{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06797264",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# set up the notebook\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import logging\n",
    "logging.basicConfig(level=logging.ERROR)\n",
    "\n",
    "from functools import partial\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ce8afb58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import json\n",
    "\n",
    "with open(\"./credential.json\", \"r\") as f:\n",
    "    credential = json.load(f)[\"openai\"]\n",
    "openai.api_key = credential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "e10ddfa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dict = {}\n",
    "\n",
    "paper_index = 0\n",
    "\n",
    "title = \"Hardness in Markov Decision Processes: Theory and Practice\"\n",
    "\n",
    "abstract = \"\"\"\n",
    "\"\"\"\n",
    "\n",
    "introduction = \"\"\"\n",
    "\"\"\"\n",
    "\n",
    "methods = \"\"\"\n",
    "\"\"\"\n",
    "\n",
    "experiments = \"\"\"\n",
    "\"\"\"\n",
    "\n",
    "conclusion = \"\"\"\n",
    "\"\"\"\n",
    "\n",
    "references = \"\"\"\n",
    "\"\"\"\n",
    "\n",
    "appendix = \"\"\"\n",
    "\"\"\"\n",
    "\n",
    "save_dict[\"paper_index\"] = paper_index\n",
    "save_dict[\"title\"] = title\n",
    "save_dict[\"abstract\"] = abstract\n",
    "save_dict[\"introduction\"] = introduction\n",
    "save_dict[\"methods\"] = methods\n",
    "save_dict[\"experiments\"] = experiments\n",
    "save_dict[\"conclusion\"] = conclusion\n",
    "save_dict[\"appendix\"] = appendix\n",
    "save_dict[\"references\"] = references"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "40c9c884",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pilot system prompt: \n",
    "system_prompt = f\"You are a computer science researcher currently reviewing a paper titled \\\"{title}\\\" for the NeurIPS computer science conference.\n",
    "\n",
    "def turn_into_input_format(role, prompt):\n",
    "    # role: \"system\", \"user\", or \"assistant\"\n",
    "    return {\"role\": role, \\\n",
    "            \"content\": prompt}\n",
    "\n",
    "system_prompt = turn_into_input_format(\"system\", system_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "837b3ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "question_section_mapping = {\n",
    "    \"1b\": \"conclusion\",\n",
    "    \"1c\": \"conclusion\",\n",
    "    \"2a\": \"methods\",\n",
    "    \"2b\": \"methods\",\n",
    "    \"3a\": \"experiments\",\n",
    "    \"3b\": \"experiments\",\n",
    "    \"3c\": \"experiments\",\n",
    "    \"3d\": \"experiments\",\n",
    "    \"4a\": \"experiments\",\n",
    "    \"4b\": \"experiments\",\n",
    "    \"4c\": \"experiments\",\n",
    "    \"4d\": \"experiments\",\n",
    "    \"4e\": \"experiments\",\n",
    "    \"5a\": \"experiments\",\n",
    "    \"5b\": \"experiments\",\n",
    "    \"5c\": \"experiments\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "bac13ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_prompts(question_section_mapping):\n",
    "\n",
    "    q1b_prompt_1 = f'The following is the {question_section_mapping[\"1b\"]} section of the paper you are reviewing:\\n' + save_dict[question_section_mapping[\"1b\"]] + \\\n",
    "                \"\\nBased on this section, do the authors describe limitations of their work?\"\n",
    "\n",
    "    q1b_prompt_2 = f'Here is the {question_section_mapping[\"1b\"]} for the paper you are reviewing. Based on the contents in the {question_section_mapping[\"1b\"]}, please answer whether the authors describe limitations of their work.\\n{question_section_mapping[\"1b\"]} section: ' + \\\n",
    "                save_dict[question_section_mapping[\"1b\"]] + \\\n",
    "                \"\\nAnswer: \"\n",
    "\n",
    "    q1b_prompt_3 = f'{question_section_mapping[\"1b\"]} section of the paper:\\n' + save_dict[question_section_mapping[\"1b\"]] + \\\n",
    "                f'\\nBased on the {question_section_mapping[\"1b\"]} section of the paper, do the authors describe any limitations of their work? Please quote a short portion of the section directly as evidence before answering.\\nQuote: '\n",
    "\n",
    "    q1c_prompt_1 = f'The following is the {question_section_mapping[\"1c\"]} section of the paper you are reviewing:\\n' + save_dict[question_section_mapping[\"1c\"]] + \\\n",
    "                \"\\nBased on this section, do the authors address potential negative societal impacts of their work?\"\n",
    "\n",
    "    q1c_prompt_2 = f'Here is the {question_section_mapping[\"1c\"]} for the paper you are reviewing. Based on the contents in the {question_section_mapping[\"1c\"]}, please answer whether the authors address potential negative societal impacts in their work.\\n{question_section_mapping[\"1c\"]} section: ' + \\\n",
    "                save_dict[question_section_mapping[\"1c\"]] + \\\n",
    "                \"\\nAnswer: \"\n",
    "\n",
    "    q1c_prompt_3 = f'{question_section_mapping[\"1c\"]} section of the paper:\\n' + save_dict[question_section_mapping[\"1c\"]] + \\\n",
    "                f'\\nBased on the {question_section_mapping[\"1c\"]} section of the paper, do the authors discuss any potential negative societal impacts of their work? Please quote a short portion of the section directly as evidence before answering.\\nQuote: '\n",
    "\n",
    "    q2a_prompt_1_1 = \"The following is the abstract of the paper you are reviewing:\\n\" + abstract + \\\n",
    "                    \"\\nBased on this abstract, do the authors have theoretical results? Please start your answer with \\\"Yes\\\", \\\"No\\\", or \\\"Unsure\\\".\"\n",
    "\n",
    "    q2a_prompt_1_2 = f'The following is the {question_section_mapping[\"2a\"]} section of the paper you are reviewing:\\n' + save_dict[question_section_mapping[\"2a\"]] + \\\n",
    "                    \"\\nBased on this section, do the authors state the assumptions for their theoretical results?\"\n",
    "\n",
    "    q2a_prompt_2_1 = \"The following is the introduction of the paper you are reviewing:\\n\" + introduction + \\\n",
    "                    \"\\nBased on the contents in the introduction, do the authors present theoretical results in the paper? Please start your answer with \\\"Yes\\\", \\\"No\\\", or \\\"Unsure\\\".\"\n",
    "    # note: a majority of the answers were just yes, no, or unsure. \n",
    "\n",
    "    q2a_prompt_2_2 = f'The following is the {question_section_mapping[\"2a\"]} section of the paper you are reviewing:\\n' + save_dict[question_section_mapping[\"2a\"]] + \\\n",
    "                    \"\\nBased on this section, do the authors state the assumptions for their theoretical results?\"\n",
    "\n",
    "    q2a_prompt_3 = f'The following is the {question_section_mapping[\"2a\"]} section of the paper you are reviewing:\\n' + save_dict[question_section_mapping[\"2a\"]] + \\\n",
    "                \"\\nBased on this section, do the authors state the assumptions for their theoretical results? If there are no theoretical results, please say so instead. \"\n",
    "\n",
    "    q2b_prompt_1_1 = \"The following is the abstract of the paper you are reviewing:\\n\" + abstract + \\\n",
    "                    \"\\nBased on this abstract, do the authors have theoretical results? Please start your answer with \\\"Yes\\\", \\\"No\\\", or \\\"Unsure\\\".\"\n",
    "\n",
    "    q2b_prompt_1_2 = f'The following is the {question_section_mapping[\"2b\"]} section of the paper you are reviewing:\\n' + save_dict[question_section_mapping[\"2b\"]] + \\\n",
    "                    \"\\nBased on this section, do the authors include complete proofs for all their theoretical results?\"\n",
    "\n",
    "    q2b_prompt_2_1 = \"The following is the introduction of the paper you are reviewing:\\n\" + introduction + \\\n",
    "                    \"\\nBased on the contents in the introduction, do the authors present theoretical results in the paper? Please start your answer with \\\"Yes\\\", \\\"No\\\", or \\\"Unsure\\\".\"\n",
    "\n",
    "    q2b_prompt_2_2 = f'The following is the {question_section_mapping[\"2b\"]} section of the paper you are reviewing:\\n' + save_dict[question_section_mapping[\"2b\"]] + \\\n",
    "                    \"\\nDo the authors include complete proofs for all their theoretical results?\"\n",
    "\n",
    "    q2b_prompt_3 = f'The following is the {question_section_mapping[\"2b\"]} section of the paper you are reviewing:\\n' + save_dict[question_section_mapping[\"2b\"]] + \\\n",
    "                    \"\\nDo the authors include complete proofs for all their theoretical results? If there are no theoretical results, please say so instead.\"\n",
    "\n",
    "    q3a_prompt_1 = f'The following is the {question_section_mapping[\"3a\"]} section of the paper you are reviewing:\\n' + save_dict[question_section_mapping[\"3a\"]] + \\\n",
    "                \"\\nBased on this section, do the authors include code, data, and instructions needed to reproduce the main experimental results, or provide them in a URL?\"\n",
    "\n",
    "    q3a_prompt_2 = f'Here is the {question_section_mapping[\"3a\"]} section for the paper you are reviewing. Based on the contents in the section, please answer whether the authors include code, data, and instructions needed to reproduce the main experimental results (either in the supplemental material or as a URL).\\n{question_section_mapping[\"3a\"]} section: ' + \\\n",
    "                save_dict[question_section_mapping[\"3a\"]] + \\\n",
    "                \"\\nAnswer: \"\n",
    "\n",
    "    q3a_prompt_3 = f'{question_section_mapping[\"3a\"]} section of the paper:\\n' + save_dict[question_section_mapping[\"3a\"]] + \\\n",
    "                f'\\nBased on the {question_section_mapping[\"3a\"]} section of the paper, do the authors include code, data, and instructions needed to reproduce the main experimental results (either in the supplemental material or as a URL)? Please quote a short portion of the section directly as evidence before answering.\\nQuote: '\n",
    "\n",
    "    q3b_prompt_1 = f'The following is the {question_section_mapping[\"3b\"]} section of the paper you are reviewing:\\n' + save_dict[question_section_mapping[\"3b\"]] + \\\n",
    "                \"Based on this section, do the authors specify all training details (e.g., data splits, hyperparameters, how they were chosen) for their results?\"\n",
    "\n",
    "    q3b_prompt_2 = f'Here is the {question_section_mapping[\"3b\"]} section for the paper you are reviewing. Based on the contents in the section, please answer whether the authors specify all the training details (e.g., data splits, hyperparameters, how they were chosen).\\n{question_section_mapping[\"3b\"]} section: ' + \\\n",
    "                save_dict[question_section_mapping[\"3b\"]] + \\\n",
    "                \"\\nAnswer: \"\n",
    "\n",
    "    q3b_prompt_3 = f'{question_section_mapping[\"3b\"]} section of the paper:\\n' + save_dict[question_section_mapping[\"3b\"]] + \\\n",
    "                f'Based on the {question_section_mapping[\"3b\"]} section of the paper, do the authors specify all the training details (e.g., data splits, hyperparameters, how they were chosen)? Please quote a short portion of the section directly as evidence before answering.\\nQuote: '\n",
    "\n",
    "    q3c_prompt_1 = f'The following is the {question_section_mapping[\"3c\"]} section of the paper you are reviewing:\\n' + save_dict[question_section_mapping[\"3c\"]] + \\\n",
    "                \"Based on this section, do the authors report error bars (e.g., with respect to the random seed after running experiments multiple times)?\"\n",
    "\n",
    "    q3c_prompt_2 = f'Here is the {question_section_mapping[\"3c\"]} section for the paper you are reviewing. Based on the contents in the section, please answer whether the authors report error bars (e.g., with respect to the random seed after running experiments multiple times).\\n{question_section_mapping[\"3c\"]} section: ' + \\\n",
    "                save_dict[question_section_mapping[\"3c\"]] + \\\n",
    "                \"\\nAnswer: \"\n",
    "\n",
    "    q3c_prompt_3 = f'{question_section_mapping[\"3c\"]} section of the paper:\\n' + save_dict[question_section_mapping[\"3c\"]] + \\\n",
    "                f'Based on the {question_section_mapping[\"3c\"]} section of the paper, do the authors report error bars (e.g., with respect to the random seed after running experiments multiple times)? Please quote a short portion of the section directly as evidence before answering.\\nQuote: '\n",
    "\n",
    "    q3d_prompt_1 = f'The following is the {question_section_mapping[\"3d\"]} section of the paper you are reviewing:\\n' + save_dict[question_section_mapping[\"3d\"]] + \\\n",
    "                \"Based on this section, do the authors include the amount of compute and type of resources used (e.g., type of GPUs, internal cluster, or cloud provider)?\"\n",
    "\n",
    "    q3d_prompt_2 = f'Here is the {question_section_mapping[\"3d\"]} section for the paper you are reviewing. Based on the contents in the section, please answer whether the authors include the amount of compute and the type of resources used (e.g., type of GPUs, internal cluster, or cloud provider).\\n{question_section_mapping[\"3d\"]} section: ' + \\\n",
    "                save_dict[question_section_mapping[\"3d\"]] + \\\n",
    "                \"\\nAnswer: \"\n",
    "\n",
    "    q3d_prompt_3 = f'{question_section_mapping[\"3d\"]} section of the paper:\\n' + save_dict[question_section_mapping[\"3d\"]] + \\\n",
    "                f'\\nBased on the {question_section_mapping[\"3d\"]} section of the paper, do the authors include the amount of compute and the type of resources used (e.g., type of GPUs, internal cluster, or cloud provider)? Please quote a short portion of the section directly as evidence before answering.\\nQuote: '\n",
    "\n",
    "    q4a_prompt_1_1 = \"The following is the methods section of the paper you are reviewing:\\n\" + methods + \\\n",
    "                    \"\\nBased on this section, do the authors use existing assets (e.g., code, data, models) that are not new contributions in their paper? Please start your answer with \\\"Yes\\\", \\\"No\\\", or \\\"Unsure\\\".\"\n",
    "\n",
    "    q4a_prompt_1_2 = f'The following is the {question_section_mapping[\"4a\"]} section of the paper you are reviewing:\\n{question_section_mapping[\"4a\"]} section: ' + save_dict[question_section_mapping[\"4a\"]] + \\\n",
    "                    f'\\nBased on the section, do the authors cite the creators of the existing assets that they use?'\n",
    "\n",
    "    q4a_prompt_2_1 = \"The following is the methods section of the paper you are reviewing:\\n\" + methods + \\\n",
    "                    \"\\nBased on this section, do the authors use existing assets (e.g., code, data, models) that are not new contributions in their paper? Please start your answer with \\\"Yes\\\", \\\"No\\\", or \\\"Unsure\\\".\"\n",
    "\n",
    "    q4a_prompt_2_2 = f'The following is the {question_section_mapping[\"4a\"]} section of the paper you are reviewing:\\n{question_section_mapping[\"4a\"]} section: ' + save_dict[question_section_mapping[\"4a\"]] + \\\n",
    "                    f'\\nBased on the section, do the authors cite the creators of the existing assets that they use? Please quote a short portion of each section directly as evidence before answering.\\nQuote: '\n",
    "\n",
    "    q4a_prompt_3 = f'The following is the {question_section_mapping[\"4a\"]} section of the paper you are reviewing:\\n{question_section_mapping[\"4a\"]} section: ' + save_dict[question_section_mapping[\"4a\"]] + \\\n",
    "                \"\\nBased on this section, if the authors are using existing assets (e.g., code, data, models), do they cite the creators of these existing assets?\"\n",
    "\n",
    "    q4b_prompt_1_1 = f'The following is the {question_section_mapping[\"4b\"]} section of the paper you are reviewing:\\n' + save_dict[question_section_mapping[\"4b\"]] + \\\n",
    "                    \"\\nBased on this section, do the authors use existing assets (e.g., code, data, models) that are not new contributions in their paper? Please start your answer with \\\"Yes\\\", \\\"No\\\", or \\\"Unsure\\\".\"\n",
    "\n",
    "    q4b_prompt_1_2 = \"\\nBased on the section, do the authors mention the licenses of the existing assets that they use?\"\n",
    "\n",
    "    q4b_prompt_2_1 = f'The following is the {question_section_mapping[\"4b\"]} section of the paper you are reviewing:\\n' + save_dict[question_section_mapping[\"4b\"]] + \\\n",
    "                    \"\\nBased on this section, do the authors use existing assets (e.g., code, data, models) that are not new contributions in their paper?  Please quote a short portion of each section directly as evidence before answering, but first start your answer with \\\"Yes\\\", \\\"No\\\", or \\\"Unsure\\\". \"\n",
    "\n",
    "    q4b_prompt_2_2 = \"\\nBased on the section, do the authors mention the licenses of the existing assets that they use? Please quote a short portion of each section directly as evidence before answering.\\nQuote: \"\n",
    "\n",
    "    q4b_prompt_3 = f'The following is the {question_section_mapping[\"4b\"]} section of the paper you are reviewing:\\n{question_section_mapping[\"4b\"]} section: ' + save_dict[question_section_mapping[\"4b\"]] + \\\n",
    "                \"\\nBased on this section, if the authors are using existing assets (e.g., code, data, models), do they mention the licenses of the existing assets that they use?\"\n",
    "\n",
    "    q4c_prompt_1_1 = \"The following is the abstract section of the paper you are reviewing:\\n\" + abstract + \\\n",
    "                    \"\\nBased on this abstract, do the authors curate/release new assets (e.g., code, data, models)? Please start your answer with \\\"Yes\\\", \\\"No\\\", or \\\"Unsure\\\".\"\n",
    "\n",
    "    q4c_prompt_1_2 = f'The following is the {question_section_mapping[\"4c\"]} section of the paper you are reviewing:\\n' + save_dict[question_section_mapping[\"4c\"]] + \\\n",
    "                    \"\\nBased on this section, do the authors include any new assets either in the supplemental material or as a URL?\"\n",
    "\n",
    "    q4c_prompt_2_1 = \"The following is the introduction section of the paper you are reviewing:\\n\" + introduction + \\\n",
    "                    \"\\nBased on the introduction, do the authors curate/release new assets (e.g., code, data, models) that are their own contribution? Please start your answer with \\\"Yes\\\", \\\"No\\\", or \\\"Unsure\\\".\"\n",
    "\n",
    "    q4c_prompt_2_2 = f'The following is the {question_section_mapping[\"4c\"]} section of the paper you are reviewing:\\n' + save_dict[question_section_mapping[\"4c\"]] + \\\n",
    "                    \"\\nBased on this section, do the authors include any new assets either in the supplemental material or as a URL?\"\n",
    "\n",
    "    q4c_prompt_3 = f'The following is the {question_section_mapping[\"4c\"]} section of the paper you are reviewing:\\n' + save_dict[question_section_mapping[\"4c\"]] + \\\n",
    "                    \"\\nBased on this section, do the authors include any new assets they curate/release either in the supplemental material or as a URL? If there are no new assets, please say so instead.\"\n",
    "\n",
    "    q4d_prompt_1_1 = \"The following is the abstract section of the paper you are reviewing:\\n\" + abstract + \\\n",
    "                    \"\\nBased on this abstract, do the authors curate/release new assets (e.g., code, data, models)? Please start your answer with \\\"Yes\\\", \\\"No\\\", or \\\"Unsure\\\".\"\n",
    "\n",
    "    q4d_prompt_1_2 = f'The following is the {question_section_mapping[\"4d\"]} section of the paper you are reviewing:\\n' + save_dict[question_section_mapping[\"4d\"]] + \\\n",
    "                    \"\\nBased on this section, do the authors discuss whether and how consent was obtained from people whose data the authors are using/curating?\"\n",
    "\n",
    "    q4d_prompt_2_1 = \"The following is the introduction section of the paper you are reviewing:\\n\" + introduction + \\\n",
    "                    \"\\nBased on the introduction, do the authors curate/release new assets (e.g., code, data, models) that are their own contribution? Please start your answer with \\\"Yes\\\", \\\"No\\\", or \\\"Unsure\\\".\"\n",
    "\n",
    "    q4d_prompt_2_2 = f'The following is the {question_section_mapping[\"4d\"]} section of the paper you are reviewing:\\n' + save_dict[question_section_mapping[\"4d\"]] + \\\n",
    "                    \"\\nBased on this section, do the authors discuss whether and how consent was obtained from people whose data the authors are using/curating? If the authors are not using human data, please say so instead.\"\n",
    "\n",
    "    q4d_prompt_3 = f'The following is the {question_section_mapping[\"4d\"]} section of the paper you are reviewing:\\n' + save_dict[question_section_mapping[\"4d\"]] + \\\n",
    "                    \"\\nBased on this section, do the authors discuss whether and how consent was obtained from people whose data the authors are using/curating? If there are no new assets, or if the authors are not using human data, please say so instead.\"\n",
    "\n",
    "    q4e_prompt_1_1 = \"The following is the abstract section of the paper you are reviewing:\\n\" + abstract + \\\n",
    "                    \"\\nBased on this abstract, do the authors curate/release new assets (e.g., code, data, models)? Please start your answer with \\\"Yes\\\", \\\"No\\\", or \\\"Unsure\\\".\"\n",
    "\n",
    "    q4e_prompt_1_2 = f'The following is the {question_section_mapping[\"4e\"]} section of the paper you are reviewing:\\n' + save_dict[question_section_mapping[\"4e\"]] + \\\n",
    "                    \"\\nBased on this section, do the authors discuss whether the data used/curated contains personally identifiable information or offensive content?\"\n",
    "\n",
    "    q4e_prompt_2_1 = \"The following is the introduction section of the paper you are reviewing:\\n\" + introduction + \\\n",
    "                    \"\\nBased on the introduction, do the authors curate/release new assets (e.g., code, data, models) that are their own contribution? Please start your answer with \\\"Yes\\\", \\\"No\\\", or \\\"Unsure\\\".\"\n",
    "\n",
    "    q4e_prompt_2_2 = f'The following is the {question_section_mapping[\"4e\"]} section of the paper you are reviewing:\\n' + save_dict[question_section_mapping[\"4e\"]] + \\\n",
    "                    \"\\nBased on this section, do the authors discuss whether the data used/curated contains personally identifiable information or offensive content?\"\n",
    "\n",
    "    q4e_prompt_3 = f'The following is the {question_section_mapping[\"4e\"]} section of the paper you are reviewing:\\n' + save_dict[question_section_mapping[\"4e\"]] + \\\n",
    "                    \"\\nBased on this section, do the authors discuss whether the data used/curated contains personally identifiable information or offensive content? If there are no new assets, please say so instead.\"\n",
    "\n",
    "    q5a_prompt_1_1 = \"The following is the abstract of the paper you are reviewing:\\n\" + abstract + \\\n",
    "                    \"\\nBased on this abstract, do the authors use crowdsourcing or conduct research with human subjects? Please start your answer with \\\"Yes\\\", \\\"No\\\", or \\\"Unsure\\\".\"\n",
    "\n",
    "    q5a_prompt_1_2 = f'The following is the {question_section_mapping[\"5a\"]} section of the paper you are reviewing:\\n' + save_dict[question_section_mapping[\"5a\"]] + \\\n",
    "                    \"\\nBased on this section, do the authors include the full text of instructions given to participants and screenshots, if applicable?\"\n",
    "\n",
    "    q5a_prompt_2_1 = \"The following is the introduction of the paper you are reviewing:\\n\" + introduction + \\\n",
    "                    \"\\nBased on the introduction, do the authors use crowdsourcing or conduct research with human subjects? Please start your answer with \\\"Yes\\\", \\\"No\\\", or \\\"Unsure\\\".\"\n",
    "\n",
    "    q5a_prompt_2_2 = f'The following is the {question_section_mapping[\"5a\"]} section of the paper you are reviewing:\\n' + save_dict[question_section_mapping[\"5a\"]] + \\\n",
    "                    \"\\nBased on this section, do the authors include the full text of instructions given to participants and screenshots, if applicable?\"\n",
    "\n",
    "    q5a_prompt_3 = f'The following is the {question_section_mapping[\"5a\"]} section of the paper you are reviewing:\\n' + save_dict[question_section_mapping[\"5a\"]] + \\\n",
    "                    \"\\nBased on the section, if the authors use crowdsourcing or conduct research with human subjects, do the authors include the full text of instructions given to participants and screenshots, if applicable? If they do not use crowdsourcing or human subject research, please say so instead.\"\n",
    "\n",
    "    q5b_prompt_1_1 = \"The following is the abstract of the paper you are reviewing:\\n\" + abstract + \\\n",
    "                    \"\\nBased on this abstract, do the authors use crowdsourcing or conduct research with human subjects? Please start your answer with \\\"Yes\\\", \\\"No\\\", or \\\"Unsure\\\".\"\n",
    "\n",
    "    q5b_prompt_1_2 = f'The following is the {question_section_mapping[\"5b\"]} section of the paper you are reviewing:\\n' + save_dict[question_section_mapping[\"5b\"]] + \\\n",
    "                    \"\\nBased on this section, do the authors describe any potential participant risks, with links to Institutional Review Board (IRB) approvals, if applicable?\"\n",
    "\n",
    "    q5b_prompt_2_1 = \"The following is the introduction of the paper you are reviewing:\\n\" + introduction + \\\n",
    "                    \"\\nBased on the introduction, do the authors use crowdsourcing or conduct research with human subjects? Please start your answer with \\\"Yes\\\", \\\"No\\\", or \\\"Unsure\\\".\"\n",
    "\n",
    "    q5b_prompt_2_2 = f'The following is the {question_section_mapping[\"5b\"]} section of the paper you are reviewing:\\n' + save_dict[question_section_mapping[\"5b\"]] + \\\n",
    "                    \"\\nBased on this section, do the authors describe any potential participant risks, with links to Institutional Review Board (IRB) approvals, if applicable?\"\n",
    "\n",
    "    q5b_prompt_3 = f'The following is the {question_section_mapping[\"5b\"]} section of the paper you are reviewing:\\n' + save_dict[question_section_mapping[\"5b\"]] + \\\n",
    "                    \"\\nBased on the section, if the authors use crowdsourcing or conduct research with human subjects, do the authors describe any potential participant risks, with links to Institutional Review Board (IRB) approvals, if applicable? If they do not use crowdsourcing or human subject research, please say so instead.\"\n",
    "\n",
    "    q5c_prompt_1_1 = \"The following is the abstract of the paper you are reviewing:\\n\" + abstract + \\\n",
    "                    \"\\nBased on this abstract, do the authors use crowdsourcing or conduct research with human subjects? Please start your answer with \\\"Yes\\\", \\\"No\\\", or \\\"Unsure\\\".\"\n",
    "\n",
    "    q5c_prompt_1_2 = f'The following is the {question_section_mapping[\"5c\"]} section of the paper you are reviewing:\\n' + save_dict[question_section_mapping[\"5c\"]] + \\\n",
    "                    \"\\nBased on this section, do the authors include the estimated hourly wage paid to participants and the total amount spent on participant compensation?\"\n",
    "\n",
    "    q5c_prompt_2_1 = \"The following is the introduction of the paper you are reviewing:\\n\" + introduction + \\\n",
    "                    \"\\nBased on the introduction, do the authors use crowdsourcing or conduct research with human subjects? Please start your answer with \\\"Yes\\\", \\\"No\\\", or \\\"Unsure\\\".\"\n",
    "\n",
    "    q5c_prompt_2_2 = f'The following is the {question_section_mapping[\"5c\"]} section of the paper you are reviewing:\\n' + save_dict[question_section_mapping[\"5c\"]] + \\\n",
    "                    \"\\nBased on this section, do the authors include the estimated hourly wage paid to participants and the total amount spent on participant compensation?\"\n",
    "\n",
    "    q5c_prompt_3 = f'The following is the {question_section_mapping[\"5c\"]} section of the paper you are reviewing:\\n' + save_dict[question_section_mapping[\"5c\"]] + \\\n",
    "                \"\\nBased on the section, if the authors use crowdsourcing or conduct research with human subjects, do the authors include the estimated hourly wage paid to participants and the total amount spent on participant compensation? If they do not use crowdsourcing and do not use human subject research, please say so instead.\"\n",
    "    \n",
    "    problems_dict = {\n",
    "        \"q1c\": [[q1c_prompt_1], [q1c_prompt_2], [q1c_prompt_3]],\n",
    "        \"q1b\": [[q1b_prompt_1], [q1b_prompt_2], [q1b_prompt_3]],\n",
    "        \"q2a\": [[q2a_prompt_1_1, q2a_prompt_1_2], [q2a_prompt_2_1, q2a_prompt_2_2], [q2a_prompt_3]],\n",
    "        \"q2b\": [[q2b_prompt_1_1, q2b_prompt_1_2], [q2b_prompt_2_1, q2b_prompt_2_2], [q2b_prompt_3]],\n",
    "        \"q3a\": [[q3a_prompt_1], [q3a_prompt_2], [q3a_prompt_3]],\n",
    "        \"q3b\": [[q3b_prompt_1], [q3b_prompt_2], [q3b_prompt_3]],\n",
    "        \"q3c\": [[q3c_prompt_1], [q3c_prompt_2], [q3c_prompt_3]],\n",
    "        \"q3d\": [[q3d_prompt_1], [q3d_prompt_2], [q3d_prompt_3]],\n",
    "        \"q4a\": [[q4a_prompt_1_1, q4a_prompt_1_2], [q4a_prompt_2_1, q4a_prompt_2_2], [q4a_prompt_3]],\n",
    "        \"q4b\": [[q4b_prompt_1_1, q4b_prompt_1_2], [q4b_prompt_2_1, q4b_prompt_2_2], [q4b_prompt_3]],\n",
    "        \"q4c\": [[q4c_prompt_1_1, q4c_prompt_1_2], [q4c_prompt_2_1, q4c_prompt_2_2], [q4c_prompt_3]],\n",
    "        \"q4d\": [[q4d_prompt_1_1, q4d_prompt_1_2], [q4d_prompt_2_1, q4d_prompt_2_2], [q4d_prompt_3]],\n",
    "        \"q4e\": [[q4e_prompt_1_1, q4e_prompt_1_2], [q4e_prompt_2_1, q4e_prompt_2_2], [q4e_prompt_3]],\n",
    "        \"q5a\": [[q5a_prompt_1_1, q5a_prompt_1_2], [q5a_prompt_2_1, q5a_prompt_2_2], [q5a_prompt_3]],\n",
    "        \"q5b\": [[q5b_prompt_1_1, q5b_prompt_1_2], [q5b_prompt_2_1, q5b_prompt_2_2], [q5b_prompt_3]],\n",
    "        \"q5c\": [[q5c_prompt_1_1, q5c_prompt_1_2], [q5c_prompt_2_1, q5c_prompt_2_2], [q5c_prompt_3]]\n",
    "    }\n",
    "\n",
    "    # put the inputs into input format for openAI API\n",
    "    for problem in problems_dict.keys(): # e.g., \"q1c\"\n",
    "        for alternative in problems_dict[problem]: # e.g., [q1c_prompt_1]\n",
    "            for ministep_index in range(len(alternative)): # e.g., q1c_prompt_1\n",
    "                alternative[ministep_index] = turn_into_input_format(\"user\", alternative[ministep_index])\n",
    "\n",
    "    return problems_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de7d586",
   "metadata": {},
   "outputs": [],
   "source": [
    "problems_dict = generate_prompts(question_section_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3665df0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_problem(system_prompt, alternative, model=\"gpt-4\", max_tokens=150, temperature=1, top_p=1, n=1, verbose=False):\n",
    "    # alternative is a list of prompts for the same prompting strategy on the same question\n",
    "    messages = [system_prompt, alternative[0]]\n",
    "    if verbose:\n",
    "        print(\"messages:\", messages)\n",
    "        print(\"temperature\", temperature, \"top_p\", top_p, \"n\", n)\n",
    "    \n",
    "    completion1 = openai.ChatCompletion.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        max_tokens=max_tokens,\n",
    "        temperature=temperature,\n",
    "        top_p=top_p, \n",
    "        n=n\n",
    "    )\n",
    "    \n",
    "    if completion1[\"usage\"][\"prompt_tokens\"] > 8000:\n",
    "        raise ValueError(\"too many input tokens.\")\n",
    "    \n",
    "    if len(alternative) == 1:\n",
    "        return [completion1[\"choices\"]]\n",
    "\n",
    "    # figure out majority answer\n",
    "    num_yes = 0\n",
    "    yes_index = None\n",
    "    for choice in completion1[\"choices\"]:\n",
    "        returned_text = choice[\"message\"][\"content\"]\n",
    "        if returned_text[:3] == \"Yes\" or returned_text[:6] == \"Unsure\":\n",
    "            num_yes += 1\n",
    "            yes_index = choice[\"index\"]\n",
    "        elif returned_text[:2] != \"No\":\n",
    "            print(\"model did not return a valid answer.\", returned_text[:30])\n",
    "        if choice[\"finish_reason\"] != \"stop\":\n",
    "            print(\"model did not finish its answer under token limit.\", returned_text[-30:])\n",
    "        if choice[\"message\"][\"role\"] != \"assistant\":\n",
    "            raise ValueError(\"model answered in the wrong role.\")\n",
    "        if verbose:\n",
    "            print(\"index\", choice[\"index\"], \"returned_text:\", returned_text)\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"num_yes:\", num_yes)\n",
    "        print(\"yes_index:\", yes_index)\n",
    "        \n",
    "    if num_yes >= 2:\n",
    "        messages = [system_prompt, alternative[0], completion1[\"choices\"][yes_index][\"message\"], alternative[1]]\n",
    "        if verbose:\n",
    "            print(\"messages:\", messages)\n",
    "        \n",
    "        completion2 = openai.ChatCompletion.create(\n",
    "            model=model,\n",
    "            messages=messages,\n",
    "            max_tokens=max_tokens,\n",
    "            temperature=temperature,\n",
    "            top_p=top_p, \n",
    "            n=n\n",
    "        )\n",
    "\n",
    "        if completion2[\"usage\"][\"prompt_tokens\"] > 8000:\n",
    "            raise ValueError(\"too many input tokens.\")\n",
    "        \n",
    "        for choice in completion2[\"choices\"]:\n",
    "            returned_text = choice[\"message\"][\"content\"]\n",
    "            if choice[\"finish_reason\"] != \"stop\":\n",
    "                print(\"model did not finish its answer under token limit.\", returned_text[-30:])\n",
    "            if choice[\"message\"][\"role\"] != \"assistant\":\n",
    "                raise ValueError(\"model answered in the wrong role.\")\n",
    "            if verbose:\n",
    "                print(\"index\", choice[\"index\"], \"returned_text:\", returned_text)\n",
    "        \n",
    "        return [completion1[\"choices\"], completion2[\"choices\"]]\n",
    "\n",
    "    return [completion1[\"choices\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6d3b6a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def pilot(verbose=False):\n",
    "    # first, pick a few temperature/top_p sampling candidates \n",
    "    # that do the best on n=3 generations on a subset of the prompts.\n",
    "    pilot_questions = [\"q1c\", \"q2a\", \"q3a\", \"q4a\", \"q5a\"]\n",
    "    for problem in pilot_questions:\n",
    "        # select just the first prompt (simplest)\n",
    "        alternative = problems_dict[problem][0]\n",
    "        for i in range(0, 21, 1):\n",
    "            temperature = i/10\n",
    "            answers = query_problem(system_prompt, alternative, temperature=temperature, n=3, verbose=verbose)\n",
    "            # save the answers to a file\n",
    "            with open(f\"pilot_data_hyp/{problem}/pilot_answers_temp_{temperature}_{problem}.json\", \"w\") as f:\n",
    "                json.dump(answers, f)\n",
    "            time.sleep(10) # OpenAI limit is like 40000 tokens/min\n",
    "        \n",
    "        for i in range(0, 11, 1):\n",
    "            top_p = i/10\n",
    "            answers = query_problem(system_prompt, alternative, top_p = top_p, n=3, verbose=verbose)\n",
    "            # save the answers to a file\n",
    "            with open(f\"pilot_data_hyp/{problem}/pilot_answers_topp_{top_p}_{problem}.json\", \"w\") as f:\n",
    "                json.dump(answers, f)\n",
    "            time.sleep(10) # OpenAI limit is like 40000 tokens/min    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "489ddd4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pilot(verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c54e48f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_output(problem, temperature=None, top_p=None):\n",
    "    if top_p is None:\n",
    "        if len(f\"{temperature}\") == 1:\n",
    "            temperature = f\"{temperature}.0\"\n",
    "        with open(f\"pilot_data_hyp/{problem}/pilot_answers_temp_{temperature}_{problem}.json\", \"r\") as f:\n",
    "           answers = json.load(f)\n",
    "    \n",
    "    if temperature is None:\n",
    "        if len(f\"{top_p}\") == 1:\n",
    "            top_p = f\"{top_p}.0\"\n",
    "        with open(f\"pilot_data_hyp/{problem}/pilot_answers_topp_{top_p}_{problem}.json\", \"r\") as f:\n",
    "            answers = json.load(f)\n",
    "\n",
    "    for i in range(len(answers[0])):\n",
    "        for j in range(len(answers)):\n",
    "            print(answers[j][i][\"message\"][\"content\"], \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8dd7fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_output(\"q5a\", top_p=0.9)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5394a5ef",
   "metadata": {},
   "source": [
    "If we optimize the system prompt jointly with the prompts:\n",
    "2 pairs of hyperparameters * 3 system prompts * 3 generations * 3 prompt alternatives * 16 questions = 864 labels -> too many\n",
    "\n",
    "Instead, we make the assumption that system prompts will not change much for different questions in the same category (same assumption made above):\n",
    "2 pairs of hyperparameters * 3 system prompts * 3 generations * 3 prompt alternatives * 5 questions = 270 labels\n",
    "\n",
    "We make another assumption that the system prompts will not interact much with the hyperparameter selection, and take t=1 (the most volatile case) to use:\n",
    "1 hyperparameter * 3 system prompts * 3 generations * 3 prompt alternatives * 5 questions = 135 labels\n",
    "\n",
    "Using this, we will see if there is significant difference between different system prompts, or pairings of system prompt + alternative that work well together. \"testing for artifacts\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
