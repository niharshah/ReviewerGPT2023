{"paper_index": 2, "title": "List-Decodable Sparse Mean Estimation", "abstract": "", "introduction": "", "methods": "\n\nL\n\n3.1 Overview of Attribute-Ef\ufb01cient Multi\ufb01ltering\n\nThe ATTRIBUTE-EFFICIENT-MULTIFILTER algorithm is presented in Algorithm 3. The starting\npoint of the algorithm is a well-known fact that if the adversary were to signi\ufb01cantly deteriorate\nour estimate on \u00b5, the spectral norm of a certain sample covariance matrix \u02dc\u2303 would become large\n[DKK+16, LRV16]. In order to achieve attribute-ef\ufb01cient sample complexity O(poly(k, log d)), it\nis however vital to control the spectral norm only on k`-sparse directions for some pre-speci\ufb01ed\n1, which can further be certi\ufb01ed by a small Frobenius norm restricted on the\npolynomial degree `\nlargest k2` entries. If the restricted Frobenius norm is suf\ufb01ciently small, it implies that the sample\ncovariance matrix behaves as a Gaussian one, and the algorithm returns the empirical mean truncated\nto be k-sparse (see Step 4). Otherwise, the algorithm will invoke either BASICMF (i.e. Algorithm 4)\n\n\u0000\n\n6\n\n \n\f(0, 1),\n\n2\n\n(0, 1),\n\n2\n\nAlgorithm 2 Main Algorithm: Attribute-Ef\ufb01cient List-Decodable Mean Estimation\nRequire: A multiset of samples T\n\nRd, parameter \u21b5\n\n(0, 1/2], failure probability \u2327\n\n1.\nCLUSTER(T,\u21b5,\u2327,` ),\n\n\u21e2\n\n2\n\n\u0000\n\ndegree of polynomials `\nT1, . . . , Tm} \n1:\n{\n=\n2: while\ndo\nL6\n(T 0,\u21b5 0)\nan element in\n3:\n4: ANS\n(i)\n(ii)\n(iii)\n\n,\n\nL\n\n;\nL   L\\{\n}\nATTRIBUTE-EFFICIENT-MULTIFILTER(T 0,\u21b5 0,\u2327/\nif ANS is a vector then add it into M .\nif ANS is a list of (Ti,\u21b5 i) then append those with \u21b5i \uf8ff\nif ANS = NO then go to the next iteration.\n\nT\n\n|\n\n|\n\n.\n\n,` ).\n\n1 to\n\n.\n\nL\n\nL {\n(T 0,\u21b5 0)\n\n(T1,\u21b5/ 2), . . . , (Tm,\u21b5/ 2)\n\n, M\n}\n\n.\n  ;\n\n5: end while\n6: return LISTREDUCTION(T,\u21b5,`, M ).\n\n(0, 1/2], failure probability \u2327\n\n2\n\n\u00b5T )>], and Pd,`(x) is the column vector of all degree-` Hermite\n\n2 (k2`\n1 [{\n\nk`) entries above the\nI,\n\n1, U 0\n\nI\n\n\u0000\n(jt, it)\n\n}t\n\n\u0000\n\n\u21e5\n\nfor large enough constant C1 > 0.\n\n\u21e2\n\n1.\n\u0000\nPd,`(T\n\nAlgorithm 3 ATTRIBUTE-EFFICIENT-MULTIFILTER(T,\u21b5,\u2327,` )\nRequire: A multiset of samples T\n\nRd, parameter \u21b5\n\ndegree of polynomials `\n\n1: \u02dc\u2303\n\nindex set of the k` diagonal entries and 1\n\n1\n\n\u00b7\n\n1\n\n2:\n\n\u0000\n\n\u00b5T )\n\nE[Pd,`(T\n\u0000\npolynomials of x.\n2 (k2`+k`)\n(it, jt)\nt\n{\n}\n\u0000\nmain diagonal of \u02dc\u2303 with largest magnitude. U\nwith I =\njt}t\n1.\n1 [{\n{\n(` + C1 log 1\n3: \u0000\u21e4sparse  \n\u21b5 )\n( \u02dc\u2303)U\n\u0000\u21e4sparse then return \u02c6\u00b5\n4: if\n\u0000\n\u0000\n5: (\u0000\u21e4, v\u21e4)\n\u0000\n\u0000\n\u0000\n\u0000\n\u0000\u21e4sparse then\n6: if \u0000\u21e4\nif ` = 1 then ANS\n7:\nwhere p1(x) := v\u21e4\n\nit}t\nC1 \u00b7\n\u21e5\nF \uf8ff\nthe largest eigenvalue and eigenvector of ( \u02dc\u2303)U 0 .\n\nlog2(2 + log 1\n\u21b5 )\ntrimk(\u00b5T ).\n\n {\n2`\n\nPd,`(x\n\nBASICMF(T,\u21b5,\u2327, p 1) else ANS\n\n(it, jt)\n\n\u00b5T ).\n\n\u0000\n\n\u0000\n\n\u0000\n\n\u21e4\n\n\u00b7\n\n}t\n\n\u0000\n\n\u00b7\nPd,`(x\n\n\u0000\n\u00b5T )>\n\n1\nA0kF \u00b7\n\nA0\nk\nHARMONICMF(T,\u21b5,\u2327, p 2).\n\n\u0000\n\n\u00b7\n\n\u0000\n\nPd,`(x\n\n\u00b7\n\n\u0000\n\n\u00b5T )\n\nwith A0 := ( \u02dc\u2303)U 0 .\n\n\u0000\n\n8: else\np2(x)\n9:\n10: ANS\n11: end if\n12: return ANS.\n\nHARMONICMF(T,\u21b5,\u2327, p 1)\n\nor HARMONICMF (i.e. Algortihm 5) to examine the concentration of a polynomial of the empirical\ndata to that of Gaussian. Both algorithms will either assert that the current sample set does not contain\na suf\ufb01ciently large amount of Gaussian samples, or will prune many corrupted samples to increase\nthe fraction of Gaussian ones. A more detailed description of the two algorithms can be found in\nSection 3.2.1 and Section 3.2.2 respectively. What is subtle in Algorithm 3 is that we will check\nthe maximum eigenvalue \u0000\u21e4 of the empirical covariance matrix \u02dc\u2303 restricted on a carefully chosen\nsubset U 0, which corresponds to the maximum eigenvalue on a certain (2k2`)-sparse direction. If \u0000\u21e4\nis too large, this indicates an easy problem since it must be the case that the adversary corrupted the\nsamples in an aggressively way. Therefore, it suf\ufb01ces to prune outliers using a degree-` polynomial\np1 which is simply the projection of Pd,`(x\n\u00b5T ) onto the span of the maximum eigenvector; see\nStep 7 in Algorithm 3. On the other hand, if \u0000\u21e4 is on a moderate scale, it indicates that the adversary\ncorrupted the samples in a very delicate way so that it passes the tests of both Frobenius norm and\nspectral norm. Now the main idea is to check the concentration of higher degree polynomials induced\nby the sample set; we show that it suf\ufb01ces to construct a degree-2` harmonic polynomial; see Step 10.\nWhile sparse mean estimation has been studied in [DKK+19] and the idea of using restricted\nFrobenius norm and \ufb01ltering was also developed, we note that their analysis only holds in the mild\ncorruption regime where \u21b5> 1/2. To establish the main results, we will leverage the tools from\n[DKS18b], with a speci\ufb01c treatment on the fact that \u00b5 is k-sparse, to ensure an attribute-ef\ufb01cient\nsample complexity bound. As we will show later, a key idea to this end is to utilize a sequence of\ncarefully chosen sparse polynomials in the sense of De\ufb01nition 7 along with sparsity-induced \ufb01lters.\n\n\u0000\n\n7\n\n \n \n \n \n \n \n \n \n \n \n \n\fThe performance guarantee of ATTRIBUTE-EFFICIENT-MULTIFILTER is as follows.\nTheorem 14 (Algorithm 3). Consider Algorithm 3 and denote by ANS its output. With probability\n\u2327 , the following holds. ANS cannot be TBD. If ANS is a k-sparse vector and if T is \u21b5-good,\n1\n\u0000\n. If ANS = NO, then T is not \u21b5-good. If ANS =\n\u00b5\nthen\nk\n1\n1\n\u21b52 ; if additionally T is\n(Ti,\u21b5 i)\n\u21b52\n{\ni \uf8ff\n\u21b5-good, then at least one Ti is \u21b5i-good. Finally, the algorithm runs in time O\n\n2` p`(` + log 1\n\u21b5 )\n2, then Ti \u21e2\n\n\u21b5\u0000\n\u02c6\u00b5\n\u0000\nm\ni=1 for some m\n}\n\nT for all i\n\u0000\n\n[m] and\n\nk2 \uf8ff\n\npoly(\n\n, d`)\n\nm\ni=1\n\n\u02dcO\n\n\uf8ff\n\n2\n\nT\n\n\u0000\n\n.\n\n1\n\nP\n\n|\n\n|\n\n3.2 Analysis of ATTRIBUTE-EFFICIENT-MULTIFILTER\n\n\u0000\n\n\u0000\n\nWe \ufb01rst show that if the restricted Frobenius norm of the sample covariance matrix is small, then the\nsample mean is a good estimate of the target mean.\nLemma 15. Consider Algorithm 3. If the algorithm returns a vector \u02c6\u00b5 at Step 4 and if T is \u21b5-good,\n(` + log 1\n\u21b5 )\nwe have that\n\nlog2(2 + log 1\n\u21b5 )\n\n2` p`\n\n\u21b5\u0000\n\nO\n\n\u00b5\n\n\u02c6\u00b5\n\n.\n\n1\n\nk\n\n\u0000\n\nk2 \uf8ff\n\n\u00b7\n\n\u00b7\n\n\u0000\n\n\u0000\n\nNext, we give performance guarantees on the remaining steps of Algorithm 3, where we consider the\ncase that the algorithm does not return at Step 4. Namely, the algorithm will either reach at Step 7\nor Step 10, and will return the ANS obtained thereof. These two steps will invoke BASICMF or\nHARMONICMF on different sparse polynomials. Observe that both algorithms may return 1) \u201cNO\u201d,\nm\nwhich certi\ufb01es that the current input set T is not \u21b5-good; 2) a list of subsets\ni=1 for some\n}\nm\n2, on which Algorithm 3 will be called in a recursive manner; or 3) TBD, which indicates that\nthe algorithm is uncertain on T being \u21b5-good. In the following, we prove that the way that we invoke\nBASICMF and HARMONICMF ensures that they will never return TBD when being called within\nAlgorithm 3. We then give performance guarantees on these two \ufb01ltering algorithms when they return\n\u201cNO\u201d or\n\nm\ni=1, thus establishing Theorem 14.\n\n(Ti,\u21b5 i)\n\n\uf8ff\n\n{\n\n(Ti,\u21b5 i)\n{\n\n}\n\nLet us consider that the algorithm reaches Step 7, i.e. the largest eigenvalue on one sparse direction is\nlarger than the threshold \u0000\u21e4sparse. It is easy to see that when ` = 1, ANS cannot be TBD since the\nonly way that BASICMF returns TBD is when Var[p(T )] is not too large, but this would violate the\ncondition that \u0000\u21e4 >\u0000 \u21e4sparse in view of our setting on \u0000\u21e4sparse. Similarly, we show that under the large\n\u0000\u21e4 regime, HARMONICMF will not return TBD either. Thus, we have the following lemma.\nLemma 16. Consider Algorithm 3. If it reaches Step 7, then ANS\n\n= TBD.\n\nNow it remains to consider the case that the algorithm reaches Step 10, which is more subtle since\nthe evidence from the magnitude of the largest restricted eigenvalue is not so strong to prune outliers.\nNote that this could happen even when T contains many outliers, since \u0000\u21e4 is not the maximum\neigenvalue on all sparse directions but on a submatrix indexed by U 0. Fortunately, if \u0000\u21e4 is not\nlarge, we show that the algorithm can still make progress by calling HARMONICMF on degree-2`\nsparse polynomials. This is because higher-degree polynomials are more sensitive to outliers than\nlow-degree polynomials, as far as we can certify the concentration of high-degree polynomials on\nclean samples. As a result, we will have the following guarantee.\nLemma 17. Consider Algorithm 3. If it reaches Step 10, then ANS\n\n= TBD.\n\n3.2.1 Basic Multi\ufb01lter for Sparse Polynomials\n\nThe BASICMF algorithm (Algorithm 4) is a key ingredient in the multi\ufb01ltering framework. It takes as\ninput a sparse polynomial p and uses it to certify whether T is \u21b5-good and suf\ufb01ciently concentrated.\nThe central idea is to measure how p(T ) distributed and compare it to that of the distribution of\np(G). We require the input p has certi\ufb01able variance on G, i.e. Var[p(G)]\n1, as otherwise, it could\n\ufb01lter away a large number of the good samples. We note that the bounded variance condition is\nalways satis\ufb01ed for degree-1 Hermite polynomials under proper normalization, while for high-degree\npolynomials, one cannot invoke BASICMF directly (see Section 3.2.2 for a remedy).\n\n\uf8ff\n\nThe way that BASICMF certi\ufb01es the input sample set T not being \u21b5-good is quite simple: if not\nall samples lie in a small L\n-ball, it returns \u201cNO\u201d at Step 2, in that this contradicts Lemma 13.\nOtherwise, the algorithm will attempt to search for a \ufb01ner interval [a, b] such that it includes most\nof the samples. If such interval exists, then either the adversary corrupted the samples such that the\nsample variance is as small as that of Gaussian while the sample mean may deviate far from the\ntarget, in which case BASICMF returns TBD at Step 5; or the sample variance is large, in which case\n\n1\n\n8\n\n6\n6\n\fRd, parameter \u21b5\n\n2\nP(Rd, l, 4`2k4`, 2`k2`) such that l\n\n\u21e2\n\n(0, 1/2], failure probability \u2327\n\n2`, Var[p(G)]\n\n\uf8ff\n\n\uf8ff\n\n(0, 1),\n2\n1, and p(x) =\n\nAlgorithm 4 BASICMF(T,\u21b5,\u2327, p )\nRequire: A multiset of samples T\n\na polynomial p\nhA(x\n\n\u00b5T ).\n\n2\n\nl/2\n\n, \u0000\np(y)\n\nlog 1\n\u21b5 )\np(x)\n\n\u0000\n(C1 \u00b7\n1: R\n2: if maxx,y\n\u0000\n3: if there is an interval [a, b] of length C1 \u00b7\np(x) : x\nC1 \u00b7\nreturn \u201cTBD\u201d.\n\nT |\nof samples in\n\n2\n` + C1 log 1\n\u21b5\n\nC0 \u00b7\n> 2k`\nq\n\u00b7\n\n{\nif Var[p(T )]\n\nthen\n\n\uf8ff\n\nT\n\n}\n\n2\n\n|\n\nl\n\nelse\n\n\u0000\nFind a threshold t > 2R such that\n\n\u0000\n\n4:\n5:\n6:\n7:\n\n\u00b7\n\nlog `d\n\u21b5\u2327 .\n\n`\n\u00b7\n\u0000l then return \u201cNO\u201d.\nlog(2 + log 1\n\nR\n\n\u00b7\nlog2(2 + log 1\n\n\u21b5 ) then\n\n\u21b5 ) that contains at least (1\n\n\u21b5\n2 )-fraction\n\n\u0000\n\nmin\n\np(x)\n\n,\n\na\n\n|\n\n|\n\n\u0000\n\np(x)\n\nb\n\n\u0000\n\n|} \u0000\n\n{|\n\nt\n\n>\n\n32\n\u21b5\n\nexp(\n\n(t\n\n\u0000\n\n\u0000\n\n2R)2/l) +\n\n2\u21b52\nk2` logl( `d\n\u21b5\u2327 )\n\n.\n\n\u21e5\nx\n\nT : min\n\n2\n(T 0,\u21b5 0)\n{\n\n.\n\n}\n\np(x)\n\n,\n\na\n\n|\n\n|\n\n\u0000\n\np(x)\n\n\u0000\n\n{|\n\n\u21e4\nb\n\n|} \uf8ff\n\n, \u21b50\n\nt\n\n}\n\n(1\n\n\u0000\n\nT\n\n|\n\n\u21b5/8)\nT 0|\n\n|\n\n|\n\n\u21b5\n\n\u00b7\n\n\u21e3\n\n+ \u21b5\n8\n\n.\n\n\u2318\n\nPr\nT\nx\n\u21e0\n\nT 0\n\n {\nreturn\n\nend if\n\n8:\n\n9:\n10:\n11: else\n12:\n\nFind t\n2\np(x) < t + R0\n\nsatisfy\n\nR, R0 > 0 such that the sets T1 :=\n\nx\n\n{\n\n2\n\nT : p(x) > t\n\nR0\n\n}\n\n\u0000\n\nand T2 :=\n\nx\n\n{\n\n2\n\nT :\n\n}\n2 +\n\nT1|\n\n|\n\n2\n\nT2|\n\n|\n\nT\n\n|\n\n\uf8ff|\n\n2 (1\n\n\u0000\n\n\u21b5/100)2 and\n\nT\n\n|\n\n|\u0000\n\nmax(\n\n,\n\nT1|\n\n|\n\n)\n\nT2|\n\n|\n\n\u21b5\n\nT\n\n|\n\n|\n\n\u0000\n\n/4.\n\n\u21b5\n\u21b5i  \n13:\nreturn\n14:\n15: end if\n\n\u00b7\n{\n\n\u21b52/100)\nT\n(1\n\u00b7 |\n(T1,\u21b5 1), (T2,\u21b5 2)\n\n\u0000\n\n|\n.\n}\n\n/\n\nTi|\n\n|\n\n, for i = 1, 2.\n\nit is possible to construct a sparsity-induced \ufb01lter to prune outliers (see Steps 7 and 8). We note that\nin Step 7, the \ufb01rst term on the right-hand side is derived from Chernoff bound for degree-l Gaussian\npolynomials and the second term is due to concentration of empirical samples to Gaussian (see\nDe\ufb01nition 8), both of which are scaled by a factor 8/\u21b5 so that the number of the samples removed\nT , which\nfrom T is 8/\u21b5 times more than that of the good samples in the representative set SG \u21e2\nmeans most of the removed samples are outliers. We show by contradiction the existence of the\nthreshold t (see Lemma 26). In fact, had such threshold t not existed, the set T must be suf\ufb01ciently\nconcentrated such that the algorithm would have returned at Step 5. This essentially relies on our\nresult of the initial clustering of Algorithm 1, which guarantees that each subset T is bounded in a\nsmall L\n-ball and the function value of p on the \u21b5-good T does not change drastically (Lemma 13).\nWe then show that equipped with such threshold t, T 0 is a subset of T and it is \u21b50-good if T is \u21b5-good\n(Lemma 28).\n\n1\n\nsuch that T1 \\\n\nWhen there is no such short interval [a, b], the algorithm splits T into two overlapping subsets\nT2 is large enough to contain most of the samples in SG. This guarantees\nT1, T2}\n{\nthat most of the samples in SG (if T is \u21b5-good) are always contained in one subset and thus there\nalways exists an \u21b5-good subset of T . We show that an appropriate threshold t can also be found at\nStep 12 (Lemma 30), and at least one Ti is \u21b5i-good if T is \u21b5-good.\n\nAs a result, we have the following guarantees for Algorithm 4; see Appendix B for the full proof.\nTheorem 18 (BASICMF). Consider Algorithm 4. Denote by ANS its return. Suppose that T being\n\u21b5-good implies Var[p(G)]\n\u2327 , the following holds. ANS is either\n2. 1) If ANS = NO, then T is not \u21b5-good. 2) If\n\u201cNO\u201d, \u201cTBD\u201d, or a list of\n; and if additionally T is \u21b5-good,\nANS = TBD, then Var[p(T )]\n\n1. Then with probability 1\nm\ni=1 with m\n}\nl\n(` + log 1\n\u21b5 )\n\n\u0000\n\nE[p(G)]\n|\nT and\n\nthen\nTi \u21e2\n\n\u0000\ni\n\nE[p(T )]\n1\n1\n\u21b52 for all i\n\u21b52\n\u0000\ni \uf8ff\n\n|\uf8ff\n\n2\n\nm\ni=1, then\n\u0000\n}\n[m]; if additionally T is \u21b5-good, then at least one Ti is \u21b5i-good.\n\nlog(2 + log 1\n\u21b5 )\n\n(Ti,\u21b5 i)\n{\n\n\u0000\n\n\u00b7\n\n\u0000\n\n\uf8ff\nlog2(2 + log 1\n\u21b5 )\n\u00b7\n. 3) If ANS =\n\n\uf8ff\n(Ti,\u21b5 i)\n{\nO\n(` + log 1\n\u21b5 )\n\n\uf8ff\nO\n\nl\n2\n\nP\n\n9\n\n \n \n \n\fAlgorithm 5 HARMONICMF(T,\u21b5,\u2327, p )\nRequire: A multiset of samples T\n\n1: for l0 = 0, 1, . . . , l do\n2:\n\nLet B(l0) be an order-2l0 tensor with\n\n2\n\npolynomial p\n\nP(Rd, l, 2`k2`, 2`k2`) such that p(x) = hA(x\n\nRd, parameter \u21b5\n\n\u21e2\n\n2\n\n(0, 1/2], failure probability \u2327\nk2 = 1.\n\u00b5T ) and\n\nA\nk\n\n\u0000\n\n(0, 1), a\n\n2\n\nB(l0)\n\ni1,...,il0\n\n,j1,...,jl0\n\n=\n\nXkl0+1,...,kl\n\nAi1...,il0\n\n,kl0+1,...,kl Aj1...,jl0\n\n,kl0+1,...,kl .\n\n3:\n\nConsider B(l0) as a dl0\ncoordinates together. Apply eigenvalue decomposition on B(l0) to obtain B(l0) =\n\ndl0 symmetric matrix by grouping each of the i1, . . . , il0 and j1, . . . , jl0\nVi.\nMULTILINEARMF(T, Vi, l0,\u21b5,\u2327/ (ldl)) for every Vi. If ANSi = NO or a list of\nfor some i, then return ANSi. If ANSi = TBD, continue.\n\ni \u0000iVi \u2326\n\nP\n\n\u2326\n\nBASICMF(T,\u21b5,\u2327, 1\n\n\u0000 hA(x\n\n\u00b5T )) with \u0000 :=\n\n\u0000\n\n(1 + log 1\n\u21b5 )\n\nlog2(2 + log 1\n\u21b5 )\n\n\u00b7\n\nIf ANS = NO or a list of (Tj,\u21b5 j), return ANS. If ANS = TBD, still return \u201cNO\u201d.\n\nC1 \u00b7\n\n\u0000\n\nl\n\n2 .\n\n\u0000\n\n4: ANSi  \n(Tj,\u21b5 j)\n{\n5: end for\n6: ANS\n\n}\n\n", "experiments": "", "conclusion": "\n\nIn this paper, we developed an attribute-ef\ufb01cient mean estimation algorithm which achieves sample\ncomplexity poly-logarithmic in the dimension with low-degree sparse polynomials under the list-\ndecodable setting. A natural question is whether the current techniques could be utilized to attribute-\nef\ufb01ciently solve the other list-decodable problems, such as learning of halfspaces and linear regression.\n\n", "appendix": " Omitted Proofs from Section 2\n\nA.1 Proof of Proposition 9\n\nProof. Fix a subset \u2326\nmost l, denoted by\nP(Rd, l, \uf8ff,  ) =\n\nM\n[\u2326 [M\n\n\u21e2\n(\u2326, l). Let P(Rd,\n(\u2326,l) P(Rd,\n\nM\n(\u2326, l), \u2326).\n\n[d] with size  , and then \ufb01x a set of \uf8ff monomials on \u2326 with degree at\n(\u2326, l), \u2326) be the induced class of polynomials. Note that\n\nIt is easy to see that for any p\n(\u2326, l), \u2326), it can be represented by a linear combinations\nof the \uf8ff monomials. Thus, the VC dimension of this class equals \uf8ff + 1. Then, we note that there are\n(\u2326, l). Therefore,\n\nchoices of \u2326, and for any given \u2326, there are\n\nchoices of\n\nM\n\n2\n\nd\nj\n\nj=0\n\nM\n\nM\nP(Rd,\n\nthe total number of the subclass P(Rd,\nP\n\n\u0000\n\n\u0000\n\n\uf8ff\nj=0\n(\u2326, l), \u2326) is at most\n\n2dl\nj\n\nP\n\n\u0000\n\n\u0000\n\n2dl\nj\n\n\uf8ff\n\ned\n\n2edl\n\uf8ff\n\n\u00b7\n\nM\n\uf8ff\n\nd\nj\n\n\u00b7\n\n\u25c6\n\nj=0 \u2713\nX\n\nj=0 \u2713\nX\n\nThe concept class union argument states that for\nbounded by O(max\n\nV, log m + V log log m\n{\n\nV }\n\nHi. In our case, we have V = \uf8ff + 1 and m\n\nall\n\u00b7\nthat the VC dimension of P(Rd, l, \uf8ff,  ) is upper bounded by\n\u21e3\n\n\uf8ff\n\n\u21e3\n\n\u2318\n\ned\n\n\u25c6\n\n\u2713\n\n\u2713\n\n\u25c6\nis upper\ni=1Hi, the VC dimension of\n), where V is an upper bound on the VC dimension of\n\nH\n\nH\n\n=\n\n\u25c6\n\n[\n\nm\n\n2edl\n\uf8ff\n\n\uf8ff\n\n\u2318\n\n. By calculation, we can show\n\n\uf8ff\n\n.\n\n(A.1)\n\ned\n\n  log\n\n+ \uf8ff log\n\n2edl\n\uf8ff\n\n+ \uf8ff + 1\n\n\uf8ff\n\n(l\uf8ff +  ) log d =: d0.\n\n(A.2)\n\nRecall that the VC theory states that for any \u270f, \u2327\nSG| \u0000\nfor some absolute constant C > 0, the following holds with probability 1\n\n(0, 1), as long as\n\n2\n\n|\n\nC\n\n\u0000\n\n\u270f.\n\nd0\n\n\u270f2 log d0\n\n\u270f + 1\n\n\u270f2 log 1\n\n\u2327\n\n\u2327 :\n\u21e3\n\n\u2318\n\n(A.3)\n\nPr[p(G)\n\n0]\n\n\u0000\n\nx\n\nPr\nSG\n\u21e0\n\n\u0000\n\n[p(x)\n\n0]\n\n\u0000\n\n\uf8ff\n\np\n\nsup\n2P(Rd,l,\uf8ff, ) \u0000\n\u0000\n\u0000\n\u0000\n\nin (A.2),\n\n\u0000\n\u0000\n\u0000\n\u0000\n\nWith the expression of d0\n\uf8ff+ ) log d\nlog (l\n(l\n\n\uf8ff+ ) log d\n\n\u00b7\n\n\u00b7\n\n\u270f2\n\n\u270f\u2327\n\nit\n\nis not hard to see that we can set\n\nSG|\n\n|\n\n= C\n\n\u00b7\n\nfor some absolute constant C > 0 to ensure that the above holds.\n\nWhen l = 2`, \uf8ff = 4`2k4`,   = 2`k2`, and \u270f =\nSG|\n\nalgebraic calculation, it suf\ufb01ces to pick\nconstant C 0. This completes the proof.\n\n|\n\n100k2`\n\n= C 0\n\n\u21b53\nlog2`( `d\n\u00b7\nk8`\n`4\n\u00b7\n\u21b56\n\n\u00b7\n\n\u21b5\u2327 ) for some natural number `\nlog6`( `d\n\n\u21b5\u2327 ) for some suf\ufb01ciently large\n\n1, by\n\n\u0000\n\n\u00b7\n\nA.2 Proof of Lemma 11\n\nProof. By the standard tail bound of Gaussian distribution, for any x drawn from N (\u00b5, Id), it holds\nt2/2). By taking union bound over both\nthat for any given index i\nt2/2).\nindex i and sample x\n2d\nxi \u0000\nChoosing t =\n\n\u00b5i| \u0000\nxi \u0000\nt]\n2\n|\nSG, we have Pr[maxx\n/\u2327 ) completes the proof.\nSG|\n\n2 exp(\n\uf8ff\nSG maxi\n\n2\n2 log(d\n\n\u0000\n[d] |\n2\n\n\u00b5i| \u0000\n\n[d], Pr[\n\nSG|\n\nexp(\n\n\uf8ff\n\n\u0000\n\nt]\n\n2\n\n|\n\n|\n\nA.3 Proof of Lemma 12\n\np\n\nProof. Let SG be the subset of T containing the samples drawn i.i.d. from N (\u00b5, Id). Since\n2\u21b5\n\n, we know that SG is a representative set with probability at least 1\n\nSG|\n\u2327 in light of Prop. 9.\n\nT\n\n|\n\n=\n\n\u0000\n\n\u00b7 |\n\n|\n\nConsider Algorithm 1. If for all x, y\n\nT , we have\n\n2\n\nx\n\nk\n\n\u0000\n\ny\n\nk1 \uf8ff\n\n6\u0000,\n\n(A.4)\n\nthen the algorithm returns only one cluster and the lemma follows immediately.\n\nIf that is not the case, we \ufb01rst note that, with probability at least 1\nEq. (A.4) due to Lemma 11. Let us condition on this event occurs from now on.\n\n\u0000\n\n\u2327 all of the samples in SG satisfy\n\n16\n\n \n \n \n \n \n \n \n\f-balls of radius 2\u0000, of which each is centered at one\nAlgorithm 1 constructs a set of disjoint L\nsample in T and contains at least an \u21b5-fraction of samples in T . Therefore, the number of such\nballs is at most m =\n. Let B0i be the ball that has the same\nb\ncenter as Bi but with `\n[m], such that\n1\nB0i is \u21b5-good.\nTi = T\n\nB1, . . . , Bm}\n-radius of 6\u0000. In the following, we show that there exists i\n\n. Denote the set by\n\n1/\u21b5\n\n2\n\n1\n\n{\n\nc\n\ny\n\nSG, for which we know that\n\nConsider a sample x\n-ball\nk1 \uf8ff\n, all of the samples in SG will be contained in Bx. In addition,\nBx :=\ny\nk\n}\n. That is,\nthere must exist one Bi that intersects Bx, as otherwise Bx will be in the set\nBi. By construction, Bx must be containted in B0i. Therefore, all samples of SG\nz\n\nB1, . . . , Bm}\n\n\u0000. Then, for the L\n\n{\nT, z\n\nk1 \uf8ff\n\nRd :\n\nx\nk\n\n2\nx\n\n2\u0000\n\n\u0000\n\n\u0000\n\n2\n\n\u00b5\n\n1\n\n{\n\n2\n\n9\nmust be included in Ti and Ti is \u21b5-good.\n\n2\n\nBx \\\n\n\\\n\nA.4 Proof of Lemma 13\n\nProof. Recall that after running Algorithm 1, every subset Ti is contained in an L\n-ball of radius 6\u0000.\n1\nT ,\nBy Jensen\u2019s inequality and the convexity of the L\n6\u0000.\n\u00b5T k1 \uf8ff\nx\nk\nN (\u00b5T ,I)[p(x)] = 0 due to the de\ufb01nition of\n2\n2. Denote z = x\nk\n\nRecall that we assumed p(x) = hA(x\nharmonic polynomials. Thus, Varx\n\n1\n\u00b5T ). Thus Ex\n\u21e0\nA\nk\n\n-norm, we have for all x\n\nN (\u00b5T ,I)[p(x)] =\n\n\u00b5T . Then,\n\n\u0000\n\n\u0000\n\n\u0000\n\n2\n\n\u21e0\n\np(x)\n\n=\n\n|\n\n|\n\nca(j)\n\n[k2`]\nXj\n2\n\n\u0000\n\u0000\n\u0000\n\u0000\n\u0000\n\u0000\n\nHea(j) (z)\n1! \u0000\na(j)\n\u0000\n\u0000\n\u0000\n\u0000\n\u0000\n\n\u0000\n\u0000\n\n0\n\n\uf8ff v\nu\nu\nu\nt\n\n[k2`]\nXj\n2\n\nc2\na(j) 1\n\n0\n\n[k2`]\nXj\n2\n\nHea(j) (z)2\na(j)\n\n1! 1\nA\n\n.\n\n(A.5)\n\n@\nwhere a(j) is a d-dimensional multi-index for the j-th monomial, and ca(j) denotes its coef\ufb01cient.\nObserve that in the \ufb01rst step, p(x) is written as a linear combination of k2` Hermite polynomials,\nsince we are considering p\n\nP(Rd, l, k2`, 2`k2`). Note also that\n\nq\u0000\n\u0000\n\n[k2`] c2\n\n\u0000\n\u0000\n\n\u0000\n\u0000\n\n@\n\nA\n\n1.\n\na(j) =\n\nj\n\nA\nk\n\nk\n\n2\n2 \uf8ff\n\n2\n\n2\n\nTo bound the second factor on the right-hand side of (A.5), we use Mehler\u2019s formula, which shows\nthat for any u with\n\n< 1 and any natural number a,\n\nP\n\nu\n\n|\n\n|\n\nHe2\n\na(zi)ua\na!\n\n=\n\np1\n\nu\n\n1+u z2\ni ,\n\ne\n\n1\n\n\u0000\n\nu2\n\n1\n\na=0\nX\n\nSince each Hea(j) (z) has degree at most l, it can be decomposed as a product of at most l univariate\nHermite polynomials. Thus, we take such product and sum over j\n\n[k2`] to obtain\n\n2\n\na(j)\ni\n\n=0\n\nHea(j)\na(j)\n\ni\n\n\u21e3\n\nua(j)\n\ni\n\n(zi)2\n\n\u00b7\n\n1!\n\nk2`\n\n(1\n\n\u00b7\n\n\u0000\n\n\u2318\n\n\uf8ff\n\nu2)\u0000\n\nl\n2\n\n\u00b7\n\nu\n1+u k\n\ne\n\ntriml(z)\n\n2\n\n2 .\n\nk\n\n[k2`] Q\n\nXj\n2\n\nTo simplify the above expression, observe that\n\n\u0000\n\u0000\n\n\u0000\n\u0000\n\n2\ntriml(z)\n2 \uf8ff\nk\ntogether gives\n\nk\n\nl\n\nz\n\u00b7 k\n\nk\n\n2\n1 \uf8ff\n\n36l\u00002. Lastly, by algebra, (1\n\nQ\n\n\u0000\n\n=0 ua(j)\n\na(j)\ni\n\na(j)\n\ni = uk\nu2)\u0000\n\nl\n2\n\n\uf8ff\n\nul.\n\n\u0000\n\nIn addition,\n\nk1\nu2 l\n2 . Putting all pieces\n\ne\n\nHea(j) (z)2\na(j)\n\n1! \uf8ff\n\nl\n\nu\u0000\n\nk2`\n\n\u00b7\n\n\u00b7\n\n\u00b7\n\nu2 l\n2\n\ne\n\n36l\u00002 u\n1+u = k2`\n\ne\n\n2 + 36l\u00002 u\nu2 l\n1+u .\n\ne\n\nl\n\nu\u0000\n\n\u00b7\n\n\u00b7\n\n[k2`]\nXj\n2\n\u0000 ; this is possible as \u0000 > 1. Then the exponent u2l\n\n\u0000\n\u0000\n\n\u0000\n\u0000\n\nWe set u = 1\n37l.\nWithout loss of generality, we may assume that \u0000 > e37; in fact, we can always ensure this by setting\n\u0000 = (C0 + e37)\n\u21b5\u2327 where C0 is the constant given in Algorithm 1. Thus, it follows that\n\n2\u00002 + 36l\n\n1+u = l\n\n1+1/\u00002 \uf8ff\n\nlog `d\n\n`\n\n2 + 36l\u00002u\n\n\u00b7\n\n\u00b7\n\nq\n\nPlugging it into (A.5) completes the proof.\n\n\u0000\n\u0000\n\n\u0000\n\u0000\n\n[k2`]\nXj\n2\n\nHea(j) (z)2\na(j)\n\n1! \uf8ff\n\nk2`\n\n\u0000l\n\n\u00b7\n\n\u00b7\n\ne37l\n\n\uf8ff\n\nk2`\n\n\u00b7\n\n\u00002l.\n\n17\n\n6\n6\n\f", "system_prompts": [{"role": "system", "content": "You are a computer science researcher currently reviewing a paper titled \"List-Decodable Sparse Mean Estimation\" for the NeurIPS computer science conference. Your goal is to try to be as objective and truthful as possible in your answers about the paper provided. Your reviews will be used for causal reasoning in determining the quality of the paper."}], "prompts": {"1b": {"role": "user", "content": "The following is the conclusion section of the paper you are reviewing:\n\n\nIn this paper, we developed an attribute-ef\ufb01cient mean estimation algorithm which achieves sample\ncomplexity poly-logarithmic in the dimension with low-degree sparse polynomials under the list-\ndecodable setting. A natural question is whether the current techniques could be utilized to attribute-\nef\ufb01ciently solve the other list-decodable problems, such as learning of halfspaces and linear regression.\n\n\n\nBased on the section(s), please answer the following question with yes, no, or n/a and provide a brief justification for your answer.\nQuestion: Do the authors describe the limitations of their work?"}, "2a": {"role": "user", "content": "The following is the methods section of the paper you are reviewing:\n\n\nL\n\n3.1 Overview of Attribute-Ef\ufb01cient Multi\ufb01ltering\n\nThe ATTRIBUTE-EFFICIENT-MULTIFILTER algorithm is presented in Algorithm 3. The starting\npoint of the algorithm is a well-known fact that if the adversary were to signi\ufb01cantly deteriorate\nour estimate on \u00b5, the spectral norm of a certain sample covariance matrix \u02dc\u2303 would become large\n[DKK+16, LRV16]. In order to achieve attribute-ef\ufb01cient sample complexity O(poly(k, log d)), it\nis however vital to control the spectral norm only on k`-sparse directions for some pre-speci\ufb01ed\n1, which can further be certi\ufb01ed by a small Frobenius norm restricted on the\npolynomial degree `\nlargest k2` entries. If the restricted Frobenius norm is suf\ufb01ciently small, it implies that the sample\ncovariance matrix behaves as a Gaussian one, and the algorithm returns the empirical mean truncated\nto be k-sparse (see Step 4). Otherwise, the algorithm will invoke either BASICMF (i.e. Algorithm 4)\n\n\u0000\n\n6\n\n \n\f(0, 1),\n\n2\n\n(0, 1),\n\n2\n\nAlgorithm 2 Main Algorithm: Attribute-Ef\ufb01cient List-Decodable Mean Estimation\nRequire: A multiset of samples T\n\nRd, parameter \u21b5\n\n(0, 1/2], failure probability \u2327\n\n1.\nCLUSTER(T,\u21b5,\u2327,` ),\n\n\u21e2\n\n2\n\n\u0000\n\ndegree of polynomials `\nT1, . . . , Tm} \n1:\n{\n=\n2: while\ndo\nL6\n(T 0,\u21b5 0)\nan element in\n3:\n4: ANS\n(i)\n(ii)\n(iii)\n\n,\n\nL\n\n;\nL   L\\{\n}\nATTRIBUTE-EFFICIENT-MULTIFILTER(T 0,\u21b5 0,\u2327/\nif ANS is a vector then add it into M .\nif ANS is a list of (Ti,\u21b5 i) then append those with \u21b5i \uf8ff\nif ANS = NO then go to the next iteration.\n\nT\n\n|\n\n|\n\n.\n\n,` ).\n\n1 to\n\n.\n\nL\n\nL {\n(T 0,\u21b5 0)\n\n(T1,\u21b5/ 2), . . . , (Tm,\u21b5/ 2)\n\n, M\n}\n\n.\n  ;\n\n5: end while\n6: return LISTREDUCTION(T,\u21b5,`, M ).\n\n(0, 1/2], failure probability \u2327\n\n2\n\n\u00b5T )>], and Pd,`(x) is the column vector of all degree-` Hermite\n\n2 (k2`\n1 [{\n\nk`) entries above the\nI,\n\n1, U 0\n\nI\n\n\u0000\n(jt, it)\n\n}t\n\n\u0000\n\n\u21e5\n\nfor large enough constant C1 > 0.\n\n\u21e2\n\n1.\n\u0000\nPd,`(T\n\nAlgorithm 3 ATTRIBUTE-EFFICIENT-MULTIFILTER(T,\u21b5,\u2327,` )\nRequire: A multiset of samples T\n\nRd, parameter \u21b5\n\ndegree of polynomials `\n\n1: \u02dc\u2303\n\nindex set of the k` diagonal entries and 1\n\n1\n\n\u00b7\n\n1\n\n2:\n\n\u0000\n\n\u00b5T )\n\nE[Pd,`(T\n\u0000\npolynomials of x.\n2 (k2`+k`)\n(it, jt)\nt\n{\n}\n\u0000\nmain diagonal of \u02dc\u2303 with largest magnitude. U\nwith I =\njt}t\n1.\n1 [{\n{\n(` + C1 log 1\n3: \u0000\u21e4sparse  \n\u21b5 )\n( \u02dc\u2303)U\n\u0000\u21e4sparse then return \u02c6\u00b5\n4: if\n\u0000\n\u0000\n5: (\u0000\u21e4, v\u21e4)\n\u0000\n\u0000\n\u0000\n\u0000\n\u0000\u21e4sparse then\n6: if \u0000\u21e4\nif ` = 1 then ANS\n7:\nwhere p1(x) := v\u21e4\n\nit}t\nC1 \u00b7\n\u21e5\nF \uf8ff\nthe largest eigenvalue and eigenvector of ( \u02dc\u2303)U 0 .\n\nlog2(2 + log 1\n\u21b5 )\ntrimk(\u00b5T ).\n\n {\n2`\n\nPd,`(x\n\nBASICMF(T,\u21b5,\u2327, p 1) else ANS\n\n(it, jt)\n\n\u00b5T ).\n\n\u0000\n\n\u0000\n\n\u0000\n\n\u21e4\n\n\u00b7\n\n}t\n\n\u0000\n\n\u00b7\nPd,`(x\n\n\u0000\n\u00b5T )>\n\n1\nA0kF \u00b7\n\nA0\nk\nHARMONICMF(T,\u21b5,\u2327, p 2).\n\n\u0000\n\n\u00b7\n\n\u0000\n\nPd,`(x\n\n\u00b7\n\n\u0000\n\n\u00b5T )\n\nwith A0 := ( \u02dc\u2303)U 0 .\n\n\u0000\n\n8: else\np2(x)\n9:\n10: ANS\n11: end if\n12: return ANS.\n\nHARMONICMF(T,\u21b5,\u2327, p 1)\n\nor HARMONICMF (i.e. Algortihm 5) to examine the concentration of a polynomial of the empirical\ndata to that of Gaussian. Both algorithms will either assert that the current sample set does not contain\na suf\ufb01ciently large amount of Gaussian samples, or will prune many corrupted samples to increase\nthe fraction of Gaussian ones. A more detailed description of the two algorithms can be found in\nSection 3.2.1 and Section 3.2.2 respectively. What is subtle in Algorithm 3 is that we will check\nthe maximum eigenvalue \u0000\u21e4 of the empirical covariance matrix \u02dc\u2303 restricted on a carefully chosen\nsubset U 0, which corresponds to the maximum eigenvalue on a certain (2k2`)-sparse direction. If \u0000\u21e4\nis too large, this indicates an easy problem since it must be the case that the adversary corrupted the\nsamples in an aggressively way. Therefore, it suf\ufb01ces to prune outliers using a degree-` polynomial\np1 which is simply the projection of Pd,`(x\n\u00b5T ) onto the span of the maximum eigenvector; see\nStep 7 in Algorithm 3. On the other hand, if \u0000\u21e4 is on a moderate scale, it indicates that the adversary\ncorrupted the samples in a very delicate way so that it passes the tests of both Frobenius norm and\nspectral norm. Now the main idea is to check the concentration of higher degree polynomials induced\nby the sample set; we show that it suf\ufb01ces to construct a degree-2` harmonic polynomial; see Step 10.\nWhile sparse mean estimation has been studied in [DKK+19] and the idea of using restricted\nFrobenius norm and \ufb01ltering was also developed, we note that their analysis only holds in the mild\ncorruption regime where \u21b5> 1/2. To establish the main results, we will leverage the tools from\n[DKS18b], with a speci\ufb01c treatment on the fact that \u00b5 is k-sparse, to ensure an attribute-ef\ufb01cient\nsample complexity bound. As we will show later, a key idea to this end is to utilize a sequence of\ncarefully chosen sparse polynomials in the sense of De\ufb01nition 7 along with sparsity-induced \ufb01lters.\n\n\u0000\n\n7\n\n \n \n \n \n \n \n \n \n \n \n \n\fThe performance guarantee of ATTRIBUTE-EFFICIENT-MULTIFILTER is as follows.\nTheorem 14 (Algorithm 3). Consider Algorithm 3 and denote by ANS its output. With probability\n\u2327 , the following holds. ANS cannot be TBD. If ANS is a k-sparse vector and if T is \u21b5-good,\n1\n\u0000\n. If ANS = NO, then T is not \u21b5-good. If ANS =\n\u00b5\nthen\nk\n1\n1\n\u21b52 ; if additionally T is\n(Ti,\u21b5 i)\n\u21b52\n{\ni \uf8ff\n\u21b5-good, then at least one Ti is \u21b5i-good. Finally, the algorithm runs in time O\n\n2` p`(` + log 1\n\u21b5 )\n2, then Ti \u21e2\n\n\u21b5\u0000\n\u02c6\u00b5\n\u0000\nm\ni=1 for some m\n}\n\nT for all i\n\u0000\n\n[m] and\n\nk2 \uf8ff\n\npoly(\n\n, d`)\n\nm\ni=1\n\n\u02dcO\n\n\uf8ff\n\n2\n\nT\n\n\u0000\n\n.\n\n1\n\nP\n\n|\n\n|\n\n3.2 Analysis of ATTRIBUTE-EFFICIENT-MULTIFILTER\n\n\u0000\n\n\u0000\n\nWe \ufb01rst show that if the restricted Frobenius norm of the sample covariance matrix is small, then the\nsample mean is a good estimate of the target mean.\nLemma 15. Consider Algorithm 3. If the algorithm returns a vector \u02c6\u00b5 at Step 4 and if T is \u21b5-good,\n(` + log 1\n\u21b5 )\nwe have that\n\nlog2(2 + log 1\n\u21b5 )\n\n2` p`\n\n\u21b5\u0000\n\nO\n\n\u00b5\n\n\u02c6\u00b5\n\n.\n\n1\n\nk\n\n\u0000\n\nk2 \uf8ff\n\n\u00b7\n\n\u00b7\n\n\u0000\n\n\u0000\n\nNext, we give performance guarantees on the remaining steps of Algorithm 3, where we consider the\ncase that the algorithm does not return at Step 4. Namely, the algorithm will either reach at Step 7\nor Step 10, and will return the ANS obtained thereof. These two steps will invoke BASICMF or\nHARMONICMF on different sparse polynomials. Observe that both algorithms may return 1) \u201cNO\u201d,\nm\nwhich certi\ufb01es that the current input set T is not \u21b5-good; 2) a list of subsets\ni=1 for some\n}\nm\n2, on which Algorithm 3 will be called in a recursive manner; or 3) TBD, which indicates that\nthe algorithm is uncertain on T being \u21b5-good. In the following, we prove that the way that we invoke\nBASICMF and HARMONICMF ensures that they will never return TBD when being called within\nAlgorithm 3. We then give performance guarantees on these two \ufb01ltering algorithms when they return\n\u201cNO\u201d or\n\nm\ni=1, thus establishing Theorem 14.\n\n(Ti,\u21b5 i)\n\n\uf8ff\n\n{\n\n(Ti,\u21b5 i)\n{\n\n}\n\nLet us consider that the algorithm reaches Step 7, i.e. the largest eigenvalue on one sparse direction is\nlarger than the threshold \u0000\u21e4sparse. It is easy to see that when ` = 1, ANS cannot be TBD since the\nonly way that BASICMF returns TBD is when Var[p(T )] is not too large, but this would violate the\ncondition that \u0000\u21e4 >\u0000 \u21e4sparse in view of our setting on \u0000\u21e4sparse. Similarly, we show that under the large\n\u0000\u21e4 regime, HARMONICMF will not return TBD either. Thus, we have the following lemma.\nLemma 16. Consider Algorithm 3. If it reaches Step 7, then ANS\n\n= TBD.\n\nNow it remains to consider the case that the algorithm reaches Step 10, which is more subtle since\nthe evidence from the magnitude of the largest restricted eigenvalue is not so strong to prune outliers.\nNote that this could happen even when T contains many outliers, since \u0000\u21e4 is not the maximum\neigenvalue on all sparse directions but on a submatrix indexed by U 0. Fortunately, if \u0000\u21e4 is not\nlarge, we show that the algorithm can still make progress by calling HARMONICMF on degree-2`\nsparse polynomials. This is because higher-degree polynomials are more sensitive to outliers than\nlow-degree polynomials, as far as we can certify the concentration of high-degree polynomials on\nclean samples. As a result, we will have the following guarantee.\nLemma 17. Consider Algorithm 3. If it reaches Step 10, then ANS\n\n= TBD.\n\n3.2.1 Basic Multi\ufb01lter for Sparse Polynomials\n\nThe BASICMF algorithm (Algorithm 4) is a key ingredient in the multi\ufb01ltering framework. It takes as\ninput a sparse polynomial p and uses it to certify whether T is \u21b5-good and suf\ufb01ciently concentrated.\nThe central idea is to measure how p(T ) distributed and compare it to that of the distribution of\np(G). We require the input p has certi\ufb01able variance on G, i.e. Var[p(G)]\n1, as otherwise, it could\n\ufb01lter away a large number of the good samples. We note that the bounded variance condition is\nalways satis\ufb01ed for degree-1 Hermite polynomials under proper normalization, while for high-degree\npolynomials, one cannot invoke BASICMF directly (see Section 3.2.2 for a remedy).\n\n\uf8ff\n\nThe way that BASICMF certi\ufb01es the input sample set T not being \u21b5-good is quite simple: if not\nall samples lie in a small L\n-ball, it returns \u201cNO\u201d at Step 2, in that this contradicts Lemma 13.\nOtherwise, the algorithm will attempt to search for a \ufb01ner interval [a, b] such that it includes most\nof the samples. If such interval exists, then either the adversary corrupted the samples such that the\nsample variance is as small as that of Gaussian while the sample mean may deviate far from the\ntarget, in which case BASICMF returns TBD at Step 5; or the sample variance is large, in which case\n\n1\n\n8\n\n6\n6\n\fRd, parameter \u21b5\n\n2\nP(Rd, l, 4`2k4`, 2`k2`) such that l\n\n\u21e2\n\n(0, 1/2], failure probability \u2327\n\n2`, Var[p(G)]\n\n\uf8ff\n\n\uf8ff\n\n(0, 1),\n2\n1, and p(x) =\n\nAlgorithm 4 BASICMF(T,\u21b5,\u2327, p )\nRequire: A multiset of samples T\n\na polynomial p\nhA(x\n\n\u00b5T ).\n\n2\n\nl/2\n\n, \u0000\np(y)\n\nlog 1\n\u21b5 )\np(x)\n\n\u0000\n(C1 \u00b7\n1: R\n2: if maxx,y\n\u0000\n3: if there is an interval [a, b] of length C1 \u00b7\np(x) : x\nC1 \u00b7\nreturn \u201cTBD\u201d.\n\nT |\nof samples in\n\n2\n` + C1 log 1\n\u21b5\n\nC0 \u00b7\n> 2k`\nq\n\u00b7\n\n{\nif Var[p(T )]\n\nthen\n\n\uf8ff\n\nT\n\n}\n\n2\n\n|\n\nl\n\nelse\n\n\u0000\nFind a threshold t > 2R such that\n\n\u0000\n\n4:\n5:\n6:\n7:\n\n\u00b7\n\nlog `d\n\u21b5\u2327 .\n\n`\n\u00b7\n\u0000l then return \u201cNO\u201d.\nlog(2 + log 1\n\nR\n\n\u00b7\nlog2(2 + log 1\n\n\u21b5 ) then\n\n\u21b5 ) that contains at least (1\n\n\u21b5\n2 )-fraction\n\n\u0000\n\nmin\n\np(x)\n\n,\n\na\n\n|\n\n|\n\n\u0000\n\np(x)\n\nb\n\n\u0000\n\n|} \u0000\n\n{|\n\nt\n\n>\n\n32\n\u21b5\n\nexp(\n\n(t\n\n\u0000\n\n\u0000\n\n2R)2/l) +\n\n2\u21b52\nk2` logl( `d\n\u21b5\u2327 )\n\n.\n\n\u21e5\nx\n\nT : min\n\n2\n(T 0,\u21b5 0)\n{\n\n.\n\n}\n\np(x)\n\n,\n\na\n\n|\n\n|\n\n\u0000\n\np(x)\n\n\u0000\n\n{|\n\n\u21e4\nb\n\n|} \uf8ff\n\n, \u21b50\n\nt\n\n}\n\n(1\n\n\u0000\n\nT\n\n|\n\n\u21b5/8)\nT 0|\n\n|\n\n|\n\n\u21b5\n\n\u00b7\n\n\u21e3\n\n+ \u21b5\n8\n\n.\n\n\u2318\n\nPr\nT\nx\n\u21e0\n\nT 0\n\n {\nreturn\n\nend if\n\n8:\n\n9:\n10:\n11: else\n12:\n\nFind t\n2\np(x) < t + R0\n\nsatisfy\n\nR, R0 > 0 such that the sets T1 :=\n\nx\n\n{\n\n2\n\nT : p(x) > t\n\nR0\n\n}\n\n\u0000\n\nand T2 :=\n\nx\n\n{\n\n2\n\nT :\n\n}\n2 +\n\nT1|\n\n|\n\n2\n\nT2|\n\n|\n\nT\n\n|\n\n\uf8ff|\n\n2 (1\n\n\u0000\n\n\u21b5/100)2 and\n\nT\n\n|\n\n|\u0000\n\nmax(\n\n,\n\nT1|\n\n|\n\n)\n\nT2|\n\n|\n\n\u21b5\n\nT\n\n|\n\n|\n\n\u0000\n\n/4.\n\n\u21b5\n\u21b5i  \n13:\nreturn\n14:\n15: end if\n\n\u00b7\n{\n\n\u21b52/100)\nT\n(1\n\u00b7 |\n(T1,\u21b5 1), (T2,\u21b5 2)\n\n\u0000\n\n|\n.\n}\n\n/\n\nTi|\n\n|\n\n, for i = 1, 2.\n\nit is possible to construct a sparsity-induced \ufb01lter to prune outliers (see Steps 7 and 8). We note that\nin Step 7, the \ufb01rst term on the right-hand side is derived from Chernoff bound for degree-l Gaussian\npolynomials and the second term is due to concentration of empirical samples to Gaussian (see\nDe\ufb01nition 8), both of which are scaled by a factor 8/\u21b5 so that the number of the samples removed\nT , which\nfrom T is 8/\u21b5 times more than that of the good samples in the representative set SG \u21e2\nmeans most of the removed samples are outliers. We show by contradiction the existence of the\nthreshold t (see Lemma 26). In fact, had such threshold t not existed, the set T must be suf\ufb01ciently\nconcentrated such that the algorithm would have returned at Step 5. This essentially relies on our\nresult of the initial clustering of Algorithm 1, which guarantees that each subset T is bounded in a\nsmall L\n-ball and the function value of p on the \u21b5-good T does not change drastically (Lemma 13).\nWe then show that equipped with such threshold t, T 0 is a subset of T and it is \u21b50-good if T is \u21b5-good\n(Lemma 28).\n\n1\n\nsuch that T1 \\\n\nWhen there is no such short interval [a, b], the algorithm splits T into two overlapping subsets\nT2 is large enough to contain most of the samples in SG. This guarantees\nT1, T2}\n{\nthat most of the samples in SG (if T is \u21b5-good) are always contained in one subset and thus there\nalways exists an \u21b5-good subset of T . We show that an appropriate threshold t can also be found at\nStep 12 (Lemma 30), and at least one Ti is \u21b5i-good if T is \u21b5-good.\n\nAs a result, we have the following guarantees for Algorithm 4; see Appendix B for the full proof.\nTheorem 18 (BASICMF). Consider Algorithm 4. Denote by ANS its return. Suppose that T being\n\u21b5-good implies Var[p(G)]\n\u2327 , the following holds. ANS is either\n2. 1) If ANS = NO, then T is not \u21b5-good. 2) If\n\u201cNO\u201d, \u201cTBD\u201d, or a list of\n; and if additionally T is \u21b5-good,\nANS = TBD, then Var[p(T )]\n\n1. Then with probability 1\nm\ni=1 with m\n}\nl\n(` + log 1\n\u21b5 )\n\n\u0000\n\nE[p(G)]\n|\nT and\n\nthen\nTi \u21e2\n\n\u0000\ni\n\nE[p(T )]\n1\n1\n\u21b52 for all i\n\u21b52\n\u0000\ni \uf8ff\n\n|\uf8ff\n\n2\n\nm\ni=1, then\n\u0000\n}\n[m]; if additionally T is \u21b5-good, then at least one Ti is \u21b5i-good.\n\nlog(2 + log 1\n\u21b5 )\n\n(Ti,\u21b5 i)\n{\n\n\u0000\n\n\u00b7\n\n\u0000\n\n\uf8ff\nlog2(2 + log 1\n\u21b5 )\n\u00b7\n. 3) If ANS =\n\n\uf8ff\n(Ti,\u21b5 i)\n{\nO\n(` + log 1\n\u21b5 )\n\n\uf8ff\nO\n\nl\n2\n\nP\n\n9\n\n \n \n \n\fAlgorithm 5 HARMONICMF(T,\u21b5,\u2327, p )\nRequire: A multiset of samples T\n\n1: for l0 = 0, 1, . . . , l do\n2:\n\nLet B(l0) be an order-2l0 tensor with\n\n2\n\npolynomial p\n\nP(Rd, l, 2`k2`, 2`k2`) such that p(x) = hA(x\n\nRd, parameter \u21b5\n\n\u21e2\n\n2\n\n(0, 1/2], failure probability \u2327\nk2 = 1.\n\u00b5T ) and\n\nA\nk\n\n\u0000\n\n(0, 1), a\n\n2\n\nB(l0)\n\ni1,...,il0\n\n,j1,...,jl0\n\n=\n\nXkl0+1,...,kl\n\nAi1...,il0\n\n,kl0+1,...,kl Aj1...,jl0\n\n,kl0+1,...,kl .\n\n3:\n\nConsider B(l0) as a dl0\ncoordinates together. Apply eigenvalue decomposition on B(l0) to obtain B(l0) =\n\ndl0 symmetric matrix by grouping each of the i1, . . . , il0 and j1, . . . , jl0\nVi.\nMULTILINEARMF(T, Vi, l0,\u21b5,\u2327/ (ldl)) for every Vi. If ANSi = NO or a list of\nfor some i, then return ANSi. If ANSi = TBD, continue.\n\ni \u0000iVi \u2326\n\nP\n\n\u2326\n\nBASICMF(T,\u21b5,\u2327, 1\n\n\u0000 hA(x\n\n\u00b5T )) with \u0000 :=\n\n\u0000\n\n(1 + log 1\n\u21b5 )\n\nlog2(2 + log 1\n\u21b5 )\n\n\u00b7\n\nIf ANS = NO or a list of (Tj,\u21b5 j), return ANS. If ANS = TBD, still return \u201cNO\u201d.\n\nC1 \u00b7\n\n\u0000\n\nl\n\n2 .\n\n\u0000\n\n4: ANSi  \n(Tj,\u21b5 j)\n{\n5: end for\n6: ANS\n\n}\n\n\n\nThe following is the appendix section of the paper you are reviewing:\n Omitted Proofs from Section 2\n\nA.1 Proof of Proposition 9\n\nProof. Fix a subset \u2326\nmost l, denoted by\nP(Rd, l, \uf8ff,  ) =\n\nM\n[\u2326 [M\n\n\u21e2\n(\u2326, l). Let P(Rd,\n(\u2326,l) P(Rd,\n\nM\n(\u2326, l), \u2326).\n\n[d] with size  , and then \ufb01x a set of \uf8ff monomials on \u2326 with degree at\n(\u2326, l), \u2326) be the induced class of polynomials. Note that\n\nIt is easy to see that for any p\n(\u2326, l), \u2326), it can be represented by a linear combinations\nof the \uf8ff monomials. Thus, the VC dimension of this class equals \uf8ff + 1. Then, we note that there are\n(\u2326, l). Therefore,\n\nchoices of \u2326, and for any given \u2326, there are\n\nchoices of\n\nM\n\n2\n\nd\nj\n\nj=0\n\nM\n\nM\nP(Rd,\n\nthe total number of the subclass P(Rd,\nP\n\n\u0000\n\n\u0000\n\n\uf8ff\nj=0\n(\u2326, l), \u2326) is at most\n\n2dl\nj\n\nP\n\n\u0000\n\n\u0000\n\n2dl\nj\n\n\uf8ff\n\ned\n\n2edl\n\uf8ff\n\n\u00b7\n\nM\n\uf8ff\n\nd\nj\n\n\u00b7\n\n\u25c6\n\nj=0 \u2713\nX\n\nj=0 \u2713\nX\n\nThe concept class union argument states that for\nbounded by O(max\n\nV, log m + V log log m\n{\n\nV }\n\nHi. In our case, we have V = \uf8ff + 1 and m\n\nall\n\u00b7\nthat the VC dimension of P(Rd, l, \uf8ff,  ) is upper bounded by\n\u21e3\n\n\uf8ff\n\n\u21e3\n\n\u2318\n\ned\n\n\u25c6\n\n\u2713\n\n\u2713\n\n\u25c6\nis upper\ni=1Hi, the VC dimension of\n), where V is an upper bound on the VC dimension of\n\nH\n\nH\n\n=\n\n\u25c6\n\n[\n\nm\n\n2edl\n\uf8ff\n\n\uf8ff\n\n\u2318\n\n. By calculation, we can show\n\n\uf8ff\n\n.\n\n(A.1)\n\ned\n\n  log\n\n+ \uf8ff log\n\n2edl\n\uf8ff\n\n+ \uf8ff + 1\n\n\uf8ff\n\n(l\uf8ff +  ) log d =: d0.\n\n(A.2)\n\nRecall that the VC theory states that for any \u270f, \u2327\nSG| \u0000\nfor some absolute constant C > 0, the following holds with probability 1\n\n(0, 1), as long as\n\n2\n\n|\n\nC\n\n\u0000\n\n\u270f.\n\nd0\n\n\u270f2 log d0\n\n\u270f + 1\n\n\u270f2 log 1\n\n\u2327\n\n\u2327 :\n\u21e3\n\n\u2318\n\n(A.3)\n\nPr[p(G)\n\n0]\n\n\u0000\n\nx\n\nPr\nSG\n\u21e0\n\n\u0000\n\n[p(x)\n\n0]\n\n\u0000\n\n\uf8ff\n\np\n\nsup\n2P(Rd,l,\uf8ff, ) \u0000\n\u0000\n\u0000\n\u0000\n\nin (A.2),\n\n\u0000\n\u0000\n\u0000\n\u0000\n\nWith the expression of d0\n\uf8ff+ ) log d\nlog (l\n(l\n\n\uf8ff+ ) log d\n\n\u00b7\n\n\u00b7\n\n\u270f2\n\n\u270f\u2327\n\nit\n\nis not hard to see that we can set\n\nSG|\n\n|\n\n= C\n\n\u00b7\n\nfor some absolute constant C > 0 to ensure that the above holds.\n\nWhen l = 2`, \uf8ff = 4`2k4`,   = 2`k2`, and \u270f =\nSG|\n\nalgebraic calculation, it suf\ufb01ces to pick\nconstant C 0. This completes the proof.\n\n|\n\n100k2`\n\n= C 0\n\n\u21b53\nlog2`( `d\n\u00b7\nk8`\n`4\n\u00b7\n\u21b56\n\n\u00b7\n\n\u21b5\u2327 ) for some natural number `\nlog6`( `d\n\n\u21b5\u2327 ) for some suf\ufb01ciently large\n\n1, by\n\n\u0000\n\n\u00b7\n\nA.2 Proof of Lemma 11\n\nProof. By the standard tail bound of Gaussian distribution, for any x drawn from N (\u00b5, Id), it holds\nt2/2). By taking union bound over both\nthat for any given index i\nt2/2).\nindex i and sample x\n2d\nxi \u0000\nChoosing t =\n\n\u00b5i| \u0000\nxi \u0000\nt]\n2\n|\nSG, we have Pr[maxx\n/\u2327 ) completes the proof.\nSG|\n\n2 exp(\n\uf8ff\nSG maxi\n\n2\n2 log(d\n\n\u0000\n[d] |\n2\n\n\u00b5i| \u0000\n\n[d], Pr[\n\nSG|\n\nexp(\n\n\uf8ff\n\n\u0000\n\nt]\n\n2\n\n|\n\n|\n\nA.3 Proof of Lemma 12\n\np\n\nProof. Let SG be the subset of T containing the samples drawn i.i.d. from N (\u00b5, Id). Since\n2\u21b5\n\n, we know that SG is a representative set with probability at least 1\n\nSG|\n\u2327 in light of Prop. 9.\n\nT\n\n|\n\n=\n\n\u0000\n\n\u00b7 |\n\n|\n\nConsider Algorithm 1. If for all x, y\n\nT , we have\n\n2\n\nx\n\nk\n\n\u0000\n\ny\n\nk1 \uf8ff\n\n6\u0000,\n\n(A.4)\n\nthen the algorithm returns only one cluster and the lemma follows immediately.\n\nIf that is not the case, we \ufb01rst note that, with probability at least 1\nEq. (A.4) due to Lemma 11. Let us condition on this event occurs from now on.\n\n\u0000\n\n\u2327 all of the samples in SG satisfy\n\n16\n\n \n \n \n \n \n \n \n\f-balls of radius 2\u0000, of which each is centered at one\nAlgorithm 1 constructs a set of disjoint L\nsample in T and contains at least an \u21b5-fraction of samples in T . Therefore, the number of such\nballs is at most m =\n. Let B0i be the ball that has the same\nb\ncenter as Bi but with `\n[m], such that\n1\nB0i is \u21b5-good.\nTi = T\n\nB1, . . . , Bm}\n-radius of 6\u0000. In the following, we show that there exists i\n\n. Denote the set by\n\n1/\u21b5\n\n2\n\n1\n\n{\n\nc\n\ny\n\nSG, for which we know that\n\nConsider a sample x\n-ball\nk1 \uf8ff\n, all of the samples in SG will be contained in Bx. In addition,\nBx :=\ny\nk\n}\n. That is,\nthere must exist one Bi that intersects Bx, as otherwise Bx will be in the set\nBi. By construction, Bx must be containted in B0i. Therefore, all samples of SG\nz\n\nB1, . . . , Bm}\n\n\u0000. Then, for the L\n\n{\nT, z\n\nk1 \uf8ff\n\nRd :\n\nx\nk\n\n2\nx\n\n2\u0000\n\n\u0000\n\n\u0000\n\n2\n\n\u00b5\n\n1\n\n{\n\n2\n\n9\nmust be included in Ti and Ti is \u21b5-good.\n\n2\n\nBx \\\n\n\\\n\nA.4 Proof of Lemma 13\n\nProof. Recall that after running Algorithm 1, every subset Ti is contained in an L\n-ball of radius 6\u0000.\n1\nT ,\nBy Jensen\u2019s inequality and the convexity of the L\n6\u0000.\n\u00b5T k1 \uf8ff\nx\nk\nN (\u00b5T ,I)[p(x)] = 0 due to the de\ufb01nition of\n2\n2. Denote z = x\nk\n\nRecall that we assumed p(x) = hA(x\nharmonic polynomials. Thus, Varx\n\n1\n\u00b5T ). Thus Ex\n\u21e0\nA\nk\n\n-norm, we have for all x\n\nN (\u00b5T ,I)[p(x)] =\n\n\u00b5T . Then,\n\n\u0000\n\n\u0000\n\n\u0000\n\n2\n\n\u21e0\n\np(x)\n\n=\n\n|\n\n|\n\nca(j)\n\n[k2`]\nXj\n2\n\n\u0000\n\u0000\n\u0000\n\u0000\n\u0000\n\u0000\n\nHea(j) (z)\n1! \u0000\na(j)\n\u0000\n\u0000\n\u0000\n\u0000\n\u0000\n\n\u0000\n\u0000\n\n0\n\n\uf8ff v\nu\nu\nu\nt\n\n[k2`]\nXj\n2\n\nc2\na(j) 1\n\n0\n\n[k2`]\nXj\n2\n\nHea(j) (z)2\na(j)\n\n1! 1\nA\n\n.\n\n(A.5)\n\n@\nwhere a(j) is a d-dimensional multi-index for the j-th monomial, and ca(j) denotes its coef\ufb01cient.\nObserve that in the \ufb01rst step, p(x) is written as a linear combination of k2` Hermite polynomials,\nsince we are considering p\n\nP(Rd, l, k2`, 2`k2`). Note also that\n\nq\u0000\n\u0000\n\n[k2`] c2\n\n\u0000\n\u0000\n\n\u0000\n\u0000\n\n@\n\nA\n\n1.\n\na(j) =\n\nj\n\nA\nk\n\nk\n\n2\n2 \uf8ff\n\n2\n\n2\n\nTo bound the second factor on the right-hand side of (A.5), we use Mehler\u2019s formula, which shows\nthat for any u with\n\n< 1 and any natural number a,\n\nP\n\nu\n\n|\n\n|\n\nHe2\n\na(zi)ua\na!\n\n=\n\np1\n\nu\n\n1+u z2\ni ,\n\ne\n\n1\n\n\u0000\n\nu2\n\n1\n\na=0\nX\n\nSince each Hea(j) (z) has degree at most l, it can be decomposed as a product of at most l univariate\nHermite polynomials. Thus, we take such product and sum over j\n\n[k2`] to obtain\n\n2\n\na(j)\ni\n\n=0\n\nHea(j)\na(j)\n\ni\n\n\u21e3\n\nua(j)\n\ni\n\n(zi)2\n\n\u00b7\n\n1!\n\nk2`\n\n(1\n\n\u00b7\n\n\u0000\n\n\u2318\n\n\uf8ff\n\nu2)\u0000\n\nl\n2\n\n\u00b7\n\nu\n1+u k\n\ne\n\ntriml(z)\n\n2\n\n2 .\n\nk\n\n[k2`] Q\n\nXj\n2\n\nTo simplify the above expression, observe that\n\n\u0000\n\u0000\n\n\u0000\n\u0000\n\n2\ntriml(z)\n2 \uf8ff\nk\ntogether gives\n\nk\n\nl\n\nz\n\u00b7 k\n\nk\n\n2\n1 \uf8ff\n\n36l\u00002. Lastly, by algebra, (1\n\nQ\n\n\u0000\n\n=0 ua(j)\n\na(j)\ni\n\na(j)\n\ni = uk\nu2)\u0000\n\nl\n2\n\n\uf8ff\n\nul.\n\n\u0000\n\nIn addition,\n\nk1\nu2 l\n2 . Putting all pieces\n\ne\n\nHea(j) (z)2\na(j)\n\n1! \uf8ff\n\nl\n\nu\u0000\n\nk2`\n\n\u00b7\n\n\u00b7\n\n\u00b7\n\nu2 l\n2\n\ne\n\n36l\u00002 u\n1+u = k2`\n\ne\n\n2 + 36l\u00002 u\nu2 l\n1+u .\n\ne\n\nl\n\nu\u0000\n\n\u00b7\n\n\u00b7\n\n[k2`]\nXj\n2\n\u0000 ; this is possible as \u0000 > 1. Then the exponent u2l\n\n\u0000\n\u0000\n\n\u0000\n\u0000\n\nWe set u = 1\n37l.\nWithout loss of generality, we may assume that \u0000 > e37; in fact, we can always ensure this by setting\n\u0000 = (C0 + e37)\n\u21b5\u2327 where C0 is the constant given in Algorithm 1. Thus, it follows that\n\n2\u00002 + 36l\n\n1+u = l\n\n1+1/\u00002 \uf8ff\n\nlog `d\n\n`\n\n2 + 36l\u00002u\n\n\u00b7\n\n\u00b7\n\nq\n\nPlugging it into (A.5) completes the proof.\n\n\u0000\n\u0000\n\n\u0000\n\u0000\n\n[k2`]\nXj\n2\n\nHea(j) (z)2\na(j)\n\n1! \uf8ff\n\nk2`\n\n\u0000l\n\n\u00b7\n\n\u00b7\n\ne37l\n\n\uf8ff\n\nk2`\n\n\u00b7\n\n\u00002l.\n\n17\n\n6\n6\n\f\n\nBased on the section(s), please answer the following question with yes, no, or n/a and provide a brief justification for your answer.\nQuestion: If the authors include theoretical results, do the authors state the full set of assumptions of all theoretical results?"}, "2b": {"role": "user", "content": "The following is the methods section of the paper you are reviewing:\n\n\nL\n\n3.1 Overview of Attribute-Ef\ufb01cient Multi\ufb01ltering\n\nThe ATTRIBUTE-EFFICIENT-MULTIFILTER algorithm is presented in Algorithm 3. The starting\npoint of the algorithm is a well-known fact that if the adversary were to signi\ufb01cantly deteriorate\nour estimate on \u00b5, the spectral norm of a certain sample covariance matrix \u02dc\u2303 would become large\n[DKK+16, LRV16]. In order to achieve attribute-ef\ufb01cient sample complexity O(poly(k, log d)), it\nis however vital to control the spectral norm only on k`-sparse directions for some pre-speci\ufb01ed\n1, which can further be certi\ufb01ed by a small Frobenius norm restricted on the\npolynomial degree `\nlargest k2` entries. If the restricted Frobenius norm is suf\ufb01ciently small, it implies that the sample\ncovariance matrix behaves as a Gaussian one, and the algorithm returns the empirical mean truncated\nto be k-sparse (see Step 4). Otherwise, the algorithm will invoke either BASICMF (i.e. Algorithm 4)\n\n\u0000\n\n6\n\n \n\f(0, 1),\n\n2\n\n(0, 1),\n\n2\n\nAlgorithm 2 Main Algorithm: Attribute-Ef\ufb01cient List-Decodable Mean Estimation\nRequire: A multiset of samples T\n\nRd, parameter \u21b5\n\n(0, 1/2], failure probability \u2327\n\n1.\nCLUSTER(T,\u21b5,\u2327,` ),\n\n\u21e2\n\n2\n\n\u0000\n\ndegree of polynomials `\nT1, . . . , Tm} \n1:\n{\n=\n2: while\ndo\nL6\n(T 0,\u21b5 0)\nan element in\n3:\n4: ANS\n(i)\n(ii)\n(iii)\n\n,\n\nL\n\n;\nL   L\\{\n}\nATTRIBUTE-EFFICIENT-MULTIFILTER(T 0,\u21b5 0,\u2327/\nif ANS is a vector then add it into M .\nif ANS is a list of (Ti,\u21b5 i) then append those with \u21b5i \uf8ff\nif ANS = NO then go to the next iteration.\n\nT\n\n|\n\n|\n\n.\n\n,` ).\n\n1 to\n\n.\n\nL\n\nL {\n(T 0,\u21b5 0)\n\n(T1,\u21b5/ 2), . . . , (Tm,\u21b5/ 2)\n\n, M\n}\n\n.\n  ;\n\n5: end while\n6: return LISTREDUCTION(T,\u21b5,`, M ).\n\n(0, 1/2], failure probability \u2327\n\n2\n\n\u00b5T )>], and Pd,`(x) is the column vector of all degree-` Hermite\n\n2 (k2`\n1 [{\n\nk`) entries above the\nI,\n\n1, U 0\n\nI\n\n\u0000\n(jt, it)\n\n}t\n\n\u0000\n\n\u21e5\n\nfor large enough constant C1 > 0.\n\n\u21e2\n\n1.\n\u0000\nPd,`(T\n\nAlgorithm 3 ATTRIBUTE-EFFICIENT-MULTIFILTER(T,\u21b5,\u2327,` )\nRequire: A multiset of samples T\n\nRd, parameter \u21b5\n\ndegree of polynomials `\n\n1: \u02dc\u2303\n\nindex set of the k` diagonal entries and 1\n\n1\n\n\u00b7\n\n1\n\n2:\n\n\u0000\n\n\u00b5T )\n\nE[Pd,`(T\n\u0000\npolynomials of x.\n2 (k2`+k`)\n(it, jt)\nt\n{\n}\n\u0000\nmain diagonal of \u02dc\u2303 with largest magnitude. U\nwith I =\njt}t\n1.\n1 [{\n{\n(` + C1 log 1\n3: \u0000\u21e4sparse  \n\u21b5 )\n( \u02dc\u2303)U\n\u0000\u21e4sparse then return \u02c6\u00b5\n4: if\n\u0000\n\u0000\n5: (\u0000\u21e4, v\u21e4)\n\u0000\n\u0000\n\u0000\n\u0000\n\u0000\u21e4sparse then\n6: if \u0000\u21e4\nif ` = 1 then ANS\n7:\nwhere p1(x) := v\u21e4\n\nit}t\nC1 \u00b7\n\u21e5\nF \uf8ff\nthe largest eigenvalue and eigenvector of ( \u02dc\u2303)U 0 .\n\nlog2(2 + log 1\n\u21b5 )\ntrimk(\u00b5T ).\n\n {\n2`\n\nPd,`(x\n\nBASICMF(T,\u21b5,\u2327, p 1) else ANS\n\n(it, jt)\n\n\u00b5T ).\n\n\u0000\n\n\u0000\n\n\u0000\n\n\u21e4\n\n\u00b7\n\n}t\n\n\u0000\n\n\u00b7\nPd,`(x\n\n\u0000\n\u00b5T )>\n\n1\nA0kF \u00b7\n\nA0\nk\nHARMONICMF(T,\u21b5,\u2327, p 2).\n\n\u0000\n\n\u00b7\n\n\u0000\n\nPd,`(x\n\n\u00b7\n\n\u0000\n\n\u00b5T )\n\nwith A0 := ( \u02dc\u2303)U 0 .\n\n\u0000\n\n8: else\np2(x)\n9:\n10: ANS\n11: end if\n12: return ANS.\n\nHARMONICMF(T,\u21b5,\u2327, p 1)\n\nor HARMONICMF (i.e. Algortihm 5) to examine the concentration of a polynomial of the empirical\ndata to that of Gaussian. Both algorithms will either assert that the current sample set does not contain\na suf\ufb01ciently large amount of Gaussian samples, or will prune many corrupted samples to increase\nthe fraction of Gaussian ones. A more detailed description of the two algorithms can be found in\nSection 3.2.1 and Section 3.2.2 respectively. What is subtle in Algorithm 3 is that we will check\nthe maximum eigenvalue \u0000\u21e4 of the empirical covariance matrix \u02dc\u2303 restricted on a carefully chosen\nsubset U 0, which corresponds to the maximum eigenvalue on a certain (2k2`)-sparse direction. If \u0000\u21e4\nis too large, this indicates an easy problem since it must be the case that the adversary corrupted the\nsamples in an aggressively way. Therefore, it suf\ufb01ces to prune outliers using a degree-` polynomial\np1 which is simply the projection of Pd,`(x\n\u00b5T ) onto the span of the maximum eigenvector; see\nStep 7 in Algorithm 3. On the other hand, if \u0000\u21e4 is on a moderate scale, it indicates that the adversary\ncorrupted the samples in a very delicate way so that it passes the tests of both Frobenius norm and\nspectral norm. Now the main idea is to check the concentration of higher degree polynomials induced\nby the sample set; we show that it suf\ufb01ces to construct a degree-2` harmonic polynomial; see Step 10.\nWhile sparse mean estimation has been studied in [DKK+19] and the idea of using restricted\nFrobenius norm and \ufb01ltering was also developed, we note that their analysis only holds in the mild\ncorruption regime where \u21b5> 1/2. To establish the main results, we will leverage the tools from\n[DKS18b], with a speci\ufb01c treatment on the fact that \u00b5 is k-sparse, to ensure an attribute-ef\ufb01cient\nsample complexity bound. As we will show later, a key idea to this end is to utilize a sequence of\ncarefully chosen sparse polynomials in the sense of De\ufb01nition 7 along with sparsity-induced \ufb01lters.\n\n\u0000\n\n7\n\n \n \n \n \n \n \n \n \n \n \n \n\fThe performance guarantee of ATTRIBUTE-EFFICIENT-MULTIFILTER is as follows.\nTheorem 14 (Algorithm 3). Consider Algorithm 3 and denote by ANS its output. With probability\n\u2327 , the following holds. ANS cannot be TBD. If ANS is a k-sparse vector and if T is \u21b5-good,\n1\n\u0000\n. If ANS = NO, then T is not \u21b5-good. If ANS =\n\u00b5\nthen\nk\n1\n1\n\u21b52 ; if additionally T is\n(Ti,\u21b5 i)\n\u21b52\n{\ni \uf8ff\n\u21b5-good, then at least one Ti is \u21b5i-good. Finally, the algorithm runs in time O\n\n2` p`(` + log 1\n\u21b5 )\n2, then Ti \u21e2\n\n\u21b5\u0000\n\u02c6\u00b5\n\u0000\nm\ni=1 for some m\n}\n\nT for all i\n\u0000\n\n[m] and\n\nk2 \uf8ff\n\npoly(\n\n, d`)\n\nm\ni=1\n\n\u02dcO\n\n\uf8ff\n\n2\n\nT\n\n\u0000\n\n.\n\n1\n\nP\n\n|\n\n|\n\n3.2 Analysis of ATTRIBUTE-EFFICIENT-MULTIFILTER\n\n\u0000\n\n\u0000\n\nWe \ufb01rst show that if the restricted Frobenius norm of the sample covariance matrix is small, then the\nsample mean is a good estimate of the target mean.\nLemma 15. Consider Algorithm 3. If the algorithm returns a vector \u02c6\u00b5 at Step 4 and if T is \u21b5-good,\n(` + log 1\n\u21b5 )\nwe have that\n\nlog2(2 + log 1\n\u21b5 )\n\n2` p`\n\n\u21b5\u0000\n\nO\n\n\u00b5\n\n\u02c6\u00b5\n\n.\n\n1\n\nk\n\n\u0000\n\nk2 \uf8ff\n\n\u00b7\n\n\u00b7\n\n\u0000\n\n\u0000\n\nNext, we give performance guarantees on the remaining steps of Algorithm 3, where we consider the\ncase that the algorithm does not return at Step 4. Namely, the algorithm will either reach at Step 7\nor Step 10, and will return the ANS obtained thereof. These two steps will invoke BASICMF or\nHARMONICMF on different sparse polynomials. Observe that both algorithms may return 1) \u201cNO\u201d,\nm\nwhich certi\ufb01es that the current input set T is not \u21b5-good; 2) a list of subsets\ni=1 for some\n}\nm\n2, on which Algorithm 3 will be called in a recursive manner; or 3) TBD, which indicates that\nthe algorithm is uncertain on T being \u21b5-good. In the following, we prove that the way that we invoke\nBASICMF and HARMONICMF ensures that they will never return TBD when being called within\nAlgorithm 3. We then give performance guarantees on these two \ufb01ltering algorithms when they return\n\u201cNO\u201d or\n\nm\ni=1, thus establishing Theorem 14.\n\n(Ti,\u21b5 i)\n\n\uf8ff\n\n{\n\n(Ti,\u21b5 i)\n{\n\n}\n\nLet us consider that the algorithm reaches Step 7, i.e. the largest eigenvalue on one sparse direction is\nlarger than the threshold \u0000\u21e4sparse. It is easy to see that when ` = 1, ANS cannot be TBD since the\nonly way that BASICMF returns TBD is when Var[p(T )] is not too large, but this would violate the\ncondition that \u0000\u21e4 >\u0000 \u21e4sparse in view of our setting on \u0000\u21e4sparse. Similarly, we show that under the large\n\u0000\u21e4 regime, HARMONICMF will not return TBD either. Thus, we have the following lemma.\nLemma 16. Consider Algorithm 3. If it reaches Step 7, then ANS\n\n= TBD.\n\nNow it remains to consider the case that the algorithm reaches Step 10, which is more subtle since\nthe evidence from the magnitude of the largest restricted eigenvalue is not so strong to prune outliers.\nNote that this could happen even when T contains many outliers, since \u0000\u21e4 is not the maximum\neigenvalue on all sparse directions but on a submatrix indexed by U 0. Fortunately, if \u0000\u21e4 is not\nlarge, we show that the algorithm can still make progress by calling HARMONICMF on degree-2`\nsparse polynomials. This is because higher-degree polynomials are more sensitive to outliers than\nlow-degree polynomials, as far as we can certify the concentration of high-degree polynomials on\nclean samples. As a result, we will have the following guarantee.\nLemma 17. Consider Algorithm 3. If it reaches Step 10, then ANS\n\n= TBD.\n\n3.2.1 Basic Multi\ufb01lter for Sparse Polynomials\n\nThe BASICMF algorithm (Algorithm 4) is a key ingredient in the multi\ufb01ltering framework. It takes as\ninput a sparse polynomial p and uses it to certify whether T is \u21b5-good and suf\ufb01ciently concentrated.\nThe central idea is to measure how p(T ) distributed and compare it to that of the distribution of\np(G). We require the input p has certi\ufb01able variance on G, i.e. Var[p(G)]\n1, as otherwise, it could\n\ufb01lter away a large number of the good samples. We note that the bounded variance condition is\nalways satis\ufb01ed for degree-1 Hermite polynomials under proper normalization, while for high-degree\npolynomials, one cannot invoke BASICMF directly (see Section 3.2.2 for a remedy).\n\n\uf8ff\n\nThe way that BASICMF certi\ufb01es the input sample set T not being \u21b5-good is quite simple: if not\nall samples lie in a small L\n-ball, it returns \u201cNO\u201d at Step 2, in that this contradicts Lemma 13.\nOtherwise, the algorithm will attempt to search for a \ufb01ner interval [a, b] such that it includes most\nof the samples. If such interval exists, then either the adversary corrupted the samples such that the\nsample variance is as small as that of Gaussian while the sample mean may deviate far from the\ntarget, in which case BASICMF returns TBD at Step 5; or the sample variance is large, in which case\n\n1\n\n8\n\n6\n6\n\fRd, parameter \u21b5\n\n2\nP(Rd, l, 4`2k4`, 2`k2`) such that l\n\n\u21e2\n\n(0, 1/2], failure probability \u2327\n\n2`, Var[p(G)]\n\n\uf8ff\n\n\uf8ff\n\n(0, 1),\n2\n1, and p(x) =\n\nAlgorithm 4 BASICMF(T,\u21b5,\u2327, p )\nRequire: A multiset of samples T\n\na polynomial p\nhA(x\n\n\u00b5T ).\n\n2\n\nl/2\n\n, \u0000\np(y)\n\nlog 1\n\u21b5 )\np(x)\n\n\u0000\n(C1 \u00b7\n1: R\n2: if maxx,y\n\u0000\n3: if there is an interval [a, b] of length C1 \u00b7\np(x) : x\nC1 \u00b7\nreturn \u201cTBD\u201d.\n\nT |\nof samples in\n\n2\n` + C1 log 1\n\u21b5\n\nC0 \u00b7\n> 2k`\nq\n\u00b7\n\n{\nif Var[p(T )]\n\nthen\n\n\uf8ff\n\nT\n\n}\n\n2\n\n|\n\nl\n\nelse\n\n\u0000\nFind a threshold t > 2R such that\n\n\u0000\n\n4:\n5:\n6:\n7:\n\n\u00b7\n\nlog `d\n\u21b5\u2327 .\n\n`\n\u00b7\n\u0000l then return \u201cNO\u201d.\nlog(2 + log 1\n\nR\n\n\u00b7\nlog2(2 + log 1\n\n\u21b5 ) then\n\n\u21b5 ) that contains at least (1\n\n\u21b5\n2 )-fraction\n\n\u0000\n\nmin\n\np(x)\n\n,\n\na\n\n|\n\n|\n\n\u0000\n\np(x)\n\nb\n\n\u0000\n\n|} \u0000\n\n{|\n\nt\n\n>\n\n32\n\u21b5\n\nexp(\n\n(t\n\n\u0000\n\n\u0000\n\n2R)2/l) +\n\n2\u21b52\nk2` logl( `d\n\u21b5\u2327 )\n\n.\n\n\u21e5\nx\n\nT : min\n\n2\n(T 0,\u21b5 0)\n{\n\n.\n\n}\n\np(x)\n\n,\n\na\n\n|\n\n|\n\n\u0000\n\np(x)\n\n\u0000\n\n{|\n\n\u21e4\nb\n\n|} \uf8ff\n\n, \u21b50\n\nt\n\n}\n\n(1\n\n\u0000\n\nT\n\n|\n\n\u21b5/8)\nT 0|\n\n|\n\n|\n\n\u21b5\n\n\u00b7\n\n\u21e3\n\n+ \u21b5\n8\n\n.\n\n\u2318\n\nPr\nT\nx\n\u21e0\n\nT 0\n\n {\nreturn\n\nend if\n\n8:\n\n9:\n10:\n11: else\n12:\n\nFind t\n2\np(x) < t + R0\n\nsatisfy\n\nR, R0 > 0 such that the sets T1 :=\n\nx\n\n{\n\n2\n\nT : p(x) > t\n\nR0\n\n}\n\n\u0000\n\nand T2 :=\n\nx\n\n{\n\n2\n\nT :\n\n}\n2 +\n\nT1|\n\n|\n\n2\n\nT2|\n\n|\n\nT\n\n|\n\n\uf8ff|\n\n2 (1\n\n\u0000\n\n\u21b5/100)2 and\n\nT\n\n|\n\n|\u0000\n\nmax(\n\n,\n\nT1|\n\n|\n\n)\n\nT2|\n\n|\n\n\u21b5\n\nT\n\n|\n\n|\n\n\u0000\n\n/4.\n\n\u21b5\n\u21b5i  \n13:\nreturn\n14:\n15: end if\n\n\u00b7\n{\n\n\u21b52/100)\nT\n(1\n\u00b7 |\n(T1,\u21b5 1), (T2,\u21b5 2)\n\n\u0000\n\n|\n.\n}\n\n/\n\nTi|\n\n|\n\n, for i = 1, 2.\n\nit is possible to construct a sparsity-induced \ufb01lter to prune outliers (see Steps 7 and 8). We note that\nin Step 7, the \ufb01rst term on the right-hand side is derived from Chernoff bound for degree-l Gaussian\npolynomials and the second term is due to concentration of empirical samples to Gaussian (see\nDe\ufb01nition 8), both of which are scaled by a factor 8/\u21b5 so that the number of the samples removed\nT , which\nfrom T is 8/\u21b5 times more than that of the good samples in the representative set SG \u21e2\nmeans most of the removed samples are outliers. We show by contradiction the existence of the\nthreshold t (see Lemma 26). In fact, had such threshold t not existed, the set T must be suf\ufb01ciently\nconcentrated such that the algorithm would have returned at Step 5. This essentially relies on our\nresult of the initial clustering of Algorithm 1, which guarantees that each subset T is bounded in a\nsmall L\n-ball and the function value of p on the \u21b5-good T does not change drastically (Lemma 13).\nWe then show that equipped with such threshold t, T 0 is a subset of T and it is \u21b50-good if T is \u21b5-good\n(Lemma 28).\n\n1\n\nsuch that T1 \\\n\nWhen there is no such short interval [a, b], the algorithm splits T into two overlapping subsets\nT2 is large enough to contain most of the samples in SG. This guarantees\nT1, T2}\n{\nthat most of the samples in SG (if T is \u21b5-good) are always contained in one subset and thus there\nalways exists an \u21b5-good subset of T . We show that an appropriate threshold t can also be found at\nStep 12 (Lemma 30), and at least one Ti is \u21b5i-good if T is \u21b5-good.\n\nAs a result, we have the following guarantees for Algorithm 4; see Appendix B for the full proof.\nTheorem 18 (BASICMF). Consider Algorithm 4. Denote by ANS its return. Suppose that T being\n\u21b5-good implies Var[p(G)]\n\u2327 , the following holds. ANS is either\n2. 1) If ANS = NO, then T is not \u21b5-good. 2) If\n\u201cNO\u201d, \u201cTBD\u201d, or a list of\n; and if additionally T is \u21b5-good,\nANS = TBD, then Var[p(T )]\n\n1. Then with probability 1\nm\ni=1 with m\n}\nl\n(` + log 1\n\u21b5 )\n\n\u0000\n\nE[p(G)]\n|\nT and\n\nthen\nTi \u21e2\n\n\u0000\ni\n\nE[p(T )]\n1\n1\n\u21b52 for all i\n\u21b52\n\u0000\ni \uf8ff\n\n|\uf8ff\n\n2\n\nm\ni=1, then\n\u0000\n}\n[m]; if additionally T is \u21b5-good, then at least one Ti is \u21b5i-good.\n\nlog(2 + log 1\n\u21b5 )\n\n(Ti,\u21b5 i)\n{\n\n\u0000\n\n\u00b7\n\n\u0000\n\n\uf8ff\nlog2(2 + log 1\n\u21b5 )\n\u00b7\n. 3) If ANS =\n\n\uf8ff\n(Ti,\u21b5 i)\n{\nO\n(` + log 1\n\u21b5 )\n\n\uf8ff\nO\n\nl\n2\n\nP\n\n9\n\n \n \n \n\fAlgorithm 5 HARMONICMF(T,\u21b5,\u2327, p )\nRequire: A multiset of samples T\n\n1: for l0 = 0, 1, . . . , l do\n2:\n\nLet B(l0) be an order-2l0 tensor with\n\n2\n\npolynomial p\n\nP(Rd, l, 2`k2`, 2`k2`) such that p(x) = hA(x\n\nRd, parameter \u21b5\n\n\u21e2\n\n2\n\n(0, 1/2], failure probability \u2327\nk2 = 1.\n\u00b5T ) and\n\nA\nk\n\n\u0000\n\n(0, 1), a\n\n2\n\nB(l0)\n\ni1,...,il0\n\n,j1,...,jl0\n\n=\n\nXkl0+1,...,kl\n\nAi1...,il0\n\n,kl0+1,...,kl Aj1...,jl0\n\n,kl0+1,...,kl .\n\n3:\n\nConsider B(l0) as a dl0\ncoordinates together. Apply eigenvalue decomposition on B(l0) to obtain B(l0) =\n\ndl0 symmetric matrix by grouping each of the i1, . . . , il0 and j1, . . . , jl0\nVi.\nMULTILINEARMF(T, Vi, l0,\u21b5,\u2327/ (ldl)) for every Vi. If ANSi = NO or a list of\nfor some i, then return ANSi. If ANSi = TBD, continue.\n\ni \u0000iVi \u2326\n\nP\n\n\u2326\n\nBASICMF(T,\u21b5,\u2327, 1\n\n\u0000 hA(x\n\n\u00b5T )) with \u0000 :=\n\n\u0000\n\n(1 + log 1\n\u21b5 )\n\nlog2(2 + log 1\n\u21b5 )\n\n\u00b7\n\nIf ANS = NO or a list of (Tj,\u21b5 j), return ANS. If ANS = TBD, still return \u201cNO\u201d.\n\nC1 \u00b7\n\n\u0000\n\nl\n\n2 .\n\n\u0000\n\n4: ANSi  \n(Tj,\u21b5 j)\n{\n5: end for\n6: ANS\n\n}\n\n\n\nThe following is the appendix section of the paper you are reviewing:\n Omitted Proofs from Section 2\n\nA.1 Proof of Proposition 9\n\nProof. Fix a subset \u2326\nmost l, denoted by\nP(Rd, l, \uf8ff,  ) =\n\nM\n[\u2326 [M\n\n\u21e2\n(\u2326, l). Let P(Rd,\n(\u2326,l) P(Rd,\n\nM\n(\u2326, l), \u2326).\n\n[d] with size  , and then \ufb01x a set of \uf8ff monomials on \u2326 with degree at\n(\u2326, l), \u2326) be the induced class of polynomials. Note that\n\nIt is easy to see that for any p\n(\u2326, l), \u2326), it can be represented by a linear combinations\nof the \uf8ff monomials. Thus, the VC dimension of this class equals \uf8ff + 1. Then, we note that there are\n(\u2326, l). Therefore,\n\nchoices of \u2326, and for any given \u2326, there are\n\nchoices of\n\nM\n\n2\n\nd\nj\n\nj=0\n\nM\n\nM\nP(Rd,\n\nthe total number of the subclass P(Rd,\nP\n\n\u0000\n\n\u0000\n\n\uf8ff\nj=0\n(\u2326, l), \u2326) is at most\n\n2dl\nj\n\nP\n\n\u0000\n\n\u0000\n\n2dl\nj\n\n\uf8ff\n\ned\n\n2edl\n\uf8ff\n\n\u00b7\n\nM\n\uf8ff\n\nd\nj\n\n\u00b7\n\n\u25c6\n\nj=0 \u2713\nX\n\nj=0 \u2713\nX\n\nThe concept class union argument states that for\nbounded by O(max\n\nV, log m + V log log m\n{\n\nV }\n\nHi. In our case, we have V = \uf8ff + 1 and m\n\nall\n\u00b7\nthat the VC dimension of P(Rd, l, \uf8ff,  ) is upper bounded by\n\u21e3\n\n\uf8ff\n\n\u21e3\n\n\u2318\n\ned\n\n\u25c6\n\n\u2713\n\n\u2713\n\n\u25c6\nis upper\ni=1Hi, the VC dimension of\n), where V is an upper bound on the VC dimension of\n\nH\n\nH\n\n=\n\n\u25c6\n\n[\n\nm\n\n2edl\n\uf8ff\n\n\uf8ff\n\n\u2318\n\n. By calculation, we can show\n\n\uf8ff\n\n.\n\n(A.1)\n\ned\n\n  log\n\n+ \uf8ff log\n\n2edl\n\uf8ff\n\n+ \uf8ff + 1\n\n\uf8ff\n\n(l\uf8ff +  ) log d =: d0.\n\n(A.2)\n\nRecall that the VC theory states that for any \u270f, \u2327\nSG| \u0000\nfor some absolute constant C > 0, the following holds with probability 1\n\n(0, 1), as long as\n\n2\n\n|\n\nC\n\n\u0000\n\n\u270f.\n\nd0\n\n\u270f2 log d0\n\n\u270f + 1\n\n\u270f2 log 1\n\n\u2327\n\n\u2327 :\n\u21e3\n\n\u2318\n\n(A.3)\n\nPr[p(G)\n\n0]\n\n\u0000\n\nx\n\nPr\nSG\n\u21e0\n\n\u0000\n\n[p(x)\n\n0]\n\n\u0000\n\n\uf8ff\n\np\n\nsup\n2P(Rd,l,\uf8ff, ) \u0000\n\u0000\n\u0000\n\u0000\n\nin (A.2),\n\n\u0000\n\u0000\n\u0000\n\u0000\n\nWith the expression of d0\n\uf8ff+ ) log d\nlog (l\n(l\n\n\uf8ff+ ) log d\n\n\u00b7\n\n\u00b7\n\n\u270f2\n\n\u270f\u2327\n\nit\n\nis not hard to see that we can set\n\nSG|\n\n|\n\n= C\n\n\u00b7\n\nfor some absolute constant C > 0 to ensure that the above holds.\n\nWhen l = 2`, \uf8ff = 4`2k4`,   = 2`k2`, and \u270f =\nSG|\n\nalgebraic calculation, it suf\ufb01ces to pick\nconstant C 0. This completes the proof.\n\n|\n\n100k2`\n\n= C 0\n\n\u21b53\nlog2`( `d\n\u00b7\nk8`\n`4\n\u00b7\n\u21b56\n\n\u00b7\n\n\u21b5\u2327 ) for some natural number `\nlog6`( `d\n\n\u21b5\u2327 ) for some suf\ufb01ciently large\n\n1, by\n\n\u0000\n\n\u00b7\n\nA.2 Proof of Lemma 11\n\nProof. By the standard tail bound of Gaussian distribution, for any x drawn from N (\u00b5, Id), it holds\nt2/2). By taking union bound over both\nthat for any given index i\nt2/2).\nindex i and sample x\n2d\nxi \u0000\nChoosing t =\n\n\u00b5i| \u0000\nxi \u0000\nt]\n2\n|\nSG, we have Pr[maxx\n/\u2327 ) completes the proof.\nSG|\n\n2 exp(\n\uf8ff\nSG maxi\n\n2\n2 log(d\n\n\u0000\n[d] |\n2\n\n\u00b5i| \u0000\n\n[d], Pr[\n\nSG|\n\nexp(\n\n\uf8ff\n\n\u0000\n\nt]\n\n2\n\n|\n\n|\n\nA.3 Proof of Lemma 12\n\np\n\nProof. Let SG be the subset of T containing the samples drawn i.i.d. from N (\u00b5, Id). Since\n2\u21b5\n\n, we know that SG is a representative set with probability at least 1\n\nSG|\n\u2327 in light of Prop. 9.\n\nT\n\n|\n\n=\n\n\u0000\n\n\u00b7 |\n\n|\n\nConsider Algorithm 1. If for all x, y\n\nT , we have\n\n2\n\nx\n\nk\n\n\u0000\n\ny\n\nk1 \uf8ff\n\n6\u0000,\n\n(A.4)\n\nthen the algorithm returns only one cluster and the lemma follows immediately.\n\nIf that is not the case, we \ufb01rst note that, with probability at least 1\nEq. (A.4) due to Lemma 11. Let us condition on this event occurs from now on.\n\n\u0000\n\n\u2327 all of the samples in SG satisfy\n\n16\n\n \n \n \n \n \n \n \n\f-balls of radius 2\u0000, of which each is centered at one\nAlgorithm 1 constructs a set of disjoint L\nsample in T and contains at least an \u21b5-fraction of samples in T . Therefore, the number of such\nballs is at most m =\n. Let B0i be the ball that has the same\nb\ncenter as Bi but with `\n[m], such that\n1\nB0i is \u21b5-good.\nTi = T\n\nB1, . . . , Bm}\n-radius of 6\u0000. In the following, we show that there exists i\n\n. Denote the set by\n\n1/\u21b5\n\n2\n\n1\n\n{\n\nc\n\ny\n\nSG, for which we know that\n\nConsider a sample x\n-ball\nk1 \uf8ff\n, all of the samples in SG will be contained in Bx. In addition,\nBx :=\ny\nk\n}\n. That is,\nthere must exist one Bi that intersects Bx, as otherwise Bx will be in the set\nBi. By construction, Bx must be containted in B0i. Therefore, all samples of SG\nz\n\nB1, . . . , Bm}\n\n\u0000. Then, for the L\n\n{\nT, z\n\nk1 \uf8ff\n\nRd :\n\nx\nk\n\n2\nx\n\n2\u0000\n\n\u0000\n\n\u0000\n\n2\n\n\u00b5\n\n1\n\n{\n\n2\n\n9\nmust be included in Ti and Ti is \u21b5-good.\n\n2\n\nBx \\\n\n\\\n\nA.4 Proof of Lemma 13\n\nProof. Recall that after running Algorithm 1, every subset Ti is contained in an L\n-ball of radius 6\u0000.\n1\nT ,\nBy Jensen\u2019s inequality and the convexity of the L\n6\u0000.\n\u00b5T k1 \uf8ff\nx\nk\nN (\u00b5T ,I)[p(x)] = 0 due to the de\ufb01nition of\n2\n2. Denote z = x\nk\n\nRecall that we assumed p(x) = hA(x\nharmonic polynomials. Thus, Varx\n\n1\n\u00b5T ). Thus Ex\n\u21e0\nA\nk\n\n-norm, we have for all x\n\nN (\u00b5T ,I)[p(x)] =\n\n\u00b5T . Then,\n\n\u0000\n\n\u0000\n\n\u0000\n\n2\n\n\u21e0\n\np(x)\n\n=\n\n|\n\n|\n\nca(j)\n\n[k2`]\nXj\n2\n\n\u0000\n\u0000\n\u0000\n\u0000\n\u0000\n\u0000\n\nHea(j) (z)\n1! \u0000\na(j)\n\u0000\n\u0000\n\u0000\n\u0000\n\u0000\n\n\u0000\n\u0000\n\n0\n\n\uf8ff v\nu\nu\nu\nt\n\n[k2`]\nXj\n2\n\nc2\na(j) 1\n\n0\n\n[k2`]\nXj\n2\n\nHea(j) (z)2\na(j)\n\n1! 1\nA\n\n.\n\n(A.5)\n\n@\nwhere a(j) is a d-dimensional multi-index for the j-th monomial, and ca(j) denotes its coef\ufb01cient.\nObserve that in the \ufb01rst step, p(x) is written as a linear combination of k2` Hermite polynomials,\nsince we are considering p\n\nP(Rd, l, k2`, 2`k2`). Note also that\n\nq\u0000\n\u0000\n\n[k2`] c2\n\n\u0000\n\u0000\n\n\u0000\n\u0000\n\n@\n\nA\n\n1.\n\na(j) =\n\nj\n\nA\nk\n\nk\n\n2\n2 \uf8ff\n\n2\n\n2\n\nTo bound the second factor on the right-hand side of (A.5), we use Mehler\u2019s formula, which shows\nthat for any u with\n\n< 1 and any natural number a,\n\nP\n\nu\n\n|\n\n|\n\nHe2\n\na(zi)ua\na!\n\n=\n\np1\n\nu\n\n1+u z2\ni ,\n\ne\n\n1\n\n\u0000\n\nu2\n\n1\n\na=0\nX\n\nSince each Hea(j) (z) has degree at most l, it can be decomposed as a product of at most l univariate\nHermite polynomials. Thus, we take such product and sum over j\n\n[k2`] to obtain\n\n2\n\na(j)\ni\n\n=0\n\nHea(j)\na(j)\n\ni\n\n\u21e3\n\nua(j)\n\ni\n\n(zi)2\n\n\u00b7\n\n1!\n\nk2`\n\n(1\n\n\u00b7\n\n\u0000\n\n\u2318\n\n\uf8ff\n\nu2)\u0000\n\nl\n2\n\n\u00b7\n\nu\n1+u k\n\ne\n\ntriml(z)\n\n2\n\n2 .\n\nk\n\n[k2`] Q\n\nXj\n2\n\nTo simplify the above expression, observe that\n\n\u0000\n\u0000\n\n\u0000\n\u0000\n\n2\ntriml(z)\n2 \uf8ff\nk\ntogether gives\n\nk\n\nl\n\nz\n\u00b7 k\n\nk\n\n2\n1 \uf8ff\n\n36l\u00002. Lastly, by algebra, (1\n\nQ\n\n\u0000\n\n=0 ua(j)\n\na(j)\ni\n\na(j)\n\ni = uk\nu2)\u0000\n\nl\n2\n\n\uf8ff\n\nul.\n\n\u0000\n\nIn addition,\n\nk1\nu2 l\n2 . Putting all pieces\n\ne\n\nHea(j) (z)2\na(j)\n\n1! \uf8ff\n\nl\n\nu\u0000\n\nk2`\n\n\u00b7\n\n\u00b7\n\n\u00b7\n\nu2 l\n2\n\ne\n\n36l\u00002 u\n1+u = k2`\n\ne\n\n2 + 36l\u00002 u\nu2 l\n1+u .\n\ne\n\nl\n\nu\u0000\n\n\u00b7\n\n\u00b7\n\n[k2`]\nXj\n2\n\u0000 ; this is possible as \u0000 > 1. Then the exponent u2l\n\n\u0000\n\u0000\n\n\u0000\n\u0000\n\nWe set u = 1\n37l.\nWithout loss of generality, we may assume that \u0000 > e37; in fact, we can always ensure this by setting\n\u0000 = (C0 + e37)\n\u21b5\u2327 where C0 is the constant given in Algorithm 1. Thus, it follows that\n\n2\u00002 + 36l\n\n1+u = l\n\n1+1/\u00002 \uf8ff\n\nlog `d\n\n`\n\n2 + 36l\u00002u\n\n\u00b7\n\n\u00b7\n\nq\n\nPlugging it into (A.5) completes the proof.\n\n\u0000\n\u0000\n\n\u0000\n\u0000\n\n[k2`]\nXj\n2\n\nHea(j) (z)2\na(j)\n\n1! \uf8ff\n\nk2`\n\n\u0000l\n\n\u00b7\n\n\u00b7\n\ne37l\n\n\uf8ff\n\nk2`\n\n\u00b7\n\n\u00002l.\n\n17\n\n6\n6\n\f\n\nBased on the section(s), please answer the following question with yes, no, or n/a and provide a brief justification for your answer.\nQuestion: If the authors include theoretical results, do the authors include complete proofs of all theoretical results?"}}}