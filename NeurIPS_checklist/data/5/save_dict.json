{"paper_index": 5, "title": "A permutation-free kernel two-sample test", "abstract": "", "introduction": "\n\n1\ni.i.d.\u223c P and Y =\nWe study the two-sample testing problem: given X = (X1, . . . , Xn)\n(Y1, . . . , Ym) i.i.d.\u223c Q, we test the null hypothesis H0 : P = Q against the alternative H1 : P \u0338= Q.\nThis is a nonparametric hypothesis problem with a composite null hypothesis and a composite\nalternative hypothesis. It finds applications in diverse areas such as testing microarray data, clinical\ndiagnosis, and database attribute matching (Gretton et al., 2012a).\n\nA popular approach to solving this problem is based on the kernel-MMD distance between the two\nempirical distributions (Gretton et al., 2006). Given a positive definite kernel k, the kernel-MMD\ndistance between two distributions P and Q on X , denoted by MMD(P, Q), is defined as\n\nMMD(P, Q) = \u2225\u00b5 \u2212 \u03bd\u2225k, where \u00b5(\u00b7) =\n\n\n\nX\n\nk(x, \u00b7)dP (x), and \u03bd(\u00b7) =\n\n\n\nX\n\nk(x, \u00b7)dQ(x).\n\n(1)\n\nAbove, \u00b5 and \u03bd are commonly called \u201ckernel mean maps\u201d, and denote the kernel mean embeddings\nof the distributions P and Q into the reproducing kernel Hilbert space (RKHS) associated with the\npositive-definite kernel k, and \u2225 \u00b7 \u2225k denotes the corresponding RKHS norm. Under mild conditions\non the positive definite kernel k (Sriperumbudur et al., 2011), MMD is a metric on the space of\n\n36th Conference on Neural Information Processing Systems (NeurIPS 2022).\n\n\fprobability distributions. Gretton et al. (2006) suggested using an empirical estimate of the squared\ndistance as the test statistic. In particular, given X and Y, define the test statistic\n\n2\n\nMMD\n\n:=\n\n1\nn(n \u2212 1)m(m \u2212 1)\n\n\n\n\n\nh(Xi, Xi\u2032, Yj, Yj\u2032),\n\n1\u2264i\u0338=i\u2032\u2264n\nwhere h(x, x\u2032, y, y\u2032) := k(x, x\u2032)\u2212k(x, y\u2032)\u2212k(y, x\u2032)+k(y, y\u2032). The above statistic has an alternative\nform that only takes quadratic time to calculate.\n\n1\u2264j\u0338=j\u2032\u2264m\n\nexceeds a suitable threshold \u03c4 \u2261 \u03c4 (\u03b1) that ensures the false\nThe MMD test rejects the null if\npositive rate is at most \u03b1. For \u201ccharacteristic kernels\u201d, this test is consistent against fixed alternatives,\nmeaning the power (the probability of rejecting the null when P \u0338= Q) increases to one as m, n \u2192 \u221e.\n\n2\n\nMMD\n\nThe difficulty in practically determining \u03c4 will play a key role in this paper. It is well known that\nwhen P = Q,\n\nis an instance of a \u201cdegenerate two-sample U-statistic\u201d, meaning that:\n\nMMD\n\n2\n\nUnder H0, EP [h(x, X \u2032, y, Y \u2032)] = EQ[h(X, x\u2032, Y, y\u2032)] = 0.\n\n(Above, x, y, x\u2032, y\u2032 are fixed, and the expectations are over X, Y, X \u2032, Y \u2032 i.i.d.\u223c P .) As a consequence,\nits (limiting) null distribution is unwieldy; it is an infinite sum of independent \u03c72 random variables\nweighted by the eigenvalues of an operator that depends on the kernel k and the underlying distribution\nP (see equation (10) in Appendix A). Since P is unknown, one cannot explicitly calculate \u03c4 .\n\nIn practice, a permutation-based approach is commonly used, where \u03c4 is set as the (1 \u2212 \u03b1)-quantile\nof the kernel-MMD statistic computed on B permuted versions of the aggregate data (X, Y). The\nresulting test has finite-sample validity, but its practical applicability is reduced due to the high\ncomputational complexity; if B = 200 permutations are used, the (permuted) test statistic must be\nrecomputed 201 times, rather than once (usually, B is chosen between 100 and 1000).\n\nDue to the high computational complexity of the permutation test, some permutation-free alternatives\nfor selecting \u03c4 have been proposed. However, as we discuss in Section 1.2, these alternatives are\neither too conservative in practice (using concentration inequalities), or heuristics with no theoretical\nguarantees (Pearson curves and Gamma approximation) or are only shown to be consistent in the\nsetting where the kernel k does not vary with n (spectral approximation). We later recap some\n\ncomputationally efficient alternatives to\n\nMMD\n\n2\n\n, but these have significantly lower power.\n\nAs far as we are aware, there exists no method in literature based on the kernel-MMD that is (i)\npermutation-free (does not require permutations), (ii) consistent against any fixed alternative, (iii)\nachieves minimax rate-optimality against local alternatives, and (iv) is correct for both the fixed\nkernel setting (k is fixed as m, n \u2192 \u221e) and the changing kernel setting (k changes as a function of\nm, n, for instance, by selecting the scale parameter of a Gaussian kernel in a data-driven manner).\n\n\u221a\n\nOur work delivers a novel and simple test satisfying all four desirable properties. We propose a\nnew variant of the kernel-MMD statistic that (after studentization) has a standard Gaussian limiting\ndistribution under the null in both the fixed and changing kernel settings, in low- and high-dimensional\n2 factor\nsettings. There is a computation-statistics tradeoff: our permutation-free test loses about a\nin power compared to the standard kernel-MMD test, but it is hundreds of times faster.\nRemark 1. Let P(X ) denote the set of all probability measures on the observation space X , where\nwe often use X = Rd for some d \u2265 1. For simplicity, in the above presentation, the distributions\nP, Q, kernel k and dimension d did not change with sample size, and this is the setting considered in\nthe majority of the literature. Later, we prove several of our results in a significantly more general\nsetting where P, Q, d, k can vary with n, m. Under the null, this provides a much more robust type-I\nerror control in high-dimensional settings, even with data-dependent kernels. Under the alternative,\nthis provides a more fine-grained power result. To elaborate on the latter, we assume that for every\nn, m, the pair (P, Q) = (Pn, Qn) \u2208 P (1)\nn : n, m \u2265 2}.\nThe class P (1)\nn is such that with increasing n and m, it contains pairs (P \u2032, Q\u2032) that are increasingly\ncloser in some distance measure \u03f1; thus the alternatives can approach the null and be equal in the\n\u03f1(P \u2032, Q\u2032) decreases with n, m, and such alternatives are\nlimit. That is, \u2206n,m := inf (P \u2032,Q\u2032)\u2208P (1)\ncalled local alternatives (as opposed to fixed alternatives). This framework allows us to characterize\nthe detection boundary of a test, that is, the smallest perturbation from the null (in terms of \u2206n,m)\nthat can be consistently detected by a test.\n\nn \u2282 P(X ) \u00d7 P(X ) for some sequence {P (1)\n\nn\n\n2\n\n\fPaper outline. We present an overview of our main results in Section 1.1 and discuss related\nwork in Section 1.2. In Section 2, we present the cross-MMD statistic and obtain its limiting null\ndistribution in Section 2.1. We demonstrate its consistency against fixed alternatives and minimax rate-\noptimality against smooth local alternatives in Section 2.2. Section 3 contains numerical experiments\nthat demonstrate our theoretical claims. All our proofs are in the supplement.\n\n1.1 Overview of our main results\n\nWe propose a variant of the quadratic time kernel-MMD statistic of (1) that relies on two key ideas:\n(i) sample splitting and (ii) studentization. In particular, we split the sample X of size n \u2265 2 into\nX1 and X2 of sizes n1 \u2265 1 and n2 \u2265 1, respectively (and Y of size m \u2265 2 into Y1 and Y2 of sizes\nm1 \u2265 1 and m2 \u2265 1), and define the two-sample cross kernel-MMD statistic xMMD\n\nas follows:\n\n2\n\nxMMD\n\n2\n\n:=\n\n1\nn1m1n2m2\n\nn1\n\nn2\n\nm1\n\nm2\n\ni=1\n\ni\u2032=1\n\nj=1\n\nj\u2032=1\n\nh(Xi, Xi\u2032, Yj, Yj\u2032).\n\n(2)\n\n2\n\n2\n\n:= xMMD\n\n/\u03c3, where \u03c3 is an empirical variance introduced in (4).\n\nOur final test statistic is \u00afxMMD\nOur first set of results show that quite generally, \u00afxMMD\nhas an N (0, 1) asymptotic null distribution.\nTheorem 4 obtains this result in the setting where both the kernel k and null distribution P are fixed.\nThis is then generalized to deal with changing kernels (for instance, Gaussian kernels with data-driven\nbandwidth choices) in Theorem 5. Finally, in Theorem 15 in Appendix A, we significantly expand\nthe scope of these results by also allowing the null distribution to change with n, and also weakening\nthe moment conditions required by Theorem 5.\n\n2\n\ny\nt\ni\ns\nn\ne\nd\ny\nt\ni\nl\ni\nb\na\nb\no\nr\nP\n\n0.4\n\n0.3\n\n0.2\n\n0.1\n\n0\n\nNull distribution of xMMD\n\nPower vs Sample-Size\n\nPower vs Computation\n\nd=10\nd=500\nN(0,1)\n\nr\ne\nw\no\nP\n\n1\n\n0.8\n\n0.6\n\n0.4\n\n0.2\n\n1\n\n0.8\n\n0.6\n\n0.4\n\n0.2\n\nr\ne\nw\no\nP\n\nMMD-perm\nxMMD\n\nMMD-perm\nxMMD\n\n\u22125\n\n0\n\n5\n\nStatistic Value\n\n200\n\n100\nSample-Size (n+m)\n\n300\n\n400\n\n10\u22123\n\n10\u22121\n10\u22122\nRunning time (seconds)\n\nFigure 1: The first figure shows the distribution of our proposed statistic \u00afxMMD\npredicted by Theo-\nrem 5 under the null for dimensions d = 10 and d = 100. The statistic is computed with Gaussian\nkernel (ksn (x, y) = exp(\u2212sn\u2225x \u2212 y\u22252\n2)) with scale parameter sn chosen by the median heuristic\nfor different choices of n, m and d, and with samples X and Y drawn from a multivariate Gaussian\ndistribution with identity covariance matrix. The second figure compares the power curves of the\ntwo-sample test using the \u00afxMMD\nstatistic with the kernel-MMD permutation test (with 200 per-\nmutations). The final figure plots the power vs computation time for the two tests. The size of the\nmarkers are proportional to the sample-size used in the test.\n\n2\n\n2\n\n2\n\nOur main methodological contribution is the \u201cxMMD test\u201d, denoted \u03a8, which rejects the null if\n\u00afxMMD\n\nexceeds z1\u2212\u03b1, which is the (1 \u2212 \u03b1)-quantile of N (0, 1). Formally,\nxMMD test: \u03a8(X, Y) = 1\n\n(3)\n\n.\n\n2\n\n\u00afxMMD\n\n\u2265z1\u2212\u03b1\n\nBy the previous results, \u03a8 has type-I error at most \u03b1, meaning that E[\u03a8(X, Y)] \u2264 \u03b1 under the null.\nWe next study the power of the xMMD test \u03a8 in Section 2.2. First, in the fixed alternative case, i.e.,\nwhen the distributions P \u0338= Q do not change with n, we show in Theorem 7, that the xMMD test\n\n3\n\n\fimplemented with any characteristic kernel is consistent under a bounded fourth moment condition.\nNext, we consider the more challenging case of local alternatives, i.e., when the distributions,\nPn \u0338= Qn, change with n. In Theorem 8, we first identify general sufficient conditions for the\nxMMD test to be uniformly consistent over a class of alternatives. Then, we specialize this to the\ncase when Pn and Qn admit densities pn and qn with \u2225pn \u2212 qn\u2225L2 \u2265 \u2206n for some \u2206n \u2192 0. We\nshow in Theorem 9, that the xMMD test with a Gaussian kernel ksn (x, y) = exp(\u2212sn\u2225x \u2212 y\u22252\n2),\nwith scale parameter sn increasing at an appropriate rate can consistently detect the local alternatives\n{\u2206n : n \u2265 1} decaying at the minimax rate.\n\nFinally, we note that while our primary focus in the paper is on the special case of kernel-MMD\nstatistic, the ideas involved in defining the xMMD statistic can be extended to the case of general\ntwo-sample U-statistics. We describe this in Appendix D.1, and obtain sufficient conditions for\nasymptotic Gaussian limit of the resulting statistic, possibly of independent interest.\n\n", "methods": "\n\nIn this section, we present our test statistic and investigate its limiting distribution. First note that\nthe squared kernel-MMD distance between two probability measures P and Q can be expressed\nas an inner product, namely \u27e8\u00b5 \u2212 \u03bd, \u00b5 \u2212 \u03bd\u27e9k. The usual kernel-MMD statistic is obtained by\nplugging the empirical kernel embeddings into this inner product expression and removing the\ndiagonal terms to make it unbiased. Our proposal instead considers pairs of empirical estimates\n((cid:98)\u00b51, (cid:98)\u00b52) and ((cid:98)\u03bd1, (cid:98)\u03bd2) constructed via sample splitting, and use the inner product between (cid:98)\u00b51 \u2212 (cid:98)\u03bd1\nand (cid:98)\u00b52 \u2212 (cid:98)\u03bd2 instead. This careful construction allows us to obtain a Gaussian limiting distribution\nafter studentization. To elaborate, recall from Section 1.1 that we partition X into X1 and X2,\nand similarly Y into Y1 and Y2. We then compute empirical kernel embeddings based on each\npartition, yielding (cid:98)\u00b51 := n\u22121\nj=1 k(Yj, \u00b7)\n(cid:80)m2\nand (cid:98)\u03bd2 := m\u22121\nj\u2032=1 k(Yj\u2032, \u00b7). Using these embeddings coupled with the kernel trick, the cross U-\nstatistic (2) can be written as x(cid:92)MMD\n= \u27e8(cid:98)\u00b51 \u2212 (cid:98)\u03bd1, (cid:98)\u00b52 \u2212 (cid:98)\u03bd2\u27e9k. To further motivate our test statistic,\ndenote UX,i := \u27e8k(Xi, \u00b7), (cid:98)\u00b52 \u2212 (cid:98)\u03bd2\u27e9k for i = 1, . . . , n1 and UY,j := \u27e8k(Yj, \u00b7), (cid:98)\u00b52 \u2212 (cid:98)\u03bd2\u27e9k for\nj = 1, . . . , m1. Then the cross U-statistic can be viewed as the difference between two sample means:\nx(cid:92)MMD\n(cid:80)m1\nj=1 UY,j. Since the summands are independent conditional on\nX2 and Y2, one may expect that x(cid:92)MMD\nis approximately Gaussian after studentization. Our results\nin Section 2.1 formalize this intuition under standard moment conditions, where it takes some care to\nremove the above conditioning, since we care about the unconditional distribution.\nLet us further denote the sample means of UX,i\u2019s and UY,j\u2019s by \u00afUX and \u00afUY , respectively, and define\n\ni\u2032=1 k(Xi\u2032, \u00b7), (cid:98)\u03bd1 := m\u22121\n\ni=1 k(Xi, \u00b7), (cid:98)\u00b52 := n\u22121\n\ni=1 UX,i \u2212 1\nm1\n\n= 1\nn1\n\n(cid:80)m1\n\n(cid:80)n1\n\n(cid:80)n2\n\n(cid:80)n1\n\n2\n\n1\n\n2\n\n2\n\n2\n\n2\n\n1\n\n(cid:98)\u03c32\nX :=\n\n1\nn1\n\nn1(cid:88)\n\ni=1\n\n(cid:0)UX,i \u2212 \u00afUX\n\n(cid:1)2\n\n, (cid:98)\u03c32\n\nY :=\n\n1\nm1\n\nm1(cid:88)\n\nj=1\n\n(cid:0)UY,j \u2212 \u00afUY\n\n(cid:1)2\n\nand (cid:98)\u03c32 :=\n\n1\nn1\n\n(cid:98)\u03c32\nX +\n\n1\nm1\n\n(cid:98)\u03c32\nY .\n\n(4)\n\nNow we have completed the description of our studentized cross U-statistic \u00afx(cid:92)MMD\n/(cid:98)\u03c3,\nand the resulting test \u03a8 in (3). The asymptotic validity of the xMMD test is guaranteed by Theorem 15\nthat establishes the asymptotic normality of \u00afx(cid:92)MMD\n\n= x(cid:92)MMD\n\nunder the null.\n\n2\n\n2\n\n2\n\n5\n\n\fFigure 2: The figures visually illustrate the main differences in computing the usual quadratic-time\nkernel-MMD statistic (left), block-MMD (center) statistic, and our new cross-MMD statistic. In\nparticular, the quadratic-time kernel-MMD statistic considers all pairwise kernel evaluations, with the\nexception of the diagonal terms. For block-MMD, we obtain the statistic by partitioning the data into\nseveral disjoint blocks; and then taking the average of the kernel-MMD statistic calculated over these\ndisjoint blocks. Finally, our cross-MMD statistic first splits the data into two disjoint parts (red and\nblack), and then uses the pairwise kernel evaluations with data from different splits. Interestingly, the\nobservation pairs included by our cross-MMD statistic are exactly complementary to those included\nby the block-MMD statistic.\n\n2\n\n2\n\n2\n\nRemark 2 (Computational Complexity). The overall cost of computing the statistic \u00afx(cid:92)MMD\nis\nO (cid:0)(n + m)2(cid:1), and in particular, both x(cid:92)MMD\nand (cid:98)\u03c3 have quadratic complexity. To see this, note\nthat x(cid:92)MMD\ncan be expanded into \u27e8(cid:98)\u00b51, (cid:98)\u00b52\u27e9k + \u27e8(cid:98)\u03bd1, (cid:98)\u03bd2\u27e9k \u2212 \u27e8(cid:98)\u00b51, (cid:98)\u03bd2\u27e9k \u2212 \u27e8(cid:98)\u03bd1, (cid:98)\u00b52\u27e9k. Each of these\nterms can be computed in O (cid:0)(n + m)2(cid:1). Similarly, each term in the summations defining (cid:98)\u03c32\nX and\nY also require O (cid:0)(n + m)2(cid:1) computation, implying that the (cid:98)\u03c3 also has O((n + m)2) complexity.\n(cid:98)\u03c32\nRemark 3. To simplify notation in what follows, we denote m as mn, where mn is some unknown\nnondecreasing sequence such that limn\u2192\u221e mn = \u221e. This still permits m, n to be separate quantities\ngrowing to infinity at potentially different rates, but it allows us to index the sequence of problems\nwith the single index n (rather than m, n). We will use kn, dn, Xn, Pn and Qn to indicate that\nquantities could (but do not have to) change as n increases, and drop the subscript when they are fixed.\nFurthermore, unless explicitly stated, we will focus on the balanced splitting scheme, i.e., n1 = \u230an/2\u230b\nand m1 = \u230am/2\u230b in what follows, because we currently see no apriori reason to split asymmetrically.\n\n2.1 Gaussian limiting distribution under the null hypothesis\nAs shown in Figure 1, the empirical distribution of \u00afx(cid:92)MMD\nresembles a standard normal distribution\nfor various choices of m, n and dimension d under the null. In this section, we formally prove this\nstatement. Recalling the mean embedding \u00b5 from (1), define\n\n2\n\nTheorem 4. Suppose that k and P do not change with n.\nX, X \u2032 i.i.d.\u223c P , then \u00afx(cid:92)MMD\n\nd\u2212\u2192 N (0, 1).\n\n2\n\n\u00afk(x, y) := \u27e8k(x, \u00b7) \u2212 \u00b5, k(y, \u00b7) \u2212 \u00b5\u27e9k.\n\n(5)\nIf 0 < EP [\u00afk(X, X \u2032)4] < \u221e for\n\nWe next present a more general result that implies Theorem 4.\nTheorem 5. Suppose P is fixed, but the kernel kn changes with n. If\n\nlim\nn\u2192\u221e\n\nEP [\u00afkn(X1, X2)4]\nEP [\u00afkn(X1, X2)2]2\n\n(cid:18) 1\nn\n\n+\n\n1\nmn\n\n(cid:19)\n\n= 0,\n\nand\n\nlim\nn\u2192\u221e\n\n\u03bb2\n1,n\n(cid:80)\u221e\nl=1 \u03bb2\nl,n\n\nexists,\n\n(6)\n\nwhere (\u03bbl,n)\u221e\n\nl=1 denote the eigenvalues of \u00afk introduced in (16), then we have \u00afx(cid:92)MMD\n\n2\n\nd\u2212\u2192 N (0, 1).\n\nIt is easy to check that condition (6) is trivially satisfied if the kernels {kn : n \u2265 1} are uniformly\nbounded by some constant; prominent examples are the Gaussian or Laplace kernel with a sample size\n\n6\n\nKernel-MMDBlock-MMDCross-MMD\f\u221a\n\n(cid:113)\n\n1,n]/\n\n(cid:113)(cid:80)\n\nn (cid:80)n\n\nEPn [W 2\n\ni=1 Wi,n/\n\ni.i.d.\u223c Pn. Define Vn =\n\ndependent bandwidth. Thus, the above condition really exists to handle unbounded kernels and heavy-\ntailed distributions. To motivate this requirement, we recall Bentkus and G\u00f6tze (1996) (see Fact 11\nin Appendix A) who proved a studentized CLT for i.i.d. random variables in a triangular array\ni(Wi,n \u2212 \u00afWn)2 where\nsetup: W1,n, W2,n, . . . , Wn,n\n\u00afWn = ((cid:80)\ni Wi,n)/n. They showed that a sufficient condition for the asymptotic normality of Vn is\nthat limn\u2192\u221e EPn[W 3\n1,n]3n = 0. (This last condition is trivially true if Pn does not\nchange with n, meaning that the triangular array setup is irrelevant and W1,n can be replaced by W1.)\nOur requirement is slightly stronger: condition (6) with \u00afkn(X1, X2) replaced by W1,n implies the\nprevious condition of Bentkus and G\u00f6tze (1996) (details in Remark 12 in Appendix A). We need\nthis stronger condition, because the terms in the definition of x(cid:92)MMD\nare not i.i.d. (indeed, not even\nindependent), and thus we cannot directly apply the result of Bentkus and G\u00f6tze (1996). Instead,\nwe take a different route by first conditioning on the second half of data (X2, Y2), then showing the\nconditional asymptotic normality of the standardized x(cid:92)MMD\n(i.e., divided by conditional standard\ndeviation instead of empirical), and finally showing that the ratio of conditional and empirical standard\ndeviations converge in probability to 1 (see Appendix B).\n\n2\n\n2\n\nFinally, we note that the result of Theorem 5 can be further generalized in several ways: (i) instead of\na fixed P and changing kn, we can consider a sequence of pairs {(Pn, kn) : n \u2265 1} changing with\nn, (ii) we can let Pn \u2208 P (0)\nn , for a class of distributions changing with n, and obtain the Gaussian\nlimit uniformly over all elements of P (0)\nn , and finally, (iii) the moment requirements on \u00afkn stated in\ncondition in (6) can also be slightly weakened. We state and prove this significantly more general\nversion of Theorem 5 in Appendix B.\nRemark 6. In the statement of the two theorems of this section, the splits (X1, Y1) and (X2, Y2)\nare assumed to be drawn i.i.d. from the same distribution P . However, a closer look at the proof\nof Theorem 5 indicates that the conclusions of the above two theorems hold even when the two\nsplits are independent and drawn i.i.d. from possibly different distributions; that is (X1, Y1) and\n(X2, Y2) are independent of each other and drawn i.i.d. from distributions P1 and P2 respectively,\n2\nwith P1 \u0338= P2. In particular, under this more general condition, the asymptotic normality of \u00afx(cid:92)MMD\nstill holds, and the resulting test \u03a8 still controls the type-1 error at the desired level. This may be\nuseful for two-sample testing in settings where the entire set of data is not i.i.d., but two different\nparts of the data were collected in two different situations. The usual MMD can also handle such\nscenarios by using a subset of permutations that do not exchange the data across the two situations.\n\n2.2 Consistency against fixed and local alternatives\n\nHere, we show that the xMMD test \u03a8 introduced in (3) is consistent against a fixed alternative and\nalso has minimax rate-optimal power against smooth local alternatives separated in L2 norm.\nWe first show that analogous to Theorem 4, xMMD is consistent against fixed alternatives.\nTheorem 7. Suppose P, Q, k do not change with n, and P \u0338= Q. If k is a characteristic kernel\nsatisfying 0 < EP [\u00afk(X1, X2)4] < \u221e, and 0 < EQ[\u00afk(Y1, Y2)4] < \u221e, then the xMMD test is\nconsistent, meaning it has asymptotic power 1.\n\nThe moment conditions required above are mild, and are satisfied trivially, for instance, by bounded\nkernels such as the Gaussian kernel. The \u201ccharacteristic\u201d condition is also needed for the consistency\nof the usual MMD test (Gretton et al., 2012a), and is also satisfied by the Gaussian kernel.\n\nRecalling Remark 1, we next consider the more challenging setting where dn, kn can change with\nn, and (Pn, Qn) can vary within a class P (1)\nn \u2282 P(Xn) \u00d7 P(Xn) that can also change with n. We\npresent a sufficient condition under which the xMMD test \u03a8 is consistent uniformly over P (1)\nn . Define\n\u03b3n := MMD(Pn, Qn), which is assumed nonzero for each n but could approach zero in the limit.\nTheorem 8. Let {\u03b4n : n \u2265 2} denote any positive sequence converging to zero. If\n\nlim\nn\u2192\u221e\n\nsup\n(Pn,Qn)\u2208P (1)\n\nn\n\nEPn,Qn [(cid:98)\u03c32]\n\u03b4n\u03b34\nn\n\n+\n\n2\n\nVPn,Qn(x(cid:92)MMD\n\u03b34\nn\n\n)\n\n= 0, where V denotes variance,\n\n(7)\n\n7\n\n\fthen limn\u2192\u221e sup(Pn,Qn)\u2208P (1)\n\nn\n\nEPn,Qn [1 \u2212 \u03a8(X, Y)] = 0, meaning the xMMD test is consistent.\n\nNote that while any sequence {\u03b4n} converging to zero suffices for the general statement above, the\ncondition (7) is easiest to satisfy for slowly decaying \u03b4n, such as \u03b4n = 1/ log log n for instance.\nThe sufficient conditions for consistency of \u03a8 stated in terms of (cid:98)\u03c3 and x(cid:92)MMD\nin (7) can also be\ntranslated into equivalent conditions on the kernel function kn, similar to (6), and we present the\ndetails in Appendix C. Importantly, if Pn, Qn, dn are fixed and kn is bounded, then both E[(cid:98)\u03c32] and\nV(x(cid:92)MMD\n) are O(1/n), and \u03b3n is a constant, so the condition is trivially satisfied, and in fact the\nabove condition is even weaker than the fourth-moment condition of the previous theorem.\n\n2\n\n2\n\n2.3 Minimax rate optimality against smooth local alternatives\n\nWe now apply the general result of Theorem 8 to the case where the distributions Pn and Qn admit\nLebesgue densities pn and qn that lie in the order \u03b2 Sobolev ball for some \u03b2 > 0, defined as\nW \u03b2,2(M ) := {f : X \u2192 R | f is a.s. continuous, and (cid:82) (1 + \u03c92)\u03b2/2\u2225F(f )(\u03c9)\u22252dw < M < \u221e}.\nFormally, we define the null and alternative class of distributions as follows:\n\nand\n\nP (0)\nP (1)\n\nn , while under H1, we assume that (Pn, Qn) \u2208 P (1)\nn .\n\nn = {P with density p : p \u2208 W \u03b2,2(M )},\nn = {(P, Q) with densities p, q \u2208 W \u03b2,2(M ) : \u2225p \u2212 q\u2225L2 \u2265 \u2206n},\nfor some sequence \u2206n decaying to zero. In particular, we assume that under H0, Pn = Qn and\nPn \u2208 P (0)\nOur next result shows that for suitably chosen scale parameter, the xMMD test \u03a8 with the Gaussian\nkernel is minimax rate-optimal for the above class of local alternatives. For simplicity, we state\nthis result with n = m, noting that the result easily extends to the case when there exist constants\n0 < c \u2264 C, such that c \u2264 n/m \u2264 C.\nTheorem 9. Consider the case when n = m, and let {\u2206n : n \u2265 1} be a sequence such that\nlimn\u2192\u221e \u2206nn2\u03b2/(d+4\u03b2) = \u221e. On applying the xMMD test \u03a8 with the Gaussian kernel ksn(x, y) =\nexp(\u2212sn\u2225x \u2212 y\u22252\n\n2), if we choose the scale as sn \u224d n4/(d+4\u03b2), then we have\n\nlim\nn\u2192\u221e\n\nsup\nPn\u2208P (0)\n\nn\n\nEPn[\u03a8(X, Y)] \u2264 \u03b1 and\n\nlim\nn\u2192\u221e\n\ninf\n(Pn,Qn)\u2208P (1)\n\nn\n\nEPn,Qn[\u03a8(X, Y)] = 1.\n\n(8)\n\nThe proof of this statement is in Appendix C, and it follows by verifying that the conditions required\nby Theorem 8 are satisfied for the above choices of \u2206n and sn.\nRemark 10. Li and Yuan (2019, Theorem 5 (ii)) showed a converse of the above statement: if\nlimn\u2192\u221e \u2206nn2\u03b2/(d+4\u03b2) < \u221e, then there exists an \u03b1 \u2208 (0, 1) such that any asymptotically level \u03b1 test\n(cid:101)\u03a8 must have limn\u2192\u221e inf (P,Q)\u2208P(\u2206n) EP,Q[ (cid:101)\u03a8(X, Y)] < 1. Hence, the sequence of {\u2206n : n \u2265 1}\nused in Theorem 9 represents the smallest L2-deviations that can be detected by any test, and (8)\nshows that our xMMD test \u03a8 can detect such changes, establishing its minimax rate-optimality.\n\n", "experiments": "\n\n2\n\nWe now present experimental validation of the theoretical claims of the previous section. In particular,\nour experiments demonstrate that (i) the limiting null distribution of \u00afx(cid:92)MMD\nis N (0, 1) under a\nwide range of choices of dimension d, sample sizes n, m and the kernel k, and (ii) the power of our\nxMMD test is competitive with the kernel-MMD permutation test. We now describe the experiments\nin more detail. Additional experimental results are reported in Appendix E.\nLimiting null distribution of \u00afx(cid:92)MMD\nhas a\nlimiting normal distribution under some mild assumptions. We empirically test this result when X\nand Y are drawn from N (0, Id) with 0 denoting the all-zeros vector in Rd, and in particular, study\nthe effects of (i) dimension: d = 10 versus d = 500, (ii) skewness of the samples: n/m = 1 versus\nn/m = 0.1, and (iii) choice of kernel: Gaussian versus Quadratic, both with data-dependent scale\nparameters using median heuristic.\n\n. We showed in Theorem 15 that the statistic \u00afx(cid:92)MMD\n\n2\n\n2\n\n8\n\n\fAs shown in the first row of Figure 3, the distribution of \u00afx(cid:92)MMD\nis robust to all these effects, and is\nclose to N (0, 1) in all cases. In contrast, the distribution of the kernel-MMD statistic scaled by its\nempirical standard deviation (obtained using 200 bootstrap samples) in the bottom row of Figure 3\nshows strong changes with these parameters. We present additional figures and details of the\nimplementation in Appendix E.\n\n2\n\nxMMD (n/m = 0.1)\n\nd = 10\n\nd = 500\nN (0, 1)\n\ny\nt\ni\ns\nn\ne\nd\n\ny\nt\ni\nl\ni\nb\na\nb\no\nr\nP\n\n2\n\n(cid:92)MMD\n\nFigure 3: The first two columns show the null distribution of the \u00afx(cid:92)MMD\nthe\nstatistic scaled by its empirical standard deviation (bottom row) using the Gaussian\nkernel with scale-parameter chosen using the median heuristic. The last two columns show the null\ndistribution for the two statistics using the Quadratic kernel with scale parameter chosen using the\n\nstatistic (top row) and\n\n2\n\nmedian heuristic. The figures demonstrate that the null distribution of\nwith dimension (d), the ratio n/m and the choice of the kernel, unlike our proposed statistic.\n\nchanges significantly\n\n2\n\n(cid:92)MMD\n\nEvaluation of the power of \u03a8. For d \u2265 1 and j \u2264 d, let a\u03f5,j denote the element of Rd with\nfirst j coordinates equal to \u03f5, and others equal to 0. We consider the two-sample testing problem\nwith P = N (0, Id) Q = N (a\u03f5,j, Id) for different choices of \u03f5 and d and j. We compare the\nperformance of our proposed test \u03a8 with the kernel-MMD permutation test, implemented with\nB = 200 permutations, and plot the power-curves (using 200 trials) in Figure 4. We also propose\na heuristic for predicting the power of the permutation test (denoted by \u03c1perm) using the power of\n\u03a8 (denoted by \u03c1\u03a8) as follows (with \u03a6 denoting the standard normal cdf, and z\u03b1 its \u03b1-quantile):\n\u221a\n\n(cid:16)\n\n(cid:98)\u03c1perm = \u03a6\n\nz\u03b1 +\n\n2 (cid:0)\u03a6\u22121(\u03c1\u03a8) \u2212 z\u03b1\n\n(cid:1)(cid:17)\n\n.\n\n(9)\n\n\u221a\n\nThis heuristic is motivated by the power expressions derived by Kim and Ramdas (2020) for the\nproblems of one-sample Gaussian mean and covariance testing (we discuss this further in Appendix E).\n2 in the above expression quantifies the price to pay for sample-splitting. As shown\nThe term\nin Figure 4, this heuristic gives us an accurate estimate of the power of the kernel-MMD permutation\ntest, without incurring the computational burden.\n\nWe now use ROC curves to compare the tradeoff between type-I and type-II errors for the usual MMD,\nlinear and block-MMDs with our \u00afx(cid:92)MMD\n. We use the same distributions P = N (0, Id) and Q =\nN (a\u03f5,j, Id) as before, and plot results for (d, j, \u03f5) \u2208 {(10, 5, 0.2), (100, 20, 0.15), (500, 100, 0.1)}\nin Figure 5. Due to sample splitting, the tradeoff achieved by our proposed statistic is slightly\n\n2\n\nworse than that of\nkernel-MMD statistics. More details about the implementation are presented in Appendix E.\n\n, but significantly better than other computationally efficient variants of\n\n2\n\n(cid:92)MMD\n\n", "conclusion": "\n\nWe proposed a variant of the kernel-MMD statistic, called cross-MMD, based on the ideas of sample-\nsplitting and studentization, and showed that it has a standard normal limiting null distribution. Using\n\n9\n\nxMMD(n/m=1)xMMD(n/m=10)xMMD(n/m=1)ProbabilitydensityMMD(n/m=0.1)MMD(n/m=1)MMD(n/m=10)MMD(n/m=1)\fFigure 4: Curves showing the variation in power versus sample-size for the xMMD test and the\nkernel-MMD permutation test. X are drawn from N (0, Id) i.i.d. and Y is drawn from N (a\u03f5,j, Id)\nwhere a\u03f5,j is obtained by perturbing the first j \u2264 d coordinates of 0 by \u03f5, the kernel used is the\nGaussian kernel with scale parameter chosen via the median heuristic. The dashed curve shows the\npredicted power of the kernel-MMD permutation test using the heuristic defined in (9).\n\nFigure 5: ROC curves highlighting the trade-off between type-I and type-II errors achieved by the\nMMD, cross-MMD, batch-MMD with batch sizes n1/2 and n1/3, and linear-MMD statistics. In all\nthe figures, we use n = m = 200.\n\nthis key result, we introduced a permutation-free (and hence computationally efficient) MMD test\nfor the two-sample problem. Experiments indicate that the power achieved by our test is within a\n\u221a\n2-factor of the power of the kernel-MMD permutation test (that requires recomputing the statistic\nhundreds of times). In other words, our results achieve the following favorable tradeoff: we get a\nsignificant reduction in computation at the price of a small reduction in power.\n\nSejdinovic et al. (2013) establish in some generality that distance-based two-sample tests (like the\nenergy distance (Sz\u00e9kely and Rizzo, 2013)) can be viewed as kernel-MMD tests with a particular\nchoice of kernel k. Hence our results are broadly applicable to distance-based statistics as well.\nSince two-sample testing and independence testing can be reduced to each other, it is an interesting\ndirection for future work to see if the ideas developed in our paper can be used for designing\npermutation-free versions of kernel-based independence tests like HSIC (Gretton et al., 2007) or\ndistance covariance (Sz\u00e9kely and Rizzo, 2009; Lyons, 2013). Our techniques seem to rely on the\nspecific structure of two-sample U-statistics of degree 2. Extending these to more general U-statistics\nof higher degrees is another important question for future work. A final question is to figure out\nwhether it is possible to achieve minimax optimal power using a sub-quadratic time test statistic. One\npotential approach would be to work with a kernel approximated by random Fourier features (Rahimi\nand Recht, 2007; Zhao and Meng, 2015). Depending on the number of random features, our test\nstatistic can be computed in sub-quadratic time and it would be interesting to see whether the resulting\ntest can still be minimax optimal in power. We leave this important question for future work.\n\n10\n\n1002003000.20.40.60.81Sample-Size(n+m)Power(d,j,,\u03f5)=(10,5,0.3)MMD-permxMMDpredicted1002003000.20.40.60.81Sample-Size(n+m)Power(d,j,,\u03f5)=(50,5,0.4)1002003000.20.40.60.81Sample-Size(n+m)Power(d,j,,\u03f5)=(100,5,0.5)00.5100.51FalsePositiveRateTruePositiveRate(d,\u03f5,j)=(10,0.15,5)MMDxMMDB-MMD(n1/2)B-MMD(n1/3)L-MMD00.5100.51FalsePositiveRate(d,\u03f5,j)=(100,0.15,10)00.5100.51FalsePositiveRate(d,\u03f5,j)=(500,0.15,50)\f", "appendix": "\n\nA Background\n\nTo keep the paper self-contained, we collect the relevant definitions and theorems from prior work\nthat are used in proving the main results of our paper.\n\nCentral Limit Theorems. We first recall a central limit theorem for studentized statistics by Ben-\ntkus and G\u00f6tze (1996).\nFact 11 (Berry Esseen CLT). For some i.i.d. \u223c P random variables W1, . . . , Wn, define the statistic\n\u00afxMMD\ni ] < \u221e, then there exists a\n\ni=1(Wi\u2212 \u00afWn)2 . If EP [Wi] = 0 and 0 < EP [W 2\n\ni=1 Wi\n\nn\n\n=\n\n\n\n2\n\nn\nuniversal constant C < \u221e such that\n\n1\nn\n\n|PP (T \u2264 x) \u2212 \u03a6(x)| \u2264 C\n\nsup\nx\u2208R\n\nEP [|W 3\n1 |]\n1 ]3/2\n\nEP [W 2\n\n\u221a\n\n.\n\nn\n\nRemark 12. Note that by Cauchy\u2013Schwarz inequality, we have\n\nEP [W 3\n\n1 ] = EP [W 2\n\n1 \u00d7 W1] \u2264\n\n\n\nEP [W 4\n\n1 ]EP [W 2\n1 ].\n\nThis implies the following\n\nEP [W 3\n1 ]\nEP [W 2\n1 ]3/2\n\n\u2264\n\n EP [W 4\n1 ]\nEP [W 2\n1 ]2\n\n1/2\n\n.\n\n14\n\n\fThus a sufficient condition for applying Fact 11 to show the convergence in distribution to N (0, 1)\nfor a triangular sequence {Wi,n : 1 \u2264 i \u2264 n, n \u2265 1}, with {Wi,n : 1 \u2264 i \u2264 n} drawn i.i.d. from\nsome distribution Pn is\n\nlim\nn\u2192\u221e\n\nEPn [W 4\nEPn [nW 2\n\n1,n]\n1,n]2 = 0.\n\nWe next recall a consequence of Lindeberg\u2019s Central Limit Theorem (CLT), as stated in (Lehmann\nand Romano, 2006, Lemma 11.3.3).\nFact 13. Let Z1, Z2, . . . be a sequence of i.i.d. zero-mean random variables with finite variance \u03c32.\nLet c1, c2, . . . be a real-valued sequence, satisfying:\n\nThen, we have\n\nlim\nn\u2192\u221e\n\nmax\n1\u2264i\u2264n\n\nc2\ni\nj=1 c2\nj\n\nn\n\n= 0.\n\nn\nn\n\ni=1 ciYi\nj=1 c2\nj\n\nd\u2212\u2192 N (0, \u03c32).\n\nNull distribution of MMD statistic. Assuming that (n, m) are such that n/m \u2192 c for some\nc > 0, and let {ul : l \u2265 1} and {vl : l \u2265 1} denote two independent sequences of i.i.d. N (0, 1)\nrandom variables. Furthermore, let {\u03bbl : l \u2265 1} denote the eigenvalues of the kernel operator\nf (\u00b7) \u2192 \nX f (x)k(\u00b7, x)dP (x). Using techniques from the theory of U-statistics, Gretton et al.\n(2012a) showed that\n\n(n + m)MMD\n\n2\n\nd\u2212\u2192\n\n\u221e\n\n\nl=1\n\n\u03bbl\n\n c1/2ul \u2212 vl\n\n2\n\n1 + c\n\n\u2212\n\n(1 + c)2\nc\n\n\n\n.\n\n(10)\n\nMMD is an infinite combination of chi-squared random\n(10) shows that the null distribution of\nvariables, weighted by the eigenvalues of the kernel operator. Due to this form, the null distribution\nhas a complex dependence on the kernel and the null distribution P .\n\nGaussian kernel calculations. Next, we recall some facts derived byLi and Yuan (2019), about the\nthe Gaussian kernel ks(x, y) := exp \u2212s\u2225x \u2212 y\u22252\n, and probability distributions that admit density\n2\nfunctions lying in the Sobolev ball \u2208 W \u03b2,2(M ).\nFact 14. Consider a Gaussian kernel that varies with sample size, kn(x, y) = exp(\u2212sn\u2225x \u2212 y\u22252\n2).\nLet \u00afkn be as defined in (15), X = Rd and X1, X2, X3, X4 \u223c Pn i.i.d., Y1, Y2 \u223c Qn, where Pn and\nQn have densities pn and qn in W \u03b2,2(M ) and \u2225pn \u2212 qn\u2225L2 = \u2206n, for some real valued sequence\n{\u2206n : n \u2265 1} converging to 0. Then, we have the following:\n\nand EQn[\u00afk2\n\nn(Y1, Y2)] \u224d s\u2212d/2\n\nn(X1, X2)] \u224d s\u2212d/2,\nn(X1, X2)] \u2272 s\u2212d/2,\nn(X1, X2)\u00afk2\n\nEPn [\u00afk2\nEPn [\u00afk4\nEPn [\u00afk2\nn(X1, X3)] \u2272 s\u22123d/4,\n\u03b3n(Pn, Qn) = MMD(Pn, Qn) \u2273 s\u2212d/2\n\nn \u2206n.\n\n(11)\n\n(12)\n\n(13)\n\n(14)\n\nAdditional Notation. We use U = oP (un) and U = OP (un) to denote that U/un\n\u2212\u2192 0 and that\nU/un is stochastically bounded. For real valued sequences, we use an \u2272 bn if there exists a constant\nC such that an \u2264 Cbn for all n. We use an \u224d bn if an \u2272 bn and bn \u2272 an.\n\np\n\nB Gaussian limiting distribution of \u00afxMMD\n\n2\n\nIn this section, we present the results about the limiting null distribution of the statistic \u00afxMMD\ngeneral outline of the section is as follows:\n\n2\n\n. The\n\n15\n\n\f\u2022 In Appendix B.1, we state the most general version of the result on the limiting distribution\n\n(Theorem 15), that we alluded to in Section 2.1. We then prove this result\n\n2\n\nof \u00afxMMD\nin Appendix B.1.1.\n\n\u2022 In Appendix B.3, we show how the general result can be used to prove Theorem 5, where\n\nthe kernel is allowed to change with n while the distribution P is fixed.\n\n\u2022 Finally, in Appendix B.3, we show how Theorem 5 can be used to conclude the result for\n\nthe case when both the kernel k and null distribution P are fixed with n.\n\nB.1 Statement of the general result (both kn and Pn changing with n)\n\nAs stated in Remark 3, we assume that m \u2261 mn is some non-decreasing function of n. We consider\na sequence of positive-definite kernels {kn : n \u2265 2}, and probability distributions {Pn : n \u2265 1, 2},\nand define\n\n\u00afk(x, y) \u2261 \u00afkn(x, y) = \u27e8kn(x, \u00b7) \u2212 \u00b5Pn , kn(y, \u00b7) \u2212 \u00b5Pn \u27e9k,\nwhere \u00b5Pn denotes the embedding of the distribution Pn into the RKHS associated with the kernel\nkn. For any fixed values of n, we use {(\u03bbl,n, \u03c6l,n) : l \u2265 1} to denote the eigenvalue-eigenfunction\nsequence associated with the integral operator g \u2192  \u00afk(\u00b7, x)g(x)dPn(x). If \u00afk happens to be square-\nintegrable (in addition to being symmetric), it has the following representation:\n\n(15)\n\n\u00afkn(x, y) =\n\n\u221e\n\n\nl=1\n\n\u03bbl,n\u03c6l,n(x)\u03c6l,n(y).\n\n(16)\n\n2\n\nWe now state the assumption required to prove the limiting normal distribution of the statistic\n\u00afxMMD\n. As we will see in Appendix B.2, in the special case of fixed P , the condition in (17) is a\nweaker version of that used in Theorem 5.\nAssumption 1. For \u00afk introduced in (5), {(\u03bbl,n, \u03c6l,n) : l \u2265 1} introduced in (16) and for a sequence\n{Pn : n \u2265 1}, we assume that\n\nEPn [\u00afk4(X1, X2)](n\u22121 + m\u22121\n\nn ) + EPn [\u00afk2(X1, X3)\u00afk2(X2, X3)]\n\n\n\n\nEPn [\u00afk2(X1, X2)]2\n\n1\nn\u22121+m\u22121\n\nn\n\n\u2192 0,\n\nand\n\n(17)\n\nlim\nn\u2192\u221e\n\n\u03bb2\n1,n\n\u221e\nl=1 \u03bb2\nl,n\n\nexists.\n\nWe now state the main result of this section.\nTheorem 15. Suppose the sequence {mn : n \u2265 1} satisfies limn\u2192\u221e n/mn exists and is non-\nzero. Let {kn : n \u2265 1} be a sequence of positive definite kernels, and let P (0)\nn denote a family of\ndistributions such that, for every n \u2265 1 and Pn \u2208 P (0)\nn , Assumption 1 is satisfied by the pair (\u00afkn, Pn)\nwith \u00afkn defined in (15). Then, we have that\n\nlim\nn\u2192\u221e\n\nsup\nPn\u2208P (0)\n\nn\n\nsup\nx\u2208R\n\n|PPn(\u00afxMMD\n\n2\n\n\u2264 x) \u2212 \u03a6(x)| = 0.\n\nWe now present the proof of this result.\n\nB.1.1 Proof of the general result with changing kn and Pn\nTo simplify the notation, we will drop the subscripts from kn, \u00afkn, Pn, \u03bbl,n,m and \u03c6l,n,m in this proof\noutline. Furthermore, note that as mentioned in Remark 3, we assume that n1 = n/2 and n1 = m/2.\n\nFor any x \u2208 X , introduce the term k(x, \u00b7) to denote k(x, \u00b7) \u2212 \u00b5. Next, we define the following terms\n\nSX = \u27e8\u00b51 \u2212 \u00b5,\n\n:=g2\n\n\n(\u00b52 \u2212 \u00b5) \u2212 (\u03bd2 \u2212 \u00b5)\u27e9k,\n\n\n\nand SY = \u27e8\u03bd1 \u2212 \u00b5, g2\u27e9k,\n\n16\n\n\fand note that we can write xMMD\n= \u00afUX \u2212 \u00afUY = SX \u2212 SY (SX differs from \u00afUX due to the extra\n\u00b5 term in the first argument of the inner product). Recall that we use \u00b5 and \u03bd to denote the kernel\nembeddings of the distributions P and Q.\n\n2\n\nWe can further rewrite SX and SY in terms of {Wi : 1 \u2264 i \u2264 n1} and {Zj : 1 \u2264 j \u2264 m1} as\nfollows:\n\nSX =\n\n1\nn1\n\nn1\n\ni=1\n\n:=Wi\n\n\n\n\u27e8k(Xi, \u00b7), g2\u27e9k,\n\nand SY =\n\n:=Zj\n\n\n\n\u27e8k(Yj, \u00b7), g2\u27e9k .\n\n1\nm1\n\nm1\n\nj=1\n\n(18)\n\nWith these terms defined, we proceed in the following steps:\n\n\u2022 Step 1: First, we consider the standardized random variables Ts,X and Ts,Y , defined as\n\nTs,X :=\n\n\u221a\n\nn1SX\ni |X2, Y2]\n\nEPn[W 2\n\n,\n\nand Ts,Y :=\n\n\u221a\n\nm1SY\nj |X2, Y2]\n\n,\n\nEPn [Z 2\n\nand prove that they converge in distribution to N (0, 1) conditioned on (X2, Y2). To\np\n\u2192 0\nprove that the limiting distribution is Gaussian, we verify that\n\nEPn [W 4\nn1EPn [W 2\n\ni |X2,Y2]\ni |X2,Y2]2\n\nand\n\nEPn [Z4\nm1EPn [Z2\n\nj |X2,Y2]\nj |X2,Y2]2\n\np\n\u2192 0. This is formally shown in Lemma 16 below.\n\n\u2022 Step 2: Next, building upon the previous result, and using the conditional independence\nof Ts,X and Ts,Y , we show in Lemma 17 below, that the standardized statistic Ts =\n1 |X2, Y2] also converges in distribution\n\n1 |X2, Y2] + m\u22121\n\nEPn [W 2\n\nEPn [Z 2\n\nn\u22121\n1\n\n\n\n1\n\n(SX \u2212 SY )/\nto N (0, 1).\n3:\nEPn [W 2\n\n\u2022 Step\nn\u22121\n1\n\nWe\n\n1 |X2,Y2]+m\u22121\nX +m\u22121\nn\u22121\n1 \u03c32\n\n1 \u03c32\n\n1\n\nY\n\nthen\nEPn [Z2\n\nprove\n1 |X2,Y2]\n\nin\n\nLemma\n\n18\n\nbelow that\n\nthe\n\nratio\n\nconverges in probability to 1.\n\nIt only remains to state and prove the three lemmas used above, which we do after this proof. Barring\nthat, combining the above three steps completes the proof of the theorem.\n\nBefore proceeding, we first introduce the terms ai = \u27e8k(Xi, \u00b7), \u00b52 \u2212\u00b5\u27e9k and bi = \u27e8k(Xi, \u00b7), \u03bd2 \u2212\u00b5\u27e9k,\nand note that we can further decompose Wi into ai \u2212 bi for 1 \u2264 i \u2264 n1. Similarly, for 1 \u2264 j \u2264 m1,\nwe can write Zj as cj \u2212 dj with cj = \u27e8k(Yj, \u00b7), \u00b52 \u2212 \u00b5\u27e9k and dj = \u27e8k(Yj, \u00b7), \u03bd2 \u2212 \u00b5\u27e9k.\nWe now state and prove the intermediate results to obtain Theorem 15.\nLemma 16. Under the conditions of Theorem 15, we have the following:\nj |X2, Y2]\nj |X2, Y2]2\n\ni |X2, Y2]\ni |X2, Y2]2\n\nEPn [Z 4\nm1EPn [Z 2\n\nEPn [W 4\nn1EPn [W 2\n\np\n\u2192 0,\n\np\n\u2192 0.\n\nand\n\nHence, as a consequence of the Lyapunov form of CLT (see Fact 11 and Remark 12 in Appendix A),\nthis means that Ts,X\n\nd\u2212\u2192 N (0, 1) conditioned on (X2, Y2).\n\nd\u2212\u2192 N (0, 1) and Ts,Y\n\nProof. We describe the steps for proving the first statement (involving Wi), noting that the other\nstatement follows in an entirely analogous manner. Throughout this proof, we will use the shorthand\nE2[\u00b7] to denote the EPn [\u00b7|X2, Y2].\nBy two applications of the AM-GM inequality, we observe that W 4\nHence, we have the following:\ni + b4\ni ]\n\ni = (ai \u2212 bi)4 \u2264 16(a4\n\ni + b4\n\nE2[a4\n\ni ).\n\nE2[W 4\ni ]\ni ]2 \u2264\n16n1E2[W 2\n\nn1E2[(ai \u2212 bi)2]\nn1E2[a4\ni ]\nEPn[\u00afk(X1, X2)2]2\n:= A1 \u00d7 A2 + B1 \u00d7 B2.\n\n\u00d7\n\n=\n\nEPn[\u00afk(X1, X2)2]2\nE2[(ai \u2212 bi)2]\nn2\n1\n\n+\n\nm1E2[b4\ni ]\nEPn[\u00afk(Y1, Y2)2]2\n\n\u00d7\n\nEPn [\u00afk(Y1, Y2)2]2\nE2[(ai \u2212 bi)2]\nm2\n1\n\n(19)\n\n(20)\nThus, to complete the proof, it suffices to show that A1 \u00d7 A2 and B1 \u00d7 B2 converge in probability to\n0. This can be shown in two steps:\n\n17\n\n\f\u2022 Under the assumptions of Theorem 15, we have A1\n\np\n\u2192 0. To prove this result,\nit suffices to show that EPn [A1] \u2192 0 and EPn[B1] \u2192 0. The result then follows by an\napplication of Markov\u2019s inequality.\n\np\n\u2192 0 and B1\n\n\u2022 A2 and B2 are bounded in probability.\n\nWe first show that EPn [A1] \u2192 0. The result for B1 follows similarly.\n\nEPn [A1] =\n\n(i)\n=\n\nn1\nEPn [\u00afk(X1, X2)2]\nn1\nEPn[\u00afk(X1, X2)2]\n\nE2\nEPn\n EPn\n\n\n\na2\n\ni\n\nn3\n1\n\n\u00afk4(X1, X2)\n\n+\n\n3n1(n1 \u2212 1)\nn4\n1\n\nE[\u00afk2(X1, X3)\u00afk2(X2, X3)]\n\n\n\n\u2264\n\n3\nEPn [\u00afk(X1, X2)2]\n\n EPn\n\n\u00afk4(X1, X2)\n\nn2\n1\n\n+\n\n1\nn1\n\nE[\u00afk2(X1, X3)\u00afk2(X2, X3)]\n\n,\n\n\n\nwhich goes to 0 as required, by invoking the condition in (17) of Assumption 1. For (i), we used the\nexpression derived by Kim and Ramdas (2020) while proving their Theorem 6.\n\nTo complete the proof, we show that A2 is bounded in probability (the result for B2 follows similarly).\nWe consider two cases, depending on whether \u03c11 := limn,m\u2192\u221e\nis equal to 0 or greater than\n0 (the existence of this limit is assumed).\n\n\u03bb2\n1\nl \u03bb2\nl\n\n\n\nCase 1: \u03c11 > 0. We first observe that as a consequence of (15) and the orthonormality of the\neigenfunctions, we have\n\nEPn [\u00afk(X1, X2)2] = EPn\n\n\uf8ee\n\n\n\n\uf8f0\n\nl,l\u2032\n\n\u03bbl\u03bbl\u2032\u03c6l(X1)\u03c6l\u2032(X1)\u03c6l(X2)\u03c6l\u2032(X2)\n\n\uf8fb =\n\n\uf8f9\n\n\u221e\n\n\nl=1\n\n\u03bb2\nl .\n\nUsing this, we obtain the following:\n\n1\n(A2)1/2\n\n=\n\nn1E2[a2\n\ni \u2212 2aibi]\n\n.\n\ni + b2\n\u221e\n\nl=1 \u03bb2\nl\n\nBy repeated use of (15), we can show that the following identities hold:\n\nE2[a2\n\ni ] =\n\n1\n(n \u2212 n1)2\n\n\u221e\n\n\nl=1\n\n\u03bb2\nl\n\nE2[b2\n\ni ] =\n\n1\n(m \u2212 m1)2\n\n\u221e\n\n\nl=1\n\n\n\n\n\ni\u2032\n\uf8eb\n\n2\n\n\u03c6l(Xi\u2032)\n\n,\n\n\uf8f6\n\n2\n\n\u03bb2\nl\n\n\n\n\uf8ed\n\n\u03c6l(Yj\u2032)\n\n\uf8f8\n\n,\n\nand\n\nE2[aibi] =\n\n1\n(n \u2212 n1)(m \u2212 m1)\n\nj\u2032\n\n\u221e\n\n\nl=1\n\n\n\n\n\n\u03bb2\nl\n\ni\u2032\n\n\u03c6l(Xi\u2032)\n\n \uf8eb\n\uf8ed\n\n\n\nj\u2032\n\n\uf8f6\n\n\u03c6l(Yj\u2032)\n\n\uf8f8 .\n\nPlugging these equalities in the expression for A2, and using \u03c1l = \u03bbl\n\n\n\nl\u2032 \u03bb2\nl\u2032\n\n, we get\n\n(A2)1/2 =\n\n\u2264\n\n1\n\nn1\n\n\n\nl \u03c1l\n\n 1\n\nn\u2212n1\n\n\n\ni\u2032 \u03c6l(Xi\u2032) \u2212 1\n\nm\u2212m1\n\n\n\nj\u2032 \u03c6l(Yj\u2032)\n\n2\n\n1\n\n \u221a\n\nn1\nn\u2212n1\n\n\u03c11\n\n\n\ni\u2032 \u03c61(Xi\u2032) \u2212\n\n\u221a\n\nn1\nm\u2212m1\n\n\n\nj\u2032 \u03c61(Yj\u2032)\n\n2\n\n18\n\n\fSince n1 = n/2, m1 = m/2, we have\nIntroduce the notation ui\u2032 = 2/n/1 + n/m and vj\u2032 = (\n\n\u221a\n\nn1/(m \u2212 m1) =\n2n/m)/1 + n/m, and note that\n\n2n/m.\n\nn1/(n \u2212 n1) = 2/n and\n\n\u221a\n\n\u221a\n\n\u221a\n\n(A2)1/2 \u2264\n\n1 + n\n\nm\n\n \u03c11\n\n\n\n1\ni\u2032 ui\u2032\u03c61(Xi\u2032) \u2212 \n\nj\u2032 vj\u2032\u03c61(Yj\u2032)\n\n2\n\n\u2264\n\n\u03c11\n\n1\n\n\n\ni\u2032 ui\u2032\u03c61(Xi\u2032) \u2212 \n\nj\u2032 vj\u2032\u03c61(Yj\u2032)\n\n2 .\n\n(21)\n\nNext, we note that\n\nlim\nn\u2192\u221e\n\nmax\ni\u2032,j\u2032\n\nu2\ni\u2032 + v2\nj\u2032\ni\u2032 + \ni\u2032 u2\nj\u2032 v2\nj\u2032\n\n\n\n= lim\nn\u2192\u221e\n\n\u2264 lim\nn\u2192\u221e\n\n2\nn + n2/m\n 1\nn\n\n1\nm\n\n+\n\n2\n\n2\nm + m2/n\n\n+\n\n\n\n= 0.\n\nThus, by an application of Lindeberg\u2019s CLT, we observe that the denominator in (21) converges in\ndistribution to N (0, \u03c11)2. This implies that A2 = OP (1), as required.\nCase 2: \u03c11 = 0. Again, we observe that\n\n(A2)\u22121/2 =\n\nn1E2[a2\ni ]\nEPn [\u00afk(X1, X2)2]\n\n+\n\nn1E2[b2\ni ]\nEPn [\u00afk(X1, X2)2]\n\n\u2212 2\n\nn1E2[aibi]\nEPn [\u00afk(X1, X2)2]\n\n.\n\nThe first two terms in the display above are 1 + oP (1), as shown in (Kim and Ramdas, 2020, pg 55,\nStep 2). For the last term, we introduce the notation g(x, y) = EPn[\u00afk(X, x)\u00afk(X, y)], and note the\nfollowing:\n\nR :=\n\nn1E2[aibi]\nEPn [\u00afk(X1, X2)2]\n\n=\n\nn1\n(n \u2212 n1)(m \u2212 m1)\n\n\n\ni\u2032,j\u2032\n\ng(Xi\u2032, Yj\u2032).\n\nEPn [R2] =\n\nSince Xi\u2032 and Yj\u2032 are independent, we observe that EPn [g(Xi\u2032, Yj\u2032)] = 0, and hence EPn [R] = 0.\nFurthermore, the variance of R satisfies\nn2\n1\n(n \u2212 n1)(m \u2212 m1)\nn2\n1\n(n \u2212 n1)(m \u2212 m1)\n\u03bb2\n1\nl\u2032 \u03bb2\nl\u2032\n\nEPn [g(X1, X2)2]\nEPn[\u00afk(X1, X2)2]\nEPn[\u00afk(X1, X3)2\u00afk(X2, X3)2]\nEPn [\u00afk(X1, X2)2]2\n\nl \u03bb4\nl\nl )2 \u2264\nl \u03bb2\n\n\u03bb2\n1\nl\u2032 \u03bb2\nl\u2032\n\n\u03bb2\nl\nl\u2032 \u03bb2\nl\u2032\n\n\u2192 \u03c11 = 0.\n\n\n(\n\n\n\n\n\n\n\n\n\n=\n\n=\n\n=\n\nl\n\nThis implies that the term R is oP (1), and hence we have\n\n(A2)1/2 =\n\n1\n2 + oP (1)\n\n= OP (1),\n\nas required. This completes the proof.\n\nNext, we show that we can use Lemma 16 to obtain the limiting distribution of the standardized\nstatistic Ts =\n\n\u221a\n\n.\n\nn\u22121\n1\n\nE[W 2\n\nSX \u2212SY\n1 |X2,Y2]+m\u22121\n\n1\n\nE[Z2\n\n1 |X2,Y2]\n\nLemma 17. Under the conditions of Theorem 15, the standardized statistic Ts converges in distribu-\ntion to N (0, 1).\n\nProof. This statement simply follows from the observation that E2[Z 2\nn\u22121\nunder the null hypothesis. Then, the term \u03b1n := (\n1\n\n\nE2[W 2\n\n1 ])/(\n\nn\u22121\n1\n\n\n\n\n\n1/(1 + n1m\u22121\n\n1 ) converges to a constant (say \u03b1 \u2208 (0, 1)).\n\n1 ] = E2[W 2\nE2[W 2\n\n1 ] almost surely\nE2[Z 2\n1 ]) =\n\n1 ] + m\u22121\n1\n\n19\n\n\fnTs,Y\n\nd\u2212\u2192 N (0, \u03b12) and\nUsing the result of Lemma 16, we can then conclude that \u03b1nTs,X\n1 \u2212 \u03b12\nd\u2212\u2192 N (0, 1 \u2212 \u03b12). This implies, due to L\u00e9vy\u2019s continuity theorem (Durrett, 2019,\nTheorem 3.3.17. (i)), the pointwise convergence of the characteristic functions of these sequences.\nIn particular, let \u03c8n,X and \u03c8n,Y denote the characteristic functions of \u03b1nTs,X and\nnTs,Y\nrespectively. Then, due to the conditional independence of Ts,X and Ts,Y given (X2, Y2), we note\nthat the characteristic function of Ts = \u03b1nTs,X = 1 \u2212 \u03b12\n\nnTs,Y , denoted by \u03c8n(t), satisfies\n\n1 \u2212 \u03b12\n\n\u03c8n(t) := EPn [exp (itTs) |X2, Y2]\n\n= EPn [exp (it \u03b1nTs,X ) |X2, Y2] \u00d7 EPn\n= \u03c8n,X (t) \u00d7 \u03c8n,Y (\u2212t).\n\nNow, taking the limit n \u2192 \u221e, we get that\n\n\nexp\n\n\n\n\u2212it 1 \u2212 \u03b12\n\nnTs,Y\n\n\n\n\n\n|X2, Y2\n\nlim\nn\u2192\u221e\n\n\u03c8n(t) = lim\nn\u2192\u221e\n\n\n\u03c8n,X (t) \u00d7 \u03c8n,Y (\u2212t)\n\n= exp\n\n\u2212\n\n\n\n= exp\n\n\u2212\n\n\n\n\u03b12t2\n\n\n\n\u00d7 exp\n\n\u2212\n\n(1 \u2212 \u03b12)t2\n\n\n\n1\n2\n\n\n\n.\n\n1\n2\nt2\n2\n\nThus, we have shown that conditioned on (X2, Y2), the characteristic function, \u03c8n of Ts converges\npointwise to the characteristic function of a N (0, 1) distribution. Hence, by the other direction of\nd\u2212\u2192 N (0, 1).\nL\u00e9vy\u2019s continuity theorem (Durrett, 2019, Theorem 3.3.17. (ii)), we conclude that Ts\nd\u2212\u2192 N (0, 1)\nFinally, we pass from the conditional statement to the unconditional one by noting that Ts\nconditioned on (X2, Y2) implies that supx\u2208R | PPn (Ts \u2264 x) \u2212 \u03a6(x)|\n\u2212\u2192 0, because the N (0, 1)\ndistribution is continuous. This fact, coupled with the boundedness of supx\u2208R | PPn (Ts \u2264 x) \u2212 \u03a6(x)|\nimplies that it also converges in expectation, as required. Thus, we have shown that the limiting\ndistribution of the standardized statistic Ts is N (0, 1) unconditionally.\n\np\n\nWe now prove that the studentized statistic also has the same limiting distribution as the standardized\nstatistic Ts by appealing to Slutsky\u2019s theorem and the continuous mapping theorem.\nE2[Z 2\nLemma 18. The ratio of \u03c32 and the conditional variance n\u22121\nprobability to 1. Stated formally,\n\n1 ] converges in\n\n1 ] + m\u22121\n1\n\nE2[W 2\n\n1\n\nX + m\u22121\n1 ] + m\u22121\n1\nRecall that we use the notation E2[\u00b7] to denote the conditional expectation on the second half of the\ndata, i.e., EPn [\u00b7|X2, Y2].\n\nn\u22121\n1 \u03c32\nE2[W 2\n\nY\nE2[Z 2\n1 ]\n\n1 \u03c32\n\n\u2212\u2192 1.\n\nn\u22121\n1\n\np\n\nProof. Since E2[W 2\nconclude the result:\n\n1 ] = E2[Z 2\n\n1 ] almost surely, it suffices to show the following two statements to\n\n\u03c32\nX\nE2[W 2\n1 ]\n\np\n\n\u2212\u2192 1,\n\nand\n\n\u03c32\nY\nE2[Z 2\n1 ]\n\np\n\n\u2212\u2192 1.\n\nWe provide the details of the first statement, since the second can be obtained similarly. Consider the\nfollowing:\n(n1 \u2212 1)\u22121 n1\n\ni=1(Wi \u2212 \u00afUX )2 \u2212 E2[W 2\n1 ]\n\nn1\n\ni=1(Wi \u2212 \u00afUX )2 \u2212 (n1 \u2212 1)E2[W 2\n1 ]\nEPn [\u00afk2(X1, X2)]\n\n\u00d7\n\nEPn[\u00afk2(X1, X2)]\n(n1 \u2212 1)E2[W 2\n1 ]\n\nE2[W 2\n1 ]\n\n=\n\n\u221a\n\nNote that C2 = n1\nn1\u22121\nof Lemma 16. Hence, to complete the proof, we will show that C1\n\nA2, where A2 was introduced in (20) and shown to be OP (1) in the proof\n\u2212\u2192 0. This can be concluded by\n\np\n\n= C1 \u00d7 C2.\n\n20\n\n\fnoting that EPn [C1] = 0, and that the variance of C1 satisfies:\n\nVPn [C1] = EPn [VPn[C1|X2, Y2]] + VPn [EPn [C1|X2, Y2]]\n\n=\n\n\u2264\n\n(n1 \u2212 1)2\nEPn [\u00afk2(X1, X2)]2\n(n1 \u2212 1)2\nEPn [\u00afk2(X1, X2)]2\n\n= 16 (A1 + B1) ,\n\n\n\n\n\nEPn\n\nVPn\n\nEPn [W 4\n1 ]\nn1\n\n\u2264\n\nn1\n\n(Wi \u2212 \u00afUX )2\n\n\n\n1\nn1 \u2212 1\n\ni=1\nn1EPn[W 4\n1 ]\nEPn [\u00afk2(X1, X2)]2\n\n\u2264 16\n\nn1EPn [a4\n1 + b4\n1]\nEPn [\u00afk2(X1, X2)]2\n\nwhere the terms A1 and B1 were introduced in (19). As mentioned during the proof of Lemma 16,\nboth of these terms can be shown to converge in probability to 0 as required.\n\nThe previous three lemmas prove that for any sequence {Pn : n \u2265 1} with Pn \u2208 P (0)\nlimn\u2192\u221e supx\u2208R |PPn\n\nn , we have\n\u2212\u03a6(x)| = 0. This is sufficient to conclude the uniform result\n\n\n\u00afxMMD\n\n\u2264 x\n\n\n\n2\n\nlim\nn\u2192\u221e\n\nsup\nPn\u2208P (0)\n\nn\n\nsup\nx\u2208R\n\n|PPn\n\n\n\u00afxMMD\n\n2\n\n\n\n\u2264 x\n\n\u2212 \u03a6(x)| = 0.\n\nThis is because we can select a sequence P \u2032\n\nn such that for all n, we have\n\n|PP \u2032\n\nn\n\nsup\nx\u2208R\n\n\n\u00afxMMD\n\n2\n\n\u2212 \u03a6(x)| \u2264 sup\n\nPn\u2208P (0)\n\nn\n\n\n\u00afxMMD\n\n2\n\n\u2212 \u03a6(x)|\n\n|PPn\n\nsup\nx\u2208R\n\n\u2264 sup\nx\u2208R\n\n|PP \u2032\n\nn\n\n\n\u00afxMMD\n\n2\n\n\u2212 \u03a6(x)| +\n\n1\nn\n\n.\n\nSince the left and right terms converge to zero, it follows that the middle term does too, as required.\nThis completes the proof of Theorem 15.\n\nB.2 Fixed P , changing kn (Theorem 5)\n\nWe note that the statement of Theorem 5 requires an additional technical assumption on the eigen-\nvalues of the kernel operator, introduced in (15). We repeat the statement of Theorem 5 with this\nadditional requirement below.\nTheorem 5\u2019. Suppose P is fixed, but the kernel kn changes with n. If\n\nlim\nn\u2192\u221e\n\nEP [\u00afkn(X1, X2)4]\nEP [\u00afkn(X1, X2)2]2\n\n 1\nn\n\n+\n\n1\nmn\n\n\n\n= 0,\n\nand\n\nlim\nn\u2192\u221e\n\n\u03bb2\n1,n\n\u221e\nl=1 \u03bb2\nl,n\n\nexists,\n\n(22)\n\nthen we have \u00afxMMD\n\n2\n\nd\u2212\u2192 N (0, 1).\n\nProof. The proof of this statement will follow the general outline of the proof of Theorem 15.\nHowever, in this special case when P is fixed, we can remove the condition that limn\u2192\u221e mn/n\nexists and is non-zero, that is required by Theorem 15.\n\nWe will carry over the notations used in the proof of Theorem 15, and in particular, we will use\nn1\n\u00afUX = 1\nj=1 Zj. Since Wi and Zj are identically distributed under\nn1\nthe null, we have EP [W 2\n2 to denote this conditional\nvariance. Then, note the following:\n\ni=1 Wi and \u00afUY = 1\nm1\ni |X2, Y2] = EP [Z 2\n\nj |X2, Y2], and we will use \u03c32\n\nm1\n\n\u00afxMMD\n\n2\n\n=\n\n\u00afUX \u2212 \u00afUY\n\u03c3\n\n=\n\n\u03c32\n\n:= T1 \u00d7 T2.\n\n\u00afUX \u2212 \u00afUY\n\n\n\n1 + m\u22121\nn\u22121\n1\n\n\n\n\u03c32\n\n1 + m\u22121\nn\u22121\n1\n\n\n\n \u00d7\n\n\u03c3\n\n(23)\n\nTo complete the proof, we will show that T1\nan application of Slutsky\u2019s theorem.\n\nd\u2212\u2192 N (0, 1) and T2\n\np\n\n\u2212\u2192 1. The result then follows by\n\n21\n\n\fFirst, we consider the term T1 in (23). Let Wi := Wi/\u03c32 and Zj := Zj/\u03c32. Then, conditioned on\n(X2, Y2), the terms Wi and Zj are independent and identically distributed. Introducing the constants\nui =\n\n m1\n\n n1\n\nm1(m1+n1) , we can write\n\nn1(m1+n1) and vj =\n\nT1 =\n\nn1\n\ni=1\n\nuiWi \u2212\n\nm1\n\nj=1\n\nvj Zj.\n\nWe can check that the constants (ui) and (vj) satisfy the property:\n\nlim\nn\u2192\u221e\n\nmax\ni,j\n\ni + v2\nu2\nj\ni\u2032 + m1\ni\u2032=1 u2\n\nn1\n\nj\u2032=1 v2\nj\u2032\n\n\u2264 lim\nn\u2192\u221e\n\nmax\ni,j\n\n1\nm1\n\n+\n\n1\nn1\n\n= 0.\n\nthis also means that\nthat is,\n\nd\u2212\u2192 N (0, 1) conditioned\nSince the limiting distribution (in this case, standard normal) is continu-\nthe T1 converges to N (0, 1) in the Kolmogorov-Smirnov met-\np\n\u2212\u2192 0. Since the random vari-\nis bounded, convergence in probability implies\nlimn\u2192\u221e EP [supx\u2208R |PP (T1 \u2264 x|X2, Y2) \u2212 \u03a6(x)|] = 0, which in turn implies that\n\nThus, by an application of Lindeberg\u2019s CLT, we note that T1\non (X2, Y2).\nous,\nric,\nable supx\u2208R |PP (T1 \u2264 x|X2, Y2) \u2212 \u03a6(x)|\nthat\nlimn\u2192\u221e supx\u2208R |EP [PP (T1 \u2264 x|X2, Y2) \u2212 \u03a6(x)] | = 0, as required.\n\nlimn\u2192\u221e supx\u2208R |PP (T1 \u2264 x|X2, Y2) \u2212 \u03a6(x)|\n\nWe now consider the second term, T2, in (23). It remains to show that T2\n1/T 2\n\np\n\n2 \u2212 1\n\n\u2212\u2192 0, and the result will follow by an application of the continuous mapping theorem.\n\n\n\n \u2212 1\n\n\n\n\n+ \u03c32\nY\nm1\n+ 1\nm1\n\n\u03c32\nX\nn1\n 1\nn1\n\n\n\n\u2212 1\n\n\n\n\n\n\u2212 1\n\n\n\n\u03c32\nX\n\u03c32\n2\n\n\u03c32\nY\n\u03c32\n2\n\n1\nT 2\n2\n\n\u2212 1\n\n\n\n\n\n\n\n\n\u03c32\n2\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n+\n\n\u2264\n\n=\n\n.\n\n(24)\n\np\n\n\u2212\u2192 1. We will show that\n\nThus, it suffices to show that both terms in (24) converge in probability to 0. This is exactly the\nresult that is proved in Lemma 18 under the two conditions listed in Assumption 1. The condition\non eigenvalues is already assumed in the statement of Theorem 5\u2019, and thus we will show that the\ncondition on the kernels, stated in (22), implies the condition (17). To prove this, we first, we note\nthat\n\nEP\n\n\u00afkn(X1, X2)2\u00afkn(X1, X3)2 \u2264 EP\n= EP\n\n\u00afkn(X1, X2)41/2 EP\n\u00afkn(X1, X2)4 .\n\n\u00afkn(X1, X3)41/2\n\nThus, the term in (17) is upper bounded by\n\u00afkn(X1, X2)4\n\u00afkn(X1, X2)22\n\nEP\nEP\n\n 1\nn\n\n+\n\n1\nmn\n\n \n\n1 +\n\n1\nn\n\n+\n\n\n\n.\n\n1\nmn\n\nSince, we have assumed that limn\u2192\u221e mn \u2192 \u221e, there exists and n0, such that for all n \u2265 n0,\n1 + 1\n\u2264 2. This implies that if (22) is satisfied, then (17) in Assumption 1 is also satisfied, as\nrequired.\n\nn + 1\nmn\n\nB.3 Fixed k, and fixed P (Theorem 4)\n\nWe prove Theorem 4 by showing that under the bounded fourth moment assumption on \u00afk, both the\nconditions required by Theorem 5\u2019 are satisfied.\nNote that since EP [\u00afk(X1, X2)] = 0, the positive and finite fourth moment also implies that the\nsecond moment of \u00afk(X1, X2) is also positive and finite. Hence, we have that\n\nThis, in turn, implies\n\nEP [\u00afk(X1, X2)4]\nEP [\u00afk(X1, X2)2]2\n\n< \u221e.\n\nlim\nn\u2192\u221e\n\nEP [\u00afk(X1, X2)4]\nEP [\u00afk(X1, X2)2]2\n\n 1\nn\n\n+\n\n1\nmn\n\n\n\n= 0,\n\n22\n\n\fas required by Theorem 5.\n\nFor the second part of the condition, we note that as kernel k and probability distribution P are\n\u03bb2\ndoesn\u2019t change with n, and hence its limit exists. Thus, both the conditions\nfixed, the term\n1\nl \u03bb2\nl\nfor Theorem 5\u2019 are satisfied, as required.\n\n\n\nC Consistency against fixed and local alternatives (Section 4)\n\nC.1 Proof of Theorem 8 (General conditions for consistency)\n\nProof. We begin by noting that\n\nEPn,Qn [1 \u2212 \u03a8(X, Y)] = PPn,Qn\n\n\n\u00afxMMD\n\n2\n\n\n\n\u2264 z1\u2212\u03b1\n\n= PPn,Qn\n\n\nxMMD\n\n2\n\n\u2264 z1\u2212\u03b1\u03c3\n\n\n\n.\n\nNow, introduce the event E = {\u03c32 \u2264 E[\u03c32]/\u03b4n}, where (\u03b4n) is a positive sequence converging to\nzero. By an application of Markov\u2019s inequality, we have PPn,Qn (E c) \u2264 \u03b4n, which implies that\n\nPPn,Qn\n\n\n\nxMMD\n\n2\n\n\u2264 z1\u2212\u03b1\n\n\u221a\n\n\n\n\u03c32\n\n= PPn,Qn\n\n\n\n\u221a\n\n\u2264 z1\u2212\u03b1\n\n2\n\n{xMMD\n\n{xMMD\n\n2\n\n\u2264 z1\u2212\u03b1\n\n+ PPn,Qn\n\n\n\n\u03c32} \u2229 E\n\u221a\n\n\u03c32} \u2229 E c\nEPn,Qn [\u03c32]/\u03b4n\n\n\n\n\n\n\n\n\u2264 PPn,Qn\n\n\u2264 PPn,Qn\n\nxMMD\n\n2\n\n\u2264 z1\u2212\u03b1\n\nxMMD\n\n2\n\n\u2264 z1\u2212\u03b1\n\n\n\n\n\n+ PPn,Qn (E c)\n\n\n\nEPn,Qn [\u03c32]/\u03b4n\n\n+ \u03b4n.\n\n(25)\n\nBy the assumption that \u03b4n \u2192 0, it suffices to show that the worst-case value of the first term in (25)\nconverges to zero to complete the proof.\n\nTo do this, we observe that (7) implies that there exists a finite value of n, say n0, such that for all\nn \u2265 n0 and m \u2265 mn0, we have\n\nsup\n(Pn,Qn)\u2208P (1)\n\nn\n\nEPn,Qn [\u03c32]\n\u03b34\nn\u03b4n\n\n\u2264\n\n1\n4z2\n\n1\u2212\u03b1\n\n,\n\nwhich implies that z1\u2212\u03b1\n\u03bd2\u27e9k, it follows that EPn,Qn [xMMD\nn \u2265 n0:\n\nEPn,Qn[\u03c32]/\u03b4n \u2264 \u03b32\n] = \u03b32\n\n2\n\nn/2. Furthermore, since xMMD\n\n= \u27e8\u00b51 \u2212 \u03bd1, \u00b52 \u2212\nn. Combining these two observations, we get for all\n\n2\n\nPPn,Qn\n\n\n\nxMMD\n\n2\n\n\n\n\u2264 z1\u2212\u03b1\n\nEPn,Qn [\u03c32]/\u03b4n\n\n\n\n\n\n\u2264 PPn,Qn\n\nxMMD\n\n2\n\n\u2212 EPn,Qn [xMMD\n\n2\n\n] \u2264\n\n\n\n\u2212 \u03b32\nn\n\n\u03b32\nn\n2\n\n(i)\n\u2264 4\n\n2\n\nVPn,Qn (xMMD\n\u03b34\nn\n\n)\n\n,\n\nwhere (i) follows from Chebychev\u2019s inequality. This implies that\n\nsup\n(Pn,Qn)\u2208P (1)\n\nn\n\nPPn,Qn\n\n\n\u00afxMMD\n\n2\n\n< z1\u2212\u03b1\n\n\n\n\u2264\n\nsup\n(Pn,Qn)\u2208P (1)\n\nn\n\n4\n\nVPn,Qn ( \u00afU )\n\u03b34\nn\n\n.\n\nThe required conclusion that sup(Pn,Qn)\u2208P (1)\nsecond term in (7).\n\nn\n\nPPn,Qn (\u00afxMMD\n\n2\n\n\u2264 z1\u2212\u03b1) \u2192 0 now follows from the\n\nC.2 Proof of Theorem 7 (Consistency against fixed alternative)\n\nWe prove Theorem 7 by showing that the sufficient conditions for consistency, as derived in Theorem 8,\nare satisfied under the assumptions of Theorem 7.\n\n23\n\n\fFirst, since the kernel is assumed to be characteristic, and Pn = P \u0338= Q = Qn, it means that\nthe kernel-MMD distance between P and Q must be strictly positive. In other words, we have\n\u03b3n = MMD(P, Q) := \u03b3 > 0 for all n \u2265 1. Hence, in order to verify the condition (7), it suffices to\nshow that the following two properties hold:\n2\nn\n\nEP,Q[\u03c32\n\nEP,Q[\u03c32\n\nY ] = 0,\n\nlim\nn\u2192\u221e\n\nX ] +\n\n(26)\n\nand\n\n2\nmn\n\n= 0.\n\n(27)\n\nEP,Q[\u03c32] = lim\nn\u2192\u221e\n2\n\nxMMD\n\nVP,Q\n\nlim\nn\u2192\u221e\n\nIn the equality in (26), we used the fact that n1 = n/2 and m1 = mn/2 (see Remark 3).\n\nVerifying (26). We begin by noting that it suffices to show that EP,Q[\u03c32\nY ] < \u221e\nto conclude (26) (this is because we have assumed in Remark 3 that limn\u2192\u221e mn = \u221e). We present\nX as the same arguments can be used to conclude the result for \u03c32\nthe details for \u03c32\nY .\nX = 1\nRecall that \u03c32\nn1\ni.i.d., this implies that\n\n, where g2 = \u00b52 \u2212 \u03bd2. Since X1, . . . , Xn1 are\n\nX ] < \u221e and EP,Q[\u03c32\n\n\u27e8k(Xi, \u00b7), g2\u27e9k \u2212 \u00afUX\n\nn1\ni=1\n\n2\n\nEP,Q[\u03c32\n\nX ] = EP,Q\n\n\n\n= EP,Q\n\n\u27e8k(X1, \u00b7), g2\u27e9k \u2212 \u00afUX\n\n2\n\n\n\nn1\n\n2\n\ni=1\n\n\u27e8k(Xi, \u00b7), g2\u27e9k \u2212 \u00afUX\n\n1\nn1\n\u27e8k(X1, \u00b7) \u2212 \u00b51, g2\u27e92\n\u2225k(X1, \u00b7) \u2212 \u00b51\u22252\n\u2225k(X1, \u00b7) \u2212 \u00b51\u22252\n\u2225k(X1, \u00b7)\u22252\n\n\u27e8k(X1, \u00b7) \u2212 \u00b51, \u00b52 \u2212 \u03bd2\u27e92\n\n= EP,Q\n\u2264 EP,Q\n\u2264 EP,Q\n\u2264 2EP,Q\nk\n\u2264 (4EP,Q [k(X1, X1)]) \u00d7 (2EP,Q [k(X2, X2) + k(Y1, Y1)]) < \u221e.\n\n = EP,Q\n\nk\u2225\u00b52 \u2212 \u03bd2\u22252\n\u2225\u00b52 \u2212 \u03bd2\u22252\n EP,Q\n \u00d7 2EP,Q\nk + \u2225\u00b51\u22252\n\n\n\u2225\u00b52\u22252\n\nk + \u2225\u03bd2\u22252\n\n\n\nk\n\nk\n\nk\n\nk\n\nk\n\nk\n\n\n\n(28)\n\n(29)\n\n(30)\n\n(31)\n\n(32)\n\n2\n\nX = \u27e8\u00b51, g2\u27e9k = \u27e8\u00b51, \u00b52 \u2212 \u03bd2\u27e9k, and the linearity of inner product,\n\nIn the above display:\n(28) uses the fact that xMMD\n(29) uses the Cauchy\u2013Schwarz inequality,\n(30) uses the fact that the two terms inside the expectation are independent,\n, and\n(31) uses the fact that \u2225a \u2212 b\u22252\n(32) uses the facts that \u2225k(X1, \u00b7)\u22252\nEP,Q[\u2225\u00b52\u22252\nEP,Q[\u2225\u03bd2\u22252\nbelow:\n\nk] \u2264 EP,Q[k(X1, X1)],\nk] \u2264 EP,Q[k(X2, X2)] for X2 \u223c P independent of X1 and EP,Q[\u2225\u00b52\u22252\nk] and\nk] \u2264 EP,Q[k(Y1, Y1)] for Y1 \u223c Q. We show the details for the bound for EP,Q[\u2225\u00b51\u22252\nk]\n\nk = k(X1, X1), EP,Q[\u2225\u00b51\u22252\n\nk \u2264 (\u2225a\u2225k + \u2225b\u2225k)2 \u2264 2 \u2225a\u22252\n\nk + \u2225b\u22252\nk\n\nEP,Q[\u2225\u00b51\u22252\n\nk] = EP,Q\n\n\uf8f0\n\n\uf8ee\n\n4\nn2\n\nn/2\n\n\nn/2\n\n\n\u27e8k(Xi, \u00b7), k(Xl, \u00b7)\u27e9k\n\ni=1\n\nl=1\n\n\uf8f9\n\n\uf8fb\n\n\u2264\n\n4\nn2\n\nn/2\n\n\nn/2\n\n\ni=1\n\nl=1\n\n(EP,Q[k(Xi, Xi)]EP,Q[k(Xl, Xl)])1/2\n\n= EP,Q[k(X1, X1)],\n\nwhere the inequality follows from an application of Cauchy\u2013Schwarz inequality. The bounds for\nEP,Q[\u2225\u00b52\u22252\nThus, we have shown that EP,Q[\u03c32\n\nX ] < \u221e. The result for EP,Q[\u03c32\n\nk] also follow from the same steps.\n\nY ] follows in an analogous manner.\n\nk] and EP,Q[\u2225\u03bd2\u22252\n\nVerifying (27). We begin by noting that the expected value of xMMD\n\u00afUX \u2212 \u00afUY is equal to MMD2(P, Q) = \u2225\u00b5 \u2212 \u03bd\u22252\nk = \u03b32. Thus, we have\n\n2\n\n= \u27e8\u00b51 \u2212 \u03bd1, \u00b52 \u2212 \u03bd2\u27e9k =\n\nVP,Q(xMMD\n\n2\n\n) = EP,Q\n\n= EP,Q\n\n\n\nxMMD\n\n2\n\n\u2212 \u27e8\u00b5 \u2212 \u03bd, \u00b5 \u2212 \u03bd\u27e9k\n\n2\n\n \u00afUX \u2212 \u27e8\u00b5, \u00b5 \u2212 \u03bd\u27e9k\n \u00afUX \u2212 \u27e8\u00b5, \u00b5 \u2212 \u03bd\u27e9k\n\n \u2212  \u00afUY \u2212 \u27e8\u03bd, \u00b5 \u2212 \u03bd\u27e9k\n2\n\n+ 2EP,Q\n\n2\n\n \u00afUY \u2212 \u27e8\u03bd, \u00b5 \u2212 \u03bd\u27e9k\n\n2\n\n.\n\n(33)\n\n= 2EP,Q\n\n24\n\n\fWe present the details for showing that the first term in (33) converges to 0 with n. The result for the\nsecond term can be proved similarly.\nBefore proceeding, we introduce some notation: we will use \u00b51 to denote \u00b51 \u2212 \u00b5, the centered version\nof \u00b51. Similarly, we will use \u00b52, \u03bd1, \u03bd2 and g2 to represent \u00b52 \u2212 \u00b5, \u03bd1 \u2212 \u03bd, \u03bd2 \u2212 \u03bd and g2 \u2212 (\u00b5 \u2212 \u03bd)\nrespectively. With these notations, note that we can write\n \u00afUX \u2212 \u27e8\u00b5, \u03bd \u2212 \u00b5\u27e9k\n\nEP,Q\n\n2\n\n\n\n= EP,Q\n\u2264 2EP,Q\n\n(\u27e8\u00b51, g2\u27e9k + \u27e8\u00b5, g2\u27e9k)2\n\u27e8\u00b51, g2\u27e92\n\n + 2EP,Q\n\nk\n\n\u27e8\u00b5, g2\u27e92\n\nk\n\n .\n\n(34)\n\nEP,Q\n\n\u27e8\u00b51, g2\u27e92\n\nWe now show that the first term of (34) is O(1/n).\n \u2264 EP,Q\n \u2264 EP,Q[\u2225\u00b51\u22252\n2 \u2225\u00b52\u22252\nk\u2225\u00b52 \u2212 \u03bd2\u22252\n (2EP,Q[k(X1, X1)] + 2EP,Q[k(Y1, Y1)])\n\u2264 EP,Q\n\uf8eb\n\uf8edEP,Q\n\n\u2225\u00b51\u22252\n\u2225\u00b51\u22252\nk\n\uf8ee\n\n\u27e8k(Xi, \u00b7), k(Xl, \u00b7)\u27e9k\n\nk]EP,Q\n\nn/2\n\n\nn/2\n\n\n= O\n\n\uf8f6\n\n\uf8f8\n\n\uf8fb\n\n\uf8f0\n\n\uf8f9\n\nk\n\nk\n\n4\nn2\n\nk + \u2225\u03bd2\u22252\n\nk\n\n\uf8eb\n\uf8edEP,Q\n\n\uf8ee\n\n\uf8f0\n\n4\nn2\n\n= O\n\ni=1\n\nl=1\n\nn/2\n\n\n\u27e8k(Xi, \u00b7), k(Xi, \u00b7)\u27e9k\n\n\uf8f9\n\n\uf8f6\n\n\uf8fb\n\n\uf8f8\n\ni=1\n\n\n\n(35)\n\n(36)\n\n(37)\n\n= O\n\n 2\nn\n\nEP,Q\n\nk(X1, X1) \u2212 \u2225\u00b5\u22252\n\nk\n\n\n\n\n= O\n\n\n\n.\n\n 1\nn\n\nk] with EP,Q[k(X1, X1)] and EP,Q[\u2225\u03bd2\u22252\n\nIn the above display:\n(35) bounds EP,Q[\u2225\u00b52\u22252\nthe same argument as in (32).\n(36) simply expands \u2225\u00b51\u22252\n(37) uses the fact that for l \u0338= i, we have EP,Q[\u27e8k(Xi, \u00b7), k(Xl, \u00b7)\u27e9kh] = 0.\nWe next show that the second term in (34) is O(1/n + 1/mn).\n\nk, and\n\nk] with EP,Q[k(Y1, Y1)] following\n\nEP,Q\n\n\u27e8\u00b5, g2\u27e92\n\nk\n\n \u2264 2EP,Q[\u2225\u00b5\u22252\n\nk] EP,Q\n 2\nn\n\n\n\n\u2225\u00b52\u22252\n\nk + \u2225\u03bd\u22252\nEP,Q[k(X1, X1) \u2212 \u2225\u00b5\u22252\n\nk\n\n\u2264 2EP,Q[\u2225\u00b5\u22252\nk]\n\nk] +\n\nEP,Q[k(Y1, Y2) \u2212 \u2225\u03bd\u22252\nk]\n\n\n\n2\nmn\n\n= O\n\n 1\nn\n\n+\n\n\n\n.\n\n1\nmn\n\nThus, since limn\u2192\u221e mn = \u221e, both the terms in (34) converge to 0 as n goes to infinity. This\ncompletes the proof that limn\u2192\u221e EP,Q[( \u00afUX \u2212 \u27e8\u00b5, \u00b5 \u2212 \u03bd\u27e9k)2] = 0. We can use the same arguments\nto show that limn\u2192\u221e EP,Q[( \u00afUY \u2212 \u27e8\u03bd, \u00b5 \u2212 \u03bd\u27e9k)2] = 0. Together, these two statements imply that\nlimn\u2192\u221e VP,Q(xMMD\n\n) = 0 following (33).\n\n2\n\nC.3 Proof of Theorem 9 (Type-I error control and consistency against local alternative)\n\nType-I error bound. To obtain the bound on the type-I error, we verify the conditions required\nby Theorem 15, by using the expressions for moments of the Gaussian kernel derived by Li and Yuan\n(2019), and recalled in Fact 14.\nFirst, we note that the scale parameters sn = n4/(d+4\u03b2), satisfies the property:\nsn\nn4/d\nIn other words, we have sn = o(n4/d). We now verify the required conditions:\n\nd+4\u03b2 ) = 0.\n\n= lim\nn\u2192\u221e\n\nlim\nn\u2192\u221e\n\nd (1\u2212 d\n\nn\u2212 4\n\n\u2022 Since we have assumed mn = n in this case, limn\u2192\u221e n/mn = 1 exists.\n\n25\n\n\f\u2022 For checking the condition on the eigenvalues, it suffices to show that\nEPn,Qn[EPn,Qn [\u00afk(X1, X2)\u00afk(X1, X3)|X2, X3]2]\nEPn,Qn[\u00afk(X1, X2)2]2\n\nlim\nn\u2192\u221e\n\n= 0,\n\nsince this is equivalent to limn\u2192\u221e\nand (13).\n\n\u03bb2\n1\nl \u03bb2\nl\n\n\n\n= 0. This result follows by a combination of (11)\n\n\u2022 We next check the condition (17). We do this in two steps. First we consider the term,\n\nEPn,Qn [\u00afkn(X1, X2)4]\nEPn,Qn[\u00afkn(X1, X2)2]2n2\n\n\u2272 s\u2212d/2\nn\n(s\u2212d/2\n)2\nn\n\n1\nn2 =\n\nsd/2\nn\nn2 \u2192 0,\n\nwhere the first inequality uses (11) and (12), while the last step uses the fact that sn =\no(n4/d). Next, we consider the quantity\nn(X1, X2)\u00afk2\n\nEPn,Qn[\u00afk2\n\nn(X1, X3)]\n\nd/4\n\n\u2272 1\nn\n\ns\u22123d/4\nn\n(s\u2212d/2\n)2\nn\n\n=\n\nsd/4\nn\nn\n\n=\n\n sn\nn4/d\n\n\u2192 0.\n\nnEPn,Qn [\u00afk2\n\nn(X1, X2)]2\n\nTogether with Theorem 15, the above conditions imply that the statistic \u00afxMMD\ncomputed using\nGaussian kernel with scale parameter sn = n4/(d+4\u03b2) has a standard normal null distribution\nuniformly over the class P (0)\nn . This implies the required result about asymptotic type-I error of the\nxMMD test \u03a8.\n\n2\n\nConsistency. To prove the consistency results, we verify that the sufficient conditions established\nby the general result, Theorem 8, are satisfied by the Gaussian kernel with scale parameter sn =\nn4/(d+4\u03b2).\nWe first check the condition on the variance of xMMD\n\n. Note that we have the following:\n\n2\n\n\u00afUX = \u27e8\u00b51, \u00b52 \u2212 \u03bd2\u27e9k = \u27e8\u00b51 + \u00b5, g2 + \u00b5 \u2212 \u03bd\u27e9k\n\n= \u27e8\u00b51, g2\u27e9k + \u27e8\u00b51, \u00b5 \u2212 \u03bd\u27e9k + \u27e8\u00b5, g2\u27e9k + \u27e8\u00b5, \u00b5 \u2212 \u03bd\u27e9k\n\nRecall that we use \u00b51 to denote \u00b51 \u2212 \u00b5, and similarly use \u00b52, \u03bd1, \u03bd2 and g2 to denote \u00b52 \u2212 \u00b5, \u03bd1 \u2212\n\u03bd, \u03bd2 \u2212 \u03bd and g2 \u2212 (\u00b5 \u2212 \u03bd) respectively. Similarly, on expanding the term \u00afUY , we get\n\u00afUY = \u27e8\u03bd1, \u00b52 \u2212 \u03bd2\u27e9k = \u27e8\u03bd1 + \u03bd, g2 + \u00b5 \u2212 \u03bd\u27e9k\n\n= \u27e8\u03bd1, g2\u27e9k + \u27e8\u03bd1, \u00b5 \u2212 \u03bd\u27e9k + \u27e8\u03bd, g2\u27e9k + \u27e8\u03bd, \u00b5 \u2212 \u03bd\u27e9k\n\n2\n\n2\n\nSince xMMD\n\n= \u00afUX \u2212 \u00afUY , we get that\nxMMD\nTherefore, the variance of xMMD\nxMMD\n= EPn,Qn\n\n2\n\nV\n\n\n\n2\n\nis\n\u27e8\u00b51 \u2212 \u03bd1, g2\u27e92\n\n= \u27e8\u00b51 \u2212 \u03bd1, g2\u27e9k + \u27e8\u00b51 \u2212 \u03bd1, \u00b5 \u2212 \u03bd\u27e9k + \u27e8\u00b5 \u2212 \u03bd, g2\u27e9k + \u03b32\nn.\n\nk + \u27e8\u00b51 \u2212 \u03bd1, \u00b5 \u2212 \u03bd\u27e92\n\nk + \u27e8\u00b5 \u2212 \u03bd, g2\u27e92\n\nk\n\n ,\n\n(38)\n\nsince all the cross terms are zero in expectation, due to the sample-splitting used in defining xMMD\nWe now obtain upper bounds on the three terms in the right-hand-side of (38).\n\n2\n\n.\n\nEPn,Qn\n\n\u27e8\u00b51 \u2212 \u03bd1, \u00b52 \u2212 \u03bd2\u27e92\n\nk\n\n \u2264 EPn,Qn\n\u2264 4 EPn,Qn\n\n\u2225\u00b51 \u2212 \u03bd1\u22252\n\u2225\u00b51\u22252\n\n EPn,Qn\n + EPn,Qn\n\nk\n\nk\n\n\u00d7 EPn,Qn\n\u2225\u00b52\u22252\n EPn,Qn[\u00afk(X, X)]\nn1\n\nk\n\n+\n\n= 4\n\n + EPn,Qn\n\nk\n\nk\n\n\u2225\u00b52 \u2212 \u03bd2\u22252\n\n\u2225\u03bd1\u22252\n\u2225\u03bd2\u22252\nEPn,Qn [\u00afk(Y, Y )]\nm1\n\n\n\nk\n\n\n\n\n\n\u00d7\n\n EPn,Qn [\u00afk(X, X)]\nn2\n\n+\n\nEPn,Qn [\u00afk(Y, Y )]\nm2\n\n\n\n\u2264\n\n32\nn2\n\n= O\n\nEPn,Qn [\u00afk(X, X)2] + EPn,Qn [\u00afk(Y, Y )2]\n\nM s\u2212d/2\nn\nn2\n\n\n\n.\n\n(39)\n\n(40)\n\n26\n\n\fIn the above display, (39) follows uses Jensen\u2019s inequality, while (40) uses the upper bound on\nthe second moment of \u00afk(X, X) and \u00afk(Y, Y ) derived by Li and Yuan (2019), and recalled in (11)\nof Fact 14. For the second term in (38), we proceed as follows:\n\nEPn,Qn\n\n\u27e8\u00b51 \u2212 \u03bd1, \u00b5 \u2212 \u03bd\u27e92\n\nk\n\n \u2264 2\u2225\u00b5 \u2212 \u03bd\u22252\n\u03b32\nn\nn\n\n\n\n\u2264\n\nk\n\nEPn,Qn\n\n\u2225\u00b51\u22252\n\n\n\nk + \u2225\u03bd1\u22252\n\n\nk\n\nEPn,Qn [\u00afk(X, X)2] +\n\nEPn,Qn [\u00afk(Y, Y )2]\n\n= O\n\n\n\nns\u2212d/4\n\u03b32\nn\nn\n\n\n\n.\n\nSimilarly, we can get the same bound on the third term of (38)\n\n\nEPn,Qn\n\n\u27e8\u00b52 \u2212 \u03bd2, \u00b5 \u2212 \u03bd\u27e92\n\nk\n\n = O\n\nns\u2212d/4\n\u03b32\nn\nn\n\n\n\n.\n\nThus, combining (40) (41) and (42), we get that\n\nsup\n(Pn,Qn)\u2208P (1)\n\nn\n\nVPn,Qn (xMMD\n\u03b34\nn\n\n2\n\n)\n\n\u2272 s\u2212d/2\nn\nn2\u03b34\nn\n\n=\n\nsd/2\nn\nn2\u22064\nn\n\n+\n\n+\n\ns\u2212d/4\nn\nn\u03b32\nn\n\n\u2272 s\u2212d/2\nn\nn2s\u2212d\nn \u22064\nn\n\n+\n\ns\u2212d/4\nn\nns\u2212d/2\nn \u22062\nn\n\nsd/4\nn\nn\u22062\nn\n\n.\n\n\n\n(41)\n\n(42)\n\n(43)\n\nThe second inequality in (43) uses (14) that says \u03b32\nn\nparameter sn \u224d n4/(d+4\u03b2), we get that\n\n\u2273 s\u2212d/2\n\nn \u22062\n\nn. Finally, using the fact that the scale\n\nlim\nn\u2192\u221e\n\nsup\n(Pn,Qn)\u2208P (1)\n\nn\n\n2\n\nVPn,Qn (xMMD\n\u03b34\nn\n\n)\n\n\n\n\u2272 lim\nn\u2192\u221e\n\n1\nn2\u03b2/(d+4\u03b2)\u2206n\n\n4 +\n\n1\nn2\u03b2/(d+4\u03b2)\u2206n\n\n2\n\n\n\n= 0,\n\nwhere the equality follows from the condition imposed on \u2206n in the statement of Theorem 9. Thus,\nwe have verified the condition on the variance of xMMD\nIt remains to verify the condition on the expected empirical variance in (7).\n\nas required by (7).\n\n2\n\nEPn,Qn\n\n\n\u03c32\n\nX\n\n = EPn,Qn\n\n\n\n1\nn1\n\nn1\n\n\n\ni=1\n\n\u27e8k(Xi, \u00b7), g2\u27e9k \u2212 \u27e8\u00b51, g2\u27e9k\n\n2\n\n\n\n \n\n\n\n1 \u2212\n\n1\nn1\n+ EPn,Qn\n EPn,Qn [\u00afk(X1, X1)]\nn2\n\n= EPn,Qn\n\n\u2264 EPn,Qn\n\n\n\u27e8k(X1, \u00b7), g2\u27e92\nk\n\n\u27e8k(X1, \u00b7), g2\u27e92\n\nk\n\n\u2264 EPn,Qn[\u00afk(X1, X1)]\n\n\u2272 s\u2212d/2\nn\nn\n\n+ \u03b32\n\nns\u2212d/4\nn\n\n.\n\n\n\u27e8k(X1, \u00b7), \u00b5 \u2212 \u03bd\u27e92\nk\n\n\n\n+\n\nEPn,Qn [\u00afk(Y1, Y1)]\nm2\n\n\n\n+ \u03b32\nn\n\nEPn,Qn [\u00afk(X1, X1)]\n\nSimilarly, we can get the same upper bound for the term EPn,Qn [\u03c32\nwe get that\n\nY ]. Since \u03c32 = n\u22121\n\n1 \u03c32\n\nX + m\u22121\n\n1 \u03c32\nY ,\n\nlim\nn\u2192\u221e\n\nsup\n(Pn,Qn)\u2208P (1)\n\nn\n\nEPn,Qn[\u03c32]\n\u03b34\nn\n\n\u2272 lim\nn\u2192\u221e\n\ns\u2212d/2\nn\nn2\u03b34\nn\n\n+\n\ns\u2212d/4\nn\nn\u03b32\nn\n\n.\n\nWe saw in (43) that this limit is equal to 0. Thus, the condition on \u03c3 as required by (7) is also satisfied\nfor sn \u224d n4/(d+4\u03b2). Hence, by an application of Theorem 8, the test \u03a8 with Gaussian kernel and sn \u224d\nn4/(d+4\u03b2) is consistent against the local alternatives with \u2206n satisfying limn\u2192\u221e \u2206nn2\u03b2/(d+4\u03b2) = \u221e.\nThis completes the proof.\n\n27\n\n\f\u00afU =\n\n1\nn1\n\n1\nm1\n\n\u03c32\nX =\n\n1\nn1\n\nn1\n\ni=1\n\nYj \u2208Y1\n\nXi\u2208X1\n\n\n\u00afq(Xi) \u2212\n\nD Gaussian Limit for General Two-Sample U-Statistic\n\nWe now generalize the asymptotic normality for kernel-MMD statistic stated in Theorem 15 to a\nlarger class of two-sample U-statistics. As before, given X = (X1, . . . , Xn) and Y = (Y1, . . . , Ym),\nwe consider the two-sample U-statistic with arbitrary kernel h defined as\n1\n\nn\n2\n\nh(Xi, Xi\u2032, Yj, Yj\u2032).\n\n1\nm\n2\n\nU =\n\n\n\n\n\n\n\ni\u2032<i\n\nj\u2032<j\n\nWe assume that h is a degenerate kernel, similar to the MMD case, and satisfies\n\nEP [h(X, x\u2032, Y, y\u2032)] = EP [h(x, X \u2032, y, Y \u2032)] = 0,\n\nwhen X, X \u2032, Y, Y \u2032 are i.i.d. random variables drawn from any distribution P .\nWith X1 = (X1, . . . , Xn1) and X2 = (Xn1+1, . . . , Xn) and Y1 = (Y1, . . . , Ym1 ) and Y2 =\n(Ym1+1, . . . , Ym), we introduce the following terms:\n\n\u03d5(x, y) :=\n\n1\nn2\n\n1\nm2\n\n\n\n\n\nXi\u2032 \u2208X2\n\nYj\u2032 \u2208Y2\n\nh(x, Xi\u2032, y, Yj\u2032), with n2 = n \u2212 n1, and m2 = m \u2212 m1(44)\n\nq(x1, x2, y2) := E[h(x1, x2, Y, y2)]\n\nand\n\n\u00afq(x) :=\n\nr(x2, y1, y2) := E[h(X, x2, y1, y2)]\n\nand\n\n\u00afr(y) :=\n\n1\nn2m2\n\n1\nn2m2\n\n\n\nXi\u2032 \u2208X2,Yj\u2032 \u2208Y2\n\n\nXi\u2032 \u2208X2,Yj\u2032 \u2208Y2\n\nq(x, Xi\u2032, Yj\u2032),\n\n(45)\n\nr(Xi\u2032, y, Yj\u2032).\n\n(46)\n\nUsing the above terms, we can now define the statistic T = \u00afU /\u03c3, with\n\n\n\n\n\n\u03d5(Xi, Yj),\n\nand \u03c32 = \u03c32\n\nX\nn1\n\n+ \u03c32\nY\nm1\n\n, where\n\n2\n\n\u00afq(Xl)\n\n,\n\n\u03c32\nY =\n\n1\nn1\n\nn1\n\nl=1\n\n1\nm1\n\nm1\n\nj=1\n\n\n\n\u00afr(Yj) \u2212\n\n2\n\n\u00afr(Yl)\n\n.\n\n1\nm1\n\nm1\n\nl=1\n\nn1\n\ni=1 \u00afq(Xi) is a centered analog of \u00afUX . Hence, the term \u03c32\n\nRemark 19. Note that the cross U-statistic written above corresponds exactly with the definition\nof the cross U-statistic for the kernel-MMD case in (2). To motivate the definitions of the em-\npirical variance terms, note that in the case of kernel-MMD statistic, we have h(x1, x2, y1, y2) =\n\u27e8k(x1, \u00b7) \u2212 k(y1, \u00b7), k(x2, \u00b7) \u2212 k(y2, \u00b7)\u27e9k. We can check that in this case, we have q(x1, x2, y2) =\n\u27e8k(x1, \u00b7), k(x1, \u00b7) \u2212 k(x2, \u00b7)\u27e9k. This implies that \u00afq(Xi) equals the term Wi introduced (18), and thus\n1\nX defined above reduces exactly to the\nn1\n\u03c32\nX introduced in (4).\nWe next state the assumptions required to show the limiting Gaussian distribution of the statistic T\nwhen X and Y are drawn independently from the same distribution.\nAssumption 2. Let (hn, Pn) be a sequence of kernel and probability distribution pairs, and let X\nand Y be two i.i.d. samples of sizes n and mn respectively, drawn independently from Pn. With \u03d5, \u00afqn\nand \u00afrn as defined in (44), (45) and (46) respectively, we assume the following are true:\n\nlim\nn\u2192\u221e\n\nEPn\n\n\n\nEPn [\u03d52(X1, Y1)|X2, Y2]\nmnEPn [\u00afq(X1)2|X2, Y2] + nEPn [\u00afr(Y1)2|X2, Y2]\n 1\nn\n\nEPn\n\nlim\nn\u2192\u221e\n\nEPn[\u00afq4(X1)|X2, Y2]\nEPn [\u00afq2(X1)|X2, Y2]2 +\nRemark 20. Note that\nin specific the case of kernel-MMD statistic, we can check that\nEPn [\u03d5(X1, Y1)2|X2, Y2] = EPn [\u00afq(X1)2 + \u00afr(Y1)2|X2, Y2]. Hence (47) always holds. The sec-\nond condition of Assumption 2, stated in (48), is a stronger version of the moment conditions used\nby Theorem 5 and Theorem 15.\n\nEPn [\u00afr4(Y1)|X2, Y2]\nEPn [\u00afr2(Y1)|X2, Y2]2\n\n1\nmn\n\n= 0.\n\n(48)\n\n\n\n\n\n= 0,\n\nand\n\n(47)\n\nWe now state the main result of this section.\n\n28\n\n\fTheorem 21. For every n \u2265 1, let X and Y denote independent samples of sizes n and mn\nrespectively, drawn from a distribution Pn. Suppose the sample-sizes are such that limn\u2192\u221e mn/n\nexists and is non-zero. Let (hn, Pn) denote a sequence satisfying the conditions of Assumption 2.\nThen, we have that\n\nlim\nn,m\u2192\u221e\n\nsup\nx\u2208R\n\n|PPn (T \u2264 x) \u2212 \u03a6(x)| = 0.\n\nD.1 Proof of Theorem 21\n\nBefore describing the details, we first present the outline of the proof.\n\n1. We first consider the standardized version of the statistic, defined as Ts = \u00afU /\u03c3P ,\nEPn [\u00afq(X1)2|X2, Y2] + m\u22121\nwhere \u03c32\nIn Lemma 22, we\n1\nshow that the difference between Ts and its projected variant, TP,s = \u00afUP /\u03c3P =\n\nn\u22121\n/\u03c3P , converges in probability to 0. Hence, we can\n1\n\nEPn [\u00afr(Y1)2|X2, Y2].\n\ni \u00afq(Xi) + m\u22121\n\n\nj \u00afr(Yj)\n\nP = n\u22121\n\n\n\n\n\n1\n\n1\n\nfocus on the term TP,s. This result uses the condition (47) of Assumption 2.\n\n2. We then show in Lemma 23, that the statistic TP,s converges in distribution to N (0, 1). This\n\ncombined with the previous result implies that Ts\n\nd\u2212\u2192 N (0, 1).\n\n3. To complete the proof, we show in Lemma 24, that the ratio of the empirical variance \u03c32\nand the conditional variance \u03c32\nP converge in probability to 1. This fact combined with\nthe continuous mapping theorem and Slutsky\u2019s theorem implies the result. The proof\nof Lemma 24 relies on the condition (48) of Assumption 2.\n\nWe now present the details of the steps outlined above.\nConsider the standardized statistic, Ts, defined as \u00afU /\u03c3P , where \u03c32\nP,X + m\u22121\nn\u22121\n1\nTP,s = \u00afUP\n\u03c3P\n\nEPn [\u00afq2(X1)|X2, Y2] + m\u22121\n1\n\nEPn [\u00afr2(Y1)|X2, Y2] := n\u22121\n\n1 \u03c32\n\n.\n\nP = VPn( \u00afUP |X2, Y2) =\n1 \u03c32\nP,Y . Introduce the term\n\nLemma 22. Under the conditions of Assumption 2, we have Tp \u2212 TP,s\n\np\n\n\u2212\u2192 0.\n\np\n\nProof. We first show that Ts \u2212 TP,s\n\u2212\u2192 0, conditioned on the second half of the observations,\n(X2, Y2). As a result of this, the conditional limiting distributions of the two random variables Ts\nand TP,s are the same. Since \u00afUP is the projection of \u00afU on the sum on independent (conditioned on\n(X2, Y2)) random variables, we have\nVPn(Ts \u2212 TP,S|X,Y2) = VPn(Ts|X,Y2) + VPn (TP,s|X,Y2) \u2212 2EPn [(TP,s + (Ts \u2212 TP,s)) TP,s|X2, Y2]\n= VPn(Ts|X,Y2) \u2212 VPn (TP,s|X,Y2) = VPn (Ts|X,Y2) \u2212 1,\n\nusing the fact that (Ts \u2212 TP,s) \u22a5 TP,s conditioned on (X2, Y2). Next, using the formula for the\nvariance of two-sample U-statistics, we have\n\u03c32\n1\nP,Y\nm1\nn1m1\nEPn [\u03d52(X1, X2)|X2, Y2]\n\u03c32\nP\n\n\u03d5(X1, X2)2|X2, Y2\n\nVPn (Ts|X2, Y2) =\n\n1\nn1m1\n\n\u03c32\nP,X\nn1\n\n= 1 +\n\n\n\n\nEPn\n\n/\u03c32\nP\n\n\n\n+\n\n+\n\n.\n\nThe result then follows by an application of the condition (47) of Assumption 2, and the fact that\nn1 = n/2 and m1 = mn/2.\n\nOur next result establishes the limiting distribution of the statistic TP,s.\n\nLemma 23. Under Assumption 2, we have TP,s\n\nd\u2212\u2192 N (0, 1).\n\nProof. Recall that TP,s = \u00afUP /\u03c3P , where \u00afUP := \u00afUP,X \u2212 \u00afUP,Y = 1\nn1\nand \u03c32\n\u00afUP,Y /\n\nP,Y . The result then follows in the following two steps:\n\nP = n\u22121\n\nm\u22121\n1 \u03c32\n\nIntroduce the terms TX = \u00afUP,X /\n\nP,X + m\u22121\n\n1 \u03c32\n\n1 \u03c32\n\nP,Y .\n\nn1\n\nm1\n\ni=1 \u00afq(Xi)\u2212 1\nj=1 \u00afr(Yj),\nm1\nn\u22121\n1 \u03c32\nP,X and TY =\n\n\n\n29\n\n\f\u2022 We first observe that TX and TY conditioned on (X2, Y2) converge in distribution to N (0, 1).\n\nThe result follows by applying Lindeberg\u2019s CLT.\n\n\u2022 Next, using the assumption that limn\u2192\u221e mn/n exists, and is non-zero, we next observe\nd\u2212\u2192 N (0, 1). The proof of this result follows from the same argument used\n\nthat TP,s\nin Lemma 17.\n\nTogether, the previous two lemmas imply that Ts\nto show that the ratio of the conditional variance \u03c32\nprobability to 1.\nLemma 24. Under Assumption 2, we have \u03c32\n\u03c32\nP\n\n\u2212\u2192 1.\n\np\n\nd\u2212\u2192 N (0, 1). To complete the proof, we need\nP , and the empirical variance \u03c32 converge in\n\nProof. We begin by noting the following\nn\u22121\n1\n\n\n\n\u03c32\n\u03c32\nP\n\n\u2212 1 =\n\nX \u2212 \u03c32\n\u03c32\n\nP,X\n\n\n\nY \u2212 \u03c32\n\u03c32\n\nP,Y\n\n\n\n1\n\n m\u22121\n\u03c32\nP\n\u03c32\nY\n\u03c32\n\nP,Y\n\n\n\n\n\u2212 1\n\n\n\n\u2264\n\n\n\n\n\n\n\nX\n\n\u03c32\n\u03c32\n\nP,X\n\n\n\n\n\u2212 1\n\n\n\n+\n\n\n\n\n\n\n\n.\n\n(49)\n\nThus it suffices to show that the two terms in (49) converge in probability to 0. Since n1/(n1 \u2212 1)\nconverges to 1, it suffices to consider\n\n(n1 \u2212 1)\u22121 n1\n\ni=1\n\nE :=\n\n\u00afq(Xi) \u2212 \u00afUP,X\nEPn [\u00afq2(X1)|X2, Y2]\n\n2\n\n\u2212 EPn[\u00afq(X1)|X2, Y2]\n\n.\n\nFirst note that EPn [E|X2, Y2] = 0. Hence, its variance can be written as\n\nVPn (E) = EPn [VPn (E|X2, Y2)] \u2264\n\n1\nn1\n\nEPn\n\n EPn[\u00afq4(X1)|X2, Y2]\nEPn [\u00afq2(X1)|X2, Y2]2\n\n\n\n.\n\n(50)\n\nThe last term in (50) converges to 0 by Assumption 2, implying that \u03c32\nconverges in the second\n\u03c32\nmoment to 1, which in turn implies their convergence in probability to 1. Following the same\narguments, we can also show that \u03c32\nY\n\u03c32\n\nalso converge in probability to 1, as required.\n\nP,X\n\nX\n\nP,Y\n\nE Additional Experiments\n\nComputing Infrastructure. All the experiments were performed on a workstation with Intel(R)\nCore(TM) i7-9700K CPU 3.60GHz and 32 GB of RAM with an NVIDIA GTX 1080 GPU.\n\nE.1 Implementation details of experiments reported in the main text\n\nDetails for Figure 1. For the null distribution, we set n = 500 and m = 625 and generated both X\nand Y from N (0, Id) for d = 10 and 100. In both cases, we computed the \u00afxMMD\nstatistic 2000\ntimes to plot the histogram.\n\n2\n\nFor the second figure, we obtain the power curves for the xMMD test and the MMD test with 200\npermutations for testing P = N (0, Id) againt Q = N (a\u03f5,j, Id). Here d = 10, j = 5 and \u03f5 = 0.2,\nand recall that a\u03f5,j is the vector in Rd obtained by setting the first j \u2264 d coordinates of 0 equal\nto \u03f5. We selected n and m from 20 equally spaced points in the intervals [10, 400] and [10, 500]\nrespectively, and ran 200 trials of the tests for every (n, m) pair to obtain the power curves. The error\nregions in the figure correspond to one bootstrap standard deviation with 200 bootstrap samples.\n\nFor the third figure, we set d = 100, j = 20, \u03f5 = 0.1, P = N (0, Id) and Q = N (a\u03f5,j, Id). We ran\nthe two tests, xMMD and MMD with 200 permutations, for 20 different (n, m) pairs in the range\n[10, 500], and repeated the experiment 200 times for every such pair. The figure plots the wall-clock\ntime, measure by Python\u2019s time.time() function, and plot the power against the average wall-clock\ntime over the 200 trials. The size of the marker is proportional to the sample size (i.e., n + m).\n\n30\n\n\fDetails for Figure 3. The two kernels used in this figure are the Gaussian and Quadratic kernels.\nThe Gaussian kernel with scale parameter s > 0 is defined as ks(x, y) = exp(\u2212s\u2225x \u2212 y\u22252\n2), while\nthe Quadratic kernel with scale s > 0 is defined as kQ(x, y) = 1 + s(xT y)2\n. With w denoting the\nmedian of the pairwise distance between all the observations, we set s = 1/(2w2) for the Gaussian\nkernel and s = 1/w for the Quadratic kernel.\n\nDetails for Figure 4. Given observations X1, X2, . . . , Xni.i.d.P , consider the problem of one-\nsample mean-testing, that is, testing H0 : E[Xi] = 0 versus H1 : E[Xi] = a \u0338= 0. When the distribu-\ntion P is a multivariate Gaussian, Kim and Ramdas (2020) showed that power of their test using a\n\n\n\n\none-sample studentized U-statistic based on a bi-linear kernel is asymptotically \u03a6\n\nz\u03b1 + aT a\n\u221a\n2\n\ntr(\u03a32)\n\n.\n\nThe power achieved by the test using the full U-statistic is \u03a6\n\n, which differs from\n\nthe previous expression by a factor of\ncovariance testing. Our heuristic in (9) is based on these two observations.\n\n2. A similar relation also holds for the problem of Gaussian\n\n\u221a\n\n\n\nz\u03b1 + aT a\u221a\n\n2tr(\u03a32)\n\n\n\nDetails for Figure 5. For plotting the ROC curves, we proceed as follows. We fix n = m =\n200, and then compute the MMD, block-MMD, linear-MMD and cross-MMD statistics for 1000\nindependent repetitions of \u2018null\u2019 and \u2018alternative\u2019 trials. For every null trial, we calculate all the\nstatistics on independent samples of sizes n and m drawn from P = N (0, Id), while for every\nalternative trial we calculate the statistics on independent samples of size n and m drawn from\nP = N (0, Id) and Q = N (a\u03f5,j, Id) respectively. Recall that a\u03f5,j is obtained by setting the first j\ncoordinates of 0 equal to \u03f5. Having obtained 2000 values for every statistic, we then plot the tradeoff\nbetween false positives (FP) and true positives (TP) as the rejection threshold is increased. The ability\nof a statistic to distinguish between the null and the alternative is quantified by the area under the\ncurve. In Figure 5, we used (d, j, \u03f5) \u2208 {(10, 5, 0.1), (100, 20, 0.1), (500, 100, 0.1)}.\n\nE.2 Additional Figures\n\nNull Distribution. Figure 6 denotes the null distribution of our proposed statistic (\u00afxMMD\n) along\nwith that of the usual MMD normalized by its empirical standard deviation. The null distribution\nin Figure 6 is Dirichlet with parameter 2 \u00d7 1 \u2208 Rd for d \u2208 {10, 500}.\n\n2\n\nPower Curves.\nIn Figure 7, we plot the power curves for the different tests using a Gaussian\nKernel, and we report the results of the same experiment with a polynomial kernel of degree 5\nin Figure 8. Recall that the polynomial kernel of degree r and scale parameter s > 0 is defined as\nk(x, y) = 1 + (xT y)/sr\n. In both instances, we selected the scale parameter using the median\nheuristic.\n\nFrom the figure, we can see that the xMMD test is competitive with the computationally more\ncostly tests, namely the MMD permutation test and the MMD-spectral test of Gretton et al. (2009).\nFurthermore, the performance of xMMD test is significantly better than the existing computationally\nefficient tests, namely block-MMD test (with block-size\n\nn) and linear-MMD test.\n\n\u221a\n\nROC curves.\nIn Figure 9, we plot some additional ROC curves for the different statistics. As\nbefore, we used 1000 \u2018null trials\u2019 and another 1000 \u2019alternative trials\u2019 with sample sizes n = 200 and\nm = 200. The data generating distributions P and Q were both Dirichlet with parameters 1 \u2208 Rd\nand (1 + \u03f5) \u00d7 1 \u2208 Rd for (d, \u03f5) \u2208 {(10, 0.4), (100, 0.2), (500, 0.15)}.\n\nE.3 Comparison with ME and SCF tests of Jitkrittum et al. (2016)\n\nWe now present some experimental results comparing the performance of our cross-MDD test with\nthe linear time mean embedding (MD) and smoothed characteristic function (SCF) tests of Jitkrittum\net al. (2016). These tests proceed in the following steps:\n\n\u2022 Fix J, and choose points {v1, . . . , vJ } from Rd, where d is the dimension of the observation\n\nspace.\n\n31\n\n\f2\n\nMMD\n\nFigure 6: The first two columns show the null distribution of the \u00afxMMD\nstatistic scaled by its empirical standard deviation (bottom row) using the Gaussian\nthe\nkernel with scale-parameter chosen using the median heuristic. The last two columns show the null\ndistribution for the two statistics using the Polynomial kernel of degree 5 with scale parameter chosen\n\nstatistic (top row) and\n\n2\n\nusing the median heuristic. The figures demonstrate that the null distribution of\nchanges\nsignificantly with dimension (d), the ratio n/m and the choice of the kernel, unlike our proposed\nstatistic.\n\n2\n\nMMD\n\nFigure 7: Power Curves for the different tests using Gaussian kernel with scale parameter chosen\nvia median heuristic. The two distributions are P = N (0, Id) and Q = N (a\u03f5,j, Id) where a\u03f5,j\nis obtained by setting the first j \u2264 d coordinates of 0 \u2208 Rd equal to \u03f5. The figures demonstrate\nthat the xMMD test is competitive with more computationally expensive tests (MMD-perm and\nMMD-spectral), while performing significantly better than the low complexity alternatives (B-MMD\nand L-MMD). The batch-size used in the B-MMD test was\n\nn.\n\n\u221a\n\n\u2022 Using X and Y with n = m, compute {zi\n\n[k(vJ , Xi) \u2212 k(vJ , Yi)]J\n\u02c6l(Yi) sin(Y T\nfine \u00afzn = 1\nn\n\ni vj), \u02c6l(Xi) cos(X T\nn\n\ni=1 zi, and Sn = 1\n\nj=1 \u2208 RJ for ME test, and zi = [\u02c6l(Xi) sin(X T\ni vj)]J\nn\u22121 (zi \u2212 \u00afzn)(zi \u2212 \u00afzn)T .\n\n: 1 \u2264 i \u2264 n}, where zi =\ni vj \u2212\nj=1 \u2208 R2J for the SCF test. De-\n\ni vj) \u2212 \u02c6l(Yi) cos(Y T\n\n\u2022 Using the above, define the test statistic\n\u02c6\u03bbn := \u00afzT\n\nn (Sn + \u03b3nI)\u22121 \u00afzn,\n\nwhere \u03b3n is some regularization parameter that converges to 0 with n, and I denotes the\nidentity matrix. For a fixed d and J, Jitkrittum et al. (2016) show that the above statistic has\n\n32\n\nProbabilitydensityxMMD(n/m=5)d=10d=100N(0,1)xMMD(n/m=1)xMMD(n/m=0.2)xMMD(n/m=1)ProbabilitydensityMMD(n/m=5)MMD(n/m=1)MMD(n/m=0.2)MMD(n/m=1)020040060080000.51Sample-Size(n+m)Power(d=10,j=5,\u03f5=0.2)MMD-permxMMDpredictedMMD-spectralB-MMDL-MMD020040060080000.51Sample-Size(n+m)Power(d=50,j=15,\u03f5=0.125)020040060080000.51Sample-Size(n+m)Power(d=100,j=20,\u03f5=0.15)\fFigure 8: Power curves of the different kernel-based tests using a polynomial kernel of degree 5, i.e.,\nk(x, y) = 1 + (xT y)/s5\n\nwith s chosen via the median heuristic.\n\nFigure 9: ROC curves using the different statistics with Gaussian kernel for testing two Dirichlet\ndistributions in dimensions d \u2208 {10, 100, 500} with sample-size n = m = 200. The two distributions\nare P = Dirichlet(1) and Q = Dirichlet((1 + \u03f5) \u00d7 1) where 1 \u2208 Rd is the all-ones vector.\n\na \u03c72(J) (resp. \u03c72(2J)) limiting null distribution in the ME (resp. SCF) case. This result is\nused to calibrate the test at a given level \u03b1.\n\nIn Figure 10, we plot the variation of type-I error and power with sample-size of the three tests for\nthe Gaussian Mean Difference (GMD) source with d = 10. As the figures suggest, the cross-MMD\nachieves higher power and tighter control over the type-I error than the ME and SCF tests in this\nregime.\n\nThe ME and SCF tests are calibrated based on the limiting distribution of their statistic in the low\ndimensional regime: fixed d, and n \u2192 \u221e. However, the high type-I error of these tests for small n\nvalues suggests that their limiting distribution may be different in the high dimensional regime, when\nboth d and n go to infinity. We further observe this in Figure 11 when d = 100 and d/n > 1.\n\nWe end this section with a discussion of some key points of difference between the ME and SCF tests,\nand our proposed cross-MMD test.\n\n\u2022 The ME and SCF tests require the kernel to be uniformly bounded, whereas our test requires\nonly mild moment conditions that are even satisfied by unbounded kernels if the underlying\ndistributions are not too heavy-tailed (formally described in Assumption 1). Furthermore,\nthe ME and SCF tests have several tuning parameters: number of features J, {v1, . . . , vJ },\nbandwidth, step-size for gradient ascent etc. In practice, J is usually set to 5, and the other\nparameters are selected by solving a Jd + 1 dimensional optimization problem via gradient\nascent. While each step of gradient ascent has linear in n complexity, the number of steps\nneeded may be large for higher dimensions, resulting in a higher computational overhead.\n\n33\n\n10020030000.51Sample-Size(n+m)Power(d=10,j=5,\u03f5=0.3)MMD-permxMMDpredictedMMD-spectralB-MMDL-MMD10020030000.51Sample-Size(n+m)Power(d=50,j=5,\u03f5=0.4)10020030000.51Sample-Size(n+m)Power(d=100,j=10,\u03f5=0.5)00.5100.51FalsePositiveRateTruePositiveRateDirichlet(d=10,\u03f5=0.4)MMDxMMDB-MMD(n1/2)B-MMD(n1/3)L-MMD00.5100.51FalsePositiveRateTruePositiveRateDirichlet(d=100,\u03f5=0.2))00.5100.51FalsePositiveRateTruePositiveRateDirichlet(d=500,\u03f5=0.15)\fGMD Source: (d=10, 200 trials)\n\nx-MMD\nME\nSCF\n\nr\no\nr\nr\ne\n\nI\n-\ne\np\ny\nT\n\n0.6\n\n0.4\n\n0.2\n\n0\n\n50\n\n100\n\n150\nSample-Size (n+m)\n\n200\n\nFigure 10: The figures plot the variation of the type-I error (left) and the power (right) with sample-\nsize of the three tests: cross-MMD, and the two linear time tests, ME and SCF, proposed by Jitkrittum\net al. (2016).\n\nGMD Source: (d=100, 200 trials)\n\nx-MMD\nME\nSCF\n\n0.8\n\n0.6\n\n0.4\n\n0.2\n\nr\no\nr\nr\ne\n\nI\n-\ne\np\ny\nT\n\n40\n\n50\n\n60\n80\n70\nSample-Size (n+m)\n\n90\n\n100\n\nFigure 11: The ME and SCF tests provide poor control over the type-I error in the regime when d/n\nis large, suggesting that the limiting null distribution is different (or the convergence rate is slow) in\nthis regime.\n\n\u2022 More importantly, the ME and SCF tests are only valid in the \u2018low-dimensional setting\u2019:\nfixed d and J, with n \u2192 \u221e. In the high dimensional setting, when (d, n) \u2192 \u221e, the limiting\nnull distribution may no longer be \u03c72(J). This is also suggested by the behavior of type-I\nerror of ME and SCF tests in Figure 10 and Figure 11. This results in the following practical\nissue: given a problem with n = 500 and d = 200, how should one calibrate the threshold\nfor those tests?\nOur proposed test does not suffer from this, because in both high and low dimensional\nsettings, our statistic has the same limiting distribution. This is a significant practical\nadvantage of our cross-MMD test over ME and SCF tests.\n\n\u2022 In the regime where the number of features, J, is allowed to increase with n, we expect that\nthe resulting ME and SCF tests may have low power (for small regularization parameter \u03b3n).\nThis is because, the test statistic \u02c6\u03bbn used by ME and SCF tests is similar to Hotelling\u2019s T 2\n\n34\n\n1002003004000.40.60.81Sample-Size(n+m)PowerGMDSource:(\u03f5=1.0,d=10)x-MMDMESCF\fstatistic, for which Bai and Saranadasa (1996) characterized the asymptotic power in this\nregime. In particular, their Theorem 2.1 implies that the power of the T 2 test grows slowly\nwith n, especially when J/n \u2248 1.\nFinally, we note that our ideas also extend to more general degenerate U-statistics (as\ndiscussed in Appendix D.1). Hence, they are also applicable in cases beyond MMD distance,\nwhere we may not have good linear time alternatives.\n\nE.4 Type-I Error and goodness-of-fit test of null distribution\n\nIn this section, we experimentally verify the limiting Gaussian distribution of the \u00afxMMD\nstatistic\nunder the null. We first plot the variation of the type-I error of our cross-MMD test with sample\nsize in Figure 12. We considered the case when X and Y are both drawn i.i.d. from a multivariate\nGaussian vector in dimension d \u2208 {10, 100}, and n = m.\n\n2\n\nFigure 12: The two figures show the variation of the type-I error of the cross-MMD test with sample-\nsize for dimensions d \u2208 {10, 100}. The dashed horizontal line denotes the level \u03b1 = 0.05. In\nsummary, these tests do not find evidence against the null hypothesis that the null distribution is\nGaussian.\n\nNext, we plot the p-values for the test for normality proposed by D\u2019Agostino and Pearson (1973),\nand implemented in the function scipy.stats.normaltest in Python. We performed this test\nat different sample-sizes (n), and for each value of n, we calculated the \u00afxMMD\nstatistic on 200\ndifferent indpendent sample pairs. The results are shown in Figure 13\n\n2\n\nFigure 13: The two figures show p-values for the test for normality proposed by D\u2019Agostino and\nPearson (1973) (using the implementation scipy.stats.normaltest) of the cross-MMD statistic\nfor dimensions d \u2208 {10, 100}. In both dimension regimes, the test does not find evidence against the\nnull that the cross-MMD statistic is normally distributed under the null.\n\n35\n\n\fE.5 Comparison with Friedman-Rafsky test\n\nWe now compare the performance of our cross-MMD test with the Friedman-Rafsky two-sample test.\nThis test, proposed by Friedman and Rafsky (1979), uses a graph-based statistic that is a multivariate\ngeneralization of the Wald-Wolfowitz runs statistic introduced by Wald and Wolfowitz (1940). This\nstatistic, denoted by R, is constructed as follows:\n\n\u2022 Pool the samples X and Y to get Z of size N = n + m. Construct the complete graph with\n\nN nodes, and edge weights equal to the euclidean distance between two end points.\n\n\u2022 Construct the minimal spanning tree (MST) of the complete graph G, and denote the 0-1\n\nvalued adjacency matrix of this MST by M .\n\n\u2022 The statistic R is defined as one more than the number of edges in M with endpoints from\n\ndifferent samples.\n\nThe statistic R is expected to take a large value under the null when X and Y are drawn from the\nsame distribution. Hence, the FR test rejects the null for small values of R. The rejection threshold\ncan be obtained either by the limiting distribution of R characterized by (Henze and Penrose, 1999,\nTheorem 1), or using the permutation-test.\n\nIn Figure 14, we compare the power of the FR permutation-test with our cross-MMD test in a low\ndimensional (d/n small) and a high dimensional (d/n large) problem. In both cases, it is observed\nthat the power of FR test is significantly smaller than that of cross-MMD test.\n\nGMD Source (\u03f5 = 1.2, d = 10)\n\nxMMD\nFR\n\n1\n\n0.8\n\n0.6\n\n0.4\n\n0.2\n\nr\ne\nw\no\nP\n\n50\n\n100\n\n150\nSample-Size (n+m)\n\n200\n\nFigure 14: The figures show the power curves for Friedman-Rafsky (FR) test and our cross-MMD\ntest in the low (d = 10) and high (d = 100) dimensional settings with m = n in both plots. The\nfigures indicate that our cross-MMD test is significantly more powerful ", "appendix_A": "\n\nB.2 Fixed P , changing kn (Theorem 5)\n\nWe note that the statement of Theorem 5 requires an additional technical assumption on the eigen-\nvalues of the kernel operator, introduced in (15). We repeat the statement of Theorem 5 with this\nadditional requirement below.\nTheorem 5\u2019. Suppose P is fixed, but the kernel kn changes with n. If\n\nlim\nn\u2192\u221e\n\nEP [\u00afkn(X1, X2)4]\nEP [\u00afkn(X1, X2)2]2\n\n 1\nn\n\n+\n\n1\nmn\n\n\n\n= 0,\n\nand\n\nlim\nn\u2192\u221e\n\n\u03bb2\n1,n\n\u221e\nl=1 \u03bb2\nl,n\n\nexists,\n\n(22)\n\nthen we have \u00afxMMD\n\n2\n\nd\u2212\u2192 N (0, 1).\n\nProof. The proof of this statement will follow the general outline of the proof of Theorem 15.\nHowever, in this special case when P is fixed, we can remove the condition that limn\u2192\u221e mn/n\nexists and is non-zero, that is required by Theorem 15.\n\nWe will carry over the notations used in the proof of Theorem 15, and in particular, we will use\nn1\n\u00afUX = 1\nj=1 Zj. Since Wi and Zj are identically distributed under\nn1\nthe null, we have EP [W 2\n2 to denote this conditional\nvariance. Then, note the following:\n\ni=1 Wi and \u00afUY = 1\nm1\ni |X2, Y2] = EP [Z 2\n\nj |X2, Y2], and we will use \u03c32\n\nm1\n\n\u00afxMMD\n\n2\n\n=\n\n\u00afUX \u2212 \u00afUY\n\u03c3\n\n=\n\n\u03c32\n\n:= T1 \u00d7 T2.\n\n\u00afUX \u2212 \u00afUY\n\n\n\n1 + m\u22121\nn\u22121\n1\n\n\n\n\u03c32\n\n1 + m\u22121\nn\u22121\n1\n\n\n\n \u00d7\n\n\u03c3\n\n(23)\n\nTo complete the proof, we will show that T1\nan application of Slutsky\u2019s theorem.\n\nd\u2212\u2192 N (0, 1) and T2\n\np\n\n\u2212\u2192 1. The result then follows by\n\n21\n\n\fFirst, we consider the term T1 in (23). Let Wi := Wi/\u03c32 and Zj := Zj/\u03c32. Then, conditioned on\n(X2, Y2), the terms Wi and Zj are independent and identically distributed. Introducing the constants\nui =\n\n m1\n\n n1\n\nm1(m1+n1) , we can write\n\nn1(m1+n1) and vj =\n\nT1 =\n\nn1\n\ni=1\n\nuiWi \u2212\n\nm1\n\nj=1\n\nvj Zj.\n\nWe can check that the constants (ui) and (vj) satisfy the property:\n\nlim\nn\u2192\u221e\n\nmax\ni,j\n\ni + v2\nu2\nj\ni\u2032 + m1\ni\u2032=1 u2\n\nn1\n\nj\u2032=1 v2\nj\u2032\n\n\u2264 lim\nn\u2192\u221e\n\nmax\ni,j\n\n1\nm1\n\n+\n\n1\nn1\n\n= 0.\n\nthis also means that\nthat is,\n\nd\u2212\u2192 N (0, 1) conditioned\nSince the limiting distribution (in this case, standard normal) is continu-\nthe T1 converges to N (0, 1) in the Kolmogorov-Smirnov met-\np\n\u2212\u2192 0. Since the random vari-\nis bounded, convergence in probability implies\nlimn\u2192\u221e EP [supx\u2208R |PP (T1 \u2264 x|X2, Y2) \u2212 \u03a6(x)|] = 0, which in turn implies that\n\nThus, by an application of Lindeberg\u2019s CLT, we note that T1\non (X2, Y2).\nous,\nric,\nable supx\u2208R |PP (T1 \u2264 x|X2, Y2) \u2212 \u03a6(x)|\nthat\nlimn\u2192\u221e supx\u2208R |EP [PP (T1 \u2264 x|X2, Y2) \u2212 \u03a6(x)] | = 0, as required.\n\nlimn\u2192\u221e supx\u2208R |PP (T1 \u2264 x|X2, Y2) \u2212 \u03a6(x)|\n\nWe now consider the second term, T2, in (23). It remains to show that T2\n1/T 2\n\np\n\n2 \u2212 1\n\n\u2212\u2192 0, and the result will follow by an application of the continuous mapping theorem.\n\n\n\n \u2212 1\n\n\n\n\n+ \u03c32\nY\nm1\n+ 1\nm1\n\n\u03c32\nX\nn1\n 1\nn1\n\n\n\n\u2212 1\n\n\n\n\n\n\u2212 1\n\n\n\n\u03c32\nX\n\u03c32\n2\n\n\u03c32\nY\n\u03c32\n2\n\n1\nT 2\n2\n\n\u2212 1\n\n\n\n\n\n\n\n\n\u03c32\n2\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n+\n\n\u2264\n\n=\n\n.\n\n(24)\n\np\n\n\u2212\u2192 1. We will show that\n\nThus, it suffices to show that both terms in (24) converge in probability to 0. This is exactly the\nresult that is proved in Lemma 18 under the two conditions listed in Assumption 1. The condition\non eigenvalues is already assumed in the statement of Theorem 5\u2019, and thus we will show that the\ncondition on the kernels, stated in (22), implies the condition (17). To prove this, we first, we note\nthat\n\nEP\n\n\u00afkn(X1, X2)2\u00afkn(X1, X3)2 \u2264 EP\n= EP\n\n\u00afkn(X1, X2)41/2 EP\n\u00afkn(X1, X2)4 .\n\n\u00afkn(X1, X3)41/2\n\nThus, the term in (17) is upper bounded by\n\u00afkn(X1, X2)4\n\u00afkn(X1, X2)22\n\nEP\nEP\n\n 1\nn\n\n+\n\n1\nmn\n\n \n\n1 +\n\n1\nn\n\n+\n\n\n\n.\n\n1\nmn\n\nSince, we have assumed that limn\u2192\u221e mn \u2192 \u221e, there exists and n0, such that for all n \u2265 n0,\n1 + 1\n\u2264 2. This implies that if (22) is satisfied, then (17) in Assumption 1 is also satisfied, as\nrequired.\n\nn + 1\nmn\n\nB.3 Fixed k, and fixed P (Theorem 4)\n\nWe prove Theorem 4 by showing that under the bounded fourth moment assumption on \u00afk, both the\nconditions required by Theorem 5\u2019 are satisfied.\nNote that since EP [\u00afk(X1, X2)] = 0, the positive and finite fourth moment also implies that the\nsecond moment of \u00afk(X1, X2) is also positive and finite. Hence, we have that\n\nThis, in turn, implies\n\nEP [\u00afk(X1, X2)4]\nEP [\u00afk(X1, X2)2]2\n\n< \u221e.\n\nlim\nn\u2192\u221e\n\nEP [\u00afk(X1, X2)4]\nEP [\u00afk(X1, X2)2]2\n\n 1\nn\n\n+\n\n1\nmn\n\n\n\n= 0,\n\n22\n\n\fas required by Theorem 5.\n\nFor the second part of the condition, we note that as kernel k and probability distribution P are\n\u03bb2\ndoesn\u2019t change with n, and hence its limit exists. Thus, both the conditions\nfixed, the term\n1\nl \u03bb2\nl\nfor Theorem 5\u2019 are satisfied, as required.\n\n\n\n", "appendix_E": "\n\nComputing Infrastructure. All the experiments were performed on a workstation with Intel(R)\nCore(TM) i7-9700K CPU 3.60GHz and 32 GB of RAM with an NVIDIA GTX 1080 GPU.\n\nE.1 Implementation details of experiments reported in the main text\n\nDetails for Figure 1. For the null distribution, we set n = 500 and m = 625 and generated both X\nand Y from N (0, Id) for d = 10 and 100. In both cases, we computed the \u00afxMMD\nstatistic 2000\ntimes to plot the histogram.\n\n2\n\nFor the second figure, we obtain the power curves for the xMMD test and the MMD test with 200\npermutations for testing P = N (0, Id) againt Q = N (a\u03f5,j, Id). Here d = 10, j = 5 and \u03f5 = 0.2,\nand recall that a\u03f5,j is the vector in Rd obtained by setting the first j \u2264 d coordinates of 0 equal\nto \u03f5. We selected n and m from 20 equally spaced points in the intervals [10, 400] and [10, 500]\nrespectively, and ran 200 trials of the tests for every (n, m) pair to obtain the power curves. The error\nregions in the figure correspond to one bootstrap standard deviation with 200 bootstrap samples.\n\nFor the third figure, we set d = 100, j = 20, \u03f5 = 0.1, P = N (0, Id) and Q = N (a\u03f5,j, Id). We ran\nthe two tests, xMMD and MMD with 200 permutations, for 20 different (n, m) pairs in the range\n[10, 500], and repeated the experiment 200 times for every such pair. The figure plots the wall-clock\ntime, measure by Python\u2019s time.time() function, and plot the power against the average wall-clock\ntime over the 200 trials. The size of the marker is proportional to the sample size (i.e., n + m).\n\n30\n\n\fDetails for Figure 3. The two kernels used in this figure are the Gaussian and Quadratic kernels.\nThe Gaussian kernel with scale parameter s > 0 is defined as ks(x, y) = exp(\u2212s\u2225x \u2212 y\u22252\n2), while\nthe Quadratic kernel with scale s > 0 is defined as kQ(x, y) = 1 + s(xT y)2\n. With w denoting the\nmedian of the pairwise distance between all the observations, we set s = 1/(2w2) for the Gaussian\nkernel and s = 1/w for the Quadratic kernel.\n\nDetails for Figure 4. Given observations X1, X2, . . . , Xni.i.d.P , consider the problem of one-\nsample mean-testing, that is, testing H0 : E[Xi] = 0 versus H1 : E[Xi] = a \u0338= 0. When the distribu-\ntion P is a multivariate Gaussian, Kim and Ramdas (2020) showed that power of their test using a\n\n\n\n\none-sample studentized U-statistic based on a bi-linear kernel is asymptotically \u03a6\n\nz\u03b1 + aT a\n\u221a\n2\n\ntr(\u03a32)\n\n.\n\nThe power achieved by the test using the full U-statistic is \u03a6\n\n, which differs from\n\nthe previous expression by a factor of\ncovariance testing. Our heuristic in (9) is based on these two observations.\n\n2. A similar relation also holds for the problem of Gaussian\n\n\u221a\n\n\n\nz\u03b1 + aT a\u221a\n\n2tr(\u03a32)\n\n\n\nDetails for Figure 5. For plotting the ROC curves, we proceed as follows. We fix n = m =\n200, and then compute the MMD, block-MMD, linear-MMD and cross-MMD statistics for 1000\nindependent repetitions of \u2018null\u2019 and \u2018alternative\u2019 trials. For every null trial, we calculate all the\nstatistics on independent samples of sizes n and m drawn from P = N (0, Id), while for every\nalternative trial we calculate the statistics on independent samples of size n and m drawn from\nP = N (0, Id) and Q = N (a\u03f5,j, Id) respectively. Recall that a\u03f5,j is obtained by setting the first j\ncoordinates of 0 equal to \u03f5. Having obtained 2000 values for every statistic, we then plot the tradeoff\nbetween false positives (FP) and true positives (TP) as the rejection threshold is increased. The ability\nof a statistic to distinguish between the null and the alternative is quantified by the area under the\ncurve. In Figure 5, we used (d, j, \u03f5) \u2208 {(10, 5, 0.1), (100, 20, 0.1), (500, 100, 0.1)}.\n\nE.2 Additional Figures\n\nNull Distribution. Figure 6 denotes the null distribution of our proposed statistic (\u00afxMMD\n) along\nwith that of the usual MMD normalized by its empirical standard deviation. The null distribution\nin Figure 6 is Dirichlet with parameter 2 \u00d7 1 \u2208 Rd for d \u2208 {10, 500}.\n\n2\n\nPower Curves.\nIn Figure 7, we plot the power curves for the different tests using a Gaussian\nKernel, and we report the results of the same experiment with a polynomial kernel of degree 5\nin Figure 8. Recall that the polynomial kernel of degree r and scale parameter s > 0 is defined as\nk(x, y) = 1 + (xT y)/sr\n. In both instances, we selected the scale parameter using the median\nheuristic.\n\nFrom the figure, we can see that the xMMD test is competitive with the computationally more\ncostly tests, namely the MMD permutation test and the MMD-spectral test of Gretton et al. (2009).\nFurthermore, the performance of xMMD test is significantly better than the existing computationally\nefficient tests, namely block-MMD test (with block-size\n\nn) and linear-MMD test.\n\n\u221a\n\nROC curves.\nIn Figure 9, we plot some additional ROC curves for the different statistics. As\nbefore, we used 1000 \u2018null trials\u2019 and another 1000 \u2019alternative trials\u2019 with sample sizes n = 200 and\nm = 200. The data generating distributions P and Q were both Dirichlet with parameters 1 \u2208 Rd\nand (1 + \u03f5) \u00d7 1 \u2208 Rd for (d, \u03f5) \u2208 {(10, 0.4), (100, 0.2), (500, 0.15)}.\n\nE.3 Comparison with ME and SCF tests of Jitkrittum et al. (2016)\n\nWe now present some experimental results comparing the performance of our cross-MDD test with\nthe linear time mean embedding (MD) and smoothed characteristic function (SCF) tests of Jitkrittum\net al. (2016). These tests proceed in the following steps:\n\n\u2022 Fix J, and choose points {v1, . . . , vJ } from Rd, where d is the dimension of the observation\n\nspace.\n\n31\n\n\f2\n\nMMD\n\nFigure 6: The first two columns show the null distribution of the \u00afxMMD\nstatistic scaled by its empirical standard deviation (bottom row) using the Gaussian\nthe\nkernel with scale-parameter chosen using the median heuristic. The last two columns show the null\ndistribution for the two statistics using the Polynomial kernel of degree 5 with scale parameter chosen\n\nstatistic (top row) and\n\n2\n\nusing the median heuristic. The figures demonstrate that the null distribution of\nchanges\nsignificantly with dimension (d), the ratio n/m and the choice of the kernel, unlike our proposed\nstatistic.\n\n2\n\nMMD\n\nFigure 7: Power Curves for the different tests using Gaussian kernel with scale parameter chosen\nvia median heuristic. The two distributions are P = N (0, Id) and Q = N (a\u03f5,j, Id) where a\u03f5,j\nis obtained by setting the first j \u2264 d coordinates of 0 \u2208 Rd equal to \u03f5. The figures demonstrate\nthat the xMMD test is competitive with more computationally expensive tests (MMD-perm and\nMMD-spectral), while performing significantly better than the low complexity alternatives (B-MMD\nand L-MMD). The batch-size used in the B-MMD test was\n\nn.\n\n\u221a\n\n\u2022 Using X and Y with n = m, compute {zi\n\n[k(vJ , Xi) \u2212 k(vJ , Yi)]J\n\u02c6l(Yi) sin(Y T\nfine \u00afzn = 1\nn\n\ni vj), \u02c6l(Xi) cos(X T\nn\n\ni=1 zi, and Sn = 1\n\nj=1 \u2208 RJ for ME test, and zi = [\u02c6l(Xi) sin(X T\ni vj)]J\nn\u22121 (zi \u2212 \u00afzn)(zi \u2212 \u00afzn)T .\n\n: 1 \u2264 i \u2264 n}, where zi =\ni vj \u2212\nj=1 \u2208 R2J for the SCF test. De-\n\ni vj) \u2212 \u02c6l(Yi) cos(Y T\n\n\u2022 Using the above, define the test statistic\n\u02c6\u03bbn := \u00afzT\n\nn (Sn + \u03b3nI)\u22121 \u00afzn,\n\nwhere \u03b3n is some regularization parameter that converges to 0 with n, and I denotes the\nidentity matrix. For a fixed d and J, Jitkrittum et al. (2016) show that the above statistic has\n\n32\n\nProbabilitydensityxMMD(n/m=5)d=10d=100N(0,1)xMMD(n/m=1)xMMD(n/m=0.2)xMMD(n/m=1)ProbabilitydensityMMD(n/m=5)MMD(n/m=1)MMD(n/m=0.2)MMD(n/m=1)020040060080000.51Sample-Size(n+m)Power(d=10,j=5,\u03f5=0.2)MMD-permxMMDpredictedMMD-spectralB-MMDL-MMD020040060080000.51Sample-Size(n+m)Power(d=50,j=15,\u03f5=0.125)020040060080000.51Sample-Size(n+m)Power(d=100,j=20,\u03f5=0.15)\fFigure 8: Power curves of the different kernel-based tests using a polynomial kernel of degree 5, i.e.,\nk(x, y) = 1 + (xT y)/s5\n\nwith s chosen via the median heuristic.\n\nFigure 9: ROC curves using the different statistics with Gaussian kernel for testing two Dirichlet\ndistributions in dimensions d \u2208 {10, 100, 500} with sample-size n = m = 200. The two distributions\nare P = Dirichlet(1) and Q = Dirichlet((1 + \u03f5) \u00d7 1) where 1 \u2208 Rd is the all-ones vector.\n\na \u03c72(J) (resp. \u03c72(2J)) limiting null distribution in the ME (resp. SCF) case. This result is\nused to calibrate the test at a given level \u03b1.\n\nIn Figure 10, we plot the variation of type-I error and power with sample-size of the three tests for\nthe Gaussian Mean Difference (GMD) source with d = 10. As the figures suggest, the cross-MMD\nachieves higher power and tighter control over the type-I error than the ME and SCF tests in this\nregime.\n\nThe ME and SCF tests are calibrated based on the limiting distribution of their statistic in the low\ndimensional regime: fixed d, and n \u2192 \u221e. However, the high type-I error of these tests for small n\nvalues suggests that their limiting distribution may be different in the high dimensional regime, when\nboth d and n go to infinity. We further observe this in Figure 11 when d = 100 and d/n > 1.\n\nWe end this section with a discussion of some key points of difference between the ME and SCF tests,\nand our proposed cross-MMD test.\n\n\u2022 The ME and SCF tests require the kernel to be uniformly bounded, whereas our test requires\nonly mild moment conditions that are even satisfied by unbounded kernels if the underlying\ndistributions are not too heavy-tailed (formally described in Assumption 1). Furthermore,\nthe ME and SCF tests have several tuning parameters: number of features J, {v1, . . . , vJ },\nbandwidth, step-size for gradient ascent etc. In practice, J is usually set to 5, and the other\nparameters are selected by solving a Jd + 1 dimensional optimization problem via gradient\nascent. While each step of gradient ascent has linear in n complexity, the number of steps\nneeded may be large for higher dimensions, resulting in a higher computational overhead.\n\n33\n\n10020030000.51Sample-Size(n+m)Power(d=10,j=5,\u03f5=0.3)MMD-permxMMDpredictedMMD-spectralB-MMDL-MMD10020030000.51Sample-Size(n+m)Power(d=50,j=5,\u03f5=0.4)10020030000.51Sample-Size(n+m)Power(d=100,j=10,\u03f5=0.5)00.5100.51FalsePositiveRateTruePositiveRateDirichlet(d=10,\u03f5=0.4)MMDxMMDB-MMD(n1/2)B-MMD(n1/3)L-MMD00.5100.51FalsePositiveRateTruePositiveRateDirichlet(d=100,\u03f5=0.2))00.5100.51FalsePositiveRateTruePositiveRateDirichlet(d=500,\u03f5=0.15)\fGMD Source: (d=10, 200 trials)\n\nx-MMD\nME\nSCF\n\nr\no\nr\nr\ne\n\nI\n-\ne\np\ny\nT\n\n0.6\n\n0.4\n\n0.2\n\n0\n\n50\n\n100\n\n150\nSample-Size (n+m)\n\n200\n\nFigure 10: The figures plot the variation of the type-I error (left) and the power (right) with sample-\nsize of the three tests: cross-MMD, and the two linear time tests, ME and SCF, proposed by Jitkrittum\net al. (2016).\n\nGMD Source: (d=100, 200 trials)\n\nx-MMD\nME\nSCF\n\n0.8\n\n0.6\n\n0.4\n\n0.2\n\nr\no\nr\nr\ne\n\nI\n-\ne\np\ny\nT\n\n40\n\n50\n\n60\n80\n70\nSample-Size (n+m)\n\n90\n\n100\n\nFigure 11: The ME and SCF tests provide poor control over the type-I error in the regime when d/n\nis large, suggesting that the limiting null distribution is different (or the convergence rate is slow) in\nthis regime.\n\n\u2022 More importantly, the ME and SCF tests are only valid in the \u2018low-dimensional setting\u2019:\nfixed d and J, with n \u2192 \u221e. In the high dimensional setting, when (d, n) \u2192 \u221e, the limiting\nnull distribution may no longer be \u03c72(J). This is also suggested by the behavior of type-I\nerror of ME and SCF tests in Figure 10 and Figure 11. This results in the following practical\nissue: given a problem with n = 500 and d = 200, how should one calibrate the threshold\nfor those tests?\nOur proposed test does not suffer from this, because in both high and low dimensional\nsettings, our statistic has the same limiting distribution. This is a significant practical\nadvantage of our cross-MMD test over ME and SCF tests.\n\n\u2022 In the regime where the number of features, J, is allowed to increase with n, we expect that\nthe resulting ME and SCF tests may have low power (for small regularization parameter \u03b3n).\nThis is because, the test statistic \u02c6\u03bbn used by ME and SCF tests is similar to Hotelling\u2019s T 2\n\n34\n\n1002003004000.40.60.81Sample-Size(n+m)PowerGMDSource:(\u03f5=1.0,d=10)x-MMDMESCF\fstatistic, for which Bai and Saranadasa (1996) characterized the asymptotic power in this\nregime. In particular, their Theorem 2.1 implies that the power of the T 2 test grows slowly\nwith n, especially when J/n \u2248 1.\nFinally, we note that our ideas also extend to more general degenerate U-statistics (as\ndiscussed in Appendix D.1). Hence, they are also applicable in cases beyond MMD distance,\nwhere we may not have good linear time alternatives.\n\nE.4 Type-I Error and goodness-of-fit test of null distribution\n\nIn this section, we experimentally verify the limiting Gaussian distribution of the \u00afxMMD\nstatistic\nunder the null. We first plot the variation of the type-I error of our cross-MMD test with sample\nsize in Figure 12. We considered the case when X and Y are both drawn i.i.d. from a multivariate\nGaussian vector in dimension d \u2208 {10, 100}, and n = m.\n\n2\n\nFigure 12: The two figures show the variation of the type-I error of the cross-MMD test with sample-\nsize for dimensions d \u2208 {10, 100}. The dashed horizontal line denotes the level \u03b1 = 0.05. In\nsummary, these tests do not find evidence against the null hypothesis that the null distribution is\nGaussian.\n\nNext, we plot the p-values for the test for normality proposed by D\u2019Agostino and Pearson (1973),\nand implemented in the function scipy.stats.normaltest in Python. We performed this test\nat different sample-sizes (n), and for each value of n, we calculated the \u00afxMMD\nstatistic on 200\ndifferent indpendent sample pairs. The results are shown in Figure 13\n\n2\n\nFigure 13: The two figures show p-values for the test for normality proposed by D\u2019Agostino and\nPearson (1973) (using the implementation scipy.stats.normaltest) of the cross-MMD statistic\nfor dimensions d \u2208 {10, 100}. In both dimension regimes, the test does not find evidence against the\nnull that the cross-MMD statistic is normally distributed under the null.\n\n35\n\n\fE.5 Comparison with Friedman-Rafsky test\n\nWe now compare the performance of our cross-MMD test with the Friedman-Rafsky two-sample test.\nThis test, proposed by Friedman and Rafsky (1979), uses a graph-based statistic that is a multivariate\ngeneralization of the Wald-Wolfowitz runs statistic introduced by Wald and Wolfowitz (1940). This\nstatistic, denoted by R, is constructed as follows:\n\n\u2022 Pool the samples X and Y to get Z of size N = n + m. Construct the complete graph with\n\nN nodes, and edge weights equal to the euclidean distance between two end points.\n\n\u2022 Construct the minimal spanning tree (MST) of the complete graph G, and denote the 0-1\n\nvalued adjacency matrix of this MST by M .\n\n\u2022 The statistic R is defined as one more than the number of edges in M with endpoints from\n\ndifferent samples.\n\nThe statistic R is expected to take a large value under the null when X and Y are drawn from the\nsame distribution. Hence, the FR test rejects the null for small values of R. The rejection threshold\ncan be obtained either by the limiting distribution of R characterized by (Henze and Penrose, 1999,\nTheorem 1), or using the permutation-test.\n\nIn Figure 14, we compare the power of the FR permutation-test with our cross-MMD test in a low\ndimensional (d/n small) and a high dimensional (d/n large) problem. In both cases, it is observed\nthat the power of FR test is significantly smaller than that of cross-MMD test.\n\nGMD Source (\u03f5 = 1.2, d = 10)\n\nxMMD\nFR\n\n1\n\n0.8\n\n0.6\n\n0.4\n\n0.2\n\nr\ne\nw\no\nP\n\n50\n\n100\n\n150\nSample-Size (n+m)\n\n200\n\nFigure 14: The figures show the power curves for Friedman-Rafsky (FR) test and our cross-MMD\ntest in the low (d = 10) and high (d = 100) dimensional settings with m = n in both plots. The\nfigures indicate that our cross-MMD test is significantly more powerful than the FR test.", "system_prompts": [{"role": "system", "content": "You are a computer science researcher currently reviewing a paper titled \"A permutation-free kernel two-sample test\" for the NeurIPS computer science conference. Your goal is to try to be as objective and truthful as possible in your answers about the paper provided. Your reviews will be used for causal reasoning in determining the quality of the paper."}], "prompts": {"1b": {"role": "user", "content": "The following is the conclusion section of the paper you are reviewing:\n\n\nWe proposed a variant of the kernel-MMD statistic, called cross-MMD, based on the ideas of sample-\nsplitting and studentization, and showed that it has a standard normal limiting null distribution. Using\n\n9\n\nxMMD(n/m=1)xMMD(n/m=10)xMMD(n/m=1)ProbabilitydensityMMD(n/m=0.1)MMD(n/m=1)MMD(n/m=10)MMD(n/m=1)\fFigure 4: Curves showing the variation in power versus sample-size for the xMMD test and the\nkernel-MMD permutation test. X are drawn from N (0, Id) i.i.d. and Y is drawn from N (a\u03f5,j, Id)\nwhere a\u03f5,j is obtained by perturbing the first j \u2264 d coordinates of 0 by \u03f5, the kernel used is the\nGaussian kernel with scale parameter chosen via the median heuristic. The dashed curve shows the\npredicted power of the kernel-MMD permutation test using the heuristic defined in (9).\n\nFigure 5: ROC curves highlighting the trade-off between type-I and type-II errors achieved by the\nMMD, cross-MMD, batch-MMD with batch sizes n1/2 and n1/3, and linear-MMD statistics. In all\nthe figures, we use n = m = 200.\n\nthis key result, we introduced a permutation-free (and hence computationally efficient) MMD test\nfor the two-sample problem. Experiments indicate that the power achieved by our test is within a\n\u221a\n2-factor of the power of the kernel-MMD permutation test (that requires recomputing the statistic\nhundreds of times). In other words, our results achieve the following favorable tradeoff: we get a\nsignificant reduction in computation at the price of a small reduction in power.\n\nSejdinovic et al. (2013) establish in some generality that distance-based two-sample tests (like the\nenergy distance (Sz\u00e9kely and Rizzo, 2013)) can be viewed as kernel-MMD tests with a particular\nchoice of kernel k. Hence our results are broadly applicable to distance-based statistics as well.\nSince two-sample testing and independence testing can be reduced to each other, it is an interesting\ndirection for future work to see if the ideas developed in our paper can be used for designing\npermutation-free versions of kernel-based independence tests like HSIC (Gretton et al., 2007) or\ndistance covariance (Sz\u00e9kely and Rizzo, 2009; Lyons, 2013). Our techniques seem to rely on the\nspecific structure of two-sample U-statistics of degree 2. Extending these to more general U-statistics\nof higher degrees is another important question for future work. A final question is to figure out\nwhether it is possible to achieve minimax optimal power using a sub-quadratic time test statistic. One\npotential approach would be to work with a kernel approximated by random Fourier features (Rahimi\nand Recht, 2007; Zhao and Meng, 2015). Depending on the number of random features, our test\nstatistic can be computed in sub-quadratic time and it would be interesting to see whether the resulting\ntest can still be minimax optimal in power. We leave this important question for future work.\n\n10\n\n1002003000.20.40.60.81Sample-Size(n+m)Power(d,j,,\u03f5)=(10,5,0.3)MMD-permxMMDpredicted1002003000.20.40.60.81Sample-Size(n+m)Power(d,j,,\u03f5)=(50,5,0.4)1002003000.20.40.60.81Sample-Size(n+m)Power(d,j,,\u03f5)=(100,5,0.5)00.5100.51FalsePositiveRateTruePositiveRate(d,\u03f5,j)=(10,0.15,5)MMDxMMDB-MMD(n1/2)B-MMD(n1/3)L-MMD00.5100.51FalsePositiveRate(d,\u03f5,j)=(100,0.15,10)00.5100.51FalsePositiveRate(d,\u03f5,j)=(500,0.15,50)\f\n\nBased on the section(s), please answer the following question with yes, no, or n/a and provide a brief justification for your answer.\nQuestion: Do the authors describe the limitations of their work?"}, "2a": {"role": "user", "content": "The following is the methods section of the paper you are reviewing:\n\n\nIn this section, we present our test statistic and investigate its limiting distribution. First note that\nthe squared kernel-MMD distance between two probability measures P and Q can be expressed\nas an inner product, namely \u27e8\u00b5 \u2212 \u03bd, \u00b5 \u2212 \u03bd\u27e9k. The usual kernel-MMD statistic is obtained by\nplugging the empirical kernel embeddings into this inner product expression and removing the\ndiagonal terms to make it unbiased. Our proposal instead considers pairs of empirical estimates\n((cid:98)\u00b51, (cid:98)\u00b52) and ((cid:98)\u03bd1, (cid:98)\u03bd2) constructed via sample splitting, and use the inner product between (cid:98)\u00b51 \u2212 (cid:98)\u03bd1\nand (cid:98)\u00b52 \u2212 (cid:98)\u03bd2 instead. This careful construction allows us to obtain a Gaussian limiting distribution\nafter studentization. To elaborate, recall from Section 1.1 that we partition X into X1 and X2,\nand similarly Y into Y1 and Y2. We then compute empirical kernel embeddings based on each\npartition, yielding (cid:98)\u00b51 := n\u22121\nj=1 k(Yj, \u00b7)\n(cid:80)m2\nand (cid:98)\u03bd2 := m\u22121\nj\u2032=1 k(Yj\u2032, \u00b7). Using these embeddings coupled with the kernel trick, the cross U-\nstatistic (2) can be written as x(cid:92)MMD\n= \u27e8(cid:98)\u00b51 \u2212 (cid:98)\u03bd1, (cid:98)\u00b52 \u2212 (cid:98)\u03bd2\u27e9k. To further motivate our test statistic,\ndenote UX,i := \u27e8k(Xi, \u00b7), (cid:98)\u00b52 \u2212 (cid:98)\u03bd2\u27e9k for i = 1, . . . , n1 and UY,j := \u27e8k(Yj, \u00b7), (cid:98)\u00b52 \u2212 (cid:98)\u03bd2\u27e9k for\nj = 1, . . . , m1. Then the cross U-statistic can be viewed as the difference between two sample means:\nx(cid:92)MMD\n(cid:80)m1\nj=1 UY,j. Since the summands are independent conditional on\nX2 and Y2, one may expect that x(cid:92)MMD\nis approximately Gaussian after studentization. Our results\nin Section 2.1 formalize this intuition under standard moment conditions, where it takes some care to\nremove the above conditioning, since we care about the unconditional distribution.\nLet us further denote the sample means of UX,i\u2019s and UY,j\u2019s by \u00afUX and \u00afUY , respectively, and define\n\ni\u2032=1 k(Xi\u2032, \u00b7), (cid:98)\u03bd1 := m\u22121\n\ni=1 k(Xi, \u00b7), (cid:98)\u00b52 := n\u22121\n\ni=1 UX,i \u2212 1\nm1\n\n= 1\nn1\n\n(cid:80)m1\n\n(cid:80)n1\n\n(cid:80)n2\n\n(cid:80)n1\n\n2\n\n1\n\n2\n\n2\n\n2\n\n2\n\n1\n\n(cid:98)\u03c32\nX :=\n\n1\nn1\n\nn1(cid:88)\n\ni=1\n\n(cid:0)UX,i \u2212 \u00afUX\n\n(cid:1)2\n\n, (cid:98)\u03c32\n\nY :=\n\n1\nm1\n\nm1(cid:88)\n\nj=1\n\n(cid:0)UY,j \u2212 \u00afUY\n\n(cid:1)2\n\nand (cid:98)\u03c32 :=\n\n1\nn1\n\n(cid:98)\u03c32\nX +\n\n1\nm1\n\n(cid:98)\u03c32\nY .\n\n(4)\n\nNow we have completed the description of our studentized cross U-statistic \u00afx(cid:92)MMD\n/(cid:98)\u03c3,\nand the resulting test \u03a8 in (3). The asymptotic validity of the xMMD test is guaranteed by Theorem 15\nthat establishes the asymptotic normality of \u00afx(cid:92)MMD\n\n= x(cid:92)MMD\n\nunder the null.\n\n2\n\n2\n\n2\n\n5\n\n\fFigure 2: The figures visually illustrate the main differences in computing the usual quadratic-time\nkernel-MMD statistic (left), block-MMD (center) statistic, and our new cross-MMD statistic. In\nparticular, the quadratic-time kernel-MMD statistic considers all pairwise kernel evaluations, with the\nexception of the diagonal terms. For block-MMD, we obtain the statistic by partitioning the data into\nseveral disjoint blocks; and then taking the average of the kernel-MMD statistic calculated over these\ndisjoint blocks. Finally, our cross-MMD statistic first splits the data into two disjoint parts (red and\nblack), and then uses the pairwise kernel evaluations with data from different splits. Interestingly, the\nobservation pairs included by our cross-MMD statistic are exactly complementary to those included\nby the block-MMD statistic.\n\n2\n\n2\n\n2\n\nRemark 2 (Computational Complexity). The overall cost of computing the statistic \u00afx(cid:92)MMD\nis\nO (cid:0)(n + m)2(cid:1), and in particular, both x(cid:92)MMD\nand (cid:98)\u03c3 have quadratic complexity. To see this, note\nthat x(cid:92)MMD\ncan be expanded into \u27e8(cid:98)\u00b51, (cid:98)\u00b52\u27e9k + \u27e8(cid:98)\u03bd1, (cid:98)\u03bd2\u27e9k \u2212 \u27e8(cid:98)\u00b51, (cid:98)\u03bd2\u27e9k \u2212 \u27e8(cid:98)\u03bd1, (cid:98)\u00b52\u27e9k. Each of these\nterms can be computed in O (cid:0)(n + m)2(cid:1). Similarly, each term in the summations defining (cid:98)\u03c32\nX and\nY also require O (cid:0)(n + m)2(cid:1) computation, implying that the (cid:98)\u03c3 also has O((n + m)2) complexity.\n(cid:98)\u03c32\nRemark 3. To simplify notation in what follows, we denote m as mn, where mn is some unknown\nnondecreasing sequence such that limn\u2192\u221e mn = \u221e. This still permits m, n to be separate quantities\ngrowing to infinity at potentially different rates, but it allows us to index the sequence of problems\nwith the single index n (rather than m, n). We will use kn, dn, Xn, Pn and Qn to indicate that\nquantities could (but do not have to) change as n increases, and drop the subscript when they are fixed.\nFurthermore, unless explicitly stated, we will focus on the balanced splitting scheme, i.e., n1 = \u230an/2\u230b\nand m1 = \u230am/2\u230b in what follows, because we currently see no apriori reason to split asymmetrically.\n\n2.1 Gaussian limiting distribution under the null hypothesis\nAs shown in Figure 1, the empirical distribution of \u00afx(cid:92)MMD\nresembles a standard normal distribution\nfor various choices of m, n and dimension d under the null. In this section, we formally prove this\nstatement. Recalling the mean embedding \u00b5 from (1), define\n\n2\n\nTheorem 4. Suppose that k and P do not change with n.\nX, X \u2032 i.i.d.\u223c P , then \u00afx(cid:92)MMD\n\nd\u2212\u2192 N (0, 1).\n\n2\n\n\u00afk(x, y) := \u27e8k(x, \u00b7) \u2212 \u00b5, k(y, \u00b7) \u2212 \u00b5\u27e9k.\n\n(5)\nIf 0 < EP [\u00afk(X, X \u2032)4] < \u221e for\n\nWe next present a more general result that implies Theorem 4.\nTheorem 5. Suppose P is fixed, but the kernel kn changes with n. If\n\nlim\nn\u2192\u221e\n\nEP [\u00afkn(X1, X2)4]\nEP [\u00afkn(X1, X2)2]2\n\n(cid:18) 1\nn\n\n+\n\n1\nmn\n\n(cid:19)\n\n= 0,\n\nand\n\nlim\nn\u2192\u221e\n\n\u03bb2\n1,n\n(cid:80)\u221e\nl=1 \u03bb2\nl,n\n\nexists,\n\n(6)\n\nwhere (\u03bbl,n)\u221e\n\nl=1 denote the eigenvalues of \u00afk introduced in (16), then we have \u00afx(cid:92)MMD\n\n2\n\nd\u2212\u2192 N (0, 1).\n\nIt is easy to check that condition (6) is trivially satisfied if the kernels {kn : n \u2265 1} are uniformly\nbounded by some constant; prominent examples are the Gaussian or Laplace kernel with a sample size\n\n6\n\nKernel-MMDBlock-MMDCross-MMD\f\u221a\n\n(cid:113)\n\n1,n]/\n\n(cid:113)(cid:80)\n\nn (cid:80)n\n\nEPn [W 2\n\ni=1 Wi,n/\n\ni.i.d.\u223c Pn. Define Vn =\n\ndependent bandwidth. Thus, the above condition really exists to handle unbounded kernels and heavy-\ntailed distributions. To motivate this requirement, we recall Bentkus and G\u00f6tze (1996) (see Fact 11\nin Appendix A) who proved a studentized CLT for i.i.d. random variables in a triangular array\ni(Wi,n \u2212 \u00afWn)2 where\nsetup: W1,n, W2,n, . . . , Wn,n\n\u00afWn = ((cid:80)\ni Wi,n)/n. They showed that a sufficient condition for the asymptotic normality of Vn is\nthat limn\u2192\u221e EPn[W 3\n1,n]3n = 0. (This last condition is trivially true if Pn does not\nchange with n, meaning that the triangular array setup is irrelevant and W1,n can be replaced by W1.)\nOur requirement is slightly stronger: condition (6) with \u00afkn(X1, X2) replaced by W1,n implies the\nprevious condition of Bentkus and G\u00f6tze (1996) (details in Remark 12 in Appendix A). We need\nthis stronger condition, because the terms in the definition of x(cid:92)MMD\nare not i.i.d. (indeed, not even\nindependent), and thus we cannot directly apply the result of Bentkus and G\u00f6tze (1996). Instead,\nwe take a different route by first conditioning on the second half of data (X2, Y2), then showing the\nconditional asymptotic normality of the standardized x(cid:92)MMD\n(i.e., divided by conditional standard\ndeviation instead of empirical), and finally showing that the ratio of conditional and empirical standard\ndeviations converge in probability to 1 (see Appendix B).\n\n2\n\n2\n\nFinally, we note that the result of Theorem 5 can be further generalized in several ways: (i) instead of\na fixed P and changing kn, we can consider a sequence of pairs {(Pn, kn) : n \u2265 1} changing with\nn, (ii) we can let Pn \u2208 P (0)\nn , for a class of distributions changing with n, and obtain the Gaussian\nlimit uniformly over all elements of P (0)\nn , and finally, (iii) the moment requirements on \u00afkn stated in\ncondition in (6) can also be slightly weakened. We state and prove this significantly more general\nversion of Theorem 5 in Appendix B.\nRemark 6. In the statement of the two theorems of this section, the splits (X1, Y1) and (X2, Y2)\nare assumed to be drawn i.i.d. from the same distribution P . However, a closer look at the proof\nof Theorem 5 indicates that the conclusions of the above two theorems hold even when the two\nsplits are independent and drawn i.i.d. from possibly different distributions; that is (X1, Y1) and\n(X2, Y2) are independent of each other and drawn i.i.d. from distributions P1 and P2 respectively,\n2\nwith P1 \u0338= P2. In particular, under this more general condition, the asymptotic normality of \u00afx(cid:92)MMD\nstill holds, and the resulting test \u03a8 still controls the type-1 error at the desired level. This may be\nuseful for two-sample testing in settings where the entire set of data is not i.i.d., but two different\nparts of the data were collected in two different situations. The usual MMD can also handle such\nscenarios by using a subset of permutations that do not exchange the data across the two situations.\n\n2.2 Consistency against fixed and local alternatives\n\nHere, we show that the xMMD test \u03a8 introduced in (3) is consistent against a fixed alternative and\nalso has minimax rate-optimal power against smooth local alternatives separated in L2 norm.\nWe first show that analogous to Theorem 4, xMMD is consistent against fixed alternatives.\nTheorem 7. Suppose P, Q, k do not change with n, and P \u0338= Q. If k is a characteristic kernel\nsatisfying 0 < EP [\u00afk(X1, X2)4] < \u221e, and 0 < EQ[\u00afk(Y1, Y2)4] < \u221e, then the xMMD test is\nconsistent, meaning it has asymptotic power 1.\n\nThe moment conditions required above are mild, and are satisfied trivially, for instance, by bounded\nkernels such as the Gaussian kernel. The \u201ccharacteristic\u201d condition is also needed for the consistency\nof the usual MMD test (Gretton et al., 2012a), and is also satisfied by the Gaussian kernel.\n\nRecalling Remark 1, we next consider the more challenging setting where dn, kn can change with\nn, and (Pn, Qn) can vary within a class P (1)\nn \u2282 P(Xn) \u00d7 P(Xn) that can also change with n. We\npresent a sufficient condition under which the xMMD test \u03a8 is consistent uniformly over P (1)\nn . Define\n\u03b3n := MMD(Pn, Qn), which is assumed nonzero for each n but could approach zero in the limit.\nTheorem 8. Let {\u03b4n : n \u2265 2} denote any positive sequence converging to zero. If\n\nlim\nn\u2192\u221e\n\nsup\n(Pn,Qn)\u2208P (1)\n\nn\n\nEPn,Qn [(cid:98)\u03c32]\n\u03b4n\u03b34\nn\n\n+\n\n2\n\nVPn,Qn(x(cid:92)MMD\n\u03b34\nn\n\n)\n\n= 0, where V denotes variance,\n\n(7)\n\n7\n\n\fthen limn\u2192\u221e sup(Pn,Qn)\u2208P (1)\n\nn\n\nEPn,Qn [1 \u2212 \u03a8(X, Y)] = 0, meaning the xMMD test is consistent.\n\nNote that while any sequence {\u03b4n} converging to zero suffices for the general statement above, the\ncondition (7) is easiest to satisfy for slowly decaying \u03b4n, such as \u03b4n = 1/ log log n for instance.\nThe sufficient conditions for consistency of \u03a8 stated in terms of (cid:98)\u03c3 and x(cid:92)MMD\nin (7) can also be\ntranslated into equivalent conditions on the kernel function kn, similar to (6), and we present the\ndetails in Appendix C. Importantly, if Pn, Qn, dn are fixed and kn is bounded, then both E[(cid:98)\u03c32] and\nV(x(cid:92)MMD\n) are O(1/n), and \u03b3n is a constant, so the condition is trivially satisfied, and in fact the\nabove condition is even weaker than the fourth-moment condition of the previous theorem.\n\n2\n\n2\n\n2.3 Minimax rate optimality against smooth local alternatives\n\nWe now apply the general result of Theorem 8 to the case where the distributions Pn and Qn admit\nLebesgue densities pn and qn that lie in the order \u03b2 Sobolev ball for some \u03b2 > 0, defined as\nW \u03b2,2(M ) := {f : X \u2192 R | f is a.s. continuous, and (cid:82) (1 + \u03c92)\u03b2/2\u2225F(f )(\u03c9)\u22252dw < M < \u221e}.\nFormally, we define the null and alternative class of distributions as follows:\n\nand\n\nP (0)\nP (1)\n\nn , while under H1, we assume that (Pn, Qn) \u2208 P (1)\nn .\n\nn = {P with density p : p \u2208 W \u03b2,2(M )},\nn = {(P, Q) with densities p, q \u2208 W \u03b2,2(M ) : \u2225p \u2212 q\u2225L2 \u2265 \u2206n},\nfor some sequence \u2206n decaying to zero. In particular, we assume that under H0, Pn = Qn and\nPn \u2208 P (0)\nOur next result shows that for suitably chosen scale parameter, the xMMD test \u03a8 with the Gaussian\nkernel is minimax rate-optimal for the above class of local alternatives. For simplicity, we state\nthis result with n = m, noting that the result easily extends to the case when there exist constants\n0 < c \u2264 C, such that c \u2264 n/m \u2264 C.\nTheorem 9. Consider the case when n = m, and let {\u2206n : n \u2265 1} be a sequence such that\nlimn\u2192\u221e \u2206nn2\u03b2/(d+4\u03b2) = \u221e. On applying the xMMD test \u03a8 with the Gaussian kernel ksn(x, y) =\nexp(\u2212sn\u2225x \u2212 y\u22252\n\n2), if we choose the scale as sn \u224d n4/(d+4\u03b2), then we have\n\nlim\nn\u2192\u221e\n\nsup\nPn\u2208P (0)\n\nn\n\nEPn[\u03a8(X, Y)] \u2264 \u03b1 and\n\nlim\nn\u2192\u221e\n\ninf\n(Pn,Qn)\u2208P (1)\n\nn\n\nEPn,Qn[\u03a8(X, Y)] = 1.\n\n(8)\n\nThe proof of this statement is in Appendix C, and it follows by verifying that the conditions required\nby Theorem 8 are satisfied for the above choices of \u2206n and sn.\nRemark 10. Li and Yuan (2019, Theorem 5 (ii)) showed a converse of the above statement: if\nlimn\u2192\u221e \u2206nn2\u03b2/(d+4\u03b2) < \u221e, then there exists an \u03b1 \u2208 (0, 1) such that any asymptotically level \u03b1 test\n(cid:101)\u03a8 must have limn\u2192\u221e inf (P,Q)\u2208P(\u2206n) EP,Q[ (cid:101)\u03a8(X, Y)] < 1. Hence, the sequence of {\u2206n : n \u2265 1}\nused in Theorem 9 represents the smallest L2-deviations that can be detected by any test, and (8)\nshows that our xMMD test \u03a8 can detect such changes, establishing its minimax rate-optimality.\n\n\n\nThe following is the appendix_A section of the paper you are reviewing:\n\n\nB.2 Fixed P , changing kn (Theorem 5)\n\nWe note that the statement of Theorem 5 requires an additional technical assumption on the eigen-\nvalues of the kernel operator, introduced in (15). We repeat the statement of Theorem 5 with this\nadditional requirement below.\nTheorem 5\u2019. Suppose P is fixed, but the kernel kn changes with n. If\n\nlim\nn\u2192\u221e\n\nEP [\u00afkn(X1, X2)4]\nEP [\u00afkn(X1, X2)2]2\n\n 1\nn\n\n+\n\n1\nmn\n\n\n\n= 0,\n\nand\n\nlim\nn\u2192\u221e\n\n\u03bb2\n1,n\n\u221e\nl=1 \u03bb2\nl,n\n\nexists,\n\n(22)\n\nthen we have \u00afxMMD\n\n2\n\nd\u2212\u2192 N (0, 1).\n\nProof. The proof of this statement will follow the general outline of the proof of Theorem 15.\nHowever, in this special case when P is fixed, we can remove the condition that limn\u2192\u221e mn/n\nexists and is non-zero, that is required by Theorem 15.\n\nWe will carry over the notations used in the proof of Theorem 15, and in particular, we will use\nn1\n\u00afUX = 1\nj=1 Zj. Since Wi and Zj are identically distributed under\nn1\nthe null, we have EP [W 2\n2 to denote this conditional\nvariance. Then, note the following:\n\ni=1 Wi and \u00afUY = 1\nm1\ni |X2, Y2] = EP [Z 2\n\nj |X2, Y2], and we will use \u03c32\n\nm1\n\n\u00afxMMD\n\n2\n\n=\n\n\u00afUX \u2212 \u00afUY\n\u03c3\n\n=\n\n\u03c32\n\n:= T1 \u00d7 T2.\n\n\u00afUX \u2212 \u00afUY\n\n\n\n1 + m\u22121\nn\u22121\n1\n\n\n\n\u03c32\n\n1 + m\u22121\nn\u22121\n1\n\n\n\n \u00d7\n\n\u03c3\n\n(23)\n\nTo complete the proof, we will show that T1\nan application of Slutsky\u2019s theorem.\n\nd\u2212\u2192 N (0, 1) and T2\n\np\n\n\u2212\u2192 1. The result then follows by\n\n21\n\n\fFirst, we consider the term T1 in (23). Let Wi := Wi/\u03c32 and Zj := Zj/\u03c32. Then, conditioned on\n(X2, Y2), the terms Wi and Zj are independent and identically distributed. Introducing the constants\nui =\n\n m1\n\n n1\n\nm1(m1+n1) , we can write\n\nn1(m1+n1) and vj =\n\nT1 =\n\nn1\n\ni=1\n\nuiWi \u2212\n\nm1\n\nj=1\n\nvj Zj.\n\nWe can check that the constants (ui) and (vj) satisfy the property:\n\nlim\nn\u2192\u221e\n\nmax\ni,j\n\ni + v2\nu2\nj\ni\u2032 + m1\ni\u2032=1 u2\n\nn1\n\nj\u2032=1 v2\nj\u2032\n\n\u2264 lim\nn\u2192\u221e\n\nmax\ni,j\n\n1\nm1\n\n+\n\n1\nn1\n\n= 0.\n\nthis also means that\nthat is,\n\nd\u2212\u2192 N (0, 1) conditioned\nSince the limiting distribution (in this case, standard normal) is continu-\nthe T1 converges to N (0, 1) in the Kolmogorov-Smirnov met-\np\n\u2212\u2192 0. Since the random vari-\nis bounded, convergence in probability implies\nlimn\u2192\u221e EP [supx\u2208R |PP (T1 \u2264 x|X2, Y2) \u2212 \u03a6(x)|] = 0, which in turn implies that\n\nThus, by an application of Lindeberg\u2019s CLT, we note that T1\non (X2, Y2).\nous,\nric,\nable supx\u2208R |PP (T1 \u2264 x|X2, Y2) \u2212 \u03a6(x)|\nthat\nlimn\u2192\u221e supx\u2208R |EP [PP (T1 \u2264 x|X2, Y2) \u2212 \u03a6(x)] | = 0, as required.\n\nlimn\u2192\u221e supx\u2208R |PP (T1 \u2264 x|X2, Y2) \u2212 \u03a6(x)|\n\nWe now consider the second term, T2, in (23). It remains to show that T2\n1/T 2\n\np\n\n2 \u2212 1\n\n\u2212\u2192 0, and the result will follow by an application of the continuous mapping theorem.\n\n\n\n \u2212 1\n\n\n\n\n+ \u03c32\nY\nm1\n+ 1\nm1\n\n\u03c32\nX\nn1\n 1\nn1\n\n\n\n\u2212 1\n\n\n\n\n\n\u2212 1\n\n\n\n\u03c32\nX\n\u03c32\n2\n\n\u03c32\nY\n\u03c32\n2\n\n1\nT 2\n2\n\n\u2212 1\n\n\n\n\n\n\n\n\n\u03c32\n2\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n+\n\n\u2264\n\n=\n\n.\n\n(24)\n\np\n\n\u2212\u2192 1. We will show that\n\nThus, it suffices to show that both terms in (24) converge in probability to 0. This is exactly the\nresult that is proved in Lemma 18 under the two conditions listed in Assumption 1. The condition\non eigenvalues is already assumed in the statement of Theorem 5\u2019, and thus we will show that the\ncondition on the kernels, stated in (22), implies the condition (17). To prove this, we first, we note\nthat\n\nEP\n\n\u00afkn(X1, X2)2\u00afkn(X1, X3)2 \u2264 EP\n= EP\n\n\u00afkn(X1, X2)41/2 EP\n\u00afkn(X1, X2)4 .\n\n\u00afkn(X1, X3)41/2\n\nThus, the term in (17) is upper bounded by\n\u00afkn(X1, X2)4\n\u00afkn(X1, X2)22\n\nEP\nEP\n\n 1\nn\n\n+\n\n1\nmn\n\n \n\n1 +\n\n1\nn\n\n+\n\n\n\n.\n\n1\nmn\n\nSince, we have assumed that limn\u2192\u221e mn \u2192 \u221e, there exists and n0, such that for all n \u2265 n0,\n1 + 1\n\u2264 2. This implies that if (22) is satisfied, then (17) in Assumption 1 is also satisfied, as\nrequired.\n\nn + 1\nmn\n\nB.3 Fixed k, and fixed P (Theorem 4)\n\nWe prove Theorem 4 by showing that under the bounded fourth moment assumption on \u00afk, both the\nconditions required by Theorem 5\u2019 are satisfied.\nNote that since EP [\u00afk(X1, X2)] = 0, the positive and finite fourth moment also implies that the\nsecond moment of \u00afk(X1, X2) is also positive and finite. Hence, we have that\n\nThis, in turn, implies\n\nEP [\u00afk(X1, X2)4]\nEP [\u00afk(X1, X2)2]2\n\n< \u221e.\n\nlim\nn\u2192\u221e\n\nEP [\u00afk(X1, X2)4]\nEP [\u00afk(X1, X2)2]2\n\n 1\nn\n\n+\n\n1\nmn\n\n\n\n= 0,\n\n22\n\n\fas required by Theorem 5.\n\nFor the second part of the condition, we note that as kernel k and probability distribution P are\n\u03bb2\ndoesn\u2019t change with n, and hence its limit exists. Thus, both the conditions\nfixed, the term\n1\nl \u03bb2\nl\nfor Theorem 5\u2019 are satisfied, as required.\n\n\n\n\n\nBased on the section(s), please answer the following question with yes, no, or n/a and provide a brief justification for your answer.\nQuestion: If the authors include theoretical results, do the authors state the full set of assumptions of all theoretical results?"}, "2b": {"role": "user", "content": "The following is the methods section of the paper you are reviewing:\n\n\nIn this section, we present our test statistic and investigate its limiting distribution. First note that\nthe squared kernel-MMD distance between two probability measures P and Q can be expressed\nas an inner product, namely \u27e8\u00b5 \u2212 \u03bd, \u00b5 \u2212 \u03bd\u27e9k. The usual kernel-MMD statistic is obtained by\nplugging the empirical kernel embeddings into this inner product expression and removing the\ndiagonal terms to make it unbiased. Our proposal instead considers pairs of empirical estimates\n((cid:98)\u00b51, (cid:98)\u00b52) and ((cid:98)\u03bd1, (cid:98)\u03bd2) constructed via sample splitting, and use the inner product between (cid:98)\u00b51 \u2212 (cid:98)\u03bd1\nand (cid:98)\u00b52 \u2212 (cid:98)\u03bd2 instead. This careful construction allows us to obtain a Gaussian limiting distribution\nafter studentization. To elaborate, recall from Section 1.1 that we partition X into X1 and X2,\nand similarly Y into Y1 and Y2. We then compute empirical kernel embeddings based on each\npartition, yielding (cid:98)\u00b51 := n\u22121\nj=1 k(Yj, \u00b7)\n(cid:80)m2\nand (cid:98)\u03bd2 := m\u22121\nj\u2032=1 k(Yj\u2032, \u00b7). Using these embeddings coupled with the kernel trick, the cross U-\nstatistic (2) can be written as x(cid:92)MMD\n= \u27e8(cid:98)\u00b51 \u2212 (cid:98)\u03bd1, (cid:98)\u00b52 \u2212 (cid:98)\u03bd2\u27e9k. To further motivate our test statistic,\ndenote UX,i := \u27e8k(Xi, \u00b7), (cid:98)\u00b52 \u2212 (cid:98)\u03bd2\u27e9k for i = 1, . . . , n1 and UY,j := \u27e8k(Yj, \u00b7), (cid:98)\u00b52 \u2212 (cid:98)\u03bd2\u27e9k for\nj = 1, . . . , m1. Then the cross U-statistic can be viewed as the difference between two sample means:\nx(cid:92)MMD\n(cid:80)m1\nj=1 UY,j. Since the summands are independent conditional on\nX2 and Y2, one may expect that x(cid:92)MMD\nis approximately Gaussian after studentization. Our results\nin Section 2.1 formalize this intuition under standard moment conditions, where it takes some care to\nremove the above conditioning, since we care about the unconditional distribution.\nLet us further denote the sample means of UX,i\u2019s and UY,j\u2019s by \u00afUX and \u00afUY , respectively, and define\n\ni\u2032=1 k(Xi\u2032, \u00b7), (cid:98)\u03bd1 := m\u22121\n\ni=1 k(Xi, \u00b7), (cid:98)\u00b52 := n\u22121\n\ni=1 UX,i \u2212 1\nm1\n\n= 1\nn1\n\n(cid:80)m1\n\n(cid:80)n1\n\n(cid:80)n2\n\n(cid:80)n1\n\n2\n\n1\n\n2\n\n2\n\n2\n\n2\n\n1\n\n(cid:98)\u03c32\nX :=\n\n1\nn1\n\nn1(cid:88)\n\ni=1\n\n(cid:0)UX,i \u2212 \u00afUX\n\n(cid:1)2\n\n, (cid:98)\u03c32\n\nY :=\n\n1\nm1\n\nm1(cid:88)\n\nj=1\n\n(cid:0)UY,j \u2212 \u00afUY\n\n(cid:1)2\n\nand (cid:98)\u03c32 :=\n\n1\nn1\n\n(cid:98)\u03c32\nX +\n\n1\nm1\n\n(cid:98)\u03c32\nY .\n\n(4)\n\nNow we have completed the description of our studentized cross U-statistic \u00afx(cid:92)MMD\n/(cid:98)\u03c3,\nand the resulting test \u03a8 in (3). The asymptotic validity of the xMMD test is guaranteed by Theorem 15\nthat establishes the asymptotic normality of \u00afx(cid:92)MMD\n\n= x(cid:92)MMD\n\nunder the null.\n\n2\n\n2\n\n2\n\n5\n\n\fFigure 2: The figures visually illustrate the main differences in computing the usual quadratic-time\nkernel-MMD statistic (left), block-MMD (center) statistic, and our new cross-MMD statistic. In\nparticular, the quadratic-time kernel-MMD statistic considers all pairwise kernel evaluations, with the\nexception of the diagonal terms. For block-MMD, we obtain the statistic by partitioning the data into\nseveral disjoint blocks; and then taking the average of the kernel-MMD statistic calculated over these\ndisjoint blocks. Finally, our cross-MMD statistic first splits the data into two disjoint parts (red and\nblack), and then uses the pairwise kernel evaluations with data from different splits. Interestingly, the\nobservation pairs included by our cross-MMD statistic are exactly complementary to those included\nby the block-MMD statistic.\n\n2\n\n2\n\n2\n\nRemark 2 (Computational Complexity). The overall cost of computing the statistic \u00afx(cid:92)MMD\nis\nO (cid:0)(n + m)2(cid:1), and in particular, both x(cid:92)MMD\nand (cid:98)\u03c3 have quadratic complexity. To see this, note\nthat x(cid:92)MMD\ncan be expanded into \u27e8(cid:98)\u00b51, (cid:98)\u00b52\u27e9k + \u27e8(cid:98)\u03bd1, (cid:98)\u03bd2\u27e9k \u2212 \u27e8(cid:98)\u00b51, (cid:98)\u03bd2\u27e9k \u2212 \u27e8(cid:98)\u03bd1, (cid:98)\u00b52\u27e9k. Each of these\nterms can be computed in O (cid:0)(n + m)2(cid:1). Similarly, each term in the summations defining (cid:98)\u03c32\nX and\nY also require O (cid:0)(n + m)2(cid:1) computation, implying that the (cid:98)\u03c3 also has O((n + m)2) complexity.\n(cid:98)\u03c32\nRemark 3. To simplify notation in what follows, we denote m as mn, where mn is some unknown\nnondecreasing sequence such that limn\u2192\u221e mn = \u221e. This still permits m, n to be separate quantities\ngrowing to infinity at potentially different rates, but it allows us to index the sequence of problems\nwith the single index n (rather than m, n). We will use kn, dn, Xn, Pn and Qn to indicate that\nquantities could (but do not have to) change as n increases, and drop the subscript when they are fixed.\nFurthermore, unless explicitly stated, we will focus on the balanced splitting scheme, i.e., n1 = \u230an/2\u230b\nand m1 = \u230am/2\u230b in what follows, because we currently see no apriori reason to split asymmetrically.\n\n2.1 Gaussian limiting distribution under the null hypothesis\nAs shown in Figure 1, the empirical distribution of \u00afx(cid:92)MMD\nresembles a standard normal distribution\nfor various choices of m, n and dimension d under the null. In this section, we formally prove this\nstatement. Recalling the mean embedding \u00b5 from (1), define\n\n2\n\nTheorem 4. Suppose that k and P do not change with n.\nX, X \u2032 i.i.d.\u223c P , then \u00afx(cid:92)MMD\n\nd\u2212\u2192 N (0, 1).\n\n2\n\n\u00afk(x, y) := \u27e8k(x, \u00b7) \u2212 \u00b5, k(y, \u00b7) \u2212 \u00b5\u27e9k.\n\n(5)\nIf 0 < EP [\u00afk(X, X \u2032)4] < \u221e for\n\nWe next present a more general result that implies Theorem 4.\nTheorem 5. Suppose P is fixed, but the kernel kn changes with n. If\n\nlim\nn\u2192\u221e\n\nEP [\u00afkn(X1, X2)4]\nEP [\u00afkn(X1, X2)2]2\n\n(cid:18) 1\nn\n\n+\n\n1\nmn\n\n(cid:19)\n\n= 0,\n\nand\n\nlim\nn\u2192\u221e\n\n\u03bb2\n1,n\n(cid:80)\u221e\nl=1 \u03bb2\nl,n\n\nexists,\n\n(6)\n\nwhere (\u03bbl,n)\u221e\n\nl=1 denote the eigenvalues of \u00afk introduced in (16), then we have \u00afx(cid:92)MMD\n\n2\n\nd\u2212\u2192 N (0, 1).\n\nIt is easy to check that condition (6) is trivially satisfied if the kernels {kn : n \u2265 1} are uniformly\nbounded by some constant; prominent examples are the Gaussian or Laplace kernel with a sample size\n\n6\n\nKernel-MMDBlock-MMDCross-MMD\f\u221a\n\n(cid:113)\n\n1,n]/\n\n(cid:113)(cid:80)\n\nn (cid:80)n\n\nEPn [W 2\n\ni=1 Wi,n/\n\ni.i.d.\u223c Pn. Define Vn =\n\ndependent bandwidth. Thus, the above condition really exists to handle unbounded kernels and heavy-\ntailed distributions. To motivate this requirement, we recall Bentkus and G\u00f6tze (1996) (see Fact 11\nin Appendix A) who proved a studentized CLT for i.i.d. random variables in a triangular array\ni(Wi,n \u2212 \u00afWn)2 where\nsetup: W1,n, W2,n, . . . , Wn,n\n\u00afWn = ((cid:80)\ni Wi,n)/n. They showed that a sufficient condition for the asymptotic normality of Vn is\nthat limn\u2192\u221e EPn[W 3\n1,n]3n = 0. (This last condition is trivially true if Pn does not\nchange with n, meaning that the triangular array setup is irrelevant and W1,n can be replaced by W1.)\nOur requirement is slightly stronger: condition (6) with \u00afkn(X1, X2) replaced by W1,n implies the\nprevious condition of Bentkus and G\u00f6tze (1996) (details in Remark 12 in Appendix A). We need\nthis stronger condition, because the terms in the definition of x(cid:92)MMD\nare not i.i.d. (indeed, not even\nindependent), and thus we cannot directly apply the result of Bentkus and G\u00f6tze (1996). Instead,\nwe take a different route by first conditioning on the second half of data (X2, Y2), then showing the\nconditional asymptotic normality of the standardized x(cid:92)MMD\n(i.e., divided by conditional standard\ndeviation instead of empirical), and finally showing that the ratio of conditional and empirical standard\ndeviations converge in probability to 1 (see Appendix B).\n\n2\n\n2\n\nFinally, we note that the result of Theorem 5 can be further generalized in several ways: (i) instead of\na fixed P and changing kn, we can consider a sequence of pairs {(Pn, kn) : n \u2265 1} changing with\nn, (ii) we can let Pn \u2208 P (0)\nn , for a class of distributions changing with n, and obtain the Gaussian\nlimit uniformly over all elements of P (0)\nn , and finally, (iii) the moment requirements on \u00afkn stated in\ncondition in (6) can also be slightly weakened. We state and prove this significantly more general\nversion of Theorem 5 in Appendix B.\nRemark 6. In the statement of the two theorems of this section, the splits (X1, Y1) and (X2, Y2)\nare assumed to be drawn i.i.d. from the same distribution P . However, a closer look at the proof\nof Theorem 5 indicates that the conclusions of the above two theorems hold even when the two\nsplits are independent and drawn i.i.d. from possibly different distributions; that is (X1, Y1) and\n(X2, Y2) are independent of each other and drawn i.i.d. from distributions P1 and P2 respectively,\n2\nwith P1 \u0338= P2. In particular, under this more general condition, the asymptotic normality of \u00afx(cid:92)MMD\nstill holds, and the resulting test \u03a8 still controls the type-1 error at the desired level. This may be\nuseful for two-sample testing in settings where the entire set of data is not i.i.d., but two different\nparts of the data were collected in two different situations. The usual MMD can also handle such\nscenarios by using a subset of permutations that do not exchange the data across the two situations.\n\n2.2 Consistency against fixed and local alternatives\n\nHere, we show that the xMMD test \u03a8 introduced in (3) is consistent against a fixed alternative and\nalso has minimax rate-optimal power against smooth local alternatives separated in L2 norm.\nWe first show that analogous to Theorem 4, xMMD is consistent against fixed alternatives.\nTheorem 7. Suppose P, Q, k do not change with n, and P \u0338= Q. If k is a characteristic kernel\nsatisfying 0 < EP [\u00afk(X1, X2)4] < \u221e, and 0 < EQ[\u00afk(Y1, Y2)4] < \u221e, then the xMMD test is\nconsistent, meaning it has asymptotic power 1.\n\nThe moment conditions required above are mild, and are satisfied trivially, for instance, by bounded\nkernels such as the Gaussian kernel. The \u201ccharacteristic\u201d condition is also needed for the consistency\nof the usual MMD test (Gretton et al., 2012a), and is also satisfied by the Gaussian kernel.\n\nRecalling Remark 1, we next consider the more challenging setting where dn, kn can change with\nn, and (Pn, Qn) can vary within a class P (1)\nn \u2282 P(Xn) \u00d7 P(Xn) that can also change with n. We\npresent a sufficient condition under which the xMMD test \u03a8 is consistent uniformly over P (1)\nn . Define\n\u03b3n := MMD(Pn, Qn), which is assumed nonzero for each n but could approach zero in the limit.\nTheorem 8. Let {\u03b4n : n \u2265 2} denote any positive sequence converging to zero. If\n\nlim\nn\u2192\u221e\n\nsup\n(Pn,Qn)\u2208P (1)\n\nn\n\nEPn,Qn [(cid:98)\u03c32]\n\u03b4n\u03b34\nn\n\n+\n\n2\n\nVPn,Qn(x(cid:92)MMD\n\u03b34\nn\n\n)\n\n= 0, where V denotes variance,\n\n(7)\n\n7\n\n\fthen limn\u2192\u221e sup(Pn,Qn)\u2208P (1)\n\nn\n\nEPn,Qn [1 \u2212 \u03a8(X, Y)] = 0, meaning the xMMD test is consistent.\n\nNote that while any sequence {\u03b4n} converging to zero suffices for the general statement above, the\ncondition (7) is easiest to satisfy for slowly decaying \u03b4n, such as \u03b4n = 1/ log log n for instance.\nThe sufficient conditions for consistency of \u03a8 stated in terms of (cid:98)\u03c3 and x(cid:92)MMD\nin (7) can also be\ntranslated into equivalent conditions on the kernel function kn, similar to (6), and we present the\ndetails in Appendix C. Importantly, if Pn, Qn, dn are fixed and kn is bounded, then both E[(cid:98)\u03c32] and\nV(x(cid:92)MMD\n) are O(1/n), and \u03b3n is a constant, so the condition is trivially satisfied, and in fact the\nabove condition is even weaker than the fourth-moment condition of the previous theorem.\n\n2\n\n2\n\n2.3 Minimax rate optimality against smooth local alternatives\n\nWe now apply the general result of Theorem 8 to the case where the distributions Pn and Qn admit\nLebesgue densities pn and qn that lie in the order \u03b2 Sobolev ball for some \u03b2 > 0, defined as\nW \u03b2,2(M ) := {f : X \u2192 R | f is a.s. continuous, and (cid:82) (1 + \u03c92)\u03b2/2\u2225F(f )(\u03c9)\u22252dw < M < \u221e}.\nFormally, we define the null and alternative class of distributions as follows:\n\nand\n\nP (0)\nP (1)\n\nn , while under H1, we assume that (Pn, Qn) \u2208 P (1)\nn .\n\nn = {P with density p : p \u2208 W \u03b2,2(M )},\nn = {(P, Q) with densities p, q \u2208 W \u03b2,2(M ) : \u2225p \u2212 q\u2225L2 \u2265 \u2206n},\nfor some sequence \u2206n decaying to zero. In particular, we assume that under H0, Pn = Qn and\nPn \u2208 P (0)\nOur next result shows that for suitably chosen scale parameter, the xMMD test \u03a8 with the Gaussian\nkernel is minimax rate-optimal for the above class of local alternatives. For simplicity, we state\nthis result with n = m, noting that the result easily extends to the case when there exist constants\n0 < c \u2264 C, such that c \u2264 n/m \u2264 C.\nTheorem 9. Consider the case when n = m, and let {\u2206n : n \u2265 1} be a sequence such that\nlimn\u2192\u221e \u2206nn2\u03b2/(d+4\u03b2) = \u221e. On applying the xMMD test \u03a8 with the Gaussian kernel ksn(x, y) =\nexp(\u2212sn\u2225x \u2212 y\u22252\n\n2), if we choose the scale as sn \u224d n4/(d+4\u03b2), then we have\n\nlim\nn\u2192\u221e\n\nsup\nPn\u2208P (0)\n\nn\n\nEPn[\u03a8(X, Y)] \u2264 \u03b1 and\n\nlim\nn\u2192\u221e\n\ninf\n(Pn,Qn)\u2208P (1)\n\nn\n\nEPn,Qn[\u03a8(X, Y)] = 1.\n\n(8)\n\nThe proof of this statement is in Appendix C, and it follows by verifying that the conditions required\nby Theorem 8 are satisfied for the above choices of \u2206n and sn.\nRemark 10. Li and Yuan (2019, Theorem 5 (ii)) showed a converse of the above statement: if\nlimn\u2192\u221e \u2206nn2\u03b2/(d+4\u03b2) < \u221e, then there exists an \u03b1 \u2208 (0, 1) such that any asymptotically level \u03b1 test\n(cid:101)\u03a8 must have limn\u2192\u221e inf (P,Q)\u2208P(\u2206n) EP,Q[ (cid:101)\u03a8(X, Y)] < 1. Hence, the sequence of {\u2206n : n \u2265 1}\nused in Theorem 9 represents the smallest L2-deviations that can be detected by any test, and (8)\nshows that our xMMD test \u03a8 can detect such changes, establishing its minimax rate-optimality.\n\n\n\nThe following is the appendix_A section of the paper you are reviewing:\n\n\nB.2 Fixed P , changing kn (Theorem 5)\n\nWe note that the statement of Theorem 5 requires an additional technical assumption on the eigen-\nvalues of the kernel operator, introduced in (15). We repeat the statement of Theorem 5 with this\nadditional requirement below.\nTheorem 5\u2019. Suppose P is fixed, but the kernel kn changes with n. If\n\nlim\nn\u2192\u221e\n\nEP [\u00afkn(X1, X2)4]\nEP [\u00afkn(X1, X2)2]2\n\n 1\nn\n\n+\n\n1\nmn\n\n\n\n= 0,\n\nand\n\nlim\nn\u2192\u221e\n\n\u03bb2\n1,n\n\u221e\nl=1 \u03bb2\nl,n\n\nexists,\n\n(22)\n\nthen we have \u00afxMMD\n\n2\n\nd\u2212\u2192 N (0, 1).\n\nProof. The proof of this statement will follow the general outline of the proof of Theorem 15.\nHowever, in this special case when P is fixed, we can remove the condition that limn\u2192\u221e mn/n\nexists and is non-zero, that is required by Theorem 15.\n\nWe will carry over the notations used in the proof of Theorem 15, and in particular, we will use\nn1\n\u00afUX = 1\nj=1 Zj. Since Wi and Zj are identically distributed under\nn1\nthe null, we have EP [W 2\n2 to denote this conditional\nvariance. Then, note the following:\n\ni=1 Wi and \u00afUY = 1\nm1\ni |X2, Y2] = EP [Z 2\n\nj |X2, Y2], and we will use \u03c32\n\nm1\n\n\u00afxMMD\n\n2\n\n=\n\n\u00afUX \u2212 \u00afUY\n\u03c3\n\n=\n\n\u03c32\n\n:= T1 \u00d7 T2.\n\n\u00afUX \u2212 \u00afUY\n\n\n\n1 + m\u22121\nn\u22121\n1\n\n\n\n\u03c32\n\n1 + m\u22121\nn\u22121\n1\n\n\n\n \u00d7\n\n\u03c3\n\n(23)\n\nTo complete the proof, we will show that T1\nan application of Slutsky\u2019s theorem.\n\nd\u2212\u2192 N (0, 1) and T2\n\np\n\n\u2212\u2192 1. The result then follows by\n\n21\n\n\fFirst, we consider the term T1 in (23). Let Wi := Wi/\u03c32 and Zj := Zj/\u03c32. Then, conditioned on\n(X2, Y2), the terms Wi and Zj are independent and identically distributed. Introducing the constants\nui =\n\n m1\n\n n1\n\nm1(m1+n1) , we can write\n\nn1(m1+n1) and vj =\n\nT1 =\n\nn1\n\ni=1\n\nuiWi \u2212\n\nm1\n\nj=1\n\nvj Zj.\n\nWe can check that the constants (ui) and (vj) satisfy the property:\n\nlim\nn\u2192\u221e\n\nmax\ni,j\n\ni + v2\nu2\nj\ni\u2032 + m1\ni\u2032=1 u2\n\nn1\n\nj\u2032=1 v2\nj\u2032\n\n\u2264 lim\nn\u2192\u221e\n\nmax\ni,j\n\n1\nm1\n\n+\n\n1\nn1\n\n= 0.\n\nthis also means that\nthat is,\n\nd\u2212\u2192 N (0, 1) conditioned\nSince the limiting distribution (in this case, standard normal) is continu-\nthe T1 converges to N (0, 1) in the Kolmogorov-Smirnov met-\np\n\u2212\u2192 0. Since the random vari-\nis bounded, convergence in probability implies\nlimn\u2192\u221e EP [supx\u2208R |PP (T1 \u2264 x|X2, Y2) \u2212 \u03a6(x)|] = 0, which in turn implies that\n\nThus, by an application of Lindeberg\u2019s CLT, we note that T1\non (X2, Y2).\nous,\nric,\nable supx\u2208R |PP (T1 \u2264 x|X2, Y2) \u2212 \u03a6(x)|\nthat\nlimn\u2192\u221e supx\u2208R |EP [PP (T1 \u2264 x|X2, Y2) \u2212 \u03a6(x)] | = 0, as required.\n\nlimn\u2192\u221e supx\u2208R |PP (T1 \u2264 x|X2, Y2) \u2212 \u03a6(x)|\n\nWe now consider the second term, T2, in (23). It remains to show that T2\n1/T 2\n\np\n\n2 \u2212 1\n\n\u2212\u2192 0, and the result will follow by an application of the continuous mapping theorem.\n\n\n\n \u2212 1\n\n\n\n\n+ \u03c32\nY\nm1\n+ 1\nm1\n\n\u03c32\nX\nn1\n 1\nn1\n\n\n\n\u2212 1\n\n\n\n\n\n\u2212 1\n\n\n\n\u03c32\nX\n\u03c32\n2\n\n\u03c32\nY\n\u03c32\n2\n\n1\nT 2\n2\n\n\u2212 1\n\n\n\n\n\n\n\n\n\u03c32\n2\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n+\n\n\u2264\n\n=\n\n.\n\n(24)\n\np\n\n\u2212\u2192 1. We will show that\n\nThus, it suffices to show that both terms in (24) converge in probability to 0. This is exactly the\nresult that is proved in Lemma 18 under the two conditions listed in Assumption 1. The condition\non eigenvalues is already assumed in the statement of Theorem 5\u2019, and thus we will show that the\ncondition on the kernels, stated in (22), implies the condition (17). To prove this, we first, we note\nthat\n\nEP\n\n\u00afkn(X1, X2)2\u00afkn(X1, X3)2 \u2264 EP\n= EP\n\n\u00afkn(X1, X2)41/2 EP\n\u00afkn(X1, X2)4 .\n\n\u00afkn(X1, X3)41/2\n\nThus, the term in (17) is upper bounded by\n\u00afkn(X1, X2)4\n\u00afkn(X1, X2)22\n\nEP\nEP\n\n 1\nn\n\n+\n\n1\nmn\n\n \n\n1 +\n\n1\nn\n\n+\n\n\n\n.\n\n1\nmn\n\nSince, we have assumed that limn\u2192\u221e mn \u2192 \u221e, there exists and n0, such that for all n \u2265 n0,\n1 + 1\n\u2264 2. This implies that if (22) is satisfied, then (17) in Assumption 1 is also satisfied, as\nrequired.\n\nn + 1\nmn\n\nB.3 Fixed k, and fixed P (Theorem 4)\n\nWe prove Theorem 4 by showing that under the bounded fourth moment assumption on \u00afk, both the\nconditions required by Theorem 5\u2019 are satisfied.\nNote that since EP [\u00afk(X1, X2)] = 0, the positive and finite fourth moment also implies that the\nsecond moment of \u00afk(X1, X2) is also positive and finite. Hence, we have that\n\nThis, in turn, implies\n\nEP [\u00afk(X1, X2)4]\nEP [\u00afk(X1, X2)2]2\n\n< \u221e.\n\nlim\nn\u2192\u221e\n\nEP [\u00afk(X1, X2)4]\nEP [\u00afk(X1, X2)2]2\n\n 1\nn\n\n+\n\n1\nmn\n\n\n\n= 0,\n\n22\n\n\fas required by Theorem 5.\n\nFor the second part of the condition, we note that as kernel k and probability distribution P are\n\u03bb2\ndoesn\u2019t change with n, and hence its limit exists. Thus, both the conditions\nfixed, the term\n1\nl \u03bb2\nl\nfor Theorem 5\u2019 are satisfied, as required.\n\n\n\n\n\nBased on the section(s), please answer the following question with yes, no, or n/a and provide a brief justification for your answer.\nQuestion: If the authors include theoretical results, do the authors include complete proofs of all theoretical results?"}, "3a": {"role": "user", "content": "The following is the experiments section of the paper you are reviewing:\n\n\n2\n\nWe now present experimental validation of the theoretical claims of the previous section. In particular,\nour experiments demonstrate that (i) the limiting null distribution of \u00afx(cid:92)MMD\nis N (0, 1) under a\nwide range of choices of dimension d, sample sizes n, m and the kernel k, and (ii) the power of our\nxMMD test is competitive with the kernel-MMD permutation test. We now describe the experiments\nin more detail. Additional experimental results are reported in Appendix E.\nLimiting null distribution of \u00afx(cid:92)MMD\nhas a\nlimiting normal distribution under some mild assumptions. We empirically test this result when X\nand Y are drawn from N (0, Id) with 0 denoting the all-zeros vector in Rd, and in particular, study\nthe effects of (i) dimension: d = 10 versus d = 500, (ii) skewness of the samples: n/m = 1 versus\nn/m = 0.1, and (iii) choice of kernel: Gaussian versus Quadratic, both with data-dependent scale\nparameters using median heuristic.\n\n. We showed in Theorem 15 that the statistic \u00afx(cid:92)MMD\n\n2\n\n2\n\n8\n\n\fAs shown in the first row of Figure 3, the distribution of \u00afx(cid:92)MMD\nis robust to all these effects, and is\nclose to N (0, 1) in all cases. In contrast, the distribution of the kernel-MMD statistic scaled by its\nempirical standard deviation (obtained using 200 bootstrap samples) in the bottom row of Figure 3\nshows strong changes with these parameters. We present additional figures and details of the\nimplementation in Appendix E.\n\n2\n\nxMMD (n/m = 0.1)\n\nd = 10\n\nd = 500\nN (0, 1)\n\ny\nt\ni\ns\nn\ne\nd\n\ny\nt\ni\nl\ni\nb\na\nb\no\nr\nP\n\n2\n\n(cid:92)MMD\n\nFigure 3: The first two columns show the null distribution of the \u00afx(cid:92)MMD\nthe\nstatistic scaled by its empirical standard deviation (bottom row) using the Gaussian\nkernel with scale-parameter chosen using the median heuristic. The last two columns show the null\ndistribution for the two statistics using the Quadratic kernel with scale parameter chosen using the\n\nstatistic (top row) and\n\n2\n\nmedian heuristic. The figures demonstrate that the null distribution of\nwith dimension (d), the ratio n/m and the choice of the kernel, unlike our proposed statistic.\n\nchanges significantly\n\n2\n\n(cid:92)MMD\n\nEvaluation of the power of \u03a8. For d \u2265 1 and j \u2264 d, let a\u03f5,j denote the element of Rd with\nfirst j coordinates equal to \u03f5, and others equal to 0. We consider the two-sample testing problem\nwith P = N (0, Id) Q = N (a\u03f5,j, Id) for different choices of \u03f5 and d and j. We compare the\nperformance of our proposed test \u03a8 with the kernel-MMD permutation test, implemented with\nB = 200 permutations, and plot the power-curves (using 200 trials) in Figure 4. We also propose\na heuristic for predicting the power of the permutation test (denoted by \u03c1perm) using the power of\n\u03a8 (denoted by \u03c1\u03a8) as follows (with \u03a6 denoting the standard normal cdf, and z\u03b1 its \u03b1-quantile):\n\u221a\n\n(cid:16)\n\n(cid:98)\u03c1perm = \u03a6\n\nz\u03b1 +\n\n2 (cid:0)\u03a6\u22121(\u03c1\u03a8) \u2212 z\u03b1\n\n(cid:1)(cid:17)\n\n.\n\n(9)\n\n\u221a\n\nThis heuristic is motivated by the power expressions derived by Kim and Ramdas (2020) for the\nproblems of one-sample Gaussian mean and covariance testing (we discuss this further in Appendix E).\n2 in the above expression quantifies the price to pay for sample-splitting. As shown\nThe term\nin Figure 4, this heuristic gives us an accurate estimate of the power of the kernel-MMD permutation\ntest, without incurring the computational burden.\n\nWe now use ROC curves to compare the tradeoff between type-I and type-II errors for the usual MMD,\nlinear and block-MMDs with our \u00afx(cid:92)MMD\n. We use the same distributions P = N (0, Id) and Q =\nN (a\u03f5,j, Id) as before, and plot results for (d, j, \u03f5) \u2208 {(10, 5, 0.2), (100, 20, 0.15), (500, 100, 0.1)}\nin Figure 5. Due to sample splitting, the tradeoff achieved by our proposed statistic is slightly\n\n2\n\nworse than that of\nkernel-MMD statistics. More details about the implementation are presented in Appendix E.\n\n, but significantly better than other computationally efficient variants of\n\n2\n\n(cid:92)MMD\n\n\n\nThe following is the appendix_E section of the paper you are reviewing:\n\n\nComputing Infrastructure. All the experiments were performed on a workstation with Intel(R)\nCore(TM) i7-9700K CPU 3.60GHz and 32 GB of RAM with an NVIDIA GTX 1080 GPU.\n\nE.1 Implementation details of experiments reported in the main text\n\nDetails for Figure 1. For the null distribution, we set n = 500 and m = 625 and generated both X\nand Y from N (0, Id) for d = 10 and 100. In both cases, we computed the \u00afxMMD\nstatistic 2000\ntimes to plot the histogram.\n\n2\n\nFor the second figure, we obtain the power curves for the xMMD test and the MMD test with 200\npermutations for testing P = N (0, Id) againt Q = N (a\u03f5,j, Id). Here d = 10, j = 5 and \u03f5 = 0.2,\nand recall that a\u03f5,j is the vector in Rd obtained by setting the first j \u2264 d coordinates of 0 equal\nto \u03f5. We selected n and m from 20 equally spaced points in the intervals [10, 400] and [10, 500]\nrespectively, and ran 200 trials of the tests for every (n, m) pair to obtain the power curves. The error\nregions in the figure correspond to one bootstrap standard deviation with 200 bootstrap samples.\n\nFor the third figure, we set d = 100, j = 20, \u03f5 = 0.1, P = N (0, Id) and Q = N (a\u03f5,j, Id). We ran\nthe two tests, xMMD and MMD with 200 permutations, for 20 different (n, m) pairs in the range\n[10, 500], and repeated the experiment 200 times for every such pair. The figure plots the wall-clock\ntime, measure by Python\u2019s time.time() function, and plot the power against the average wall-clock\ntime over the 200 trials. The size of the marker is proportional to the sample size (i.e., n + m).\n\n30\n\n\fDetails for Figure 3. The two kernels used in this figure are the Gaussian and Quadratic kernels.\nThe Gaussian kernel with scale parameter s > 0 is defined as ks(x, y) = exp(\u2212s\u2225x \u2212 y\u22252\n2), while\nthe Quadratic kernel with scale s > 0 is defined as kQ(x, y) = 1 + s(xT y)2\n. With w denoting the\nmedian of the pairwise distance between all the observations, we set s = 1/(2w2) for the Gaussian\nkernel and s = 1/w for the Quadratic kernel.\n\nDetails for Figure 4. Given observations X1, X2, . . . , Xni.i.d.P , consider the problem of one-\nsample mean-testing, that is, testing H0 : E[Xi] = 0 versus H1 : E[Xi] = a \u0338= 0. When the distribu-\ntion P is a multivariate Gaussian, Kim and Ramdas (2020) showed that power of their test using a\n\n\n\n\none-sample studentized U-statistic based on a bi-linear kernel is asymptotically \u03a6\n\nz\u03b1 + aT a\n\u221a\n2\n\ntr(\u03a32)\n\n.\n\nThe power achieved by the test using the full U-statistic is \u03a6\n\n, which differs from\n\nthe previous expression by a factor of\ncovariance testing. Our heuristic in (9) is based on these two observations.\n\n2. A similar relation also holds for the problem of Gaussian\n\n\u221a\n\n\n\nz\u03b1 + aT a\u221a\n\n2tr(\u03a32)\n\n\n\nDetails for Figure 5. For plotting the ROC curves, we proceed as follows. We fix n = m =\n200, and then compute the MMD, block-MMD, linear-MMD and cross-MMD statistics for 1000\nindependent repetitions of \u2018null\u2019 and \u2018alternative\u2019 trials. For every null trial, we calculate all the\nstatistics on independent samples of sizes n and m drawn from P = N (0, Id), while for every\nalternative trial we calculate the statistics on independent samples of size n and m drawn from\nP = N (0, Id) and Q = N (a\u03f5,j, Id) respectively. Recall that a\u03f5,j is obtained by setting the first j\ncoordinates of 0 equal to \u03f5. Having obtained 2000 values for every statistic, we then plot the tradeoff\nbetween false positives (FP) and true positives (TP) as the rejection threshold is increased. The ability\nof a statistic to distinguish between the null and the alternative is quantified by the area under the\ncurve. In Figure 5, we used (d, j, \u03f5) \u2208 {(10, 5, 0.1), (100, 20, 0.1), (500, 100, 0.1)}.\n\nE.2 Additional Figures\n\nNull Distribution. Figure 6 denotes the null distribution of our proposed statistic (\u00afxMMD\n) along\nwith that of the usual MMD normalized by its empirical standard deviation. The null distribution\nin Figure 6 is Dirichlet with parameter 2 \u00d7 1 \u2208 Rd for d \u2208 {10, 500}.\n\n2\n\nPower Curves.\nIn Figure 7, we plot the power curves for the different tests using a Gaussian\nKernel, and we report the results of the same experiment with a polynomial kernel of degree 5\nin Figure 8. Recall that the polynomial kernel of degree r and scale parameter s > 0 is defined as\nk(x, y) = 1 + (xT y)/sr\n. In both instances, we selected the scale parameter using the median\nheuristic.\n\nFrom the figure, we can see that the xMMD test is competitive with the computationally more\ncostly tests, namely the MMD permutation test and the MMD-spectral test of Gretton et al. (2009).\nFurthermore, the performance of xMMD test is significantly better than the existing computationally\nefficient tests, namely block-MMD test (with block-size\n\nn) and linear-MMD test.\n\n\u221a\n\nROC curves.\nIn Figure 9, we plot some additional ROC curves for the different statistics. As\nbefore, we used 1000 \u2018null trials\u2019 and another 1000 \u2019alternative trials\u2019 with sample sizes n = 200 and\nm = 200. The data generating distributions P and Q were both Dirichlet with parameters 1 \u2208 Rd\nand (1 + \u03f5) \u00d7 1 \u2208 Rd for (d, \u03f5) \u2208 {(10, 0.4), (100, 0.2), (500, 0.15)}.\n\nE.3 Comparison with ME and SCF tests of Jitkrittum et al. (2016)\n\nWe now present some experimental results comparing the performance of our cross-MDD test with\nthe linear time mean embedding (MD) and smoothed characteristic function (SCF) tests of Jitkrittum\net al. (2016). These tests proceed in the following steps:\n\n\u2022 Fix J, and choose points {v1, . . . , vJ } from Rd, where d is the dimension of the observation\n\nspace.\n\n31\n\n\f2\n\nMMD\n\nFigure 6: The first two columns show the null distribution of the \u00afxMMD\nstatistic scaled by its empirical standard deviation (bottom row) using the Gaussian\nthe\nkernel with scale-parameter chosen using the median heuristic. The last two columns show the null\ndistribution for the two statistics using the Polynomial kernel of degree 5 with scale parameter chosen\n\nstatistic (top row) and\n\n2\n\nusing the median heuristic. The figures demonstrate that the null distribution of\nchanges\nsignificantly with dimension (d), the ratio n/m and the choice of the kernel, unlike our proposed\nstatistic.\n\n2\n\nMMD\n\nFigure 7: Power Curves for the different tests using Gaussian kernel with scale parameter chosen\nvia median heuristic. The two distributions are P = N (0, Id) and Q = N (a\u03f5,j, Id) where a\u03f5,j\nis obtained by setting the first j \u2264 d coordinates of 0 \u2208 Rd equal to \u03f5. The figures demonstrate\nthat the xMMD test is competitive with more computationally expensive tests (MMD-perm and\nMMD-spectral), while performing significantly better than the low complexity alternatives (B-MMD\nand L-MMD). The batch-size used in the B-MMD test was\n\nn.\n\n\u221a\n\n\u2022 Using X and Y with n = m, compute {zi\n\n[k(vJ , Xi) \u2212 k(vJ , Yi)]J\n\u02c6l(Yi) sin(Y T\nfine \u00afzn = 1\nn\n\ni vj), \u02c6l(Xi) cos(X T\nn\n\ni=1 zi, and Sn = 1\n\nj=1 \u2208 RJ for ME test, and zi = [\u02c6l(Xi) sin(X T\ni vj)]J\nn\u22121 (zi \u2212 \u00afzn)(zi \u2212 \u00afzn)T .\n\n: 1 \u2264 i \u2264 n}, where zi =\ni vj \u2212\nj=1 \u2208 R2J for the SCF test. De-\n\ni vj) \u2212 \u02c6l(Yi) cos(Y T\n\n\u2022 Using the above, define the test statistic\n\u02c6\u03bbn := \u00afzT\n\nn (Sn + \u03b3nI)\u22121 \u00afzn,\n\nwhere \u03b3n is some regularization parameter that converges to 0 with n, and I denotes the\nidentity matrix. For a fixed d and J, Jitkrittum et al. (2016) show that the above statistic has\n\n32\n\nProbabilitydensityxMMD(n/m=5)d=10d=100N(0,1)xMMD(n/m=1)xMMD(n/m=0.2)xMMD(n/m=1)ProbabilitydensityMMD(n/m=5)MMD(n/m=1)MMD(n/m=0.2)MMD(n/m=1)020040060080000.51Sample-Size(n+m)Power(d=10,j=5,\u03f5=0.2)MMD-permxMMDpredictedMMD-spectralB-MMDL-MMD020040060080000.51Sample-Size(n+m)Power(d=50,j=15,\u03f5=0.125)020040060080000.51Sample-Size(n+m)Power(d=100,j=20,\u03f5=0.15)\fFigure 8: Power curves of the different kernel-based tests using a polynomial kernel of degree 5, i.e.,\nk(x, y) = 1 + (xT y)/s5\n\nwith s chosen via the median heuristic.\n\nFigure 9: ROC curves using the different statistics with Gaussian kernel for testing two Dirichlet\ndistributions in dimensions d \u2208 {10, 100, 500} with sample-size n = m = 200. The two distributions\nare P = Dirichlet(1) and Q = Dirichlet((1 + \u03f5) \u00d7 1) where 1 \u2208 Rd is the all-ones vector.\n\na \u03c72(J) (resp. \u03c72(2J)) limiting null distribution in the ME (resp. SCF) case. This result is\nused to calibrate the test at a given level \u03b1.\n\nIn Figure 10, we plot the variation of type-I error and power with sample-size of the three tests for\nthe Gaussian Mean Difference (GMD) source with d = 10. As the figures suggest, the cross-MMD\nachieves higher power and tighter control over the type-I error than the ME and SCF tests in this\nregime.\n\nThe ME and SCF tests are calibrated based on the limiting distribution of their statistic in the low\ndimensional regime: fixed d, and n \u2192 \u221e. However, the high type-I error of these tests for small n\nvalues suggests that their limiting distribution may be different in the high dimensional regime, when\nboth d and n go to infinity. We further observe this in Figure 11 when d = 100 and d/n > 1.\n\nWe end this section with a discussion of some key points of difference between the ME and SCF tests,\nand our proposed cross-MMD test.\n\n\u2022 The ME and SCF tests require the kernel to be uniformly bounded, whereas our test requires\nonly mild moment conditions that are even satisfied by unbounded kernels if the underlying\ndistributions are not too heavy-tailed (formally described in Assumption 1). Furthermore,\nthe ME and SCF tests have several tuning parameters: number of features J, {v1, . . . , vJ },\nbandwidth, step-size for gradient ascent etc. In practice, J is usually set to 5, and the other\nparameters are selected by solving a Jd + 1 dimensional optimization problem via gradient\nascent. While each step of gradient ascent has linear in n complexity, the number of steps\nneeded may be large for higher dimensions, resulting in a higher computational overhead.\n\n33\n\n10020030000.51Sample-Size(n+m)Power(d=10,j=5,\u03f5=0.3)MMD-permxMMDpredictedMMD-spectralB-MMDL-MMD10020030000.51Sample-Size(n+m)Power(d=50,j=5,\u03f5=0.4)10020030000.51Sample-Size(n+m)Power(d=100,j=10,\u03f5=0.5)00.5100.51FalsePositiveRateTruePositiveRateDirichlet(d=10,\u03f5=0.4)MMDxMMDB-MMD(n1/2)B-MMD(n1/3)L-MMD00.5100.51FalsePositiveRateTruePositiveRateDirichlet(d=100,\u03f5=0.2))00.5100.51FalsePositiveRateTruePositiveRateDirichlet(d=500,\u03f5=0.15)\fGMD Source: (d=10, 200 trials)\n\nx-MMD\nME\nSCF\n\nr\no\nr\nr\ne\n\nI\n-\ne\np\ny\nT\n\n0.6\n\n0.4\n\n0.2\n\n0\n\n50\n\n100\n\n150\nSample-Size (n+m)\n\n200\n\nFigure 10: The figures plot the variation of the type-I error (left) and the power (right) with sample-\nsize of the three tests: cross-MMD, and the two linear time tests, ME and SCF, proposed by Jitkrittum\net al. (2016).\n\nGMD Source: (d=100, 200 trials)\n\nx-MMD\nME\nSCF\n\n0.8\n\n0.6\n\n0.4\n\n0.2\n\nr\no\nr\nr\ne\n\nI\n-\ne\np\ny\nT\n\n40\n\n50\n\n60\n80\n70\nSample-Size (n+m)\n\n90\n\n100\n\nFigure 11: The ME and SCF tests provide poor control over the type-I error in the regime when d/n\nis large, suggesting that the limiting null distribution is different (or the convergence rate is slow) in\nthis regime.\n\n\u2022 More importantly, the ME and SCF tests are only valid in the \u2018low-dimensional setting\u2019:\nfixed d and J, with n \u2192 \u221e. In the high dimensional setting, when (d, n) \u2192 \u221e, the limiting\nnull distribution may no longer be \u03c72(J). This is also suggested by the behavior of type-I\nerror of ME and SCF tests in Figure 10 and Figure 11. This results in the following practical\nissue: given a problem with n = 500 and d = 200, how should one calibrate the threshold\nfor those tests?\nOur proposed test does not suffer from this, because in both high and low dimensional\nsettings, our statistic has the same limiting distribution. This is a significant practical\nadvantage of our cross-MMD test over ME and SCF tests.\n\n\u2022 In the regime where the number of features, J, is allowed to increase with n, we expect that\nthe resulting ME and SCF tests may have low power (for small regularization parameter \u03b3n).\nThis is because, the test statistic \u02c6\u03bbn used by ME and SCF tests is similar to Hotelling\u2019s T 2\n\n34\n\n1002003004000.40.60.81Sample-Size(n+m)PowerGMDSource:(\u03f5=1.0,d=10)x-MMDMESCF\fstatistic, for which Bai and Saranadasa (1996) characterized the asymptotic power in this\nregime. In particular, their Theorem 2.1 implies that the power of the T 2 test grows slowly\nwith n, especially when J/n \u2248 1.\nFinally, we note that our ideas also extend to more general degenerate U-statistics (as\ndiscussed in Appendix D.1). Hence, they are also applicable in cases beyond MMD distance,\nwhere we may not have good linear time alternatives.\n\nE.4 Type-I Error and goodness-of-fit test of null distribution\n\nIn this section, we experimentally verify the limiting Gaussian distribution of the \u00afxMMD\nstatistic\nunder the null. We first plot the variation of the type-I error of our cross-MMD test with sample\nsize in Figure 12. We considered the case when X and Y are both drawn i.i.d. from a multivariate\nGaussian vector in dimension d \u2208 {10, 100}, and n = m.\n\n2\n\nFigure 12: The two figures show the variation of the type-I error of the cross-MMD test with sample-\nsize for dimensions d \u2208 {10, 100}. The dashed horizontal line denotes the level \u03b1 = 0.05. In\nsummary, these tests do not find evidence against the null hypothesis that the null distribution is\nGaussian.\n\nNext, we plot the p-values for the test for normality proposed by D\u2019Agostino and Pearson (1973),\nand implemented in the function scipy.stats.normaltest in Python. We performed this test\nat different sample-sizes (n), and for each value of n, we calculated the \u00afxMMD\nstatistic on 200\ndifferent indpendent sample pairs. The results are shown in Figure 13\n\n2\n\nFigure 13: The two figures show p-values for the test for normality proposed by D\u2019Agostino and\nPearson (1973) (using the implementation scipy.stats.normaltest) of the cross-MMD statistic\nfor dimensions d \u2208 {10, 100}. In both dimension regimes, the test does not find evidence against the\nnull that the cross-MMD statistic is normally distributed under the null.\n\n35\n\n\fE.5 Comparison with Friedman-Rafsky test\n\nWe now compare the performance of our cross-MMD test with the Friedman-Rafsky two-sample test.\nThis test, proposed by Friedman and Rafsky (1979), uses a graph-based statistic that is a multivariate\ngeneralization of the Wald-Wolfowitz runs statistic introduced by Wald and Wolfowitz (1940). This\nstatistic, denoted by R, is constructed as follows:\n\n\u2022 Pool the samples X and Y to get Z of size N = n + m. Construct the complete graph with\n\nN nodes, and edge weights equal to the euclidean distance between two end points.\n\n\u2022 Construct the minimal spanning tree (MST) of the complete graph G, and denote the 0-1\n\nvalued adjacency matrix of this MST by M .\n\n\u2022 The statistic R is defined as one more than the number of edges in M with endpoints from\n\ndifferent samples.\n\nThe statistic R is expected to take a large value under the null when X and Y are drawn from the\nsame distribution. Hence, the FR test rejects the null for small values of R. The rejection threshold\ncan be obtained either by the limiting distribution of R characterized by (Henze and Penrose, 1999,\nTheorem 1), or using the permutation-test.\n\nIn Figure 14, we compare the power of the FR permutation-test with our cross-MMD test in a low\ndimensional (d/n small) and a high dimensional (d/n large) problem. In both cases, it is observed\nthat the power of FR test is significantly smaller than that of cross-MMD test.\n\nGMD Source (\u03f5 = 1.2, d = 10)\n\nxMMD\nFR\n\n1\n\n0.8\n\n0.6\n\n0.4\n\n0.2\n\nr\ne\nw\no\nP\n\n50\n\n100\n\n150\nSample-Size (n+m)\n\n200\n\nFigure 14: The figures show the power curves for Friedman-Rafsky (FR) test and our cross-MMD\ntest in the low (d = 10) and high (d = 100) dimensional settings with m = n in both plots. The\nfigures indicate that our cross-MMD test is significantly more powerful than the FR test.\n\nBased on the section(s), please answer the following question with yes, no, or n/a and provide a brief justification for your answer.\nQuestion: If the authors ran experiments, do the authors include the code, data, and instructions needed to reproduce the main experimental results (either in the supplemental material or as a URL)?"}, "3b": {"role": "user", "content": "The following is the experiments section of the paper you are reviewing:\n\n\n2\n\nWe now present experimental validation of the theoretical claims of the previous section. In particular,\nour experiments demonstrate that (i) the limiting null distribution of \u00afx(cid:92)MMD\nis N (0, 1) under a\nwide range of choices of dimension d, sample sizes n, m and the kernel k, and (ii) the power of our\nxMMD test is competitive with the kernel-MMD permutation test. We now describe the experiments\nin more detail. Additional experimental results are reported in Appendix E.\nLimiting null distribution of \u00afx(cid:92)MMD\nhas a\nlimiting normal distribution under some mild assumptions. We empirically test this result when X\nand Y are drawn from N (0, Id) with 0 denoting the all-zeros vector in Rd, and in particular, study\nthe effects of (i) dimension: d = 10 versus d = 500, (ii) skewness of the samples: n/m = 1 versus\nn/m = 0.1, and (iii) choice of kernel: Gaussian versus Quadratic, both with data-dependent scale\nparameters using median heuristic.\n\n. We showed in Theorem 15 that the statistic \u00afx(cid:92)MMD\n\n2\n\n2\n\n8\n\n\fAs shown in the first row of Figure 3, the distribution of \u00afx(cid:92)MMD\nis robust to all these effects, and is\nclose to N (0, 1) in all cases. In contrast, the distribution of the kernel-MMD statistic scaled by its\nempirical standard deviation (obtained using 200 bootstrap samples) in the bottom row of Figure 3\nshows strong changes with these parameters. We present additional figures and details of the\nimplementation in Appendix E.\n\n2\n\nxMMD (n/m = 0.1)\n\nd = 10\n\nd = 500\nN (0, 1)\n\ny\nt\ni\ns\nn\ne\nd\n\ny\nt\ni\nl\ni\nb\na\nb\no\nr\nP\n\n2\n\n(cid:92)MMD\n\nFigure 3: The first two columns show the null distribution of the \u00afx(cid:92)MMD\nthe\nstatistic scaled by its empirical standard deviation (bottom row) using the Gaussian\nkernel with scale-parameter chosen using the median heuristic. The last two columns show the null\ndistribution for the two statistics using the Quadratic kernel with scale parameter chosen using the\n\nstatistic (top row) and\n\n2\n\nmedian heuristic. The figures demonstrate that the null distribution of\nwith dimension (d), the ratio n/m and the choice of the kernel, unlike our proposed statistic.\n\nchanges significantly\n\n2\n\n(cid:92)MMD\n\nEvaluation of the power of \u03a8. For d \u2265 1 and j \u2264 d, let a\u03f5,j denote the element of Rd with\nfirst j coordinates equal to \u03f5, and others equal to 0. We consider the two-sample testing problem\nwith P = N (0, Id) Q = N (a\u03f5,j, Id) for different choices of \u03f5 and d and j. We compare the\nperformance of our proposed test \u03a8 with the kernel-MMD permutation test, implemented with\nB = 200 permutations, and plot the power-curves (using 200 trials) in Figure 4. We also propose\na heuristic for predicting the power of the permutation test (denoted by \u03c1perm) using the power of\n\u03a8 (denoted by \u03c1\u03a8) as follows (with \u03a6 denoting the standard normal cdf, and z\u03b1 its \u03b1-quantile):\n\u221a\n\n(cid:16)\n\n(cid:98)\u03c1perm = \u03a6\n\nz\u03b1 +\n\n2 (cid:0)\u03a6\u22121(\u03c1\u03a8) \u2212 z\u03b1\n\n(cid:1)(cid:17)\n\n.\n\n(9)\n\n\u221a\n\nThis heuristic is motivated by the power expressions derived by Kim and Ramdas (2020) for the\nproblems of one-sample Gaussian mean and covariance testing (we discuss this further in Appendix E).\n2 in the above expression quantifies the price to pay for sample-splitting. As shown\nThe term\nin Figure 4, this heuristic gives us an accurate estimate of the power of the kernel-MMD permutation\ntest, without incurring the computational burden.\n\nWe now use ROC curves to compare the tradeoff between type-I and type-II errors for the usual MMD,\nlinear and block-MMDs with our \u00afx(cid:92)MMD\n. We use the same distributions P = N (0, Id) and Q =\nN (a\u03f5,j, Id) as before, and plot results for (d, j, \u03f5) \u2208 {(10, 5, 0.2), (100, 20, 0.15), (500, 100, 0.1)}\nin Figure 5. Due to sample splitting, the tradeoff achieved by our proposed statistic is slightly\n\n2\n\nworse than that of\nkernel-MMD statistics. More details about the implementation are presented in Appendix E.\n\n, but significantly better than other computationally efficient variants of\n\n2\n\n(cid:92)MMD\n\n\n\nThe following is the appendix_E section of the paper you are reviewing:\n\n\nComputing Infrastructure. All the experiments were performed on a workstation with Intel(R)\nCore(TM) i7-9700K CPU 3.60GHz and 32 GB of RAM with an NVIDIA GTX 1080 GPU.\n\nE.1 Implementation details of experiments reported in the main text\n\nDetails for Figure 1. For the null distribution, we set n = 500 and m = 625 and generated both X\nand Y from N (0, Id) for d = 10 and 100. In both cases, we computed the \u00afxMMD\nstatistic 2000\ntimes to plot the histogram.\n\n2\n\nFor the second figure, we obtain the power curves for the xMMD test and the MMD test with 200\npermutations for testing P = N (0, Id) againt Q = N (a\u03f5,j, Id). Here d = 10, j = 5 and \u03f5 = 0.2,\nand recall that a\u03f5,j is the vector in Rd obtained by setting the first j \u2264 d coordinates of 0 equal\nto \u03f5. We selected n and m from 20 equally spaced points in the intervals [10, 400] and [10, 500]\nrespectively, and ran 200 trials of the tests for every (n, m) pair to obtain the power curves. The error\nregions in the figure correspond to one bootstrap standard deviation with 200 bootstrap samples.\n\nFor the third figure, we set d = 100, j = 20, \u03f5 = 0.1, P = N (0, Id) and Q = N (a\u03f5,j, Id). We ran\nthe two tests, xMMD and MMD with 200 permutations, for 20 different (n, m) pairs in the range\n[10, 500], and repeated the experiment 200 times for every such pair. The figure plots the wall-clock\ntime, measure by Python\u2019s time.time() function, and plot the power against the average wall-clock\ntime over the 200 trials. The size of the marker is proportional to the sample size (i.e., n + m).\n\n30\n\n\fDetails for Figure 3. The two kernels used in this figure are the Gaussian and Quadratic kernels.\nThe Gaussian kernel with scale parameter s > 0 is defined as ks(x, y) = exp(\u2212s\u2225x \u2212 y\u22252\n2), while\nthe Quadratic kernel with scale s > 0 is defined as kQ(x, y) = 1 + s(xT y)2\n. With w denoting the\nmedian of the pairwise distance between all the observations, we set s = 1/(2w2) for the Gaussian\nkernel and s = 1/w for the Quadratic kernel.\n\nDetails for Figure 4. Given observations X1, X2, . . . , Xni.i.d.P , consider the problem of one-\nsample mean-testing, that is, testing H0 : E[Xi] = 0 versus H1 : E[Xi] = a \u0338= 0. When the distribu-\ntion P is a multivariate Gaussian, Kim and Ramdas (2020) showed that power of their test using a\n\n\n\n\none-sample studentized U-statistic based on a bi-linear kernel is asymptotically \u03a6\n\nz\u03b1 + aT a\n\u221a\n2\n\ntr(\u03a32)\n\n.\n\nThe power achieved by the test using the full U-statistic is \u03a6\n\n, which differs from\n\nthe previous expression by a factor of\ncovariance testing. Our heuristic in (9) is based on these two observations.\n\n2. A similar relation also holds for the problem of Gaussian\n\n\u221a\n\n\n\nz\u03b1 + aT a\u221a\n\n2tr(\u03a32)\n\n\n\nDetails for Figure 5. For plotting the ROC curves, we proceed as follows. We fix n = m =\n200, and then compute the MMD, block-MMD, linear-MMD and cross-MMD statistics for 1000\nindependent repetitions of \u2018null\u2019 and \u2018alternative\u2019 trials. For every null trial, we calculate all the\nstatistics on independent samples of sizes n and m drawn from P = N (0, Id), while for every\nalternative trial we calculate the statistics on independent samples of size n and m drawn from\nP = N (0, Id) and Q = N (a\u03f5,j, Id) respectively. Recall that a\u03f5,j is obtained by setting the first j\ncoordinates of 0 equal to \u03f5. Having obtained 2000 values for every statistic, we then plot the tradeoff\nbetween false positives (FP) and true positives (TP) as the rejection threshold is increased. The ability\nof a statistic to distinguish between the null and the alternative is quantified by the area under the\ncurve. In Figure 5, we used (d, j, \u03f5) \u2208 {(10, 5, 0.1), (100, 20, 0.1), (500, 100, 0.1)}.\n\nE.2 Additional Figures\n\nNull Distribution. Figure 6 denotes the null distribution of our proposed statistic (\u00afxMMD\n) along\nwith that of the usual MMD normalized by its empirical standard deviation. The null distribution\nin Figure 6 is Dirichlet with parameter 2 \u00d7 1 \u2208 Rd for d \u2208 {10, 500}.\n\n2\n\nPower Curves.\nIn Figure 7, we plot the power curves for the different tests using a Gaussian\nKernel, and we report the results of the same experiment with a polynomial kernel of degree 5\nin Figure 8. Recall that the polynomial kernel of degree r and scale parameter s > 0 is defined as\nk(x, y) = 1 + (xT y)/sr\n. In both instances, we selected the scale parameter using the median\nheuristic.\n\nFrom the figure, we can see that the xMMD test is competitive with the computationally more\ncostly tests, namely the MMD permutation test and the MMD-spectral test of Gretton et al. (2009).\nFurthermore, the performance of xMMD test is significantly better than the existing computationally\nefficient tests, namely block-MMD test (with block-size\n\nn) and linear-MMD test.\n\n\u221a\n\nROC curves.\nIn Figure 9, we plot some additional ROC curves for the different statistics. As\nbefore, we used 1000 \u2018null trials\u2019 and another 1000 \u2019alternative trials\u2019 with sample sizes n = 200 and\nm = 200. The data generating distributions P and Q were both Dirichlet with parameters 1 \u2208 Rd\nand (1 + \u03f5) \u00d7 1 \u2208 Rd for (d, \u03f5) \u2208 {(10, 0.4), (100, 0.2), (500, 0.15)}.\n\nE.3 Comparison with ME and SCF tests of Jitkrittum et al. (2016)\n\nWe now present some experimental results comparing the performance of our cross-MDD test with\nthe linear time mean embedding (MD) and smoothed characteristic function (SCF) tests of Jitkrittum\net al. (2016). These tests proceed in the following steps:\n\n\u2022 Fix J, and choose points {v1, . . . , vJ } from Rd, where d is the dimension of the observation\n\nspace.\n\n31\n\n\f2\n\nMMD\n\nFigure 6: The first two columns show the null distribution of the \u00afxMMD\nstatistic scaled by its empirical standard deviation (bottom row) using the Gaussian\nthe\nkernel with scale-parameter chosen using the median heuristic. The last two columns show the null\ndistribution for the two statistics using the Polynomial kernel of degree 5 with scale parameter chosen\n\nstatistic (top row) and\n\n2\n\nusing the median heuristic. The figures demonstrate that the null distribution of\nchanges\nsignificantly with dimension (d), the ratio n/m and the choice of the kernel, unlike our proposed\nstatistic.\n\n2\n\nMMD\n\nFigure 7: Power Curves for the different tests using Gaussian kernel with scale parameter chosen\nvia median heuristic. The two distributions are P = N (0, Id) and Q = N (a\u03f5,j, Id) where a\u03f5,j\nis obtained by setting the first j \u2264 d coordinates of 0 \u2208 Rd equal to \u03f5. The figures demonstrate\nthat the xMMD test is competitive with more computationally expensive tests (MMD-perm and\nMMD-spectral), while performing significantly better than the low complexity alternatives (B-MMD\nand L-MMD). The batch-size used in the B-MMD test was\n\nn.\n\n\u221a\n\n\u2022 Using X and Y with n = m, compute {zi\n\n[k(vJ , Xi) \u2212 k(vJ , Yi)]J\n\u02c6l(Yi) sin(Y T\nfine \u00afzn = 1\nn\n\ni vj), \u02c6l(Xi) cos(X T\nn\n\ni=1 zi, and Sn = 1\n\nj=1 \u2208 RJ for ME test, and zi = [\u02c6l(Xi) sin(X T\ni vj)]J\nn\u22121 (zi \u2212 \u00afzn)(zi \u2212 \u00afzn)T .\n\n: 1 \u2264 i \u2264 n}, where zi =\ni vj \u2212\nj=1 \u2208 R2J for the SCF test. De-\n\ni vj) \u2212 \u02c6l(Yi) cos(Y T\n\n\u2022 Using the above, define the test statistic\n\u02c6\u03bbn := \u00afzT\n\nn (Sn + \u03b3nI)\u22121 \u00afzn,\n\nwhere \u03b3n is some regularization parameter that converges to 0 with n, and I denotes the\nidentity matrix. For a fixed d and J, Jitkrittum et al. (2016) show that the above statistic has\n\n32\n\nProbabilitydensityxMMD(n/m=5)d=10d=100N(0,1)xMMD(n/m=1)xMMD(n/m=0.2)xMMD(n/m=1)ProbabilitydensityMMD(n/m=5)MMD(n/m=1)MMD(n/m=0.2)MMD(n/m=1)020040060080000.51Sample-Size(n+m)Power(d=10,j=5,\u03f5=0.2)MMD-permxMMDpredictedMMD-spectralB-MMDL-MMD020040060080000.51Sample-Size(n+m)Power(d=50,j=15,\u03f5=0.125)020040060080000.51Sample-Size(n+m)Power(d=100,j=20,\u03f5=0.15)\fFigure 8: Power curves of the different kernel-based tests using a polynomial kernel of degree 5, i.e.,\nk(x, y) = 1 + (xT y)/s5\n\nwith s chosen via the median heuristic.\n\nFigure 9: ROC curves using the different statistics with Gaussian kernel for testing two Dirichlet\ndistributions in dimensions d \u2208 {10, 100, 500} with sample-size n = m = 200. The two distributions\nare P = Dirichlet(1) and Q = Dirichlet((1 + \u03f5) \u00d7 1) where 1 \u2208 Rd is the all-ones vector.\n\na \u03c72(J) (resp. \u03c72(2J)) limiting null distribution in the ME (resp. SCF) case. This result is\nused to calibrate the test at a given level \u03b1.\n\nIn Figure 10, we plot the variation of type-I error and power with sample-size of the three tests for\nthe Gaussian Mean Difference (GMD) source with d = 10. As the figures suggest, the cross-MMD\nachieves higher power and tighter control over the type-I error than the ME and SCF tests in this\nregime.\n\nThe ME and SCF tests are calibrated based on the limiting distribution of their statistic in the low\ndimensional regime: fixed d, and n \u2192 \u221e. However, the high type-I error of these tests for small n\nvalues suggests that their limiting distribution may be different in the high dimensional regime, when\nboth d and n go to infinity. We further observe this in Figure 11 when d = 100 and d/n > 1.\n\nWe end this section with a discussion of some key points of difference between the ME and SCF tests,\nand our proposed cross-MMD test.\n\n\u2022 The ME and SCF tests require the kernel to be uniformly bounded, whereas our test requires\nonly mild moment conditions that are even satisfied by unbounded kernels if the underlying\ndistributions are not too heavy-tailed (formally described in Assumption 1). Furthermore,\nthe ME and SCF tests have several tuning parameters: number of features J, {v1, . . . , vJ },\nbandwidth, step-size for gradient ascent etc. In practice, J is usually set to 5, and the other\nparameters are selected by solving a Jd + 1 dimensional optimization problem via gradient\nascent. While each step of gradient ascent has linear in n complexity, the number of steps\nneeded may be large for higher dimensions, resulting in a higher computational overhead.\n\n33\n\n10020030000.51Sample-Size(n+m)Power(d=10,j=5,\u03f5=0.3)MMD-permxMMDpredictedMMD-spectralB-MMDL-MMD10020030000.51Sample-Size(n+m)Power(d=50,j=5,\u03f5=0.4)10020030000.51Sample-Size(n+m)Power(d=100,j=10,\u03f5=0.5)00.5100.51FalsePositiveRateTruePositiveRateDirichlet(d=10,\u03f5=0.4)MMDxMMDB-MMD(n1/2)B-MMD(n1/3)L-MMD00.5100.51FalsePositiveRateTruePositiveRateDirichlet(d=100,\u03f5=0.2))00.5100.51FalsePositiveRateTruePositiveRateDirichlet(d=500,\u03f5=0.15)\fGMD Source: (d=10, 200 trials)\n\nx-MMD\nME\nSCF\n\nr\no\nr\nr\ne\n\nI\n-\ne\np\ny\nT\n\n0.6\n\n0.4\n\n0.2\n\n0\n\n50\n\n100\n\n150\nSample-Size (n+m)\n\n200\n\nFigure 10: The figures plot the variation of the type-I error (left) and the power (right) with sample-\nsize of the three tests: cross-MMD, and the two linear time tests, ME and SCF, proposed by Jitkrittum\net al. (2016).\n\nGMD Source: (d=100, 200 trials)\n\nx-MMD\nME\nSCF\n\n0.8\n\n0.6\n\n0.4\n\n0.2\n\nr\no\nr\nr\ne\n\nI\n-\ne\np\ny\nT\n\n40\n\n50\n\n60\n80\n70\nSample-Size (n+m)\n\n90\n\n100\n\nFigure 11: The ME and SCF tests provide poor control over the type-I error in the regime when d/n\nis large, suggesting that the limiting null distribution is different (or the convergence rate is slow) in\nthis regime.\n\n\u2022 More importantly, the ME and SCF tests are only valid in the \u2018low-dimensional setting\u2019:\nfixed d and J, with n \u2192 \u221e. In the high dimensional setting, when (d, n) \u2192 \u221e, the limiting\nnull distribution may no longer be \u03c72(J). This is also suggested by the behavior of type-I\nerror of ME and SCF tests in Figure 10 and Figure 11. This results in the following practical\nissue: given a problem with n = 500 and d = 200, how should one calibrate the threshold\nfor those tests?\nOur proposed test does not suffer from this, because in both high and low dimensional\nsettings, our statistic has the same limiting distribution. This is a significant practical\nadvantage of our cross-MMD test over ME and SCF tests.\n\n\u2022 In the regime where the number of features, J, is allowed to increase with n, we expect that\nthe resulting ME and SCF tests may have low power (for small regularization parameter \u03b3n).\nThis is because, the test statistic \u02c6\u03bbn used by ME and SCF tests is similar to Hotelling\u2019s T 2\n\n34\n\n1002003004000.40.60.81Sample-Size(n+m)PowerGMDSource:(\u03f5=1.0,d=10)x-MMDMESCF\fstatistic, for which Bai and Saranadasa (1996) characterized the asymptotic power in this\nregime. In particular, their Theorem 2.1 implies that the power of the T 2 test grows slowly\nwith n, especially when J/n \u2248 1.\nFinally, we note that our ideas also extend to more general degenerate U-statistics (as\ndiscussed in Appendix D.1). Hence, they are also applicable in cases beyond MMD distance,\nwhere we may not have good linear time alternatives.\n\nE.4 Type-I Error and goodness-of-fit test of null distribution\n\nIn this section, we experimentally verify the limiting Gaussian distribution of the \u00afxMMD\nstatistic\nunder the null. We first plot the variation of the type-I error of our cross-MMD test with sample\nsize in Figure 12. We considered the case when X and Y are both drawn i.i.d. from a multivariate\nGaussian vector in dimension d \u2208 {10, 100}, and n = m.\n\n2\n\nFigure 12: The two figures show the variation of the type-I error of the cross-MMD test with sample-\nsize for dimensions d \u2208 {10, 100}. The dashed horizontal line denotes the level \u03b1 = 0.05. In\nsummary, these tests do not find evidence against the null hypothesis that the null distribution is\nGaussian.\n\nNext, we plot the p-values for the test for normality proposed by D\u2019Agostino and Pearson (1973),\nand implemented in the function scipy.stats.normaltest in Python. We performed this test\nat different sample-sizes (n), and for each value of n, we calculated the \u00afxMMD\nstatistic on 200\ndifferent indpendent sample pairs. The results are shown in Figure 13\n\n2\n\nFigure 13: The two figures show p-values for the test for normality proposed by D\u2019Agostino and\nPearson (1973) (using the implementation scipy.stats.normaltest) of the cross-MMD statistic\nfor dimensions d \u2208 {10, 100}. In both dimension regimes, the test does not find evidence against the\nnull that the cross-MMD statistic is normally distributed under the null.\n\n35\n\n\fE.5 Comparison with Friedman-Rafsky test\n\nWe now compare the performance of our cross-MMD test with the Friedman-Rafsky two-sample test.\nThis test, proposed by Friedman and Rafsky (1979), uses a graph-based statistic that is a multivariate\ngeneralization of the Wald-Wolfowitz runs statistic introduced by Wald and Wolfowitz (1940). This\nstatistic, denoted by R, is constructed as follows:\n\n\u2022 Pool the samples X and Y to get Z of size N = n + m. Construct the complete graph with\n\nN nodes, and edge weights equal to the euclidean distance between two end points.\n\n\u2022 Construct the minimal spanning tree (MST) of the complete graph G, and denote the 0-1\n\nvalued adjacency matrix of this MST by M .\n\n\u2022 The statistic R is defined as one more than the number of edges in M with endpoints from\n\ndifferent samples.\n\nThe statistic R is expected to take a large value under the null when X and Y are drawn from the\nsame distribution. Hence, the FR test rejects the null for small values of R. The rejection threshold\ncan be obtained either by the limiting distribution of R characterized by (Henze and Penrose, 1999,\nTheorem 1), or using the permutation-test.\n\nIn Figure 14, we compare the power of the FR permutation-test with our cross-MMD test in a low\ndimensional (d/n small) and a high dimensional (d/n large) problem. In both cases, it is observed\nthat the power of FR test is significantly smaller than that of cross-MMD test.\n\nGMD Source (\u03f5 = 1.2, d = 10)\n\nxMMD\nFR\n\n1\n\n0.8\n\n0.6\n\n0.4\n\n0.2\n\nr\ne\nw\no\nP\n\n50\n\n100\n\n150\nSample-Size (n+m)\n\n200\n\nFigure 14: The figures show the power curves for Friedman-Rafsky (FR) test and our cross-MMD\ntest in the low (d = 10) and high (d = 100) dimensional settings with m = n in both plots. The\nfigures indicate that our cross-MMD test is significantly more powerful than the FR test.\n\nBased on the section(s), please answer the following question with yes, no, or n/a and provide a brief justification for your answer.\nQuestion: If the authors ran experiments, do the authors specify all the training details (e.g., data splits, hyperparameters, how they were chosen)?"}, "3c": {"role": "user", "content": "The following is the introduction section of the paper you are reviewing:\n\n\n1\ni.i.d.\u223c P and Y =\nWe study the two-sample testing problem: given X = (X1, . . . , Xn)\n(Y1, . . . , Ym) i.i.d.\u223c Q, we test the null hypothesis H0 : P = Q against the alternative H1 : P \u0338= Q.\nThis is a nonparametric hypothesis problem with a composite null hypothesis and a composite\nalternative hypothesis. It finds applications in diverse areas such as testing microarray data, clinical\ndiagnosis, and database attribute matching (Gretton et al., 2012a).\n\nA popular approach to solving this problem is based on the kernel-MMD distance between the two\nempirical distributions (Gretton et al., 2006). Given a positive definite kernel k, the kernel-MMD\ndistance between two distributions P and Q on X , denoted by MMD(P, Q), is defined as\n\nMMD(P, Q) = \u2225\u00b5 \u2212 \u03bd\u2225k, where \u00b5(\u00b7) =\n\n\n\nX\n\nk(x, \u00b7)dP (x), and \u03bd(\u00b7) =\n\n\n\nX\n\nk(x, \u00b7)dQ(x).\n\n(1)\n\nAbove, \u00b5 and \u03bd are commonly called \u201ckernel mean maps\u201d, and denote the kernel mean embeddings\nof the distributions P and Q into the reproducing kernel Hilbert space (RKHS) associated with the\npositive-definite kernel k, and \u2225 \u00b7 \u2225k denotes the corresponding RKHS norm. Under mild conditions\non the positive definite kernel k (Sriperumbudur et al., 2011), MMD is a metric on the space of\n\n36th Conference on Neural Information Processing Systems (NeurIPS 2022).\n\n\fprobability distributions. Gretton et al. (2006) suggested using an empirical estimate of the squared\ndistance as the test statistic. In particular, given X and Y, define the test statistic\n\n2\n\nMMD\n\n:=\n\n1\nn(n \u2212 1)m(m \u2212 1)\n\n\n\n\n\nh(Xi, Xi\u2032, Yj, Yj\u2032),\n\n1\u2264i\u0338=i\u2032\u2264n\nwhere h(x, x\u2032, y, y\u2032) := k(x, x\u2032)\u2212k(x, y\u2032)\u2212k(y, x\u2032)+k(y, y\u2032). The above statistic has an alternative\nform that only takes quadratic time to calculate.\n\n1\u2264j\u0338=j\u2032\u2264m\n\nexceeds a suitable threshold \u03c4 \u2261 \u03c4 (\u03b1) that ensures the false\nThe MMD test rejects the null if\npositive rate is at most \u03b1. For \u201ccharacteristic kernels\u201d, this test is consistent against fixed alternatives,\nmeaning the power (the probability of rejecting the null when P \u0338= Q) increases to one as m, n \u2192 \u221e.\n\n2\n\nMMD\n\nThe difficulty in practically determining \u03c4 will play a key role in this paper. It is well known that\nwhen P = Q,\n\nis an instance of a \u201cdegenerate two-sample U-statistic\u201d, meaning that:\n\nMMD\n\n2\n\nUnder H0, EP [h(x, X \u2032, y, Y \u2032)] = EQ[h(X, x\u2032, Y, y\u2032)] = 0.\n\n(Above, x, y, x\u2032, y\u2032 are fixed, and the expectations are over X, Y, X \u2032, Y \u2032 i.i.d.\u223c P .) As a consequence,\nits (limiting) null distribution is unwieldy; it is an infinite sum of independent \u03c72 random variables\nweighted by the eigenvalues of an operator that depends on the kernel k and the underlying distribution\nP (see equation (10) in Appendix A). Since P is unknown, one cannot explicitly calculate \u03c4 .\n\nIn practice, a permutation-based approach is commonly used, where \u03c4 is set as the (1 \u2212 \u03b1)-quantile\nof the kernel-MMD statistic computed on B permuted versions of the aggregate data (X, Y). The\nresulting test has finite-sample validity, but its practical applicability is reduced due to the high\ncomputational complexity; if B = 200 permutations are used, the (permuted) test statistic must be\nrecomputed 201 times, rather than once (usually, B is chosen between 100 and 1000).\n\nDue to the high computational complexity of the permutation test, some permutation-free alternatives\nfor selecting \u03c4 have been proposed. However, as we discuss in Section 1.2, these alternatives are\neither too conservative in practice (using concentration inequalities), or heuristics with no theoretical\nguarantees (Pearson curves and Gamma approximation) or are only shown to be consistent in the\nsetting where the kernel k does not vary with n (spectral approximation). We later recap some\n\ncomputationally efficient alternatives to\n\nMMD\n\n2\n\n, but these have significantly lower power.\n\nAs far as we are aware, there exists no method in literature based on the kernel-MMD that is (i)\npermutation-free (does not require permutations), (ii) consistent against any fixed alternative, (iii)\nachieves minimax rate-optimality against local alternatives, and (iv) is correct for both the fixed\nkernel setting (k is fixed as m, n \u2192 \u221e) and the changing kernel setting (k changes as a function of\nm, n, for instance, by selecting the scale parameter of a Gaussian kernel in a data-driven manner).\n\n\u221a\n\nOur work delivers a novel and simple test satisfying all four desirable properties. We propose a\nnew variant of the kernel-MMD statistic that (after studentization) has a standard Gaussian limiting\ndistribution under the null in both the fixed and changing kernel settings, in low- and high-dimensional\n2 factor\nsettings. There is a computation-statistics tradeoff: our permutation-free test loses about a\nin power compared to the standard kernel-MMD test, but it is hundreds of times faster.\nRemark 1. Let P(X ) denote the set of all probability measures on the observation space X , where\nwe often use X = Rd for some d \u2265 1. For simplicity, in the above presentation, the distributions\nP, Q, kernel k and dimension d did not change with sample size, and this is the setting considered in\nthe majority of the literature. Later, we prove several of our results in a significantly more general\nsetting where P, Q, d, k can vary with n, m. Under the null, this provides a much more robust type-I\nerror control in high-dimensional settings, even with data-dependent kernels. Under the alternative,\nthis provides a more fine-grained power result. To elaborate on the latter, we assume that for every\nn, m, the pair (P, Q) = (Pn, Qn) \u2208 P (1)\nn : n, m \u2265 2}.\nThe class P (1)\nn is such that with increasing n and m, it contains pairs (P \u2032, Q\u2032) that are increasingly\ncloser in some distance measure \u03f1; thus the alternatives can approach the null and be equal in the\n\u03f1(P \u2032, Q\u2032) decreases with n, m, and such alternatives are\nlimit. That is, \u2206n,m := inf (P \u2032,Q\u2032)\u2208P (1)\ncalled local alternatives (as opposed to fixed alternatives). This framework allows us to characterize\nthe detection boundary of a test, that is, the smallest perturbation from the null (in terms of \u2206n,m)\nthat can be consistently detected by a test.\n\nn \u2282 P(X ) \u00d7 P(X ) for some sequence {P (1)\n\nn\n\n2\n\n\fPaper outline. We present an overview of our main results in Section 1.1 and discuss related\nwork in Section 1.2. In Section 2, we present the cross-MMD statistic and obtain its limiting null\ndistribution in Section 2.1. We demonstrate its consistency against fixed alternatives and minimax rate-\noptimality against smooth local alternatives in Section 2.2. Section 3 contains numerical experiments\nthat demonstrate our theoretical claims. All our proofs are in the supplement.\n\n1.1 Overview of our main results\n\nWe propose a variant of the quadratic time kernel-MMD statistic of (1) that relies on two key ideas:\n(i) sample splitting and (ii) studentization. In particular, we split the sample X of size n \u2265 2 into\nX1 and X2 of sizes n1 \u2265 1 and n2 \u2265 1, respectively (and Y of size m \u2265 2 into Y1 and Y2 of sizes\nm1 \u2265 1 and m2 \u2265 1), and define the two-sample cross kernel-MMD statistic xMMD\n\nas follows:\n\n2\n\nxMMD\n\n2\n\n:=\n\n1\nn1m1n2m2\n\nn1\n\nn2\n\nm1\n\nm2\n\ni=1\n\ni\u2032=1\n\nj=1\n\nj\u2032=1\n\nh(Xi, Xi\u2032, Yj, Yj\u2032).\n\n(2)\n\n2\n\n2\n\n:= xMMD\n\n/\u03c3, where \u03c3 is an empirical variance introduced in (4).\n\nOur final test statistic is \u00afxMMD\nOur first set of results show that quite generally, \u00afxMMD\nhas an N (0, 1) asymptotic null distribution.\nTheorem 4 obtains this result in the setting where both the kernel k and null distribution P are fixed.\nThis is then generalized to deal with changing kernels (for instance, Gaussian kernels with data-driven\nbandwidth choices) in Theorem 5. Finally, in Theorem 15 in Appendix A, we significantly expand\nthe scope of these results by also allowing the null distribution to change with n, and also weakening\nthe moment conditions required by Theorem 5.\n\n2\n\ny\nt\ni\ns\nn\ne\nd\ny\nt\ni\nl\ni\nb\na\nb\no\nr\nP\n\n0.4\n\n0.3\n\n0.2\n\n0.1\n\n0\n\nNull distribution of xMMD\n\nPower vs Sample-Size\n\nPower vs Computation\n\nd=10\nd=500\nN(0,1)\n\nr\ne\nw\no\nP\n\n1\n\n0.8\n\n0.6\n\n0.4\n\n0.2\n\n1\n\n0.8\n\n0.6\n\n0.4\n\n0.2\n\nr\ne\nw\no\nP\n\nMMD-perm\nxMMD\n\nMMD-perm\nxMMD\n\n\u22125\n\n0\n\n5\n\nStatistic Value\n\n200\n\n100\nSample-Size (n+m)\n\n300\n\n400\n\n10\u22123\n\n10\u22121\n10\u22122\nRunning time (seconds)\n\nFigure 1: The first figure shows the distribution of our proposed statistic \u00afxMMD\npredicted by Theo-\nrem 5 under the null for dimensions d = 10 and d = 100. The statistic is computed with Gaussian\nkernel (ksn (x, y) = exp(\u2212sn\u2225x \u2212 y\u22252\n2)) with scale parameter sn chosen by the median heuristic\nfor different choices of n, m and d, and with samples X and Y drawn from a multivariate Gaussian\ndistribution with identity covariance matrix. The second figure compares the power curves of the\ntwo-sample test using the \u00afxMMD\nstatistic with the kernel-MMD permutation test (with 200 per-\nmutations). The final figure plots the power vs computation time for the two tests. The size of the\nmarkers are proportional to the sample-size used in the test.\n\n2\n\n2\n\n2\n\nOur main methodological contribution is the \u201cxMMD test\u201d, denoted \u03a8, which rejects the null if\n\u00afxMMD\n\nexceeds z1\u2212\u03b1, which is the (1 \u2212 \u03b1)-quantile of N (0, 1). Formally,\nxMMD test: \u03a8(X, Y) = 1\n\n(3)\n\n.\n\n2\n\n\u00afxMMD\n\n\u2265z1\u2212\u03b1\n\nBy the previous results, \u03a8 has type-I error at most \u03b1, meaning that E[\u03a8(X, Y)] \u2264 \u03b1 under the null.\nWe next study the power of the xMMD test \u03a8 in Section 2.2. First, in the fixed alternative case, i.e.,\nwhen the distributions P \u0338= Q do not change with n, we show in Theorem 7, that the xMMD test\n\n3\n\n\fimplemented with any characteristic kernel is consistent under a bounded fourth moment condition.\nNext, we consider the more challenging case of local alternatives, i.e., when the distributions,\nPn \u0338= Qn, change with n. In Theorem 8, we first identify general sufficient conditions for the\nxMMD test to be uniformly consistent over a class of alternatives. Then, we specialize this to the\ncase when Pn and Qn admit densities pn and qn with \u2225pn \u2212 qn\u2225L2 \u2265 \u2206n for some \u2206n \u2192 0. We\nshow in Theorem 9, that the xMMD test with a Gaussian kernel ksn (x, y) = exp(\u2212sn\u2225x \u2212 y\u22252\n2),\nwith scale parameter sn increasing at an appropriate rate can consistently detect the local alternatives\n{\u2206n : n \u2265 1} decaying at the minimax rate.\n\nFinally, we note that while our primary focus in the paper is on the special case of kernel-MMD\nstatistic, the ideas involved in defining the xMMD statistic can be extended to the case of general\ntwo-sample U-statistics. We describe this in Appendix D.1, and obtain sufficient conditions for\nasymptotic Gaussian limit of the resulting statistic, possibly of independent interest.\n\n\n\nThe following is the appendix_E section of the paper you are reviewing:\n\n\nComputing Infrastructure. All the experiments were performed on a workstation with Intel(R)\nCore(TM) i7-9700K CPU 3.60GHz and 32 GB of RAM with an NVIDIA GTX 1080 GPU.\n\nE.1 Implementation details of experiments reported in the main text\n\nDetails for Figure 1. For the null distribution, we set n = 500 and m = 625 and generated both X\nand Y from N (0, Id) for d = 10 and 100. In both cases, we computed the \u00afxMMD\nstatistic 2000\ntimes to plot the histogram.\n\n2\n\nFor the second figure, we obtain the power curves for the xMMD test and the MMD test with 200\npermutations for testing P = N (0, Id) againt Q = N (a\u03f5,j, Id). Here d = 10, j = 5 and \u03f5 = 0.2,\nand recall that a\u03f5,j is the vector in Rd obtained by setting the first j \u2264 d coordinates of 0 equal\nto \u03f5. We selected n and m from 20 equally spaced points in the intervals [10, 400] and [10, 500]\nrespectively, and ran 200 trials of the tests for every (n, m) pair to obtain the power curves. The error\nregions in the figure correspond to one bootstrap standard deviation with 200 bootstrap samples.\n\nFor the third figure, we set d = 100, j = 20, \u03f5 = 0.1, P = N (0, Id) and Q = N (a\u03f5,j, Id). We ran\nthe two tests, xMMD and MMD with 200 permutations, for 20 different (n, m) pairs in the range\n[10, 500], and repeated the experiment 200 times for every such pair. The figure plots the wall-clock\ntime, measure by Python\u2019s time.time() function, and plot the power against the average wall-clock\ntime over the 200 trials. The size of the marker is proportional to the sample size (i.e., n + m).\n\n30\n\n\fDetails for Figure 3. The two kernels used in this figure are the Gaussian and Quadratic kernels.\nThe Gaussian kernel with scale parameter s > 0 is defined as ks(x, y) = exp(\u2212s\u2225x \u2212 y\u22252\n2), while\nthe Quadratic kernel with scale s > 0 is defined as kQ(x, y) = 1 + s(xT y)2\n. With w denoting the\nmedian of the pairwise distance between all the observations, we set s = 1/(2w2) for the Gaussian\nkernel and s = 1/w for the Quadratic kernel.\n\nDetails for Figure 4. Given observations X1, X2, . . . , Xni.i.d.P , consider the problem of one-\nsample mean-testing, that is, testing H0 : E[Xi] = 0 versus H1 : E[Xi] = a \u0338= 0. When the distribu-\ntion P is a multivariate Gaussian, Kim and Ramdas (2020) showed that power of their test using a\n\n\n\n\none-sample studentized U-statistic based on a bi-linear kernel is asymptotically \u03a6\n\nz\u03b1 + aT a\n\u221a\n2\n\ntr(\u03a32)\n\n.\n\nThe power achieved by the test using the full U-statistic is \u03a6\n\n, which differs from\n\nthe previous expression by a factor of\ncovariance testing. Our heuristic in (9) is based on these two observations.\n\n2. A similar relation also holds for the problem of Gaussian\n\n\u221a\n\n\n\nz\u03b1 + aT a\u221a\n\n2tr(\u03a32)\n\n\n\nDetails for Figure 5. For plotting the ROC curves, we proceed as follows. We fix n = m =\n200, and then compute the MMD, block-MMD, linear-MMD and cross-MMD statistics for 1000\nindependent repetitions of \u2018null\u2019 and \u2018alternative\u2019 trials. For every null trial, we calculate all the\nstatistics on independent samples of sizes n and m drawn from P = N (0, Id), while for every\nalternative trial we calculate the statistics on independent samples of size n and m drawn from\nP = N (0, Id) and Q = N (a\u03f5,j, Id) respectively. Recall that a\u03f5,j is obtained by setting the first j\ncoordinates of 0 equal to \u03f5. Having obtained 2000 values for every statistic, we then plot the tradeoff\nbetween false positives (FP) and true positives (TP) as the rejection threshold is increased. The ability\nof a statistic to distinguish between the null and the alternative is quantified by the area under the\ncurve. In Figure 5, we used (d, j, \u03f5) \u2208 {(10, 5, 0.1), (100, 20, 0.1), (500, 100, 0.1)}.\n\nE.2 Additional Figures\n\nNull Distribution. Figure 6 denotes the null distribution of our proposed statistic (\u00afxMMD\n) along\nwith that of the usual MMD normalized by its empirical standard deviation. The null distribution\nin Figure 6 is Dirichlet with parameter 2 \u00d7 1 \u2208 Rd for d \u2208 {10, 500}.\n\n2\n\nPower Curves.\nIn Figure 7, we plot the power curves for the different tests using a Gaussian\nKernel, and we report the results of the same experiment with a polynomial kernel of degree 5\nin Figure 8. Recall that the polynomial kernel of degree r and scale parameter s > 0 is defined as\nk(x, y) = 1 + (xT y)/sr\n. In both instances, we selected the scale parameter using the median\nheuristic.\n\nFrom the figure, we can see that the xMMD test is competitive with the computationally more\ncostly tests, namely the MMD permutation test and the MMD-spectral test of Gretton et al. (2009).\nFurthermore, the performance of xMMD test is significantly better than the existing computationally\nefficient tests, namely block-MMD test (with block-size\n\nn) and linear-MMD test.\n\n\u221a\n\nROC curves.\nIn Figure 9, we plot some additional ROC curves for the different statistics. As\nbefore, we used 1000 \u2018null trials\u2019 and another 1000 \u2019alternative trials\u2019 with sample sizes n = 200 and\nm = 200. The data generating distributions P and Q were both Dirichlet with parameters 1 \u2208 Rd\nand (1 + \u03f5) \u00d7 1 \u2208 Rd for (d, \u03f5) \u2208 {(10, 0.4), (100, 0.2), (500, 0.15)}.\n\nE.3 Comparison with ME and SCF tests of Jitkrittum et al. (2016)\n\nWe now present some experimental results comparing the performance of our cross-MDD test with\nthe linear time mean embedding (MD) and smoothed characteristic function (SCF) tests of Jitkrittum\net al. (2016). These tests proceed in the following steps:\n\n\u2022 Fix J, and choose points {v1, . . . , vJ } from Rd, where d is the dimension of the observation\n\nspace.\n\n31\n\n\f2\n\nMMD\n\nFigure 6: The first two columns show the null distribution of the \u00afxMMD\nstatistic scaled by its empirical standard deviation (bottom row) using the Gaussian\nthe\nkernel with scale-parameter chosen using the median heuristic. The last two columns show the null\ndistribution for the two statistics using the Polynomial kernel of degree 5 with scale parameter chosen\n\nstatistic (top row) and\n\n2\n\nusing the median heuristic. The figures demonstrate that the null distribution of\nchanges\nsignificantly with dimension (d), the ratio n/m and the choice of the kernel, unlike our proposed\nstatistic.\n\n2\n\nMMD\n\nFigure 7: Power Curves for the different tests using Gaussian kernel with scale parameter chosen\nvia median heuristic. The two distributions are P = N (0, Id) and Q = N (a\u03f5,j, Id) where a\u03f5,j\nis obtained by setting the first j \u2264 d coordinates of 0 \u2208 Rd equal to \u03f5. The figures demonstrate\nthat the xMMD test is competitive with more computationally expensive tests (MMD-perm and\nMMD-spectral), while performing significantly better than the low complexity alternatives (B-MMD\nand L-MMD). The batch-size used in the B-MMD test was\n\nn.\n\n\u221a\n\n\u2022 Using X and Y with n = m, compute {zi\n\n[k(vJ , Xi) \u2212 k(vJ , Yi)]J\n\u02c6l(Yi) sin(Y T\nfine \u00afzn = 1\nn\n\ni vj), \u02c6l(Xi) cos(X T\nn\n\ni=1 zi, and Sn = 1\n\nj=1 \u2208 RJ for ME test, and zi = [\u02c6l(Xi) sin(X T\ni vj)]J\nn\u22121 (zi \u2212 \u00afzn)(zi \u2212 \u00afzn)T .\n\n: 1 \u2264 i \u2264 n}, where zi =\ni vj \u2212\nj=1 \u2208 R2J for the SCF test. De-\n\ni vj) \u2212 \u02c6l(Yi) cos(Y T\n\n\u2022 Using the above, define the test statistic\n\u02c6\u03bbn := \u00afzT\n\nn (Sn + \u03b3nI)\u22121 \u00afzn,\n\nwhere \u03b3n is some regularization parameter that converges to 0 with n, and I denotes the\nidentity matrix. For a fixed d and J, Jitkrittum et al. (2016) show that the above statistic has\n\n32\n\nProbabilitydensityxMMD(n/m=5)d=10d=100N(0,1)xMMD(n/m=1)xMMD(n/m=0.2)xMMD(n/m=1)ProbabilitydensityMMD(n/m=5)MMD(n/m=1)MMD(n/m=0.2)MMD(n/m=1)020040060080000.51Sample-Size(n+m)Power(d=10,j=5,\u03f5=0.2)MMD-permxMMDpredictedMMD-spectralB-MMDL-MMD020040060080000.51Sample-Size(n+m)Power(d=50,j=15,\u03f5=0.125)020040060080000.51Sample-Size(n+m)Power(d=100,j=20,\u03f5=0.15)\fFigure 8: Power curves of the different kernel-based tests using a polynomial kernel of degree 5, i.e.,\nk(x, y) = 1 + (xT y)/s5\n\nwith s chosen via the median heuristic.\n\nFigure 9: ROC curves using the different statistics with Gaussian kernel for testing two Dirichlet\ndistributions in dimensions d \u2208 {10, 100, 500} with sample-size n = m = 200. The two distributions\nare P = Dirichlet(1) and Q = Dirichlet((1 + \u03f5) \u00d7 1) where 1 \u2208 Rd is the all-ones vector.\n\na \u03c72(J) (resp. \u03c72(2J)) limiting null distribution in the ME (resp. SCF) case. This result is\nused to calibrate the test at a given level \u03b1.\n\nIn Figure 10, we plot the variation of type-I error and power with sample-size of the three tests for\nthe Gaussian Mean Difference (GMD) source with d = 10. As the figures suggest, the cross-MMD\nachieves higher power and tighter control over the type-I error than the ME and SCF tests in this\nregime.\n\nThe ME and SCF tests are calibrated based on the limiting distribution of their statistic in the low\ndimensional regime: fixed d, and n \u2192 \u221e. However, the high type-I error of these tests for small n\nvalues suggests that their limiting distribution may be different in the high dimensional regime, when\nboth d and n go to infinity. We further observe this in Figure 11 when d = 100 and d/n > 1.\n\nWe end this section with a discussion of some key points of difference between the ME and SCF tests,\nand our proposed cross-MMD test.\n\n\u2022 The ME and SCF tests require the kernel to be uniformly bounded, whereas our test requires\nonly mild moment conditions that are even satisfied by unbounded kernels if the underlying\ndistributions are not too heavy-tailed (formally described in Assumption 1). Furthermore,\nthe ME and SCF tests have several tuning parameters: number of features J, {v1, . . . , vJ },\nbandwidth, step-size for gradient ascent etc. In practice, J is usually set to 5, and the other\nparameters are selected by solving a Jd + 1 dimensional optimization problem via gradient\nascent. While each step of gradient ascent has linear in n complexity, the number of steps\nneeded may be large for higher dimensions, resulting in a higher computational overhead.\n\n33\n\n10020030000.51Sample-Size(n+m)Power(d=10,j=5,\u03f5=0.3)MMD-permxMMDpredictedMMD-spectralB-MMDL-MMD10020030000.51Sample-Size(n+m)Power(d=50,j=5,\u03f5=0.4)10020030000.51Sample-Size(n+m)Power(d=100,j=10,\u03f5=0.5)00.5100.51FalsePositiveRateTruePositiveRateDirichlet(d=10,\u03f5=0.4)MMDxMMDB-MMD(n1/2)B-MMD(n1/3)L-MMD00.5100.51FalsePositiveRateTruePositiveRateDirichlet(d=100,\u03f5=0.2))00.5100.51FalsePositiveRateTruePositiveRateDirichlet(d=500,\u03f5=0.15)\fGMD Source: (d=10, 200 trials)\n\nx-MMD\nME\nSCF\n\nr\no\nr\nr\ne\n\nI\n-\ne\np\ny\nT\n\n0.6\n\n0.4\n\n0.2\n\n0\n\n50\n\n100\n\n150\nSample-Size (n+m)\n\n200\n\nFigure 10: The figures plot the variation of the type-I error (left) and the power (right) with sample-\nsize of the three tests: cross-MMD, and the two linear time tests, ME and SCF, proposed by Jitkrittum\net al. (2016).\n\nGMD Source: (d=100, 200 trials)\n\nx-MMD\nME\nSCF\n\n0.8\n\n0.6\n\n0.4\n\n0.2\n\nr\no\nr\nr\ne\n\nI\n-\ne\np\ny\nT\n\n40\n\n50\n\n60\n80\n70\nSample-Size (n+m)\n\n90\n\n100\n\nFigure 11: The ME and SCF tests provide poor control over the type-I error in the regime when d/n\nis large, suggesting that the limiting null distribution is different (or the convergence rate is slow) in\nthis regime.\n\n\u2022 More importantly, the ME and SCF tests are only valid in the \u2018low-dimensional setting\u2019:\nfixed d and J, with n \u2192 \u221e. In the high dimensional setting, when (d, n) \u2192 \u221e, the limiting\nnull distribution may no longer be \u03c72(J). This is also suggested by the behavior of type-I\nerror of ME and SCF tests in Figure 10 and Figure 11. This results in the following practical\nissue: given a problem with n = 500 and d = 200, how should one calibrate the threshold\nfor those tests?\nOur proposed test does not suffer from this, because in both high and low dimensional\nsettings, our statistic has the same limiting distribution. This is a significant practical\nadvantage of our cross-MMD test over ME and SCF tests.\n\n\u2022 In the regime where the number of features, J, is allowed to increase with n, we expect that\nthe resulting ME and SCF tests may have low power (for small regularization parameter \u03b3n).\nThis is because, the test statistic \u02c6\u03bbn used by ME and SCF tests is similar to Hotelling\u2019s T 2\n\n34\n\n1002003004000.40.60.81Sample-Size(n+m)PowerGMDSource:(\u03f5=1.0,d=10)x-MMDMESCF\fstatistic, for which Bai and Saranadasa (1996) characterized the asymptotic power in this\nregime. In particular, their Theorem 2.1 implies that the power of the T 2 test grows slowly\nwith n, especially when J/n \u2248 1.\nFinally, we note that our ideas also extend to more general degenerate U-statistics (as\ndiscussed in Appendix D.1). Hence, they are also applicable in cases beyond MMD distance,\nwhere we may not have good linear time alternatives.\n\nE.4 Type-I Error and goodness-of-fit test of null distribution\n\nIn this section, we experimentally verify the limiting Gaussian distribution of the \u00afxMMD\nstatistic\nunder the null. We first plot the variation of the type-I error of our cross-MMD test with sample\nsize in Figure 12. We considered the case when X and Y are both drawn i.i.d. from a multivariate\nGaussian vector in dimension d \u2208 {10, 100}, and n = m.\n\n2\n\nFigure 12: The two figures show the variation of the type-I error of the cross-MMD test with sample-\nsize for dimensions d \u2208 {10, 100}. The dashed horizontal line denotes the level \u03b1 = 0.05. In\nsummary, these tests do not find evidence against the null hypothesis that the null distribution is\nGaussian.\n\nNext, we plot the p-values for the test for normality proposed by D\u2019Agostino and Pearson (1973),\nand implemented in the function scipy.stats.normaltest in Python. We performed this test\nat different sample-sizes (n), and for each value of n, we calculated the \u00afxMMD\nstatistic on 200\ndifferent indpendent sample pairs. The results are shown in Figure 13\n\n2\n\nFigure 13: The two figures show p-values for the test for normality proposed by D\u2019Agostino and\nPearson (1973) (using the implementation scipy.stats.normaltest) of the cross-MMD statistic\nfor dimensions d \u2208 {10, 100}. In both dimension regimes, the test does not find evidence against the\nnull that the cross-MMD statistic is normally distributed under the null.\n\n35\n\n\fE.5 Comparison with Friedman-Rafsky test\n\nWe now compare the performance of our cross-MMD test with the Friedman-Rafsky two-sample test.\nThis test, proposed by Friedman and Rafsky (1979), uses a graph-based statistic that is a multivariate\ngeneralization of the Wald-Wolfowitz runs statistic introduced by Wald and Wolfowitz (1940). This\nstatistic, denoted by R, is constructed as follows:\n\n\u2022 Pool the samples X and Y to get Z of size N = n + m. Construct the complete graph with\n\nN nodes, and edge weights equal to the euclidean distance between two end points.\n\n\u2022 Construct the minimal spanning tree (MST) of the complete graph G, and denote the 0-1\n\nvalued adjacency matrix of this MST by M .\n\n\u2022 The statistic R is defined as one more than the number of edges in M with endpoints from\n\ndifferent samples.\n\nThe statistic R is expected to take a large value under the null when X and Y are drawn from the\nsame distribution. Hence, the FR test rejects the null for small values of R. The rejection threshold\ncan be obtained either by the limiting distribution of R characterized by (Henze and Penrose, 1999,\nTheorem 1), or using the permutation-test.\n\nIn Figure 14, we compare the power of the FR permutation-test with our cross-MMD test in a low\ndimensional (d/n small) and a high dimensional (d/n large) problem. In both cases, it is observed\nthat the power of FR test is significantly smaller than that of cross-MMD test.\n\nGMD Source (\u03f5 = 1.2, d = 10)\n\nxMMD\nFR\n\n1\n\n0.8\n\n0.6\n\n0.4\n\n0.2\n\nr\ne\nw\no\nP\n\n50\n\n100\n\n150\nSample-Size (n+m)\n\n200\n\nFigure 14: The figures show the power curves for Friedman-Rafsky (FR) test and our cross-MMD\ntest in the low (d = 10) and high (d = 100) dimensional settings with m = n in both plots. The\nfigures indicate that our cross-MMD test is significantly more powerful than the FR test.\n\nBased on the section(s), please answer the following question with yes, no, or n/a and provide a brief justification for your answer.\nQuestion: If the authors ran experiments, do the authors report error bars (e.g., with respect to the random seed after running experiments multiple times)?"}, "3d": {"role": "user", "content": "The following is the experiments section of the paper you are reviewing:\n\n\n2\n\nWe now present experimental validation of the theoretical claims of the previous section. In particular,\nour experiments demonstrate that (i) the limiting null distribution of \u00afx(cid:92)MMD\nis N (0, 1) under a\nwide range of choices of dimension d, sample sizes n, m and the kernel k, and (ii) the power of our\nxMMD test is competitive with the kernel-MMD permutation test. We now describe the experiments\nin more detail. Additional experimental results are reported in Appendix E.\nLimiting null distribution of \u00afx(cid:92)MMD\nhas a\nlimiting normal distribution under some mild assumptions. We empirically test this result when X\nand Y are drawn from N (0, Id) with 0 denoting the all-zeros vector in Rd, and in particular, study\nthe effects of (i) dimension: d = 10 versus d = 500, (ii) skewness of the samples: n/m = 1 versus\nn/m = 0.1, and (iii) choice of kernel: Gaussian versus Quadratic, both with data-dependent scale\nparameters using median heuristic.\n\n. We showed in Theorem 15 that the statistic \u00afx(cid:92)MMD\n\n2\n\n2\n\n8\n\n\fAs shown in the first row of Figure 3, the distribution of \u00afx(cid:92)MMD\nis robust to all these effects, and is\nclose to N (0, 1) in all cases. In contrast, the distribution of the kernel-MMD statistic scaled by its\nempirical standard deviation (obtained using 200 bootstrap samples) in the bottom row of Figure 3\nshows strong changes with these parameters. We present additional figures and details of the\nimplementation in Appendix E.\n\n2\n\nxMMD (n/m = 0.1)\n\nd = 10\n\nd = 500\nN (0, 1)\n\ny\nt\ni\ns\nn\ne\nd\n\ny\nt\ni\nl\ni\nb\na\nb\no\nr\nP\n\n2\n\n(cid:92)MMD\n\nFigure 3: The first two columns show the null distribution of the \u00afx(cid:92)MMD\nthe\nstatistic scaled by its empirical standard deviation (bottom row) using the Gaussian\nkernel with scale-parameter chosen using the median heuristic. The last two columns show the null\ndistribution for the two statistics using the Quadratic kernel with scale parameter chosen using the\n\nstatistic (top row) and\n\n2\n\nmedian heuristic. The figures demonstrate that the null distribution of\nwith dimension (d), the ratio n/m and the choice of the kernel, unlike our proposed statistic.\n\nchanges significantly\n\n2\n\n(cid:92)MMD\n\nEvaluation of the power of \u03a8. For d \u2265 1 and j \u2264 d, let a\u03f5,j denote the element of Rd with\nfirst j coordinates equal to \u03f5, and others equal to 0. We consider the two-sample testing problem\nwith P = N (0, Id) Q = N (a\u03f5,j, Id) for different choices of \u03f5 and d and j. We compare the\nperformance of our proposed test \u03a8 with the kernel-MMD permutation test, implemented with\nB = 200 permutations, and plot the power-curves (using 200 trials) in Figure 4. We also propose\na heuristic for predicting the power of the permutation test (denoted by \u03c1perm) using the power of\n\u03a8 (denoted by \u03c1\u03a8) as follows (with \u03a6 denoting the standard normal cdf, and z\u03b1 its \u03b1-quantile):\n\u221a\n\n(cid:16)\n\n(cid:98)\u03c1perm = \u03a6\n\nz\u03b1 +\n\n2 (cid:0)\u03a6\u22121(\u03c1\u03a8) \u2212 z\u03b1\n\n(cid:1)(cid:17)\n\n.\n\n(9)\n\n\u221a\n\nThis heuristic is motivated by the power expressions derived by Kim and Ramdas (2020) for the\nproblems of one-sample Gaussian mean and covariance testing (we discuss this further in Appendix E).\n2 in the above expression quantifies the price to pay for sample-splitting. As shown\nThe term\nin Figure 4, this heuristic gives us an accurate estimate of the power of the kernel-MMD permutation\ntest, without incurring the computational burden.\n\nWe now use ROC curves to compare the tradeoff between type-I and type-II errors for the usual MMD,\nlinear and block-MMDs with our \u00afx(cid:92)MMD\n. We use the same distributions P = N (0, Id) and Q =\nN (a\u03f5,j, Id) as before, and plot results for (d, j, \u03f5) \u2208 {(10, 5, 0.2), (100, 20, 0.15), (500, 100, 0.1)}\nin Figure 5. Due to sample splitting, the tradeoff achieved by our proposed statistic is slightly\n\n2\n\nworse than that of\nkernel-MMD statistics. More details about the implementation are presented in Appendix E.\n\n, but significantly better than other computationally efficient variants of\n\n2\n\n(cid:92)MMD\n\n\n\nThe following is the appendix_E section of the paper you are reviewing:\n\n\nComputing Infrastructure. All the experiments were performed on a workstation with Intel(R)\nCore(TM) i7-9700K CPU 3.60GHz and 32 GB of RAM with an NVIDIA GTX 1080 GPU.\n\nE.1 Implementation details of experiments reported in the main text\n\nDetails for Figure 1. For the null distribution, we set n = 500 and m = 625 and generated both X\nand Y from N (0, Id) for d = 10 and 100. In both cases, we computed the \u00afxMMD\nstatistic 2000\ntimes to plot the histogram.\n\n2\n\nFor the second figure, we obtain the power curves for the xMMD test and the MMD test with 200\npermutations for testing P = N (0, Id) againt Q = N (a\u03f5,j, Id). Here d = 10, j = 5 and \u03f5 = 0.2,\nand recall that a\u03f5,j is the vector in Rd obtained by setting the first j \u2264 d coordinates of 0 equal\nto \u03f5. We selected n and m from 20 equally spaced points in the intervals [10, 400] and [10, 500]\nrespectively, and ran 200 trials of the tests for every (n, m) pair to obtain the power curves. The error\nregions in the figure correspond to one bootstrap standard deviation with 200 bootstrap samples.\n\nFor the third figure, we set d = 100, j = 20, \u03f5 = 0.1, P = N (0, Id) and Q = N (a\u03f5,j, Id). We ran\nthe two tests, xMMD and MMD with 200 permutations, for 20 different (n, m) pairs in the range\n[10, 500], and repeated the experiment 200 times for every such pair. The figure plots the wall-clock\ntime, measure by Python\u2019s time.time() function, and plot the power against the average wall-clock\ntime over the 200 trials. The size of the marker is proportional to the sample size (i.e., n + m).\n\n30\n\n\fDetails for Figure 3. The two kernels used in this figure are the Gaussian and Quadratic kernels.\nThe Gaussian kernel with scale parameter s > 0 is defined as ks(x, y) = exp(\u2212s\u2225x \u2212 y\u22252\n2), while\nthe Quadratic kernel with scale s > 0 is defined as kQ(x, y) = 1 + s(xT y)2\n. With w denoting the\nmedian of the pairwise distance between all the observations, we set s = 1/(2w2) for the Gaussian\nkernel and s = 1/w for the Quadratic kernel.\n\nDetails for Figure 4. Given observations X1, X2, . . . , Xni.i.d.P , consider the problem of one-\nsample mean-testing, that is, testing H0 : E[Xi] = 0 versus H1 : E[Xi] = a \u0338= 0. When the distribu-\ntion P is a multivariate Gaussian, Kim and Ramdas (2020) showed that power of their test using a\n\n\n\n\none-sample studentized U-statistic based on a bi-linear kernel is asymptotically \u03a6\n\nz\u03b1 + aT a\n\u221a\n2\n\ntr(\u03a32)\n\n.\n\nThe power achieved by the test using the full U-statistic is \u03a6\n\n, which differs from\n\nthe previous expression by a factor of\ncovariance testing. Our heuristic in (9) is based on these two observations.\n\n2. A similar relation also holds for the problem of Gaussian\n\n\u221a\n\n\n\nz\u03b1 + aT a\u221a\n\n2tr(\u03a32)\n\n\n\nDetails for Figure 5. For plotting the ROC curves, we proceed as follows. We fix n = m =\n200, and then compute the MMD, block-MMD, linear-MMD and cross-MMD statistics for 1000\nindependent repetitions of \u2018null\u2019 and \u2018alternative\u2019 trials. For every null trial, we calculate all the\nstatistics on independent samples of sizes n and m drawn from P = N (0, Id), while for every\nalternative trial we calculate the statistics on independent samples of size n and m drawn from\nP = N (0, Id) and Q = N (a\u03f5,j, Id) respectively. Recall that a\u03f5,j is obtained by setting the first j\ncoordinates of 0 equal to \u03f5. Having obtained 2000 values for every statistic, we then plot the tradeoff\nbetween false positives (FP) and true positives (TP) as the rejection threshold is increased. The ability\nof a statistic to distinguish between the null and the alternative is quantified by the area under the\ncurve. In Figure 5, we used (d, j, \u03f5) \u2208 {(10, 5, 0.1), (100, 20, 0.1), (500, 100, 0.1)}.\n\nE.2 Additional Figures\n\nNull Distribution. Figure 6 denotes the null distribution of our proposed statistic (\u00afxMMD\n) along\nwith that of the usual MMD normalized by its empirical standard deviation. The null distribution\nin Figure 6 is Dirichlet with parameter 2 \u00d7 1 \u2208 Rd for d \u2208 {10, 500}.\n\n2\n\nPower Curves.\nIn Figure 7, we plot the power curves for the different tests using a Gaussian\nKernel, and we report the results of the same experiment with a polynomial kernel of degree 5\nin Figure 8. Recall that the polynomial kernel of degree r and scale parameter s > 0 is defined as\nk(x, y) = 1 + (xT y)/sr\n. In both instances, we selected the scale parameter using the median\nheuristic.\n\nFrom the figure, we can see that the xMMD test is competitive with the computationally more\ncostly tests, namely the MMD permutation test and the MMD-spectral test of Gretton et al. (2009).\nFurthermore, the performance of xMMD test is significantly better than the existing computationally\nefficient tests, namely block-MMD test (with block-size\n\nn) and linear-MMD test.\n\n\u221a\n\nROC curves.\nIn Figure 9, we plot some additional ROC curves for the different statistics. As\nbefore, we used 1000 \u2018null trials\u2019 and another 1000 \u2019alternative trials\u2019 with sample sizes n = 200 and\nm = 200. The data generating distributions P and Q were both Dirichlet with parameters 1 \u2208 Rd\nand (1 + \u03f5) \u00d7 1 \u2208 Rd for (d, \u03f5) \u2208 {(10, 0.4), (100, 0.2), (500, 0.15)}.\n\nE.3 Comparison with ME and SCF tests of Jitkrittum et al. (2016)\n\nWe now present some experimental results comparing the performance of our cross-MDD test with\nthe linear time mean embedding (MD) and smoothed characteristic function (SCF) tests of Jitkrittum\net al. (2016). These tests proceed in the following steps:\n\n\u2022 Fix J, and choose points {v1, . . . , vJ } from Rd, where d is the dimension of the observation\n\nspace.\n\n31\n\n\f2\n\nMMD\n\nFigure 6: The first two columns show the null distribution of the \u00afxMMD\nstatistic scaled by its empirical standard deviation (bottom row) using the Gaussian\nthe\nkernel with scale-parameter chosen using the median heuristic. The last two columns show the null\ndistribution for the two statistics using the Polynomial kernel of degree 5 with scale parameter chosen\n\nstatistic (top row) and\n\n2\n\nusing the median heuristic. The figures demonstrate that the null distribution of\nchanges\nsignificantly with dimension (d), the ratio n/m and the choice of the kernel, unlike our proposed\nstatistic.\n\n2\n\nMMD\n\nFigure 7: Power Curves for the different tests using Gaussian kernel with scale parameter chosen\nvia median heuristic. The two distributions are P = N (0, Id) and Q = N (a\u03f5,j, Id) where a\u03f5,j\nis obtained by setting the first j \u2264 d coordinates of 0 \u2208 Rd equal to \u03f5. The figures demonstrate\nthat the xMMD test is competitive with more computationally expensive tests (MMD-perm and\nMMD-spectral), while performing significantly better than the low complexity alternatives (B-MMD\nand L-MMD). The batch-size used in the B-MMD test was\n\nn.\n\n\u221a\n\n\u2022 Using X and Y with n = m, compute {zi\n\n[k(vJ , Xi) \u2212 k(vJ , Yi)]J\n\u02c6l(Yi) sin(Y T\nfine \u00afzn = 1\nn\n\ni vj), \u02c6l(Xi) cos(X T\nn\n\ni=1 zi, and Sn = 1\n\nj=1 \u2208 RJ for ME test, and zi = [\u02c6l(Xi) sin(X T\ni vj)]J\nn\u22121 (zi \u2212 \u00afzn)(zi \u2212 \u00afzn)T .\n\n: 1 \u2264 i \u2264 n}, where zi =\ni vj \u2212\nj=1 \u2208 R2J for the SCF test. De-\n\ni vj) \u2212 \u02c6l(Yi) cos(Y T\n\n\u2022 Using the above, define the test statistic\n\u02c6\u03bbn := \u00afzT\n\nn (Sn + \u03b3nI)\u22121 \u00afzn,\n\nwhere \u03b3n is some regularization parameter that converges to 0 with n, and I denotes the\nidentity matrix. For a fixed d and J, Jitkrittum et al. (2016) show that the above statistic has\n\n32\n\nProbabilitydensityxMMD(n/m=5)d=10d=100N(0,1)xMMD(n/m=1)xMMD(n/m=0.2)xMMD(n/m=1)ProbabilitydensityMMD(n/m=5)MMD(n/m=1)MMD(n/m=0.2)MMD(n/m=1)020040060080000.51Sample-Size(n+m)Power(d=10,j=5,\u03f5=0.2)MMD-permxMMDpredictedMMD-spectralB-MMDL-MMD020040060080000.51Sample-Size(n+m)Power(d=50,j=15,\u03f5=0.125)020040060080000.51Sample-Size(n+m)Power(d=100,j=20,\u03f5=0.15)\fFigure 8: Power curves of the different kernel-based tests using a polynomial kernel of degree 5, i.e.,\nk(x, y) = 1 + (xT y)/s5\n\nwith s chosen via the median heuristic.\n\nFigure 9: ROC curves using the different statistics with Gaussian kernel for testing two Dirichlet\ndistributions in dimensions d \u2208 {10, 100, 500} with sample-size n = m = 200. The two distributions\nare P = Dirichlet(1) and Q = Dirichlet((1 + \u03f5) \u00d7 1) where 1 \u2208 Rd is the all-ones vector.\n\na \u03c72(J) (resp. \u03c72(2J)) limiting null distribution in the ME (resp. SCF) case. This result is\nused to calibrate the test at a given level \u03b1.\n\nIn Figure 10, we plot the variation of type-I error and power with sample-size of the three tests for\nthe Gaussian Mean Difference (GMD) source with d = 10. As the figures suggest, the cross-MMD\nachieves higher power and tighter control over the type-I error than the ME and SCF tests in this\nregime.\n\nThe ME and SCF tests are calibrated based on the limiting distribution of their statistic in the low\ndimensional regime: fixed d, and n \u2192 \u221e. However, the high type-I error of these tests for small n\nvalues suggests that their limiting distribution may be different in the high dimensional regime, when\nboth d and n go to infinity. We further observe this in Figure 11 when d = 100 and d/n > 1.\n\nWe end this section with a discussion of some key points of difference between the ME and SCF tests,\nand our proposed cross-MMD test.\n\n\u2022 The ME and SCF tests require the kernel to be uniformly bounded, whereas our test requires\nonly mild moment conditions that are even satisfied by unbounded kernels if the underlying\ndistributions are not too heavy-tailed (formally described in Assumption 1). Furthermore,\nthe ME and SCF tests have several tuning parameters: number of features J, {v1, . . . , vJ },\nbandwidth, step-size for gradient ascent etc. In practice, J is usually set to 5, and the other\nparameters are selected by solving a Jd + 1 dimensional optimization problem via gradient\nascent. While each step of gradient ascent has linear in n complexity, the number of steps\nneeded may be large for higher dimensions, resulting in a higher computational overhead.\n\n33\n\n10020030000.51Sample-Size(n+m)Power(d=10,j=5,\u03f5=0.3)MMD-permxMMDpredictedMMD-spectralB-MMDL-MMD10020030000.51Sample-Size(n+m)Power(d=50,j=5,\u03f5=0.4)10020030000.51Sample-Size(n+m)Power(d=100,j=10,\u03f5=0.5)00.5100.51FalsePositiveRateTruePositiveRateDirichlet(d=10,\u03f5=0.4)MMDxMMDB-MMD(n1/2)B-MMD(n1/3)L-MMD00.5100.51FalsePositiveRateTruePositiveRateDirichlet(d=100,\u03f5=0.2))00.5100.51FalsePositiveRateTruePositiveRateDirichlet(d=500,\u03f5=0.15)\fGMD Source: (d=10, 200 trials)\n\nx-MMD\nME\nSCF\n\nr\no\nr\nr\ne\n\nI\n-\ne\np\ny\nT\n\n0.6\n\n0.4\n\n0.2\n\n0\n\n50\n\n100\n\n150\nSample-Size (n+m)\n\n200\n\nFigure 10: The figures plot the variation of the type-I error (left) and the power (right) with sample-\nsize of the three tests: cross-MMD, and the two linear time tests, ME and SCF, proposed by Jitkrittum\net al. (2016).\n\nGMD Source: (d=100, 200 trials)\n\nx-MMD\nME\nSCF\n\n0.8\n\n0.6\n\n0.4\n\n0.2\n\nr\no\nr\nr\ne\n\nI\n-\ne\np\ny\nT\n\n40\n\n50\n\n60\n80\n70\nSample-Size (n+m)\n\n90\n\n100\n\nFigure 11: The ME and SCF tests provide poor control over the type-I error in the regime when d/n\nis large, suggesting that the limiting null distribution is different (or the convergence rate is slow) in\nthis regime.\n\n\u2022 More importantly, the ME and SCF tests are only valid in the \u2018low-dimensional setting\u2019:\nfixed d and J, with n \u2192 \u221e. In the high dimensional setting, when (d, n) \u2192 \u221e, the limiting\nnull distribution may no longer be \u03c72(J). This is also suggested by the behavior of type-I\nerror of ME and SCF tests in Figure 10 and Figure 11. This results in the following practical\nissue: given a problem with n = 500 and d = 200, how should one calibrate the threshold\nfor those tests?\nOur proposed test does not suffer from this, because in both high and low dimensional\nsettings, our statistic has the same limiting distribution. This is a significant practical\nadvantage of our cross-MMD test over ME and SCF tests.\n\n\u2022 In the regime where the number of features, J, is allowed to increase with n, we expect that\nthe resulting ME and SCF tests may have low power (for small regularization parameter \u03b3n).\nThis is because, the test statistic \u02c6\u03bbn used by ME and SCF tests is similar to Hotelling\u2019s T 2\n\n34\n\n1002003004000.40.60.81Sample-Size(n+m)PowerGMDSource:(\u03f5=1.0,d=10)x-MMDMESCF\fstatistic, for which Bai and Saranadasa (1996) characterized the asymptotic power in this\nregime. In particular, their Theorem 2.1 implies that the power of the T 2 test grows slowly\nwith n, especially when J/n \u2248 1.\nFinally, we note that our ideas also extend to more general degenerate U-statistics (as\ndiscussed in Appendix D.1). Hence, they are also applicable in cases beyond MMD distance,\nwhere we may not have good linear time alternatives.\n\nE.4 Type-I Error and goodness-of-fit test of null distribution\n\nIn this section, we experimentally verify the limiting Gaussian distribution of the \u00afxMMD\nstatistic\nunder the null. We first plot the variation of the type-I error of our cross-MMD test with sample\nsize in Figure 12. We considered the case when X and Y are both drawn i.i.d. from a multivariate\nGaussian vector in dimension d \u2208 {10, 100}, and n = m.\n\n2\n\nFigure 12: The two figures show the variation of the type-I error of the cross-MMD test with sample-\nsize for dimensions d \u2208 {10, 100}. The dashed horizontal line denotes the level \u03b1 = 0.05. In\nsummary, these tests do not find evidence against the null hypothesis that the null distribution is\nGaussian.\n\nNext, we plot the p-values for the test for normality proposed by D\u2019Agostino and Pearson (1973),\nand implemented in the function scipy.stats.normaltest in Python. We performed this test\nat different sample-sizes (n), and for each value of n, we calculated the \u00afxMMD\nstatistic on 200\ndifferent indpendent sample pairs. The results are shown in Figure 13\n\n2\n\nFigure 13: The two figures show p-values for the test for normality proposed by D\u2019Agostino and\nPearson (1973) (using the implementation scipy.stats.normaltest) of the cross-MMD statistic\nfor dimensions d \u2208 {10, 100}. In both dimension regimes, the test does not find evidence against the\nnull that the cross-MMD statistic is normally distributed under the null.\n\n35\n\n\fE.5 Comparison with Friedman-Rafsky test\n\nWe now compare the performance of our cross-MMD test with the Friedman-Rafsky two-sample test.\nThis test, proposed by Friedman and Rafsky (1979), uses a graph-based statistic that is a multivariate\ngeneralization of the Wald-Wolfowitz runs statistic introduced by Wald and Wolfowitz (1940). This\nstatistic, denoted by R, is constructed as follows:\n\n\u2022 Pool the samples X and Y to get Z of size N = n + m. Construct the complete graph with\n\nN nodes, and edge weights equal to the euclidean distance between two end points.\n\n\u2022 Construct the minimal spanning tree (MST) of the complete graph G, and denote the 0-1\n\nvalued adjacency matrix of this MST by M .\n\n\u2022 The statistic R is defined as one more than the number of edges in M with endpoints from\n\ndifferent samples.\n\nThe statistic R is expected to take a large value under the null when X and Y are drawn from the\nsame distribution. Hence, the FR test rejects the null for small values of R. The rejection threshold\ncan be obtained either by the limiting distribution of R characterized by (Henze and Penrose, 1999,\nTheorem 1), or using the permutation-test.\n\nIn Figure 14, we compare the power of the FR permutation-test with our cross-MMD test in a low\ndimensional (d/n small) and a high dimensional (d/n large) problem. In both cases, it is observed\nthat the power of FR test is significantly smaller than that of cross-MMD test.\n\nGMD Source (\u03f5 = 1.2, d = 10)\n\nxMMD\nFR\n\n1\n\n0.8\n\n0.6\n\n0.4\n\n0.2\n\nr\ne\nw\no\nP\n\n50\n\n100\n\n150\nSample-Size (n+m)\n\n200\n\nFigure 14: The figures show the power curves for Friedman-Rafsky (FR) test and our cross-MMD\ntest in the low (d = 10) and high (d = 100) dimensional settings with m = n in both plots. The\nfigures indicate that our cross-MMD test is significantly more powerful than the FR test.\n\nBased on the section(s), please answer the following question with yes, no, or n/a and provide a brief justification for your answer.\nQuestion: If the authors ran experiments, do the authors include the total amount of compute and the type of resources used (e.g., type of GPUs, internal cluster, or cloud provider)?"}}}