{"paper_index": 0, "title": "FedPop: A Bayesian Approach for Personalised Federated Learning", "abstract": "", "introduction": "", "methods": "\n\nIn this section, we present non-asymptotic convergence guarantees for Algorithm 1 when the family\nof Markov kernels {Q(i)\n\u03b3,\u03b8 : \u03b3 \u2208 (0, \u00af\u03b3], \u03b8 \u2208 \u0398, i \u2208 [b]} is associated to unadjusted, i.e. without\nMetropolis acceptance step, overdamped Langevin dynamics (Durmus and Moulines, 2017; Dalalyan,\n2017). The bounds we derive allow to showcase explicitly the impact of FL constraints, namely\npartial participation and compression. Results for general unadjusted Markov kernels are postponed\nto the supplement.\n\nTo show our theoretical results and resort to standard assumptions made in the stochastic approxima-\ntion literature, we consider a minimisation problem and rewrite the opposite of the objective function\n(3) for any \u03b8 \u2208 \u0398 as\n\nf (\u03b8) = b\u22121\n\nb\n(cid:88)\n\ni=1\n\nfi(\u03b8) , where fi (\u03b8) = \u2212 log p(\u03c6, \u03b2) \u2212 b log p (Di | \u03c6, \u03b2) .\n\n(6)\n\n5\n\n\fAlgorithm 1 FL via Stochastic Optimisation using Unadjusted Kernel (FedSOUK)\n1: Input: nb. outer iterations K, nb. local iterations M , Markov kernels {Q(i)\n\n\u03b3,\u03b8}\u03b3,\u03b8,i, step-sizes\n\n{\u03b7(1)\n\nk , \u03b7(2)\n\nk }k\u2208[K],i\u2208[b] and initial points Z (0)\n\n0 \u2208 Rd, \u03b20 \u2208 B and \u03c60 \u2208 \u03a6.\n\n| Di, \u03c6k, \u03b2k) +\n\n\u221a\n\n2\u03b3\u03be(i,m+1)\nk\n\n.\n\n2: for k = 0 to K \u2212 1 do\n3:\n4:\n5:\n6:\n7:\n8:\n9:\n10:\n11:\n\nServer sends {\u03b2k+1, \u03c6k+1} to clients belonging to Ak+1.\nfor i \u2208 Ak+1 // On active clients Ak+1 do\n\n// Warm-start of the SA scheme if possible\nif k \u2265 1 then\nSet Z (i,0)\n\nk = Z (i,M )\nk\u22121 .\n\nend if\n// Computation of key quantities using MCMC\nfor m = 0 to M \u2212 1 do\nDraw Z (i,m+1)\nZ (i,m)\nk\nk\n// For Langevin dynamics\n// Draw \u03be(i,m+1)\n// Set Z (i,m+1)\nk\n\n\u223c N(0d, Id).\n= Z (i,m)\nk\n\n+ \u03b3\u2207z log p(Z (i,m)\n\n\u223c Q(i)\n\u03b3,\u03b8k\n\n, \u00b7\n\n(cid:17)\n\n(cid:16)\n\nk\n\nk\n\n.\n\n(cid:80)M\n\nend for\n// Communication with the server\nSet I (i)\nSet J (i)\nSend I (i)\n\nk = 1\nM\nk = 1\nM\nk and Ck+1\n\nm=1 \u2207\u03c6 log p\nJ (i)\nk\n\n(cid:16)\nZ (i,m)\n| \u03b2k\nk\n(cid:16)\nDi | Z (i,m)\nk\n\nm=1 \u2207\u03b2 log p\n\n(cid:80)M\n\n(cid:16)\n\n(cid:17)\n\nto the central server.\n\n(cid:17)\n\n.\n\n(cid:17)\n\n.\n\n, \u03c6k\n\n12:\n\n13:\n\n14:\n15:\n16:\n17:\n\n18:\n\n19:\n\n20:\n21:\n\nend for\nSet \u03b2k+1 = \u03a0B\n\n(cid:16)\n\n(cid:104)\n\n\u03b2k + \u03b7(1)\nk+1\n(cid:16)\n\u03c6k + \u03b7(2)\nk+1\n\n\u2207\u03b2 log p(\u03c6k, \u03b2k) + b\n(cid:104)\n\u2207\u03c6 log p(\u03c6k, \u03b2k) + b\n\n|Ak+1|\n\n|Ak+1|\n\n(cid:80)\n\ni\u2208Ak+1\n\n(cid:80)\n\ni\u2208Ak+1\n\n(cid:105)(cid:17)\n\nI (i)\nk\nCk+1\n\n.\n(cid:16)\n\n(cid:17)(cid:105)(cid:17)\n\n.\n\nJ (i)\nk\n\nSet \u03c6k+1 = \u03a0\u03a6\n\n22:\n23: end for\n24: Output: {\u03c6K, \u03b2K} and samples {Z (1:b,m)\n\nK\u22121 }M\n\nm=1.\n\nNon-Asymptotic Convergence Bounds. For the sake of better readability, we only detail in the\nmain paper assumptions regarding the objective function, compression operators and the partial\nparticipation scenario. Technical assumptions related to the Markov kernels {Q(i)\n\u03b3,\u03b8} are postponed to\nthe supplement. In spirit, we require, for any i \u2208 [b], \u03b8 \u2208 \u0398 and \u03b3, that Q(i)\n\u03b3,\u03b8 satis\ufb01es some ergodic\ncondition and can provide samples suf\ufb01ciently close to the local posterior distribution p(z(i) | Di, \u03b8).\nFor the sake of simplicity, we also assume that for any k \u2208 N\u2217, \u03b7(1)\nk = \u03b7k, see Algorithm 1.\nWe make the following assumptions on \u0398 and the family of functions {fi : i \u2208 [b]}.\nH1. \u0398 is convex, closed subset of Rd\u0398 and \u0398 \u2282 B(0, R\u0398) for R\u0398 > 0.\nH2. For any i \u2208 [b], the following conditions hold.\n\nk = \u03b7(2)\n\n(i) The function fi de\ufb01ned in (6) is convex.\n(ii) There exist an open set U \u2208 Rd\u0398 and Lf > 0 such that \u0398 \u2282 U, fi \u2208 C1(U, R) and for any\n\u03b81, \u03b82 \u2208 \u0398,\n\n(cid:107)\u2207fi(\u03b82) \u2212 \u2207fi(\u03b81)(cid:107) \u2264 Lf (cid:107)\u03b82 \u2212 \u03b81(cid:107) .\n\nThe assumption below requires compression operators {Ck}k\u2208N\u2217 to be unbiased and to have a\nbounded variance. Such an assumption is for instance veri\ufb01ed by stochastic quantisation operators,\nsee Alistarh et al. (2017).\nH3. The compression operators {Ck}k\u2208N\u2217 are independent and satisfy the following conditions.\n(i) For any k \u2208 N\u2217, v \u2208 Rd, E[Ck(v)] = v.\n(ii) There exists \u03c9 \u2265 1, such that for any k \u2208 N\u2217, v \u2208 Rd, E[(cid:107)Ck(v) \u2212 v(cid:107)2] \u2264 \u03c9 (cid:107)v(cid:107)2.\n\n6\n\n\fWe \ufb01nally assume that each client has probability p \u2208 (0, 1] to be active at each communication\nround. We would like to point out that this partial participation assumption can be associated to a\nspeci\ufb01c compression operator satisfying H3.\nH4. For any k \u2208 N\u2217, Ak = {i \u2208 [b] : Bi,k = 1} where for any i \u2208 [b], {Bi,k : k \u2208 N\u2217} is a family\nof i.i.d. Bernouilli random variables with success probability p \u2208 (0, 1].\n\nthese assumptions,\n\nUnder\n(cid:80)k\n\nj=1 \u03b7j\u03b8j/((cid:80)k\n\nj=1 \u03b7j) converges towards an element of arg min\u0398 f .\n\nthe next\n\nresult establishes that (\u00af\u03b8k)k\u2208N de\ufb01ned by \u00af\u03b8k =\n\nTheorem 1. Assume H1-H4 along with A8 detailed in the supplement and let for any k \u2208 [K],\n\u03b7k \u2208 (0, 1/Lf ]. Then, for any K \u2208 N\u2217, we have\n\nE (cid:2)f (\u00af\u03b8k) \u2212 f (\u03b8(cid:63))(cid:3) \u2264 E\n\n(cid:34) (cid:80)K\n\nk=1 \u03b7k{f (\u03b8k) \u2212 f (\u03b8(cid:63))}\nk=1 \u03b7k\n\n(cid:80)K\n\n(cid:35)\n\n\u2264 A(\u03b3) +\n\nEK\nk=1 \u03b7k\n\n(cid:80)K\n\n,\n\nwhere EK depends linearly on (\u03c9/p) (cid:80)K\nof \u03c9, p and (\u03b7k). Closed-form formulas for these constants are provided in the supplement.\n\nk; and A(\u03b3) = C\u03b3\u03b1 with \u03b1 > 0 and C is independent\n\nk=1 \u03b72\n\nAn interesting feature of Algorithm 1 is that convergence towards a minimum of f is possible and\nthe impact of partial participation and compression vanishes when limk\u2192\u221e \u03b7k = 0. More precisely,\nlim supk\u2192\u221e EK/((cid:80)K\nk=1 \u03b7k) = 0 and lim\u03b3\u21920+ A(\u03b3) = 0 which shows that we can tend towards a\nminimum of f with arbitrary precision (cid:15) > 0 by setting the step-size \u03b3 to a small enough value.\n\n", "experiments": "\n\nIn this section, we illustrate the bene\ufb01ts of our methodology on several FL benchmarks associated to\nboth synthetic and real data. Since existing Bayesian FL approaches are not suited for personalisation\n(see Table 1), we only compare the performances of Algorithm 1 with personalised FL methods. In\nall our experiments, we use overdamped Langevin dynamics to sample locally and call this speci\ufb01c\ninstance of Algorithm 1, FedSOUL. In addition, we set p(z(i) | \u03b2) = N(\u00b5, \u03c32Id) with \u03b2 = {\u00b5, \u03c3} for\nsimplicity. To be comparable with existing personalised FL approaches that only consider periodic\ncommunication via multiple local steps, we do not resort to the proposed compression mechanism\nalthough the latter could be of interest for real-world applications. Additional experiments and details\nabout experimental design are provided in the supplement.\n\nSynthetic Data. We start by showcasing the bene\ufb01ts of FedSOUL for clients having small and highly\nheterogeneous data sets as pointed out in Section 1 and Section 2. To this end, we consider a similar\nexperimental setting as in Collins et al. (2021) where synthetic observations {y(i)\nj }j\u2208[Ni] \u2208 Di\n\n8\n\n\ftrue\u03c6(cid:62)\n\ntruex(i)\n\nj \u223c N(z(i)\n\nj \u223c N(0k, Ik) and y(i)\n\nare generated via the following procedure: x(i)\nj , 0.1).\nThe ground-truth parameters z(i)\ntrue \u2208 Rd and \u03c6true \u2208 Rk\u00d7d have been randomly generated\nbeforehand with (d, k) = (2, 20). Compared to Collins et al. (2021), we use heterogeneous\ndata partitions across clients so that 90% of the b = 100 clients have small data sets of size\n5 and the remaining 10% have data sets of size 10. We compare our results with FedRep\n(Collins et al., 2021) and FedAvg (McMahan et al., 2017) since they stand for two limiting in-\nstances of the proposed methodology, see Section 4 and Gelman and Hill (2007, Section 12).\nFigure 2 compares the different ap-\nproaches by computing the princi-\nple angle distance1 (respectively the\n(cid:96)2 norm) between \u03c6true (respectively\nz(i)\nthe\ntrue) and its estimated value;\nlesser the better. In contrast to its main\ncompetitors and based on both met-\nrics, FedSOUL provides an impressive\nimprovement. This illustrates the ben-\ne\ufb01ts of the introduction of a common\nprior p(z(i) | \u03b2) which allows to pre-\nvent from over\ufb01tting on clients with small data sets while performing personalisation. Additional\nresults with other choices for (b, d, k) and data partitioning strategies are available in the supplement.\n\nFigure 2: Small data sets - synthetic data.\n\nMoreover, to compare our algorithm with a non-FL setting, we perform a non-distributed and non-\nfederated stochastic approximation algorithm to \ufb01nd \u03b8\u2217 using a large number of iterations to get an\naccurate approximation of the optimal parameter \u03b8\u2217. Then, we use FedPop to obtain an estimate \u02dc\u03b8\u2217\nand measure the relative error in l2- distance between \u03b8\u2217 and \u02dc\u03b8\u2217. For some outer iterations T = 100,\nthe relative error was less than 10\u22123, which illustrates the relevance of our theoretical results. We\nalso test the performances of the proposed approach when the warm-start strategy is not used. In this\ncase, we have to set M = 50 to achieve the same performances as in the stateful variant of FedSOUL.\n\nReal Data. We consider now real image data sets, namely CIFAR-10 and CIFAR-100 (Krizhevsky,\n2009). For our likelihood model de\ufb01ned by p(Di | \u03c6, z(i)), we use 5-layer convolutional neural\nnetworks and perform personalisation for the last layer. We set b = 100 for convenience and control\ndata heterogeneity by assigning to each client Ni images belonging to only S different classes.\n\nSmall data sets. Under this setting,\nwe \ufb01rst consider (10%, 50%, 90%)\nof clients having small data sets of\nsize either Ni = 5 or Ni = 10;\nwhile remaining clients have larger\ndata sets of size Ni = 25. We com-\npare our approach with FedRep since\nit stands for the state-of-the-art person-\nalised FL approach. The algorithms\nare trained ful\ufb01lling the same com-\nputational budget. Figure 3 shows\nthe average accuracy across clients for\nthe two approaches on both CIFAR-\n10 and CIFAR-100. We can see that FedSOUL is consistently better than FedRep over different\ncon\ufb01gurations.\n\nFigure 3: (right) CIFAR-10 with S = 5 and (left) CIFAR-\n100 with S = 20. The x-axis refers to the percentage of\nclients having Ni \u2208 {5, 10} images.\n\nFull data sets. In addition to show that the proposed approach achieves state-of-the-art performances\non small data sets (which is common in the cross-device scenario), we now illustrate that FedSOUL\nis also competitive on larger data sets. To this end, we use all training images in CIFAR-10 and\nCIFAR-100 image data sets and consider the same data partitioning as in Collins et al. (2021). More\nprecisely, in this case the number of observations and the number of classes per client are uniformly\nshared over the clients. Table 2 shows our results in comparison with state-of-the-art personalised\nFL approaches. We can see that that our model outperforms other methods on both CIFAR-10\n\n1de\ufb01ned in (Collins et al., 2021, De\ufb01nition 1)\n\n9\n\nFedRepFedAvgFedSOUL05101520253035l2 norm of the estimation error010020030040050010\u22121100Principal Angle DistanceFedRepFedAvgFedSOUL1050900.00.10.20.30.4AccuracyFedSOUL,5FedRep,5FedSOUL,10FedRep,101050900.000.020.040.060.080.100.120.14AccuracyFedSOUL,5FedRep,5FedSOUL,10FedRep,10\fTable 2: Real data - Full data sets. Accuracy (in %) on test samples. FedAvg and SCAFFOLD are not\npersonalised FL approaches but stand for well-known FL benchmarks.\n\n(# clients b, # classes per client S)\n\n(100, 2)\n\n(100, 5)\n\n(100, 5)\n\n(100, 20)\n\nCIFAR-10\n\nCIFAR-100\n\nLocal learning only\n\nFedAvg (McMahan et al., 2017)\nSCAFFOLD (Karimireddy et al., 2020)\n\nLG-FedAvg (Liang et al., 2019)\nPer-FedAvg (Fallah et al., 2020)\nL2GD (Hanzely and Richt\u00e1rik, 2020)\nAPFL (Deng et al., 2021b)\nDITTO (Li et al., 2021)\nFedRep (Collins et al., 2021)\nFedAvg + fine-tuning (FT)\n\nFedSOUL (this paper)\n\n89.79\n\n42.65\n37.72\n\n84.14\n82.27\n81.04\n83.77\n85.39\n87.70\n85.63\n\n91.12\n\n70.68\n\n51.78\n47.33\n\n63.02\n67.20\n59.98\n72.29\n70.34\n75.68\n71.32\n\n79.48\n\n75.29\n\n23.94\n20.32\n\n72.44\n72.05\n72.13\n78.20\n78.91\n79.15\n79.03\n\n79.56\n\n41.29\n\n31.97\n22.52\n\n38.76\n52.49\n42.84\n55.44\n56.34\n56.10\n56.19\n\n59.73\n\nand CIFAR-100 by a large margin. Additional results with other personalised FL algorithms are\npostponed to the supplement.\n\nUncertainty Quanti\ufb01cation on Real Data. As highlighted in Table 1, one advantage of the\nproposed approach compared to existing personalised FL methods is the ability to perform uncertainty\nquanti\ufb01cation by sampling locally from the posterior p(z(i) | Di, \u03c6K, \u03b2K), see Algorithm 1. We\nillustrate this feature by computing on CIFAR-10 calibration curves and scores (e.g. expected\ncalibration error aka ECE) on a speci\ufb01c client; and by performing an out-of-distribution analysis\non MNIST/FashionMNIST data sets. Figure 4 shows that the proposed approach provides relevant\nuncertainty diagnosis. Additional results on uncertainty quanti\ufb01cation can be found in the supplement.\n\n higher diameter and value norm.", "conclusion": "\n\nIn this paper, we proposed a general\nBayesian methodology based on a nat-\nural mixed-effects modeling approach\nto model personalisation in federated\nlearning. Our FL method is the \ufb01rst\nthat allows for both personalisation\nand cheap uncertainty quanti\ufb01cation for\n(cross-device) federated learning. By in-\ntroducing a common prior on the local pa-\nrameters, we tackle the local over\ufb01tting\nproblem in the scenario where clients\nhave highly heterogeneous and small\ndata sets. In addition, we have shown that\nthe proposed approach has favorable con-\nvergence properties. Some limitations of\nFedPop pave the way for more advanced\npersonalised FL approaches. As an ex-\nample, our model does not allow for training heterogeneous architectures across clients because of\nthe introduced common prior, and only satisfy \ufb01rst-order privacy guarantees. Regarding the latter,\nfurther works include for instance deriving differentially private versions of our framework.\n\nFigure 4: (right) Calibration on CIFAR-10 for a speci\ufb01c\nclient and (left) OOD analysis with MNIST training &\nFashionMNIST inference \u2013 one curve corresponds to one\nclient.\n\n", "appendix_1": "\n\n[b] and \u2713\n2\nr\u2713 log p(Di, z(i)\nb\n\n\u0000\n\n2\n\nWe make the following assumption on \u21e5 and the family of functions\nA1. \u21e5 is a convex, closed subset of Rd\u21e5 and \u21e5\nA2. For any i\n\n\u21e2\n[b], the following conditions hold.\n\nB(0, R\u21e5) for R\u21e5 > 0.\n\nfi : i\n{\n\n[b]\n\n.\n}\n\n2\n\n2\n\n(i) The function fi de\ufb01ned in (S1) is convex.\n(ii) There exist an open set U\n\u27131,\u2713 2 2\n\n\u21e5,\n\n2\nfi(\u27132)\n\nRd\u21e5 and Lf > 0 such that \u21e5\n\nkr\n\nfi(\u27131)\n\nLf k\n\n\u27132 \u0000\n\nk \uf8ff\n\n\u0000 r\n\nC1(U, R) and for any\n\nU, fi 2\n.\n\n\u21e2\n\u27131k\n\n2\n\n\fNote that A2-(ii) implies that the objective function f de\ufb01ned in (S1) is gradient-Lipschitz with\nLipschitz constant Lf .\n\n2\n\n[b].\n\nWe now consider assumptions on the family of compression and partial participation operators\nCi, Si}i\n{\nA3. There exists a probability measure \u232b1 on a measurable space (X1,\nfunctions\n}i\n2\n[b],\n\nX1) and a family of measurable\n\n[b] such that the following conditions hold.\n\nCi : Rd\u0000\n\nRd\u0000\n\n\u21e5\n\n{\n\n(i) For any v\n2\n(ii) There exist\n\nX1 !\nRd\u0000 and any i\n!i 2\nR+}i\n{\n\n2\n\nX1\n[b], such that for any v\n\nCi(v, x(1)) \u232b1(dx(1)) = v.\nRd\u0000 and any i\n\n2\n\nR\n\nCi(v, x(1))\n\n2\n\nv\n\n\u0000\n\n2\n\u232b1(dx(1))\n\n[b],\n\n2\n2 .\n\nv\n\n!i k\n\nk\n\n\uf8ff\n\nZX1 \u0000\n\u0000\n\u0000\n1, each client has a probability pi 2\n[b], the unbiased partial participation operator Si : Rd\u21e5\n\nIn addition, recall that we consider the partial device participation context where at each communica-\ntion round k\n(0, 1] of participating, independently from other\nclients.\nA4. For any i\nfor any \u2713\n\nX2 with X2 = [0, 1]b by\n\nRd\u21e5 is de\ufb01ned,\n\nRd\u21e5 and x(2) =\n\nX2 !\n\n\u0000\n\u0000\n\u0000\n\n\u21e5\n\n\u0000\n\n2\n\n2\n\nx(2)\ni }i\n[b] 2\n{\nSi(\u2713, x(2)) = 1\n\n2\n\nx(2)\ni \uf8ff\n{\n\npi}\n\n\u2713/pi ,\n\n) a measurable function. We consider the following assumption on the family\n\u21e5, i\n\n[b]\n\n(0, 1].\n\n[1,\n!\n\u2713 ) : \u2713\n\nwhere pi 2\nNote that the assumption A4 is equivalent to H4 in the main paper.\nLet V : Rd\n\u2713 ,\u21e1 (i)\n(H (i)\n{\nA5. For any i\n\n1\n2\n[b], the following conditions hold.\n2\n\u21e5, \u21e1(i)\n\u2713 (\n(i) For any \u2713\n(ii) There exists LH \u0000\n\nH (i)\n) <\n\u2713 k\nk\n0 such that for any z\n\nand (\u2713, z (i))\n\n.\n}\n\n\u21e5,\n\n1\n\n2\n\n2\n\n2\n\nH (i)\n\u27132\n\n(z)\n\nH (i)\n\u27131\n\n(z)\n\n\u0000\n\n\u0000\n\u0000\n\u0000\nS1.3 Stochastic Approximation Framework\n\n\u0000\n\u0000\n\u0000\n\n\u2713 (z(i)) is measurable.\n\nH (i)\n7!\nRd and \u27131,\u2713 2 2\n\u27131k\n\nLH k\n\n\u27132 \u0000\n\n\uf8ff\n\nV 1/2(z) .\n\n[b] a sequence of independent an identically distributed (i.i.d.) random variables\n[b] which is i.i.d. and with uniform dis-\n\n)k\n\nk\n\n2\n\n)k\n\n2N,i\n\nLet (X (i,1)\nwith distribution \u232b1 independent of the sequence (X (i,2)\ntribution on [0, 1]. We consider a family of unadjusted Markov kernels\n[b]\n. Let (\u0000k)k\nsamples from \u21e1(i)\n\n2N\u21e4 2\n\u2713 using Q(i)\n\u0000,\u2713 .\n\n2N,i\n\n}\n\n2\n\nk\n\n2\n(R\u21e4+)N\u21e4 a sequence of step-sizes which will be used to obtain approximate\n\n2\n\n2\n\n{\n\nQ(i)\n\n\u0000,\u2713 : \u0000\n\n(0, \u00af\u0000],\u2713\n\n\u21e5, i\n\nWe now recast the proposed approach detailed in Algorithm 1 into a stochastic approximation\nframework.\nStarting from some initialization (Z (1,0)\n, P), the sequence ((Z (1,m)\n(\u2326,\n\nRbd\n2N via the recursion for k\n\n\u21e5, we de\ufb01ne on a probability space\n\n, . . . , Z (b,0)\n0\n)m\n\n,\u2713 0)\n2\n[M ],\u2713 k)k\n\n, . . . , Z (b,m)\n\nN,\n\n\u21e5\n\nk\n\nk\n\n0\n\n2\n\nF\n\n2\n\n1, (Z (i,m)\nk\n\n)m\n\n0,...,M\n\n}\n\n2{\n\nis a Markov chain with Markov kernel Q(i)\n\n\u0000k,\u2713k\n\nfor any i\n2\nwith Z (i,0)\n\n[b], given\nk = Z (i,M )\n\u2713k \u0000\n\n\u0000\n\nk\n\n1\n\n\u2713k+1 =\u21e7 \u21e5\n\n\u0000\n\nFk\n,\n\n\u2318k+1 \u0000\n\n\u0000\u2713k\n\n\u21e3\n\nk+1 , X (1)\nZ (1:M )\n\nk+1, X (2)\n\nk+1\n\n,\n\n\u2318i\n\nh\n\nwhere\n\n\u0000\n0, . . . , k\n{\n\ndenotes the Hadamard product and for any k\n\n, i\n\n[b]\n\n) and\n}\n\n2\n\n}\n\nF\u0000\n\n1 = \u0000(\u27130,\n\nZ (i,0)\n0\n\n: i\n\n{\n\n2\n\n3\n\n2\n[b]\n\nFk = \u0000(\u27130,\n\nZ (i,m)\nN,\nl\n). In addition, for any k\n}\n\n{{\n\n(S5)\n\n: l\n\n[M ]\n2\n2\nN, \u2318k+1 =\n\n}m\n2\n\n\f(\u2318(1)\nx(1)\n\nk+1,\u2318 (2)\n\nk+1 = ([Z (1,1:M )\nk+1)>, Z (1:M )\nX1, x(2)\nX2 ,\n\nk+1\n\n2\n\n2\nz(1:M ), x(1), x(2)\n\n\u0000\u2713\n\n\u21e3\n\nwhere\n\n\u0000 , \u0000(i)\n\n\u0000(i)\n{\n\n\u0000 }i\n\n[b] de\ufb01ned by\n\n2\n\n\u0000(i)\n\n\u0000 (z(i,1:M )) =\n\n\u0000(i)\n\n\u0000 (z(i,1:M )) =\n\n1\nM\n\n1\nM\n\n\u0000\n\n\u0000\n\nS1.4 Main Result\n\n]>, . . . , [Z (b,1:M )\n\nk\n\n]>)> and for any \u2713\n\n\u21e5, z(1:M )\n\nRM d,\n\n2\n\n2\n\n\u2318\n\n=\n\n=\n\n\u0000\u0000\n\n,\n\nz(1:M ), x(1), x(2)\nz(1:M ), x(2)\n\u0000(i)\n\u0000 (z(i,1:M )), x(i,1)\n\u0000\n\u0000(i)\n\u2318\n\u0000 (z(i,1:M )), x(i,2)\n\n\u0000\u0000\n\u0000\nb\n\u0000\ni=1 Si\nb\nh\n\u21e3\ni=1 Si\n\nCi\n\n\u25c6\n\n\u0000\n\nP\n\nP\n\nh\n\n\u2713\n\n0\n\n@\n\nM\n\n,\n\n(S6)\n\n, x(i,2)\n\ni\n\n1\n\nA\n\ni\n\n(1/b)\n\nr\u0000p(\u0000) +\n\nr\u0000 log p(z(i,m)\n\n|\n\n\u0000)\n\n(1/b)\n\nr\u0000p(\u0000) +\n\nr\u0000 log p(Di |\n\nz(i,m),\u0000 )\n\n.\n\nm=1\nX\nM\n\n\u0000\n\nm=1\nX\n\n\u0000\n\n2N. These conditions are stated hereafter.\n\nIn order to show non-asymptotic convergence guarantees for FedSOUK detailed in Algorithm 1, we\nneed additional assumptions ensuring some stability of the sequence (Z (i,m)\n[b])k\nA6. For any i\n(i) There exists A1 \u0000\n[Q(i)\n\n1 such that for any p, k\nZ (i,0)\n0\n\nN and m\n2{\nA1V (Z (i,0)\n\n[b], the following conditions hold.\n\n,\n}\nV (Z (i,0)\n0\n\n]pV (Z (i,m)\nk\n\n0, . . . , M\n\n0, . . . , M\n\n) , E\n\n: m\n\n2{\n\n, i\n\n<\n\n2\n\n2\n\n2\n\nE\n\n}\n\n)\n\n)\n\nk\n\n0\n\n,\n\n\u0000k,\u2713k\n\n|\n\n\uf8ff\n\nh\n\ni\n\n1\n\nk\n\nh\n: m\n\nwhere (Z (i,m)\n(ii) There exists A2, A3 \u0000\n\u0000,\u2713 admits \u21e1(i)\nQ(i)\n\n0, . . . , M\n1, \u21e2\n\u0000,\u2713 as stationary distribution and\n\n, i\n[0, 1) such that for any \u0000\n\ni\n2N is de\ufb01ned in (S5).\n2\n\n[b])k\n\n}\n2\n\n2{\n\n2\n\n(0, \u00af\u0000], \u2713\n\n\u21e5, z\n\n2\n\n2\n\nRd and k\n\nN,\n\n2\n\n), \u00001 : (R\u21e4+)2\nRd, a\n\u21e5, z\n\n2\n\nR+ and \u00002 : (R\u21e4+)2\nR+\n[1/4, 1/2], we have for any\n\n!\n\n!\n2\n\nA2\u21e2k\u0000 V (z)\n\n\u0000\n\n\u0000,\u2713 ]k\n\n\u0000z[Q(i)\n\n\u21e1(i)\n\u0000,\u2713\n\u0000\n\u21e1(i)\n\u0000,\u2713 (V )\n\u0000\n\u0000\nR+ such that for any \u0000\n\n\u0000\n\u0000\n\u0000\n\nV \uf8ff\n\nA3 .\n\n\uf8ff\n\n(0, \u00af\u0000] and \u2713\n\n\u21e5,\n\n2\n\n2\n\n (\u0000) .\n\n(iii) There exists   : R\u21e4+ !\n\n\u21e1(i)\n\u0000,\u2713 \u0000\n\n\u21e1(i)\n\u2713\nA7. There exists a measurable function V : Rd\nsuch that for any \u00001,\u0000 2 2\ni\n2\n\n1\n(0, \u00af\u0000] with \u00002 <\u0000 1, \u27131,\u2713 2 2\n\u0000zQ(i)\n\n\u0000zQ(i)\n\n\u0000\n\u0000\n!\n\u0000\n\nV 1/2 \uf8ff\n[1,\n\n[b],\n\n\u0000\n\u0000\n\u0000\n\n\u00002,\u27132 \u0000\n\n\u00001,\u27131\n\nV a \uf8ff\n\n[\u00001(\u00001,\u0000 2) + \u00002(\u00001,\u0000 2)\n\n\u27132 \u0000\nk\n\n\u27131k\n\n]V 2a(z) .\n\n\u0000\n\u0000\n\u0000\nk+1 = \u2318k+1 and, for any i\n\n\u0000\n\u0000\nWe are now ready to show our main result. To ease the presentation, assume for any k\n\u0000\n\u2318(1)\nk+1 = \u2318(2)\n2\nTheorem S2. Assume A1, A2, A3, A4, A5, A6 and A7 and let for any k\nRd and i\nIn addition, for any \u2713\nK\nN\u21e4, we have\n\n[b], assume that\n\nk+1 = \u0000k+1.\n\nH (i)\nk\n\n[b],\u0000 (i)\n\n\u2713 (z)\n\n\u21e5, z\n\nk \uf8ff\n\n2\n\n2\n\n2\n\n[K], \u2318k 2\n\n(0, 1/Lf ].\n2\nV 1/4(z). Then, for any\n\nN that\n\n2\n\n2\n\nK\n\nk=1 \u2318k{\n\nf (\u2713k)\n\u0000\nK\nk=1 \u2318k\n\nE\n\n\" P\n\nf (\u2713?)\n\n}\n\n# \uf8ff\n\nEK\nK\nk=1 \u2318k\n\n,\n\nP\n\nP\n\nwhere, for any K\n\nN\u21e4,\n\n2\n\nEK = 2R2\n\n\u21e5 + 2A1\n\nsup\n[b],m\n\n2\n\ni\n\n2\n\n[M ]\n\nV 1/2(Z (i,m)\n\n0\n\n)\n\nE\n\nn\n\nh\n\nK\n\n\u23182\nk\n\nio\n\nXk=1\n\n8bL2\n\nf R2\n\n\u21e5 +\n\nb\n\ni=1\nX\n\n(!i + 1 + pi)\npi\n\n!\n\n4\n\n \n \n \n\f+ b\n\nsup\n[b],m\n\n2\n\ni\n\n2\n\n[M ]\n\nn\n\nC (i,m)\n3\n\n+ b.A1Cc,2\n\nsup\n[b],m\n\n2\n\ni\n\n2\n\n[M ]\n\nK\n\n+ b\n\n\u2318k (\u0000k\n\n1) ,\n\n\u0000\n\nK\n\nK\n\n\u2318k \u0000\n\n|\n\n\u2318k\n\n1\n1 +\n\n\u0000\u0000\nk\n\u0000\n\n1|\n\n\u0000\n\n\"\n\no\n\nE\n\nXk=1\nV (Z (i,m)\n0\n\nn\n\nh\n\nK\n\n)\n\nio\n\nXk=1\n\nXk=1\n1\n\n\u21e5\n\n1\n\n\u23182\nk\u0000\u0000\nk\n\u0000\n\n1 + \u2318K/\u0000K \u0000\n\n\u23181/\u00001\n\n#\n\n\u2318k\u0000\u0000\nk\n\n1\n\u0000\u0000\nk {\n\n\u21e41(\u0000k\n\n1,\u0000 k) + \u21e42(\u0000k\n\n\u0000\n\n1,\u0000 k)\u2318k}\n\n\u0000\n\n+ \u2318k\n\n\u21e4\n\nXk=1\nC (i,m)\n3\n{\n\nwith\n\n[b],m\n\n}i\n\n2\n\n2\n\n[M ] de\ufb01ned in Lemma S5 and Cc,2 de\ufb01ned in Lemma S6.\n\nProof. The proof follows by using the fact that (S23) is a (\ncombining Lemma S1-S7.\n\nFk\n\n\u0000\n\n1)k\n\n2N\u21e4 -martingale increment and by\n\n", "appendix_2": " FedSOUL\n\n\u0000,\u2713 is associated with a Gaussian probability density function q(i)\n\nWe now apply Theorem S2 to FedSOUL where for any i\nkernel Q(i)\nrz log p(z(i)\n\u0000\nposterior distributions\n\n[b] such that A6 and A7 are satis\ufb01ed.\n\n\u0000\nDi,\u2713 ) and variance 2\u0000Id. To this end, we show explicit conditions on the family of\n\n(0, \u00af\u0000] and \u2713\n\u0000,\u2713 (z(i),\n\n2\n) with mean z(i)\n\n\u21e5, the Markov\n\n[b], \u0000\n\n2\n\n2\n\n|\n\n\u00b7\n\n\u21e1(i)\n\u2713 }i\n\n{\n\n2\n\nS2.1 Assumptions\n\n[b], let U (i)\nFor any i\n\u2713\nour case, this boils down to set U (i)\nA8. For any i\n\n: Rd\n\n!\n\n2\n\n2\n\n(i) Assume that (\u2713, z (i))\n\u27131,\u2713 2 2\n\n7!\n\u21e5 and there exists L\n\nsup\n\u21e5\n\u2713\n\n2\n\n\u0000\n\u0000\n\u0000\n\nand\n\n{rzU (i)\n\n\u2713 (0) : \u2713\n\n\u21e5\n\n}\n\n2\n\nis bounded.\n\nR such that for any z(i)\nlog p(z(i)\n\u2713 (z(i)) =\n\n\u0000\n\nRd, \u21e1(i)\n\u2713 (z(i))\n2\nDi,\u0000,\u0000 ) for any z(i)\n|\n\n/\n\nexp\n\n{\u0000\nRd.\n2\n\nU (i)\n\n\u2713 (z(i))\n\n. In\n}\n\n[b], the following conditions hold.\n\nU (i)\n\n\u2713 (z(i)) is differentiable for any\n\n7!\nRd,\n\n\u0000\n\nU\u2713(z(i)) is continuous, z(i)\n0 such that for any z1, z2 2\n\u0000 rzU (i)\nrzU (i)\n\n\u2713 (z1)\n\n\u2713 (z2)\n\nL\n\n\u27132 \u0000\nk\n\n\u27131k\n\n,\n\n\uf8ff\n\n\u0000\n\u0000\n\u0000\n\n11\n\n\f(ii) There exist m1, m2 > 0 and c, R\n\nhrzU (i)\n\n\u2713 (z), z\n\ni \u0000\n\n\u0000\nz\nm1 k\n\n0 such that for any \u2713\n\n1B(0,R)c (z) + m2\n\nk\nRd and \u27131,\u2713 2 2\nLU k\n(z)\n\n\uf8ff\n\n\u27131\n\n\u0000 rzU (i)\n\n2\n\n0 such that z\nrzU (i)\n\n(z)\n\n\u27132\n\n(iii) There exists LU \u0000\n\nwhere V : Rd\n\n!\n\n\u0000\n\u0000\n\u0000\n\nR is de\ufb01ned under A8-(ii), for any z\n\n\u0000\n\u0000\n\u0000\n\n2\n\n\u21e5 and z\n\n2\n\n2\n\nrzU (i)\n\n\u2713 (z)\n\nRd,\n2\n\n\u0000\n\n\u0000\n\u0000\n\u0000\n\nc .\n\nV (z)1/2 ,\n\n\u27131k\n\n\u21e5,\n\n\u0000\n\u0000\n\u0000\n\u27132 \u0000\nRd, as\n\nV (z) = exp\n\nm1\n\n1 +\n\n\u21e2\n\nq\n\nz\n\nk\n\n2/4\nk\n\n\u0000\n\n.\n\n(S24)\n\nS2.2 Veri\ufb01cation of A6 and A7\n\nLemma S8. Assume A8. Then, A6 and A7 are satis\ufb01ed with V de\ufb01ned in (S24) and\n\n1/2\n\nL ,\n\n\u02dcD1\n\n2\n\n!\n\n)\n\n\u0000\n\u0000\n\u0000\n\n,\n\n{\n\n1, 2m2}\n\n\u00af\u0000< min\n\u02dcm1 = m1/4 ,\nb = \u02dcm1(d + c + p2\u02dcm1) exp(\u02dcm2\n1{\n\u02dcm2\n1[p2\n\u0000 = exp(\n\u0000\n\u0000\n1, 2(d + c)/m1, R\nr = max\n{\n\u00001/\u00002 \u0000\n1 ,\n\u00001 : (\u00001,\u0000 2)\n\u00001/2\n,\n2\n\n\u00002 : (\u00001,\u0000 2)\n\n1]) ,\n\n7!\n\n}\n\n,\n\n7!\n\n(d + c + \u02dcm1\u00af\u0000 +\n\n1 + r2\n\n) ,\n}\n\np\n\nrzU (i)\n\n\u2713 (0)\n\n[b]\n\n\u0000\n\u0000\n\u0000\n1(1/\u0000) ,\n\n  : \u0000\n\n2C(1\n\n\u0000\n\n7!\n\n\u21e0)\u0000\n\n1\u00001/2 \u02dcD1/2\n\n1\n\n(1 + \u00af\u0000)1/2\n\nd + 2\u00af\u0000\n\n(\n\nL2MV + sup\n\u21e5,i\n2\n\n2\n\n\u2713\n\np2~m1 exp(~m1\n\n\u02dcD1 =\n\n1 + max\n\n1, R\n{\n3~m2\n1\n\np\n\n2)(1 + ~m1 + c + d)\n}\n\n+ b\u0000\u0000\n\n\u00af\u0000 log\u0000\n\nwith MV = supz\n\n2Rd\n\n(1 +\n{\n\nz\nk\n\nk\n\n)2/V (z)\n\n, C\n}\n\n\u0000\n\n0, \u21e0\n\n2\n\n(0, 1).\n\nProof. The proof follows from De Bortoli et al. [11, Theorem 5].\n\n12\n\n \n\f\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n \n\n\n\n\n\n\n\n \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure S1: Small data sets - synthetic data. b = 50 clients.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n \n\n \n\n\n\n\n\n\n\n \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure S2: Small data sets - synthetic data. b = 200 clients.\n\n   \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n \n\n \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure S3: Small data sets - synthetic data. Raw data dimensionality is k = 50.\n\n \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n \n\n \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure S4: Small data sets - synthetic data. Raw data dimensionality is k = 5.\n\nS3  unsurprising given the MDP structure.", "appendix_3": "\n\nS3.1 Synthetic datasets\n\nIn this section, following the experiments from the main paper, we will show additional con\ufb01gurations\nof the toy example. We still use the same model (see Section 5 and Singhal et al. [47], Collins et al.\n[8]), but we choose different values of (d, k, b). First, let us test, how the total number of clients b\nimpacts the performances of the different approaches. Figure S1 and Figure S2 depict our results for\nb\n, with the size of the minimal dataset being 5 and the share of clients with the minimal\n}\ndataset 90%. We can see that in both cases, FedSOUL outperforms its competitors.\n\n50, 200\n\n2{\n\nSecond, we test, how the dimensionality of raw data impacts the result. Figure S3 and Figure S4\nshow our results with k\n\n. All others parameters are the same as before.\n}\n\nOne more experiment we conducted is the dependence on latent dimensionality d. We test two\noptions d = 2 (as in original experiments) and d = 5 in Figure S5 and Figure S6. Again, the more\n\n5, 50\n\n2{\n\n13\n\n\f\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure S5: Small data sets - synthetic data. Latent space dimensionality is d = 5.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure S6: Small data sets - synthetic data. Latent space dimensionality is d = 2.\n\ny\nc\na\nr\nu\nc\nc\nA\n\n0.7\n\n0.6\n\n0.5\n\n0.4\n\n0.3\n\n0.2\n\n0.1\n\n0.0\n\ny\nc\na\nr\nu\nc\nc\nA\n\n0.4\n\n0.3\n\n0.2\n\n0.1\n\n0.0\n\n10\n\n50\n\nFedSOUL\nFedRep\nAPFL\n\n90\n\n10\n\n50\n\nFedSOUL\nFedRep\nAPFL\n\n90\n\nFigure S7: Small image datasets. The minimal local dataset size is 2 (top) or 5 (bottom).\n\nparameters we have to learn (given the same small data budget), the better Bayesian methods (i.e.\nFedSOUL) are better.\n\nS3.2 Image datasets classi\ufb01cation\n\nIn this section, we provide an additional baseline for the experiments with personalization, in case\nwe have only a few heterogeneous data. Speci\ufb01cally, we consider APFL [13] which is another\npersonalized federated learning approach. We consider the CIFAR-10 dataset with 100 clients.\nAmong these clients, there are 10, 50, or 90 which have a local dataset of either 5 (one setup) or 10\n(another setup). Else of size 25.\n\nWe see in Figure S7 that FedSOUL typically performs better than FedRep, but on par with APFL. It\nis surprising, that APFL is a very good baseline in this type of problem, which it was not specially\ndesigned for.\n\nS3.3 Image datasets uncertainty quanti\ufb01cation\n\nIn this section, we provide additional experiments on image uncertainty with CIFAR-10 (in distribu-\ntion) and SVHN (out of distribution) datasets. As a measure of uncertainty, we will use predictive\nentropy. On ", "system_prompts": [{"role": "system", "content": "You are a computer science researcher currently reviewing a paper titled \"FedPop: A Bayesian Approach for Personalised Federated Learning\" for the NeurIPS computer science conference. Your goal is to try to be as objective and truthful as possible in your answers about the paper provided. Your reviews will be used for causal reasoning in determining the quality of the paper."}], "prompts": {"1b": {"role": "user", "content": "The following is the conclusion section of the paper you are reviewing:\n\n\nIn this paper, we proposed a general\nBayesian methodology based on a nat-\nural mixed-effects modeling approach\nto model personalisation in federated\nlearning. Our FL method is the \ufb01rst\nthat allows for both personalisation\nand cheap uncertainty quanti\ufb01cation for\n(cross-device) federated learning. By in-\ntroducing a common prior on the local pa-\nrameters, we tackle the local over\ufb01tting\nproblem in the scenario where clients\nhave highly heterogeneous and small\ndata sets. In addition, we have shown that\nthe proposed approach has favorable con-\nvergence properties. Some limitations of\nFedPop pave the way for more advanced\npersonalised FL approaches. As an ex-\nample, our model does not allow for training heterogeneous architectures across clients because of\nthe introduced common prior, and only satisfy \ufb01rst-order privacy guarantees. Regarding the latter,\nfurther works include for instance deriving differentially private versions of our framework.\n\nFigure 4: (right) Calibration on CIFAR-10 for a speci\ufb01c\nclient and (left) OOD analysis with MNIST training &\nFashionMNIST inference \u2013 one curve corresponds to one\nclient.\n\n\n\nBased on the section(s), please answer the following question with yes, no, or n/a and provide a brief justification for your answer.\nQuestion: Do the authors describe the limitations of their work?"}, "2a": {"role": "user", "content": "The following is the methods section of the paper you are reviewing:\n\n\nIn this section, we present non-asymptotic convergence guarantees for Algorithm 1 when the family\nof Markov kernels {Q(i)\n\u03b3,\u03b8 : \u03b3 \u2208 (0, \u00af\u03b3], \u03b8 \u2208 \u0398, i \u2208 [b]} is associated to unadjusted, i.e. without\nMetropolis acceptance step, overdamped Langevin dynamics (Durmus and Moulines, 2017; Dalalyan,\n2017). The bounds we derive allow to showcase explicitly the impact of FL constraints, namely\npartial participation and compression. Results for general unadjusted Markov kernels are postponed\nto the supplement.\n\nTo show our theoretical results and resort to standard assumptions made in the stochastic approxima-\ntion literature, we consider a minimisation problem and rewrite the opposite of the objective function\n(3) for any \u03b8 \u2208 \u0398 as\n\nf (\u03b8) = b\u22121\n\nb\n(cid:88)\n\ni=1\n\nfi(\u03b8) , where fi (\u03b8) = \u2212 log p(\u03c6, \u03b2) \u2212 b log p (Di | \u03c6, \u03b2) .\n\n(6)\n\n5\n\n\fAlgorithm 1 FL via Stochastic Optimisation using Unadjusted Kernel (FedSOUK)\n1: Input: nb. outer iterations K, nb. local iterations M , Markov kernels {Q(i)\n\n\u03b3,\u03b8}\u03b3,\u03b8,i, step-sizes\n\n{\u03b7(1)\n\nk , \u03b7(2)\n\nk }k\u2208[K],i\u2208[b] and initial points Z (0)\n\n0 \u2208 Rd, \u03b20 \u2208 B and \u03c60 \u2208 \u03a6.\n\n| Di, \u03c6k, \u03b2k) +\n\n\u221a\n\n2\u03b3\u03be(i,m+1)\nk\n\n.\n\n2: for k = 0 to K \u2212 1 do\n3:\n4:\n5:\n6:\n7:\n8:\n9:\n10:\n11:\n\nServer sends {\u03b2k+1, \u03c6k+1} to clients belonging to Ak+1.\nfor i \u2208 Ak+1 // On active clients Ak+1 do\n\n// Warm-start of the SA scheme if possible\nif k \u2265 1 then\nSet Z (i,0)\n\nk = Z (i,M )\nk\u22121 .\n\nend if\n// Computation of key quantities using MCMC\nfor m = 0 to M \u2212 1 do\nDraw Z (i,m+1)\nZ (i,m)\nk\nk\n// For Langevin dynamics\n// Draw \u03be(i,m+1)\n// Set Z (i,m+1)\nk\n\n\u223c N(0d, Id).\n= Z (i,m)\nk\n\n+ \u03b3\u2207z log p(Z (i,m)\n\n\u223c Q(i)\n\u03b3,\u03b8k\n\n, \u00b7\n\n(cid:17)\n\n(cid:16)\n\nk\n\nk\n\n.\n\n(cid:80)M\n\nend for\n// Communication with the server\nSet I (i)\nSet J (i)\nSend I (i)\n\nk = 1\nM\nk = 1\nM\nk and Ck+1\n\nm=1 \u2207\u03c6 log p\nJ (i)\nk\n\n(cid:16)\nZ (i,m)\n| \u03b2k\nk\n(cid:16)\nDi | Z (i,m)\nk\n\nm=1 \u2207\u03b2 log p\n\n(cid:80)M\n\n(cid:16)\n\n(cid:17)\n\nto the central server.\n\n(cid:17)\n\n.\n\n(cid:17)\n\n.\n\n, \u03c6k\n\n12:\n\n13:\n\n14:\n15:\n16:\n17:\n\n18:\n\n19:\n\n20:\n21:\n\nend for\nSet \u03b2k+1 = \u03a0B\n\n(cid:16)\n\n(cid:104)\n\n\u03b2k + \u03b7(1)\nk+1\n(cid:16)\n\u03c6k + \u03b7(2)\nk+1\n\n\u2207\u03b2 log p(\u03c6k, \u03b2k) + b\n(cid:104)\n\u2207\u03c6 log p(\u03c6k, \u03b2k) + b\n\n|Ak+1|\n\n|Ak+1|\n\n(cid:80)\n\ni\u2208Ak+1\n\n(cid:80)\n\ni\u2208Ak+1\n\n(cid:105)(cid:17)\n\nI (i)\nk\nCk+1\n\n.\n(cid:16)\n\n(cid:17)(cid:105)(cid:17)\n\n.\n\nJ (i)\nk\n\nSet \u03c6k+1 = \u03a0\u03a6\n\n22:\n23: end for\n24: Output: {\u03c6K, \u03b2K} and samples {Z (1:b,m)\n\nK\u22121 }M\n\nm=1.\n\nNon-Asymptotic Convergence Bounds. For the sake of better readability, we only detail in the\nmain paper assumptions regarding the objective function, compression operators and the partial\nparticipation scenario. Technical assumptions related to the Markov kernels {Q(i)\n\u03b3,\u03b8} are postponed to\nthe supplement. In spirit, we require, for any i \u2208 [b], \u03b8 \u2208 \u0398 and \u03b3, that Q(i)\n\u03b3,\u03b8 satis\ufb01es some ergodic\ncondition and can provide samples suf\ufb01ciently close to the local posterior distribution p(z(i) | Di, \u03b8).\nFor the sake of simplicity, we also assume that for any k \u2208 N\u2217, \u03b7(1)\nk = \u03b7k, see Algorithm 1.\nWe make the following assumptions on \u0398 and the family of functions {fi : i \u2208 [b]}.\nH1. \u0398 is convex, closed subset of Rd\u0398 and \u0398 \u2282 B(0, R\u0398) for R\u0398 > 0.\nH2. For any i \u2208 [b], the following conditions hold.\n\nk = \u03b7(2)\n\n(i) The function fi de\ufb01ned in (6) is convex.\n(ii) There exist an open set U \u2208 Rd\u0398 and Lf > 0 such that \u0398 \u2282 U, fi \u2208 C1(U, R) and for any\n\u03b81, \u03b82 \u2208 \u0398,\n\n(cid:107)\u2207fi(\u03b82) \u2212 \u2207fi(\u03b81)(cid:107) \u2264 Lf (cid:107)\u03b82 \u2212 \u03b81(cid:107) .\n\nThe assumption below requires compression operators {Ck}k\u2208N\u2217 to be unbiased and to have a\nbounded variance. Such an assumption is for instance veri\ufb01ed by stochastic quantisation operators,\nsee Alistarh et al. (2017).\nH3. The compression operators {Ck}k\u2208N\u2217 are independent and satisfy the following conditions.\n(i) For any k \u2208 N\u2217, v \u2208 Rd, E[Ck(v)] = v.\n(ii) There exists \u03c9 \u2265 1, such that for any k \u2208 N\u2217, v \u2208 Rd, E[(cid:107)Ck(v) \u2212 v(cid:107)2] \u2264 \u03c9 (cid:107)v(cid:107)2.\n\n6\n\n\fWe \ufb01nally assume that each client has probability p \u2208 (0, 1] to be active at each communication\nround. We would like to point out that this partial participation assumption can be associated to a\nspeci\ufb01c compression operator satisfying H3.\nH4. For any k \u2208 N\u2217, Ak = {i \u2208 [b] : Bi,k = 1} where for any i \u2208 [b], {Bi,k : k \u2208 N\u2217} is a family\nof i.i.d. Bernouilli random variables with success probability p \u2208 (0, 1].\n\nthese assumptions,\n\nUnder\n(cid:80)k\n\nj=1 \u03b7j\u03b8j/((cid:80)k\n\nj=1 \u03b7j) converges towards an element of arg min\u0398 f .\n\nthe next\n\nresult establishes that (\u00af\u03b8k)k\u2208N de\ufb01ned by \u00af\u03b8k =\n\nTheorem 1. Assume H1-H4 along with A8 detailed in the supplement and let for any k \u2208 [K],\n\u03b7k \u2208 (0, 1/Lf ]. Then, for any K \u2208 N\u2217, we have\n\nE (cid:2)f (\u00af\u03b8k) \u2212 f (\u03b8(cid:63))(cid:3) \u2264 E\n\n(cid:34) (cid:80)K\n\nk=1 \u03b7k{f (\u03b8k) \u2212 f (\u03b8(cid:63))}\nk=1 \u03b7k\n\n(cid:80)K\n\n(cid:35)\n\n\u2264 A(\u03b3) +\n\nEK\nk=1 \u03b7k\n\n(cid:80)K\n\n,\n\nwhere EK depends linearly on (\u03c9/p) (cid:80)K\nof \u03c9, p and (\u03b7k). Closed-form formulas for these constants are provided in the supplement.\n\nk; and A(\u03b3) = C\u03b3\u03b1 with \u03b1 > 0 and C is independent\n\nk=1 \u03b72\n\nAn interesting feature of Algorithm 1 is that convergence towards a minimum of f is possible and\nthe impact of partial participation and compression vanishes when limk\u2192\u221e \u03b7k = 0. More precisely,\nlim supk\u2192\u221e EK/((cid:80)K\nk=1 \u03b7k) = 0 and lim\u03b3\u21920+ A(\u03b3) = 0 which shows that we can tend towards a\nminimum of f with arbitrary precision (cid:15) > 0 by setting the step-size \u03b3 to a small enough value.\n\n\n\nThe following is the appendix_1 section of the paper you are reviewing:\n\n\n[b] and \u2713\n2\nr\u2713 log p(Di, z(i)\nb\n\n\u0000\n\n2\n\nWe make the following assumption on \u21e5 and the family of functions\nA1. \u21e5 is a convex, closed subset of Rd\u21e5 and \u21e5\nA2. For any i\n\n\u21e2\n[b], the following conditions hold.\n\nB(0, R\u21e5) for R\u21e5 > 0.\n\nfi : i\n{\n\n[b]\n\n.\n}\n\n2\n\n2\n\n(i) The function fi de\ufb01ned in (S1) is convex.\n(ii) There exist an open set U\n\u27131,\u2713 2 2\n\n\u21e5,\n\n2\nfi(\u27132)\n\nRd\u21e5 and Lf > 0 such that \u21e5\n\nkr\n\nfi(\u27131)\n\nLf k\n\n\u27132 \u0000\n\nk \uf8ff\n\n\u0000 r\n\nC1(U, R) and for any\n\nU, fi 2\n.\n\n\u21e2\n\u27131k\n\n2\n\n\fNote that A2-(ii) implies that the objective function f de\ufb01ned in (S1) is gradient-Lipschitz with\nLipschitz constant Lf .\n\n2\n\n[b].\n\nWe now consider assumptions on the family of compression and partial participation operators\nCi, Si}i\n{\nA3. There exists a probability measure \u232b1 on a measurable space (X1,\nfunctions\n}i\n2\n[b],\n\nX1) and a family of measurable\n\n[b] such that the following conditions hold.\n\nCi : Rd\u0000\n\nRd\u0000\n\n\u21e5\n\n{\n\n(i) For any v\n2\n(ii) There exist\n\nX1 !\nRd\u0000 and any i\n!i 2\nR+}i\n{\n\n2\n\nX1\n[b], such that for any v\n\nCi(v, x(1)) \u232b1(dx(1)) = v.\nRd\u0000 and any i\n\n2\n\nR\n\nCi(v, x(1))\n\n2\n\nv\n\n\u0000\n\n2\n\u232b1(dx(1))\n\n[b],\n\n2\n2 .\n\nv\n\n!i k\n\nk\n\n\uf8ff\n\nZX1 \u0000\n\u0000\n\u0000\n1, each client has a probability pi 2\n[b], the unbiased partial participation operator Si : Rd\u21e5\n\nIn addition, recall that we consider the partial device participation context where at each communica-\ntion round k\n(0, 1] of participating, independently from other\nclients.\nA4. For any i\nfor any \u2713\n\nX2 with X2 = [0, 1]b by\n\nRd\u21e5 is de\ufb01ned,\n\nRd\u21e5 and x(2) =\n\nX2 !\n\n\u0000\n\u0000\n\u0000\n\n\u21e5\n\n\u0000\n\n2\n\n2\n\nx(2)\ni }i\n[b] 2\n{\nSi(\u2713, x(2)) = 1\n\n2\n\nx(2)\ni \uf8ff\n{\n\npi}\n\n\u2713/pi ,\n\n) a measurable function. We consider the following assumption on the family\n\u21e5, i\n\n[b]\n\n(0, 1].\n\n[1,\n!\n\u2713 ) : \u2713\n\nwhere pi 2\nNote that the assumption A4 is equivalent to H4 in the main paper.\nLet V : Rd\n\u2713 ,\u21e1 (i)\n(H (i)\n{\nA5. For any i\n\n1\n2\n[b], the following conditions hold.\n2\n\u21e5, \u21e1(i)\n\u2713 (\n(i) For any \u2713\n(ii) There exists LH \u0000\n\nH (i)\n) <\n\u2713 k\nk\n0 such that for any z\n\nand (\u2713, z (i))\n\n.\n}\n\n\u21e5,\n\n1\n\n2\n\n2\n\n2\n\nH (i)\n\u27132\n\n(z)\n\nH (i)\n\u27131\n\n(z)\n\n\u0000\n\n\u0000\n\u0000\n\u0000\nS1.3 Stochastic Approximation Framework\n\n\u0000\n\u0000\n\u0000\n\n\u2713 (z(i)) is measurable.\n\nH (i)\n7!\nRd and \u27131,\u2713 2 2\n\u27131k\n\nLH k\n\n\u27132 \u0000\n\n\uf8ff\n\nV 1/2(z) .\n\n[b] a sequence of independent an identically distributed (i.i.d.) random variables\n[b] which is i.i.d. and with uniform dis-\n\n)k\n\nk\n\n2\n\n)k\n\n2N,i\n\nLet (X (i,1)\nwith distribution \u232b1 independent of the sequence (X (i,2)\ntribution on [0, 1]. We consider a family of unadjusted Markov kernels\n[b]\n. Let (\u0000k)k\nsamples from \u21e1(i)\n\n2N\u21e4 2\n\u2713 using Q(i)\n\u0000,\u2713 .\n\n2N,i\n\n}\n\n2\n\nk\n\n2\n(R\u21e4+)N\u21e4 a sequence of step-sizes which will be used to obtain approximate\n\n2\n\n2\n\n{\n\nQ(i)\n\n\u0000,\u2713 : \u0000\n\n(0, \u00af\u0000],\u2713\n\n\u21e5, i\n\nWe now recast the proposed approach detailed in Algorithm 1 into a stochastic approximation\nframework.\nStarting from some initialization (Z (1,0)\n, P), the sequence ((Z (1,m)\n(\u2326,\n\nRbd\n2N via the recursion for k\n\n\u21e5, we de\ufb01ne on a probability space\n\n, . . . , Z (b,0)\n0\n)m\n\n,\u2713 0)\n2\n[M ],\u2713 k)k\n\n, . . . , Z (b,m)\n\nN,\n\n\u21e5\n\nk\n\nk\n\n0\n\n2\n\nF\n\n2\n\n1, (Z (i,m)\nk\n\n)m\n\n0,...,M\n\n}\n\n2{\n\nis a Markov chain with Markov kernel Q(i)\n\n\u0000k,\u2713k\n\nfor any i\n2\nwith Z (i,0)\n\n[b], given\nk = Z (i,M )\n\u2713k \u0000\n\n\u0000\n\nk\n\n1\n\n\u2713k+1 =\u21e7 \u21e5\n\n\u0000\n\nFk\n,\n\n\u2318k+1 \u0000\n\n\u0000\u2713k\n\n\u21e3\n\nk+1 , X (1)\nZ (1:M )\n\nk+1, X (2)\n\nk+1\n\n,\n\n\u2318i\n\nh\n\nwhere\n\n\u0000\n0, . . . , k\n{\n\ndenotes the Hadamard product and for any k\n\n, i\n\n[b]\n\n) and\n}\n\n2\n\n}\n\nF\u0000\n\n1 = \u0000(\u27130,\n\nZ (i,0)\n0\n\n: i\n\n{\n\n2\n\n3\n\n2\n[b]\n\nFk = \u0000(\u27130,\n\nZ (i,m)\nN,\nl\n). In addition, for any k\n}\n\n{{\n\n(S5)\n\n: l\n\n[M ]\n2\n2\nN, \u2318k+1 =\n\n}m\n2\n\n\f(\u2318(1)\nx(1)\n\nk+1,\u2318 (2)\n\nk+1 = ([Z (1,1:M )\nk+1)>, Z (1:M )\nX1, x(2)\nX2 ,\n\nk+1\n\n2\n\n2\nz(1:M ), x(1), x(2)\n\n\u0000\u2713\n\n\u21e3\n\nwhere\n\n\u0000 , \u0000(i)\n\n\u0000(i)\n{\n\n\u0000 }i\n\n[b] de\ufb01ned by\n\n2\n\n\u0000(i)\n\n\u0000 (z(i,1:M )) =\n\n\u0000(i)\n\n\u0000 (z(i,1:M )) =\n\n1\nM\n\n1\nM\n\n\u0000\n\n\u0000\n\nS1.4 Main Result\n\n]>, . . . , [Z (b,1:M )\n\nk\n\n]>)> and for any \u2713\n\n\u21e5, z(1:M )\n\nRM d,\n\n2\n\n2\n\n\u2318\n\n=\n\n=\n\n\u0000\u0000\n\n,\n\nz(1:M ), x(1), x(2)\nz(1:M ), x(2)\n\u0000(i)\n\u0000 (z(i,1:M )), x(i,1)\n\u0000\n\u0000(i)\n\u2318\n\u0000 (z(i,1:M )), x(i,2)\n\n\u0000\u0000\n\u0000\nb\n\u0000\ni=1 Si\nb\nh\n\u21e3\ni=1 Si\n\nCi\n\n\u25c6\n\n\u0000\n\nP\n\nP\n\nh\n\n\u2713\n\n0\n\n@\n\nM\n\n,\n\n(S6)\n\n, x(i,2)\n\ni\n\n1\n\nA\n\ni\n\n(1/b)\n\nr\u0000p(\u0000) +\n\nr\u0000 log p(z(i,m)\n\n|\n\n\u0000)\n\n(1/b)\n\nr\u0000p(\u0000) +\n\nr\u0000 log p(Di |\n\nz(i,m),\u0000 )\n\n.\n\nm=1\nX\nM\n\n\u0000\n\nm=1\nX\n\n\u0000\n\n2N. These conditions are stated hereafter.\n\nIn order to show non-asymptotic convergence guarantees for FedSOUK detailed in Algorithm 1, we\nneed additional assumptions ensuring some stability of the sequence (Z (i,m)\n[b])k\nA6. For any i\n(i) There exists A1 \u0000\n[Q(i)\n\n1 such that for any p, k\nZ (i,0)\n0\n\nN and m\n2{\nA1V (Z (i,0)\n\n[b], the following conditions hold.\n\n,\n}\nV (Z (i,0)\n0\n\n]pV (Z (i,m)\nk\n\n0, . . . , M\n\n0, . . . , M\n\n) , E\n\n: m\n\n2{\n\n, i\n\n<\n\n2\n\n2\n\n2\n\nE\n\n}\n\n)\n\n)\n\nk\n\n0\n\n,\n\n\u0000k,\u2713k\n\n|\n\n\uf8ff\n\nh\n\ni\n\n1\n\nk\n\nh\n: m\n\nwhere (Z (i,m)\n(ii) There exists A2, A3 \u0000\n\u0000,\u2713 admits \u21e1(i)\nQ(i)\n\n0, . . . , M\n1, \u21e2\n\u0000,\u2713 as stationary distribution and\n\n, i\n[0, 1) such that for any \u0000\n\ni\n2N is de\ufb01ned in (S5).\n2\n\n[b])k\n\n}\n2\n\n2{\n\n2\n\n(0, \u00af\u0000], \u2713\n\n\u21e5, z\n\n2\n\n2\n\nRd and k\n\nN,\n\n2\n\n), \u00001 : (R\u21e4+)2\nRd, a\n\u21e5, z\n\n2\n\nR+ and \u00002 : (R\u21e4+)2\nR+\n[1/4, 1/2], we have for any\n\n!\n\n!\n2\n\nA2\u21e2k\u0000 V (z)\n\n\u0000\n\n\u0000,\u2713 ]k\n\n\u0000z[Q(i)\n\n\u21e1(i)\n\u0000,\u2713\n\u0000\n\u21e1(i)\n\u0000,\u2713 (V )\n\u0000\n\u0000\nR+ such that for any \u0000\n\n\u0000\n\u0000\n\u0000\n\nV \uf8ff\n\nA3 .\n\n\uf8ff\n\n(0, \u00af\u0000] and \u2713\n\n\u21e5,\n\n2\n\n2\n\n (\u0000) .\n\n(iii) There exists   : R\u21e4+ !\n\n\u21e1(i)\n\u0000,\u2713 \u0000\n\n\u21e1(i)\n\u2713\nA7. There exists a measurable function V : Rd\nsuch that for any \u00001,\u0000 2 2\ni\n2\n\n1\n(0, \u00af\u0000] with \u00002 <\u0000 1, \u27131,\u2713 2 2\n\u0000zQ(i)\n\n\u0000zQ(i)\n\n\u0000\n\u0000\n!\n\u0000\n\nV 1/2 \uf8ff\n[1,\n\n[b],\n\n\u0000\n\u0000\n\u0000\n\n\u00002,\u27132 \u0000\n\n\u00001,\u27131\n\nV a \uf8ff\n\n[\u00001(\u00001,\u0000 2) + \u00002(\u00001,\u0000 2)\n\n\u27132 \u0000\nk\n\n\u27131k\n\n]V 2a(z) .\n\n\u0000\n\u0000\n\u0000\nk+1 = \u2318k+1 and, for any i\n\n\u0000\n\u0000\nWe are now ready to show our main result. To ease the presentation, assume for any k\n\u0000\n\u2318(1)\nk+1 = \u2318(2)\n2\nTheorem S2. Assume A1, A2, A3, A4, A5, A6 and A7 and let for any k\nRd and i\nIn addition, for any \u2713\nK\nN\u21e4, we have\n\n[b], assume that\n\nk+1 = \u0000k+1.\n\nH (i)\nk\n\n[b],\u0000 (i)\n\n\u2713 (z)\n\n\u21e5, z\n\nk \uf8ff\n\n2\n\n2\n\n2\n\n[K], \u2318k 2\n\n(0, 1/Lf ].\n2\nV 1/4(z). Then, for any\n\nN that\n\n2\n\n2\n\nK\n\nk=1 \u2318k{\n\nf (\u2713k)\n\u0000\nK\nk=1 \u2318k\n\nE\n\n\" P\n\nf (\u2713?)\n\n}\n\n# \uf8ff\n\nEK\nK\nk=1 \u2318k\n\n,\n\nP\n\nP\n\nwhere, for any K\n\nN\u21e4,\n\n2\n\nEK = 2R2\n\n\u21e5 + 2A1\n\nsup\n[b],m\n\n2\n\ni\n\n2\n\n[M ]\n\nV 1/2(Z (i,m)\n\n0\n\n)\n\nE\n\nn\n\nh\n\nK\n\n\u23182\nk\n\nio\n\nXk=1\n\n8bL2\n\nf R2\n\n\u21e5 +\n\nb\n\ni=1\nX\n\n(!i + 1 + pi)\npi\n\n!\n\n4\n\n \n \n \n\f+ b\n\nsup\n[b],m\n\n2\n\ni\n\n2\n\n[M ]\n\nn\n\nC (i,m)\n3\n\n+ b.A1Cc,2\n\nsup\n[b],m\n\n2\n\ni\n\n2\n\n[M ]\n\nK\n\n+ b\n\n\u2318k (\u0000k\n\n1) ,\n\n\u0000\n\nK\n\nK\n\n\u2318k \u0000\n\n|\n\n\u2318k\n\n1\n1 +\n\n\u0000\u0000\nk\n\u0000\n\n1|\n\n\u0000\n\n\"\n\no\n\nE\n\nXk=1\nV (Z (i,m)\n0\n\nn\n\nh\n\nK\n\n)\n\nio\n\nXk=1\n\nXk=1\n1\n\n\u21e5\n\n1\n\n\u23182\nk\u0000\u0000\nk\n\u0000\n\n1 + \u2318K/\u0000K \u0000\n\n\u23181/\u00001\n\n#\n\n\u2318k\u0000\u0000\nk\n\n1\n\u0000\u0000\nk {\n\n\u21e41(\u0000k\n\n1,\u0000 k) + \u21e42(\u0000k\n\n\u0000\n\n1,\u0000 k)\u2318k}\n\n\u0000\n\n+ \u2318k\n\n\u21e4\n\nXk=1\nC (i,m)\n3\n{\n\nwith\n\n[b],m\n\n}i\n\n2\n\n2\n\n[M ] de\ufb01ned in Lemma S5 and Cc,2 de\ufb01ned in Lemma S6.\n\nProof. The proof follows by using the fact that (S23) is a (\ncombining Lemma S1-S7.\n\nFk\n\n\u0000\n\n1)k\n\n2N\u21e4 -martingale increment and by\n\n\n\nThe following is the appendix_2 section of the paper you are reviewing:\n FedSOUL\n\n\u0000,\u2713 is associated with a Gaussian probability density function q(i)\n\nWe now apply Theorem S2 to FedSOUL where for any i\nkernel Q(i)\nrz log p(z(i)\n\u0000\nposterior distributions\n\n[b] such that A6 and A7 are satis\ufb01ed.\n\n\u0000\nDi,\u2713 ) and variance 2\u0000Id. To this end, we show explicit conditions on the family of\n\n(0, \u00af\u0000] and \u2713\n\u0000,\u2713 (z(i),\n\n2\n) with mean z(i)\n\n\u21e5, the Markov\n\n[b], \u0000\n\n2\n\n2\n\n|\n\n\u00b7\n\n\u21e1(i)\n\u2713 }i\n\n{\n\n2\n\nS2.1 Assumptions\n\n[b], let U (i)\nFor any i\n\u2713\nour case, this boils down to set U (i)\nA8. For any i\n\n: Rd\n\n!\n\n2\n\n2\n\n(i) Assume that (\u2713, z (i))\n\u27131,\u2713 2 2\n\n7!\n\u21e5 and there exists L\n\nsup\n\u21e5\n\u2713\n\n2\n\n\u0000\n\u0000\n\u0000\n\nand\n\n{rzU (i)\n\n\u2713 (0) : \u2713\n\n\u21e5\n\n}\n\n2\n\nis bounded.\n\nR such that for any z(i)\nlog p(z(i)\n\u2713 (z(i)) =\n\n\u0000\n\nRd, \u21e1(i)\n\u2713 (z(i))\n2\nDi,\u0000,\u0000 ) for any z(i)\n|\n\n/\n\nexp\n\n{\u0000\nRd.\n2\n\nU (i)\n\n\u2713 (z(i))\n\n. In\n}\n\n[b], the following conditions hold.\n\nU (i)\n\n\u2713 (z(i)) is differentiable for any\n\n7!\nRd,\n\n\u0000\n\nU\u2713(z(i)) is continuous, z(i)\n0 such that for any z1, z2 2\n\u0000 rzU (i)\nrzU (i)\n\n\u2713 (z1)\n\n\u2713 (z2)\n\nL\n\n\u27132 \u0000\nk\n\n\u27131k\n\n,\n\n\uf8ff\n\n\u0000\n\u0000\n\u0000\n\n11\n\n\f(ii) There exist m1, m2 > 0 and c, R\n\nhrzU (i)\n\n\u2713 (z), z\n\ni \u0000\n\n\u0000\nz\nm1 k\n\n0 such that for any \u2713\n\n1B(0,R)c (z) + m2\n\nk\nRd and \u27131,\u2713 2 2\nLU k\n(z)\n\n\uf8ff\n\n\u27131\n\n\u0000 rzU (i)\n\n2\n\n0 such that z\nrzU (i)\n\n(z)\n\n\u27132\n\n(iii) There exists LU \u0000\n\nwhere V : Rd\n\n!\n\n\u0000\n\u0000\n\u0000\n\nR is de\ufb01ned under A8-(ii), for any z\n\n\u0000\n\u0000\n\u0000\n\n2\n\n\u21e5 and z\n\n2\n\n2\n\nrzU (i)\n\n\u2713 (z)\n\nRd,\n2\n\n\u0000\n\n\u0000\n\u0000\n\u0000\n\nc .\n\nV (z)1/2 ,\n\n\u27131k\n\n\u21e5,\n\n\u0000\n\u0000\n\u0000\n\u27132 \u0000\nRd, as\n\nV (z) = exp\n\nm1\n\n1 +\n\n\u21e2\n\nq\n\nz\n\nk\n\n2/4\nk\n\n\u0000\n\n.\n\n(S24)\n\nS2.2 Veri\ufb01cation of A6 and A7\n\nLemma S8. Assume A8. Then, A6 and A7 are satis\ufb01ed with V de\ufb01ned in (S24) and\n\n1/2\n\nL ,\n\n\u02dcD1\n\n2\n\n!\n\n)\n\n\u0000\n\u0000\n\u0000\n\n,\n\n{\n\n1, 2m2}\n\n\u00af\u0000< min\n\u02dcm1 = m1/4 ,\nb = \u02dcm1(d + c + p2\u02dcm1) exp(\u02dcm2\n1{\n\u02dcm2\n1[p2\n\u0000 = exp(\n\u0000\n\u0000\n1, 2(d + c)/m1, R\nr = max\n{\n\u00001/\u00002 \u0000\n1 ,\n\u00001 : (\u00001,\u0000 2)\n\u00001/2\n,\n2\n\n\u00002 : (\u00001,\u0000 2)\n\n1]) ,\n\n7!\n\n}\n\n,\n\n7!\n\n(d + c + \u02dcm1\u00af\u0000 +\n\n1 + r2\n\n) ,\n}\n\np\n\nrzU (i)\n\n\u2713 (0)\n\n[b]\n\n\u0000\n\u0000\n\u0000\n1(1/\u0000) ,\n\n  : \u0000\n\n2C(1\n\n\u0000\n\n7!\n\n\u21e0)\u0000\n\n1\u00001/2 \u02dcD1/2\n\n1\n\n(1 + \u00af\u0000)1/2\n\nd + 2\u00af\u0000\n\n(\n\nL2MV + sup\n\u21e5,i\n2\n\n2\n\n\u2713\n\np2~m1 exp(~m1\n\n\u02dcD1 =\n\n1 + max\n\n1, R\n{\n3~m2\n1\n\np\n\n2)(1 + ~m1 + c + d)\n}\n\n+ b\u0000\u0000\n\n\u00af\u0000 log\u0000\n\nwith MV = supz\n\n2Rd\n\n(1 +\n{\n\nz\nk\n\nk\n\n)2/V (z)\n\n, C\n}\n\n\u0000\n\n0, \u21e0\n\n2\n\n(0, 1).\n\nProof. The proof follows from De Bortoli et al. [11, Theorem 5].\n\n12\n\n \n\f\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n \n\n\n\n\n\n\n\n \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure S1: Small data sets - synthetic data. b = 50 clients.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n \n\n \n\n\n\n\n\n\n\n \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure S2: Small data sets - synthetic data. b = 200 clients.\n\n   \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n \n\n \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure S3: Small data sets - synthetic data. Raw data dimensionality is k = 50.\n\n \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n \n\n \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure S4: Small data sets - synthetic data. Raw data dimensionality is k = 5.\n\nS3  unsurprising given the MDP structure.\n\nBased on the section(s), please answer the following question with yes, no, or n/a and provide a brief justification for your answer.\nQuestion: If the authors include theoretical results, do the authors state the full set of assumptions of all theoretical results?"}, "2b": {"role": "user", "content": "The following is the methods section of the paper you are reviewing:\n\n\nIn this section, we present non-asymptotic convergence guarantees for Algorithm 1 when the family\nof Markov kernels {Q(i)\n\u03b3,\u03b8 : \u03b3 \u2208 (0, \u00af\u03b3], \u03b8 \u2208 \u0398, i \u2208 [b]} is associated to unadjusted, i.e. without\nMetropolis acceptance step, overdamped Langevin dynamics (Durmus and Moulines, 2017; Dalalyan,\n2017). The bounds we derive allow to showcase explicitly the impact of FL constraints, namely\npartial participation and compression. Results for general unadjusted Markov kernels are postponed\nto the supplement.\n\nTo show our theoretical results and resort to standard assumptions made in the stochastic approxima-\ntion literature, we consider a minimisation problem and rewrite the opposite of the objective function\n(3) for any \u03b8 \u2208 \u0398 as\n\nf (\u03b8) = b\u22121\n\nb\n(cid:88)\n\ni=1\n\nfi(\u03b8) , where fi (\u03b8) = \u2212 log p(\u03c6, \u03b2) \u2212 b log p (Di | \u03c6, \u03b2) .\n\n(6)\n\n5\n\n\fAlgorithm 1 FL via Stochastic Optimisation using Unadjusted Kernel (FedSOUK)\n1: Input: nb. outer iterations K, nb. local iterations M , Markov kernels {Q(i)\n\n\u03b3,\u03b8}\u03b3,\u03b8,i, step-sizes\n\n{\u03b7(1)\n\nk , \u03b7(2)\n\nk }k\u2208[K],i\u2208[b] and initial points Z (0)\n\n0 \u2208 Rd, \u03b20 \u2208 B and \u03c60 \u2208 \u03a6.\n\n| Di, \u03c6k, \u03b2k) +\n\n\u221a\n\n2\u03b3\u03be(i,m+1)\nk\n\n.\n\n2: for k = 0 to K \u2212 1 do\n3:\n4:\n5:\n6:\n7:\n8:\n9:\n10:\n11:\n\nServer sends {\u03b2k+1, \u03c6k+1} to clients belonging to Ak+1.\nfor i \u2208 Ak+1 // On active clients Ak+1 do\n\n// Warm-start of the SA scheme if possible\nif k \u2265 1 then\nSet Z (i,0)\n\nk = Z (i,M )\nk\u22121 .\n\nend if\n// Computation of key quantities using MCMC\nfor m = 0 to M \u2212 1 do\nDraw Z (i,m+1)\nZ (i,m)\nk\nk\n// For Langevin dynamics\n// Draw \u03be(i,m+1)\n// Set Z (i,m+1)\nk\n\n\u223c N(0d, Id).\n= Z (i,m)\nk\n\n+ \u03b3\u2207z log p(Z (i,m)\n\n\u223c Q(i)\n\u03b3,\u03b8k\n\n, \u00b7\n\n(cid:17)\n\n(cid:16)\n\nk\n\nk\n\n.\n\n(cid:80)M\n\nend for\n// Communication with the server\nSet I (i)\nSet J (i)\nSend I (i)\n\nk = 1\nM\nk = 1\nM\nk and Ck+1\n\nm=1 \u2207\u03c6 log p\nJ (i)\nk\n\n(cid:16)\nZ (i,m)\n| \u03b2k\nk\n(cid:16)\nDi | Z (i,m)\nk\n\nm=1 \u2207\u03b2 log p\n\n(cid:80)M\n\n(cid:16)\n\n(cid:17)\n\nto the central server.\n\n(cid:17)\n\n.\n\n(cid:17)\n\n.\n\n, \u03c6k\n\n12:\n\n13:\n\n14:\n15:\n16:\n17:\n\n18:\n\n19:\n\n20:\n21:\n\nend for\nSet \u03b2k+1 = \u03a0B\n\n(cid:16)\n\n(cid:104)\n\n\u03b2k + \u03b7(1)\nk+1\n(cid:16)\n\u03c6k + \u03b7(2)\nk+1\n\n\u2207\u03b2 log p(\u03c6k, \u03b2k) + b\n(cid:104)\n\u2207\u03c6 log p(\u03c6k, \u03b2k) + b\n\n|Ak+1|\n\n|Ak+1|\n\n(cid:80)\n\ni\u2208Ak+1\n\n(cid:80)\n\ni\u2208Ak+1\n\n(cid:105)(cid:17)\n\nI (i)\nk\nCk+1\n\n.\n(cid:16)\n\n(cid:17)(cid:105)(cid:17)\n\n.\n\nJ (i)\nk\n\nSet \u03c6k+1 = \u03a0\u03a6\n\n22:\n23: end for\n24: Output: {\u03c6K, \u03b2K} and samples {Z (1:b,m)\n\nK\u22121 }M\n\nm=1.\n\nNon-Asymptotic Convergence Bounds. For the sake of better readability, we only detail in the\nmain paper assumptions regarding the objective function, compression operators and the partial\nparticipation scenario. Technical assumptions related to the Markov kernels {Q(i)\n\u03b3,\u03b8} are postponed to\nthe supplement. In spirit, we require, for any i \u2208 [b], \u03b8 \u2208 \u0398 and \u03b3, that Q(i)\n\u03b3,\u03b8 satis\ufb01es some ergodic\ncondition and can provide samples suf\ufb01ciently close to the local posterior distribution p(z(i) | Di, \u03b8).\nFor the sake of simplicity, we also assume that for any k \u2208 N\u2217, \u03b7(1)\nk = \u03b7k, see Algorithm 1.\nWe make the following assumptions on \u0398 and the family of functions {fi : i \u2208 [b]}.\nH1. \u0398 is convex, closed subset of Rd\u0398 and \u0398 \u2282 B(0, R\u0398) for R\u0398 > 0.\nH2. For any i \u2208 [b], the following conditions hold.\n\nk = \u03b7(2)\n\n(i) The function fi de\ufb01ned in (6) is convex.\n(ii) There exist an open set U \u2208 Rd\u0398 and Lf > 0 such that \u0398 \u2282 U, fi \u2208 C1(U, R) and for any\n\u03b81, \u03b82 \u2208 \u0398,\n\n(cid:107)\u2207fi(\u03b82) \u2212 \u2207fi(\u03b81)(cid:107) \u2264 Lf (cid:107)\u03b82 \u2212 \u03b81(cid:107) .\n\nThe assumption below requires compression operators {Ck}k\u2208N\u2217 to be unbiased and to have a\nbounded variance. Such an assumption is for instance veri\ufb01ed by stochastic quantisation operators,\nsee Alistarh et al. (2017).\nH3. The compression operators {Ck}k\u2208N\u2217 are independent and satisfy the following conditions.\n(i) For any k \u2208 N\u2217, v \u2208 Rd, E[Ck(v)] = v.\n(ii) There exists \u03c9 \u2265 1, such that for any k \u2208 N\u2217, v \u2208 Rd, E[(cid:107)Ck(v) \u2212 v(cid:107)2] \u2264 \u03c9 (cid:107)v(cid:107)2.\n\n6\n\n\fWe \ufb01nally assume that each client has probability p \u2208 (0, 1] to be active at each communication\nround. We would like to point out that this partial participation assumption can be associated to a\nspeci\ufb01c compression operator satisfying H3.\nH4. For any k \u2208 N\u2217, Ak = {i \u2208 [b] : Bi,k = 1} where for any i \u2208 [b], {Bi,k : k \u2208 N\u2217} is a family\nof i.i.d. Bernouilli random variables with success probability p \u2208 (0, 1].\n\nthese assumptions,\n\nUnder\n(cid:80)k\n\nj=1 \u03b7j\u03b8j/((cid:80)k\n\nj=1 \u03b7j) converges towards an element of arg min\u0398 f .\n\nthe next\n\nresult establishes that (\u00af\u03b8k)k\u2208N de\ufb01ned by \u00af\u03b8k =\n\nTheorem 1. Assume H1-H4 along with A8 detailed in the supplement and let for any k \u2208 [K],\n\u03b7k \u2208 (0, 1/Lf ]. Then, for any K \u2208 N\u2217, we have\n\nE (cid:2)f (\u00af\u03b8k) \u2212 f (\u03b8(cid:63))(cid:3) \u2264 E\n\n(cid:34) (cid:80)K\n\nk=1 \u03b7k{f (\u03b8k) \u2212 f (\u03b8(cid:63))}\nk=1 \u03b7k\n\n(cid:80)K\n\n(cid:35)\n\n\u2264 A(\u03b3) +\n\nEK\nk=1 \u03b7k\n\n(cid:80)K\n\n,\n\nwhere EK depends linearly on (\u03c9/p) (cid:80)K\nof \u03c9, p and (\u03b7k). Closed-form formulas for these constants are provided in the supplement.\n\nk; and A(\u03b3) = C\u03b3\u03b1 with \u03b1 > 0 and C is independent\n\nk=1 \u03b72\n\nAn interesting feature of Algorithm 1 is that convergence towards a minimum of f is possible and\nthe impact of partial participation and compression vanishes when limk\u2192\u221e \u03b7k = 0. More precisely,\nlim supk\u2192\u221e EK/((cid:80)K\nk=1 \u03b7k) = 0 and lim\u03b3\u21920+ A(\u03b3) = 0 which shows that we can tend towards a\nminimum of f with arbitrary precision (cid:15) > 0 by setting the step-size \u03b3 to a small enough value.\n\n\n\nThe following is the appendix_1 section of the paper you are reviewing:\n\n\n[b] and \u2713\n2\nr\u2713 log p(Di, z(i)\nb\n\n\u0000\n\n2\n\nWe make the following assumption on \u21e5 and the family of functions\nA1. \u21e5 is a convex, closed subset of Rd\u21e5 and \u21e5\nA2. For any i\n\n\u21e2\n[b], the following conditions hold.\n\nB(0, R\u21e5) for R\u21e5 > 0.\n\nfi : i\n{\n\n[b]\n\n.\n}\n\n2\n\n2\n\n(i) The function fi de\ufb01ned in (S1) is convex.\n(ii) There exist an open set U\n\u27131,\u2713 2 2\n\n\u21e5,\n\n2\nfi(\u27132)\n\nRd\u21e5 and Lf > 0 such that \u21e5\n\nkr\n\nfi(\u27131)\n\nLf k\n\n\u27132 \u0000\n\nk \uf8ff\n\n\u0000 r\n\nC1(U, R) and for any\n\nU, fi 2\n.\n\n\u21e2\n\u27131k\n\n2\n\n\fNote that A2-(ii) implies that the objective function f de\ufb01ned in (S1) is gradient-Lipschitz with\nLipschitz constant Lf .\n\n2\n\n[b].\n\nWe now consider assumptions on the family of compression and partial participation operators\nCi, Si}i\n{\nA3. There exists a probability measure \u232b1 on a measurable space (X1,\nfunctions\n}i\n2\n[b],\n\nX1) and a family of measurable\n\n[b] such that the following conditions hold.\n\nCi : Rd\u0000\n\nRd\u0000\n\n\u21e5\n\n{\n\n(i) For any v\n2\n(ii) There exist\n\nX1 !\nRd\u0000 and any i\n!i 2\nR+}i\n{\n\n2\n\nX1\n[b], such that for any v\n\nCi(v, x(1)) \u232b1(dx(1)) = v.\nRd\u0000 and any i\n\n2\n\nR\n\nCi(v, x(1))\n\n2\n\nv\n\n\u0000\n\n2\n\u232b1(dx(1))\n\n[b],\n\n2\n2 .\n\nv\n\n!i k\n\nk\n\n\uf8ff\n\nZX1 \u0000\n\u0000\n\u0000\n1, each client has a probability pi 2\n[b], the unbiased partial participation operator Si : Rd\u21e5\n\nIn addition, recall that we consider the partial device participation context where at each communica-\ntion round k\n(0, 1] of participating, independently from other\nclients.\nA4. For any i\nfor any \u2713\n\nX2 with X2 = [0, 1]b by\n\nRd\u21e5 is de\ufb01ned,\n\nRd\u21e5 and x(2) =\n\nX2 !\n\n\u0000\n\u0000\n\u0000\n\n\u21e5\n\n\u0000\n\n2\n\n2\n\nx(2)\ni }i\n[b] 2\n{\nSi(\u2713, x(2)) = 1\n\n2\n\nx(2)\ni \uf8ff\n{\n\npi}\n\n\u2713/pi ,\n\n) a measurable function. We consider the following assumption on the family\n\u21e5, i\n\n[b]\n\n(0, 1].\n\n[1,\n!\n\u2713 ) : \u2713\n\nwhere pi 2\nNote that the assumption A4 is equivalent to H4 in the main paper.\nLet V : Rd\n\u2713 ,\u21e1 (i)\n(H (i)\n{\nA5. For any i\n\n1\n2\n[b], the following conditions hold.\n2\n\u21e5, \u21e1(i)\n\u2713 (\n(i) For any \u2713\n(ii) There exists LH \u0000\n\nH (i)\n) <\n\u2713 k\nk\n0 such that for any z\n\nand (\u2713, z (i))\n\n.\n}\n\n\u21e5,\n\n1\n\n2\n\n2\n\n2\n\nH (i)\n\u27132\n\n(z)\n\nH (i)\n\u27131\n\n(z)\n\n\u0000\n\n\u0000\n\u0000\n\u0000\nS1.3 Stochastic Approximation Framework\n\n\u0000\n\u0000\n\u0000\n\n\u2713 (z(i)) is measurable.\n\nH (i)\n7!\nRd and \u27131,\u2713 2 2\n\u27131k\n\nLH k\n\n\u27132 \u0000\n\n\uf8ff\n\nV 1/2(z) .\n\n[b] a sequence of independent an identically distributed (i.i.d.) random variables\n[b] which is i.i.d. and with uniform dis-\n\n)k\n\nk\n\n2\n\n)k\n\n2N,i\n\nLet (X (i,1)\nwith distribution \u232b1 independent of the sequence (X (i,2)\ntribution on [0, 1]. We consider a family of unadjusted Markov kernels\n[b]\n. Let (\u0000k)k\nsamples from \u21e1(i)\n\n2N\u21e4 2\n\u2713 using Q(i)\n\u0000,\u2713 .\n\n2N,i\n\n}\n\n2\n\nk\n\n2\n(R\u21e4+)N\u21e4 a sequence of step-sizes which will be used to obtain approximate\n\n2\n\n2\n\n{\n\nQ(i)\n\n\u0000,\u2713 : \u0000\n\n(0, \u00af\u0000],\u2713\n\n\u21e5, i\n\nWe now recast the proposed approach detailed in Algorithm 1 into a stochastic approximation\nframework.\nStarting from some initialization (Z (1,0)\n, P), the sequence ((Z (1,m)\n(\u2326,\n\nRbd\n2N via the recursion for k\n\n\u21e5, we de\ufb01ne on a probability space\n\n, . . . , Z (b,0)\n0\n)m\n\n,\u2713 0)\n2\n[M ],\u2713 k)k\n\n, . . . , Z (b,m)\n\nN,\n\n\u21e5\n\nk\n\nk\n\n0\n\n2\n\nF\n\n2\n\n1, (Z (i,m)\nk\n\n)m\n\n0,...,M\n\n}\n\n2{\n\nis a Markov chain with Markov kernel Q(i)\n\n\u0000k,\u2713k\n\nfor any i\n2\nwith Z (i,0)\n\n[b], given\nk = Z (i,M )\n\u2713k \u0000\n\n\u0000\n\nk\n\n1\n\n\u2713k+1 =\u21e7 \u21e5\n\n\u0000\n\nFk\n,\n\n\u2318k+1 \u0000\n\n\u0000\u2713k\n\n\u21e3\n\nk+1 , X (1)\nZ (1:M )\n\nk+1, X (2)\n\nk+1\n\n,\n\n\u2318i\n\nh\n\nwhere\n\n\u0000\n0, . . . , k\n{\n\ndenotes the Hadamard product and for any k\n\n, i\n\n[b]\n\n) and\n}\n\n2\n\n}\n\nF\u0000\n\n1 = \u0000(\u27130,\n\nZ (i,0)\n0\n\n: i\n\n{\n\n2\n\n3\n\n2\n[b]\n\nFk = \u0000(\u27130,\n\nZ (i,m)\nN,\nl\n). In addition, for any k\n}\n\n{{\n\n(S5)\n\n: l\n\n[M ]\n2\n2\nN, \u2318k+1 =\n\n}m\n2\n\n\f(\u2318(1)\nx(1)\n\nk+1,\u2318 (2)\n\nk+1 = ([Z (1,1:M )\nk+1)>, Z (1:M )\nX1, x(2)\nX2 ,\n\nk+1\n\n2\n\n2\nz(1:M ), x(1), x(2)\n\n\u0000\u2713\n\n\u21e3\n\nwhere\n\n\u0000 , \u0000(i)\n\n\u0000(i)\n{\n\n\u0000 }i\n\n[b] de\ufb01ned by\n\n2\n\n\u0000(i)\n\n\u0000 (z(i,1:M )) =\n\n\u0000(i)\n\n\u0000 (z(i,1:M )) =\n\n1\nM\n\n1\nM\n\n\u0000\n\n\u0000\n\nS1.4 Main Result\n\n]>, . . . , [Z (b,1:M )\n\nk\n\n]>)> and for any \u2713\n\n\u21e5, z(1:M )\n\nRM d,\n\n2\n\n2\n\n\u2318\n\n=\n\n=\n\n\u0000\u0000\n\n,\n\nz(1:M ), x(1), x(2)\nz(1:M ), x(2)\n\u0000(i)\n\u0000 (z(i,1:M )), x(i,1)\n\u0000\n\u0000(i)\n\u2318\n\u0000 (z(i,1:M )), x(i,2)\n\n\u0000\u0000\n\u0000\nb\n\u0000\ni=1 Si\nb\nh\n\u21e3\ni=1 Si\n\nCi\n\n\u25c6\n\n\u0000\n\nP\n\nP\n\nh\n\n\u2713\n\n0\n\n@\n\nM\n\n,\n\n(S6)\n\n, x(i,2)\n\ni\n\n1\n\nA\n\ni\n\n(1/b)\n\nr\u0000p(\u0000) +\n\nr\u0000 log p(z(i,m)\n\n|\n\n\u0000)\n\n(1/b)\n\nr\u0000p(\u0000) +\n\nr\u0000 log p(Di |\n\nz(i,m),\u0000 )\n\n.\n\nm=1\nX\nM\n\n\u0000\n\nm=1\nX\n\n\u0000\n\n2N. These conditions are stated hereafter.\n\nIn order to show non-asymptotic convergence guarantees for FedSOUK detailed in Algorithm 1, we\nneed additional assumptions ensuring some stability of the sequence (Z (i,m)\n[b])k\nA6. For any i\n(i) There exists A1 \u0000\n[Q(i)\n\n1 such that for any p, k\nZ (i,0)\n0\n\nN and m\n2{\nA1V (Z (i,0)\n\n[b], the following conditions hold.\n\n,\n}\nV (Z (i,0)\n0\n\n]pV (Z (i,m)\nk\n\n0, . . . , M\n\n0, . . . , M\n\n) , E\n\n: m\n\n2{\n\n, i\n\n<\n\n2\n\n2\n\n2\n\nE\n\n}\n\n)\n\n)\n\nk\n\n0\n\n,\n\n\u0000k,\u2713k\n\n|\n\n\uf8ff\n\nh\n\ni\n\n1\n\nk\n\nh\n: m\n\nwhere (Z (i,m)\n(ii) There exists A2, A3 \u0000\n\u0000,\u2713 admits \u21e1(i)\nQ(i)\n\n0, . . . , M\n1, \u21e2\n\u0000,\u2713 as stationary distribution and\n\n, i\n[0, 1) such that for any \u0000\n\ni\n2N is de\ufb01ned in (S5).\n2\n\n[b])k\n\n}\n2\n\n2{\n\n2\n\n(0, \u00af\u0000], \u2713\n\n\u21e5, z\n\n2\n\n2\n\nRd and k\n\nN,\n\n2\n\n), \u00001 : (R\u21e4+)2\nRd, a\n\u21e5, z\n\n2\n\nR+ and \u00002 : (R\u21e4+)2\nR+\n[1/4, 1/2], we have for any\n\n!\n\n!\n2\n\nA2\u21e2k\u0000 V (z)\n\n\u0000\n\n\u0000,\u2713 ]k\n\n\u0000z[Q(i)\n\n\u21e1(i)\n\u0000,\u2713\n\u0000\n\u21e1(i)\n\u0000,\u2713 (V )\n\u0000\n\u0000\nR+ such that for any \u0000\n\n\u0000\n\u0000\n\u0000\n\nV \uf8ff\n\nA3 .\n\n\uf8ff\n\n(0, \u00af\u0000] and \u2713\n\n\u21e5,\n\n2\n\n2\n\n (\u0000) .\n\n(iii) There exists   : R\u21e4+ !\n\n\u21e1(i)\n\u0000,\u2713 \u0000\n\n\u21e1(i)\n\u2713\nA7. There exists a measurable function V : Rd\nsuch that for any \u00001,\u0000 2 2\ni\n2\n\n1\n(0, \u00af\u0000] with \u00002 <\u0000 1, \u27131,\u2713 2 2\n\u0000zQ(i)\n\n\u0000zQ(i)\n\n\u0000\n\u0000\n!\n\u0000\n\nV 1/2 \uf8ff\n[1,\n\n[b],\n\n\u0000\n\u0000\n\u0000\n\n\u00002,\u27132 \u0000\n\n\u00001,\u27131\n\nV a \uf8ff\n\n[\u00001(\u00001,\u0000 2) + \u00002(\u00001,\u0000 2)\n\n\u27132 \u0000\nk\n\n\u27131k\n\n]V 2a(z) .\n\n\u0000\n\u0000\n\u0000\nk+1 = \u2318k+1 and, for any i\n\n\u0000\n\u0000\nWe are now ready to show our main result. To ease the presentation, assume for any k\n\u0000\n\u2318(1)\nk+1 = \u2318(2)\n2\nTheorem S2. Assume A1, A2, A3, A4, A5, A6 and A7 and let for any k\nRd and i\nIn addition, for any \u2713\nK\nN\u21e4, we have\n\n[b], assume that\n\nk+1 = \u0000k+1.\n\nH (i)\nk\n\n[b],\u0000 (i)\n\n\u2713 (z)\n\n\u21e5, z\n\nk \uf8ff\n\n2\n\n2\n\n2\n\n[K], \u2318k 2\n\n(0, 1/Lf ].\n2\nV 1/4(z). Then, for any\n\nN that\n\n2\n\n2\n\nK\n\nk=1 \u2318k{\n\nf (\u2713k)\n\u0000\nK\nk=1 \u2318k\n\nE\n\n\" P\n\nf (\u2713?)\n\n}\n\n# \uf8ff\n\nEK\nK\nk=1 \u2318k\n\n,\n\nP\n\nP\n\nwhere, for any K\n\nN\u21e4,\n\n2\n\nEK = 2R2\n\n\u21e5 + 2A1\n\nsup\n[b],m\n\n2\n\ni\n\n2\n\n[M ]\n\nV 1/2(Z (i,m)\n\n0\n\n)\n\nE\n\nn\n\nh\n\nK\n\n\u23182\nk\n\nio\n\nXk=1\n\n8bL2\n\nf R2\n\n\u21e5 +\n\nb\n\ni=1\nX\n\n(!i + 1 + pi)\npi\n\n!\n\n4\n\n \n \n \n\f+ b\n\nsup\n[b],m\n\n2\n\ni\n\n2\n\n[M ]\n\nn\n\nC (i,m)\n3\n\n+ b.A1Cc,2\n\nsup\n[b],m\n\n2\n\ni\n\n2\n\n[M ]\n\nK\n\n+ b\n\n\u2318k (\u0000k\n\n1) ,\n\n\u0000\n\nK\n\nK\n\n\u2318k \u0000\n\n|\n\n\u2318k\n\n1\n1 +\n\n\u0000\u0000\nk\n\u0000\n\n1|\n\n\u0000\n\n\"\n\no\n\nE\n\nXk=1\nV (Z (i,m)\n0\n\nn\n\nh\n\nK\n\n)\n\nio\n\nXk=1\n\nXk=1\n1\n\n\u21e5\n\n1\n\n\u23182\nk\u0000\u0000\nk\n\u0000\n\n1 + \u2318K/\u0000K \u0000\n\n\u23181/\u00001\n\n#\n\n\u2318k\u0000\u0000\nk\n\n1\n\u0000\u0000\nk {\n\n\u21e41(\u0000k\n\n1,\u0000 k) + \u21e42(\u0000k\n\n\u0000\n\n1,\u0000 k)\u2318k}\n\n\u0000\n\n+ \u2318k\n\n\u21e4\n\nXk=1\nC (i,m)\n3\n{\n\nwith\n\n[b],m\n\n}i\n\n2\n\n2\n\n[M ] de\ufb01ned in Lemma S5 and Cc,2 de\ufb01ned in Lemma S6.\n\nProof. The proof follows by using the fact that (S23) is a (\ncombining Lemma S1-S7.\n\nFk\n\n\u0000\n\n1)k\n\n2N\u21e4 -martingale increment and by\n\n\n\nThe following is the appendix_2 section of the paper you are reviewing:\n FedSOUL\n\n\u0000,\u2713 is associated with a Gaussian probability density function q(i)\n\nWe now apply Theorem S2 to FedSOUL where for any i\nkernel Q(i)\nrz log p(z(i)\n\u0000\nposterior distributions\n\n[b] such that A6 and A7 are satis\ufb01ed.\n\n\u0000\nDi,\u2713 ) and variance 2\u0000Id. To this end, we show explicit conditions on the family of\n\n(0, \u00af\u0000] and \u2713\n\u0000,\u2713 (z(i),\n\n2\n) with mean z(i)\n\n\u21e5, the Markov\n\n[b], \u0000\n\n2\n\n2\n\n|\n\n\u00b7\n\n\u21e1(i)\n\u2713 }i\n\n{\n\n2\n\nS2.1 Assumptions\n\n[b], let U (i)\nFor any i\n\u2713\nour case, this boils down to set U (i)\nA8. For any i\n\n: Rd\n\n!\n\n2\n\n2\n\n(i) Assume that (\u2713, z (i))\n\u27131,\u2713 2 2\n\n7!\n\u21e5 and there exists L\n\nsup\n\u21e5\n\u2713\n\n2\n\n\u0000\n\u0000\n\u0000\n\nand\n\n{rzU (i)\n\n\u2713 (0) : \u2713\n\n\u21e5\n\n}\n\n2\n\nis bounded.\n\nR such that for any z(i)\nlog p(z(i)\n\u2713 (z(i)) =\n\n\u0000\n\nRd, \u21e1(i)\n\u2713 (z(i))\n2\nDi,\u0000,\u0000 ) for any z(i)\n|\n\n/\n\nexp\n\n{\u0000\nRd.\n2\n\nU (i)\n\n\u2713 (z(i))\n\n. In\n}\n\n[b], the following conditions hold.\n\nU (i)\n\n\u2713 (z(i)) is differentiable for any\n\n7!\nRd,\n\n\u0000\n\nU\u2713(z(i)) is continuous, z(i)\n0 such that for any z1, z2 2\n\u0000 rzU (i)\nrzU (i)\n\n\u2713 (z1)\n\n\u2713 (z2)\n\nL\n\n\u27132 \u0000\nk\n\n\u27131k\n\n,\n\n\uf8ff\n\n\u0000\n\u0000\n\u0000\n\n11\n\n\f(ii) There exist m1, m2 > 0 and c, R\n\nhrzU (i)\n\n\u2713 (z), z\n\ni \u0000\n\n\u0000\nz\nm1 k\n\n0 such that for any \u2713\n\n1B(0,R)c (z) + m2\n\nk\nRd and \u27131,\u2713 2 2\nLU k\n(z)\n\n\uf8ff\n\n\u27131\n\n\u0000 rzU (i)\n\n2\n\n0 such that z\nrzU (i)\n\n(z)\n\n\u27132\n\n(iii) There exists LU \u0000\n\nwhere V : Rd\n\n!\n\n\u0000\n\u0000\n\u0000\n\nR is de\ufb01ned under A8-(ii), for any z\n\n\u0000\n\u0000\n\u0000\n\n2\n\n\u21e5 and z\n\n2\n\n2\n\nrzU (i)\n\n\u2713 (z)\n\nRd,\n2\n\n\u0000\n\n\u0000\n\u0000\n\u0000\n\nc .\n\nV (z)1/2 ,\n\n\u27131k\n\n\u21e5,\n\n\u0000\n\u0000\n\u0000\n\u27132 \u0000\nRd, as\n\nV (z) = exp\n\nm1\n\n1 +\n\n\u21e2\n\nq\n\nz\n\nk\n\n2/4\nk\n\n\u0000\n\n.\n\n(S24)\n\nS2.2 Veri\ufb01cation of A6 and A7\n\nLemma S8. Assume A8. Then, A6 and A7 are satis\ufb01ed with V de\ufb01ned in (S24) and\n\n1/2\n\nL ,\n\n\u02dcD1\n\n2\n\n!\n\n)\n\n\u0000\n\u0000\n\u0000\n\n,\n\n{\n\n1, 2m2}\n\n\u00af\u0000< min\n\u02dcm1 = m1/4 ,\nb = \u02dcm1(d + c + p2\u02dcm1) exp(\u02dcm2\n1{\n\u02dcm2\n1[p2\n\u0000 = exp(\n\u0000\n\u0000\n1, 2(d + c)/m1, R\nr = max\n{\n\u00001/\u00002 \u0000\n1 ,\n\u00001 : (\u00001,\u0000 2)\n\u00001/2\n,\n2\n\n\u00002 : (\u00001,\u0000 2)\n\n1]) ,\n\n7!\n\n}\n\n,\n\n7!\n\n(d + c + \u02dcm1\u00af\u0000 +\n\n1 + r2\n\n) ,\n}\n\np\n\nrzU (i)\n\n\u2713 (0)\n\n[b]\n\n\u0000\n\u0000\n\u0000\n1(1/\u0000) ,\n\n  : \u0000\n\n2C(1\n\n\u0000\n\n7!\n\n\u21e0)\u0000\n\n1\u00001/2 \u02dcD1/2\n\n1\n\n(1 + \u00af\u0000)1/2\n\nd + 2\u00af\u0000\n\n(\n\nL2MV + sup\n\u21e5,i\n2\n\n2\n\n\u2713\n\np2~m1 exp(~m1\n\n\u02dcD1 =\n\n1 + max\n\n1, R\n{\n3~m2\n1\n\np\n\n2)(1 + ~m1 + c + d)\n}\n\n+ b\u0000\u0000\n\n\u00af\u0000 log\u0000\n\nwith MV = supz\n\n2Rd\n\n(1 +\n{\n\nz\nk\n\nk\n\n)2/V (z)\n\n, C\n}\n\n\u0000\n\n0, \u21e0\n\n2\n\n(0, 1).\n\nProof. The proof follows from De Bortoli et al. [11, Theorem 5].\n\n12\n\n \n\f\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n \n\n\n\n\n\n\n\n \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure S1: Small data sets - synthetic data. b = 50 clients.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n \n\n \n\n\n\n\n\n\n\n \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure S2: Small data sets - synthetic data. b = 200 clients.\n\n   \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n \n\n \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure S3: Small data sets - synthetic data. Raw data dimensionality is k = 50.\n\n \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n \n\n \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure S4: Small data sets - synthetic data. Raw data dimensionality is k = 5.\n\nS3  unsurprising given the MDP structure.\n\nBased on the section(s), please answer the following question with yes, no, or n/a and provide a brief justification for your answer.\nQuestion: If the authors include theoretical results, do the authors include complete proofs of all theoretical results?"}, "3a": {"role": "user", "content": "The following is the experiments section of the paper you are reviewing:\n\n\nIn this section, we illustrate the bene\ufb01ts of our methodology on several FL benchmarks associated to\nboth synthetic and real data. Since existing Bayesian FL approaches are not suited for personalisation\n(see Table 1), we only compare the performances of Algorithm 1 with personalised FL methods. In\nall our experiments, we use overdamped Langevin dynamics to sample locally and call this speci\ufb01c\ninstance of Algorithm 1, FedSOUL. In addition, we set p(z(i) | \u03b2) = N(\u00b5, \u03c32Id) with \u03b2 = {\u00b5, \u03c3} for\nsimplicity. To be comparable with existing personalised FL approaches that only consider periodic\ncommunication via multiple local steps, we do not resort to the proposed compression mechanism\nalthough the latter could be of interest for real-world applications. Additional experiments and details\nabout experimental design are provided in the supplement.\n\nSynthetic Data. We start by showcasing the bene\ufb01ts of FedSOUL for clients having small and highly\nheterogeneous data sets as pointed out in Section 1 and Section 2. To this end, we consider a similar\nexperimental setting as in Collins et al. (2021) where synthetic observations {y(i)\nj }j\u2208[Ni] \u2208 Di\n\n8\n\n\ftrue\u03c6(cid:62)\n\ntruex(i)\n\nj \u223c N(z(i)\n\nj \u223c N(0k, Ik) and y(i)\n\nare generated via the following procedure: x(i)\nj , 0.1).\nThe ground-truth parameters z(i)\ntrue \u2208 Rd and \u03c6true \u2208 Rk\u00d7d have been randomly generated\nbeforehand with (d, k) = (2, 20). Compared to Collins et al. (2021), we use heterogeneous\ndata partitions across clients so that 90% of the b = 100 clients have small data sets of size\n5 and the remaining 10% have data sets of size 10. We compare our results with FedRep\n(Collins et al., 2021) and FedAvg (McMahan et al., 2017) since they stand for two limiting in-\nstances of the proposed methodology, see Section 4 and Gelman and Hill (2007, Section 12).\nFigure 2 compares the different ap-\nproaches by computing the princi-\nple angle distance1 (respectively the\n(cid:96)2 norm) between \u03c6true (respectively\nz(i)\nthe\ntrue) and its estimated value;\nlesser the better. In contrast to its main\ncompetitors and based on both met-\nrics, FedSOUL provides an impressive\nimprovement. This illustrates the ben-\ne\ufb01ts of the introduction of a common\nprior p(z(i) | \u03b2) which allows to pre-\nvent from over\ufb01tting on clients with small data sets while performing personalisation. Additional\nresults with other choices for (b, d, k) and data partitioning strategies are available in the supplement.\n\nFigure 2: Small data sets - synthetic data.\n\nMoreover, to compare our algorithm with a non-FL setting, we perform a non-distributed and non-\nfederated stochastic approximation algorithm to \ufb01nd \u03b8\u2217 using a large number of iterations to get an\naccurate approximation of the optimal parameter \u03b8\u2217. Then, we use FedPop to obtain an estimate \u02dc\u03b8\u2217\nand measure the relative error in l2- distance between \u03b8\u2217 and \u02dc\u03b8\u2217. For some outer iterations T = 100,\nthe relative error was less than 10\u22123, which illustrates the relevance of our theoretical results. We\nalso test the performances of the proposed approach when the warm-start strategy is not used. In this\ncase, we have to set M = 50 to achieve the same performances as in the stateful variant of FedSOUL.\n\nReal Data. We consider now real image data sets, namely CIFAR-10 and CIFAR-100 (Krizhevsky,\n2009). For our likelihood model de\ufb01ned by p(Di | \u03c6, z(i)), we use 5-layer convolutional neural\nnetworks and perform personalisation for the last layer. We set b = 100 for convenience and control\ndata heterogeneity by assigning to each client Ni images belonging to only S different classes.\n\nSmall data sets. Under this setting,\nwe \ufb01rst consider (10%, 50%, 90%)\nof clients having small data sets of\nsize either Ni = 5 or Ni = 10;\nwhile remaining clients have larger\ndata sets of size Ni = 25. We com-\npare our approach with FedRep since\nit stands for the state-of-the-art person-\nalised FL approach. The algorithms\nare trained ful\ufb01lling the same com-\nputational budget. Figure 3 shows\nthe average accuracy across clients for\nthe two approaches on both CIFAR-\n10 and CIFAR-100. We can see that FedSOUL is consistently better than FedRep over different\ncon\ufb01gurations.\n\nFigure 3: (right) CIFAR-10 with S = 5 and (left) CIFAR-\n100 with S = 20. The x-axis refers to the percentage of\nclients having Ni \u2208 {5, 10} images.\n\nFull data sets. In addition to show that the proposed approach achieves state-of-the-art performances\non small data sets (which is common in the cross-device scenario), we now illustrate that FedSOUL\nis also competitive on larger data sets. To this end, we use all training images in CIFAR-10 and\nCIFAR-100 image data sets and consider the same data partitioning as in Collins et al. (2021). More\nprecisely, in this case the number of observations and the number of classes per client are uniformly\nshared over the clients. Table 2 shows our results in comparison with state-of-the-art personalised\nFL approaches. We can see that that our model outperforms other methods on both CIFAR-10\n\n1de\ufb01ned in (Collins et al., 2021, De\ufb01nition 1)\n\n9\n\nFedRepFedAvgFedSOUL05101520253035l2 norm of the estimation error010020030040050010\u22121100Principal Angle DistanceFedRepFedAvgFedSOUL1050900.00.10.20.30.4AccuracyFedSOUL,5FedRep,5FedSOUL,10FedRep,101050900.000.020.040.060.080.100.120.14AccuracyFedSOUL,5FedRep,5FedSOUL,10FedRep,10\fTable 2: Real data - Full data sets. Accuracy (in %) on test samples. FedAvg and SCAFFOLD are not\npersonalised FL approaches but stand for well-known FL benchmarks.\n\n(# clients b, # classes per client S)\n\n(100, 2)\n\n(100, 5)\n\n(100, 5)\n\n(100, 20)\n\nCIFAR-10\n\nCIFAR-100\n\nLocal learning only\n\nFedAvg (McMahan et al., 2017)\nSCAFFOLD (Karimireddy et al., 2020)\n\nLG-FedAvg (Liang et al., 2019)\nPer-FedAvg (Fallah et al., 2020)\nL2GD (Hanzely and Richt\u00e1rik, 2020)\nAPFL (Deng et al., 2021b)\nDITTO (Li et al., 2021)\nFedRep (Collins et al., 2021)\nFedAvg + fine-tuning (FT)\n\nFedSOUL (this paper)\n\n89.79\n\n42.65\n37.72\n\n84.14\n82.27\n81.04\n83.77\n85.39\n87.70\n85.63\n\n91.12\n\n70.68\n\n51.78\n47.33\n\n63.02\n67.20\n59.98\n72.29\n70.34\n75.68\n71.32\n\n79.48\n\n75.29\n\n23.94\n20.32\n\n72.44\n72.05\n72.13\n78.20\n78.91\n79.15\n79.03\n\n79.56\n\n41.29\n\n31.97\n22.52\n\n38.76\n52.49\n42.84\n55.44\n56.34\n56.10\n56.19\n\n59.73\n\nand CIFAR-100 by a large margin. Additional results with other personalised FL algorithms are\npostponed to the supplement.\n\nUncertainty Quanti\ufb01cation on Real Data. As highlighted in Table 1, one advantage of the\nproposed approach compared to existing personalised FL methods is the ability to perform uncertainty\nquanti\ufb01cation by sampling locally from the posterior p(z(i) | Di, \u03c6K, \u03b2K), see Algorithm 1. We\nillustrate this feature by computing on CIFAR-10 calibration curves and scores (e.g. expected\ncalibration error aka ECE) on a speci\ufb01c client; and by performing an out-of-distribution analysis\non MNIST/FashionMNIST data sets. Figure 4 shows that the proposed approach provides relevant\nuncertainty diagnosis. Additional results on uncertainty quanti\ufb01cation can be found in the supplement.\n\n higher diameter and value norm.\n\nThe following is the appendix_3 section of the paper you are reviewing:\n\n\nS3.1 Synthetic datasets\n\nIn this section, following the experiments from the main paper, we will show additional con\ufb01gurations\nof the toy example. We still use the same model (see Section 5 and Singhal et al. [47], Collins et al.\n[8]), but we choose different values of (d, k, b). First, let us test, how the total number of clients b\nimpacts the performances of the different approaches. Figure S1 and Figure S2 depict our results for\nb\n, with the size of the minimal dataset being 5 and the share of clients with the minimal\n}\ndataset 90%. We can see that in both cases, FedSOUL outperforms its competitors.\n\n50, 200\n\n2{\n\nSecond, we test, how the dimensionality of raw data impacts the result. Figure S3 and Figure S4\nshow our results with k\n\n. All others parameters are the same as before.\n}\n\nOne more experiment we conducted is the dependence on latent dimensionality d. We test two\noptions d = 2 (as in original experiments) and d = 5 in Figure S5 and Figure S6. Again, the more\n\n5, 50\n\n2{\n\n13\n\n\f\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure S5: Small data sets - synthetic data. Latent space dimensionality is d = 5.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure S6: Small data sets - synthetic data. Latent space dimensionality is d = 2.\n\ny\nc\na\nr\nu\nc\nc\nA\n\n0.7\n\n0.6\n\n0.5\n\n0.4\n\n0.3\n\n0.2\n\n0.1\n\n0.0\n\ny\nc\na\nr\nu\nc\nc\nA\n\n0.4\n\n0.3\n\n0.2\n\n0.1\n\n0.0\n\n10\n\n50\n\nFedSOUL\nFedRep\nAPFL\n\n90\n\n10\n\n50\n\nFedSOUL\nFedRep\nAPFL\n\n90\n\nFigure S7: Small image datasets. The minimal local dataset size is 2 (top) or 5 (bottom).\n\nparameters we have to learn (given the same small data budget), the better Bayesian methods (i.e.\nFedSOUL) are better.\n\nS3.2 Image datasets classi\ufb01cation\n\nIn this section, we provide an additional baseline for the experiments with personalization, in case\nwe have only a few heterogeneous data. Speci\ufb01cally, we consider APFL [13] which is another\npersonalized federated learning approach. We consider the CIFAR-10 dataset with 100 clients.\nAmong these clients, there are 10, 50, or 90 which have a local dataset of either 5 (one setup) or 10\n(another setup). Else of size 25.\n\nWe see in Figure S7 that FedSOUL typically performs better than FedRep, but on par with APFL. It\nis surprising, that APFL is a very good baseline in this type of problem, which it was not specially\ndesigned for.\n\nS3.3 Image datasets uncertainty quanti\ufb01cation\n\nIn this section, we provide additional experiments on image uncertainty with CIFAR-10 (in distribu-\ntion) and SVHN (out of distribution) datasets. As a measure of uncertainty, we will use predictive\nentropy. On \n\nBased on the section(s), please answer the following question with yes, no, or n/a and provide a brief justification for your answer.\nQuestion: If the authors ran experiments, do the authors include the code, data, and instructions needed to reproduce the main experimental results (either in the supplemental material or as a URL)?"}, "3b": {"role": "user", "content": "The following is the experiments section of the paper you are reviewing:\n\n\nIn this section, we illustrate the bene\ufb01ts of our methodology on several FL benchmarks associated to\nboth synthetic and real data. Since existing Bayesian FL approaches are not suited for personalisation\n(see Table 1), we only compare the performances of Algorithm 1 with personalised FL methods. In\nall our experiments, we use overdamped Langevin dynamics to sample locally and call this speci\ufb01c\ninstance of Algorithm 1, FedSOUL. In addition, we set p(z(i) | \u03b2) = N(\u00b5, \u03c32Id) with \u03b2 = {\u00b5, \u03c3} for\nsimplicity. To be comparable with existing personalised FL approaches that only consider periodic\ncommunication via multiple local steps, we do not resort to the proposed compression mechanism\nalthough the latter could be of interest for real-world applications. Additional experiments and details\nabout experimental design are provided in the supplement.\n\nSynthetic Data. We start by showcasing the bene\ufb01ts of FedSOUL for clients having small and highly\nheterogeneous data sets as pointed out in Section 1 and Section 2. To this end, we consider a similar\nexperimental setting as in Collins et al. (2021) where synthetic observations {y(i)\nj }j\u2208[Ni] \u2208 Di\n\n8\n\n\ftrue\u03c6(cid:62)\n\ntruex(i)\n\nj \u223c N(z(i)\n\nj \u223c N(0k, Ik) and y(i)\n\nare generated via the following procedure: x(i)\nj , 0.1).\nThe ground-truth parameters z(i)\ntrue \u2208 Rd and \u03c6true \u2208 Rk\u00d7d have been randomly generated\nbeforehand with (d, k) = (2, 20). Compared to Collins et al. (2021), we use heterogeneous\ndata partitions across clients so that 90% of the b = 100 clients have small data sets of size\n5 and the remaining 10% have data sets of size 10. We compare our results with FedRep\n(Collins et al., 2021) and FedAvg (McMahan et al., 2017) since they stand for two limiting in-\nstances of the proposed methodology, see Section 4 and Gelman and Hill (2007, Section 12).\nFigure 2 compares the different ap-\nproaches by computing the princi-\nple angle distance1 (respectively the\n(cid:96)2 norm) between \u03c6true (respectively\nz(i)\nthe\ntrue) and its estimated value;\nlesser the better. In contrast to its main\ncompetitors and based on both met-\nrics, FedSOUL provides an impressive\nimprovement. This illustrates the ben-\ne\ufb01ts of the introduction of a common\nprior p(z(i) | \u03b2) which allows to pre-\nvent from over\ufb01tting on clients with small data sets while performing personalisation. Additional\nresults with other choices for (b, d, k) and data partitioning strategies are available in the supplement.\n\nFigure 2: Small data sets - synthetic data.\n\nMoreover, to compare our algorithm with a non-FL setting, we perform a non-distributed and non-\nfederated stochastic approximation algorithm to \ufb01nd \u03b8\u2217 using a large number of iterations to get an\naccurate approximation of the optimal parameter \u03b8\u2217. Then, we use FedPop to obtain an estimate \u02dc\u03b8\u2217\nand measure the relative error in l2- distance between \u03b8\u2217 and \u02dc\u03b8\u2217. For some outer iterations T = 100,\nthe relative error was less than 10\u22123, which illustrates the relevance of our theoretical results. We\nalso test the performances of the proposed approach when the warm-start strategy is not used. In this\ncase, we have to set M = 50 to achieve the same performances as in the stateful variant of FedSOUL.\n\nReal Data. We consider now real image data sets, namely CIFAR-10 and CIFAR-100 (Krizhevsky,\n2009). For our likelihood model de\ufb01ned by p(Di | \u03c6, z(i)), we use 5-layer convolutional neural\nnetworks and perform personalisation for the last layer. We set b = 100 for convenience and control\ndata heterogeneity by assigning to each client Ni images belonging to only S different classes.\n\nSmall data sets. Under this setting,\nwe \ufb01rst consider (10%, 50%, 90%)\nof clients having small data sets of\nsize either Ni = 5 or Ni = 10;\nwhile remaining clients have larger\ndata sets of size Ni = 25. We com-\npare our approach with FedRep since\nit stands for the state-of-the-art person-\nalised FL approach. The algorithms\nare trained ful\ufb01lling the same com-\nputational budget. Figure 3 shows\nthe average accuracy across clients for\nthe two approaches on both CIFAR-\n10 and CIFAR-100. We can see that FedSOUL is consistently better than FedRep over different\ncon\ufb01gurations.\n\nFigure 3: (right) CIFAR-10 with S = 5 and (left) CIFAR-\n100 with S = 20. The x-axis refers to the percentage of\nclients having Ni \u2208 {5, 10} images.\n\nFull data sets. In addition to show that the proposed approach achieves state-of-the-art performances\non small data sets (which is common in the cross-device scenario), we now illustrate that FedSOUL\nis also competitive on larger data sets. To this end, we use all training images in CIFAR-10 and\nCIFAR-100 image data sets and consider the same data partitioning as in Collins et al. (2021). More\nprecisely, in this case the number of observations and the number of classes per client are uniformly\nshared over the clients. Table 2 shows our results in comparison with state-of-the-art personalised\nFL approaches. We can see that that our model outperforms other methods on both CIFAR-10\n\n1de\ufb01ned in (Collins et al., 2021, De\ufb01nition 1)\n\n9\n\nFedRepFedAvgFedSOUL05101520253035l2 norm of the estimation error010020030040050010\u22121100Principal Angle DistanceFedRepFedAvgFedSOUL1050900.00.10.20.30.4AccuracyFedSOUL,5FedRep,5FedSOUL,10FedRep,101050900.000.020.040.060.080.100.120.14AccuracyFedSOUL,5FedRep,5FedSOUL,10FedRep,10\fTable 2: Real data - Full data sets. Accuracy (in %) on test samples. FedAvg and SCAFFOLD are not\npersonalised FL approaches but stand for well-known FL benchmarks.\n\n(# clients b, # classes per client S)\n\n(100, 2)\n\n(100, 5)\n\n(100, 5)\n\n(100, 20)\n\nCIFAR-10\n\nCIFAR-100\n\nLocal learning only\n\nFedAvg (McMahan et al., 2017)\nSCAFFOLD (Karimireddy et al., 2020)\n\nLG-FedAvg (Liang et al., 2019)\nPer-FedAvg (Fallah et al., 2020)\nL2GD (Hanzely and Richt\u00e1rik, 2020)\nAPFL (Deng et al., 2021b)\nDITTO (Li et al., 2021)\nFedRep (Collins et al., 2021)\nFedAvg + fine-tuning (FT)\n\nFedSOUL (this paper)\n\n89.79\n\n42.65\n37.72\n\n84.14\n82.27\n81.04\n83.77\n85.39\n87.70\n85.63\n\n91.12\n\n70.68\n\n51.78\n47.33\n\n63.02\n67.20\n59.98\n72.29\n70.34\n75.68\n71.32\n\n79.48\n\n75.29\n\n23.94\n20.32\n\n72.44\n72.05\n72.13\n78.20\n78.91\n79.15\n79.03\n\n79.56\n\n41.29\n\n31.97\n22.52\n\n38.76\n52.49\n42.84\n55.44\n56.34\n56.10\n56.19\n\n59.73\n\nand CIFAR-100 by a large margin. Additional results with other personalised FL algorithms are\npostponed to the supplement.\n\nUncertainty Quanti\ufb01cation on Real Data. As highlighted in Table 1, one advantage of the\nproposed approach compared to existing personalised FL methods is the ability to perform uncertainty\nquanti\ufb01cation by sampling locally from the posterior p(z(i) | Di, \u03c6K, \u03b2K), see Algorithm 1. We\nillustrate this feature by computing on CIFAR-10 calibration curves and scores (e.g. expected\ncalibration error aka ECE) on a speci\ufb01c client; and by performing an out-of-distribution analysis\non MNIST/FashionMNIST data sets. Figure 4 shows that the proposed approach provides relevant\nuncertainty diagnosis. Additional results on uncertainty quanti\ufb01cation can be found in the supplement.\n\n higher diameter and value norm.\n\nThe following is the appendix_3 section of the paper you are reviewing:\n\n\nS3.1 Synthetic datasets\n\nIn this section, following the experiments from the main paper, we will show additional con\ufb01gurations\nof the toy example. We still use the same model (see Section 5 and Singhal et al. [47], Collins et al.\n[8]), but we choose different values of (d, k, b). First, let us test, how the total number of clients b\nimpacts the performances of the different approaches. Figure S1 and Figure S2 depict our results for\nb\n, with the size of the minimal dataset being 5 and the share of clients with the minimal\n}\ndataset 90%. We can see that in both cases, FedSOUL outperforms its competitors.\n\n50, 200\n\n2{\n\nSecond, we test, how the dimensionality of raw data impacts the result. Figure S3 and Figure S4\nshow our results with k\n\n. All others parameters are the same as before.\n}\n\nOne more experiment we conducted is the dependence on latent dimensionality d. We test two\noptions d = 2 (as in original experiments) and d = 5 in Figure S5 and Figure S6. Again, the more\n\n5, 50\n\n2{\n\n13\n\n\f\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure S5: Small data sets - synthetic data. Latent space dimensionality is d = 5.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure S6: Small data sets - synthetic data. Latent space dimensionality is d = 2.\n\ny\nc\na\nr\nu\nc\nc\nA\n\n0.7\n\n0.6\n\n0.5\n\n0.4\n\n0.3\n\n0.2\n\n0.1\n\n0.0\n\ny\nc\na\nr\nu\nc\nc\nA\n\n0.4\n\n0.3\n\n0.2\n\n0.1\n\n0.0\n\n10\n\n50\n\nFedSOUL\nFedRep\nAPFL\n\n90\n\n10\n\n50\n\nFedSOUL\nFedRep\nAPFL\n\n90\n\nFigure S7: Small image datasets. The minimal local dataset size is 2 (top) or 5 (bottom).\n\nparameters we have to learn (given the same small data budget), the better Bayesian methods (i.e.\nFedSOUL) are better.\n\nS3.2 Image datasets classi\ufb01cation\n\nIn this section, we provide an additional baseline for the experiments with personalization, in case\nwe have only a few heterogeneous data. Speci\ufb01cally, we consider APFL [13] which is another\npersonalized federated learning approach. We consider the CIFAR-10 dataset with 100 clients.\nAmong these clients, there are 10, 50, or 90 which have a local dataset of either 5 (one setup) or 10\n(another setup). Else of size 25.\n\nWe see in Figure S7 that FedSOUL typically performs better than FedRep, but on par with APFL. It\nis surprising, that APFL is a very good baseline in this type of problem, which it was not specially\ndesigned for.\n\nS3.3 Image datasets uncertainty quanti\ufb01cation\n\nIn this section, we provide additional experiments on image uncertainty with CIFAR-10 (in distribu-\ntion) and SVHN (out of distribution) datasets. As a measure of uncertainty, we will use predictive\nentropy. On \n\nBased on the section(s), please answer the following question with yes, no, or n/a and provide a brief justification for your answer.\nQuestion: If the authors ran experiments, do the authors specify all the training details (e.g., data splits, hyperparameters, how they were chosen)?"}, "3c": {"role": "user", "content": "The following is the experiments section of the paper you are reviewing:\n\n\nIn this section, we illustrate the bene\ufb01ts of our methodology on several FL benchmarks associated to\nboth synthetic and real data. Since existing Bayesian FL approaches are not suited for personalisation\n(see Table 1), we only compare the performances of Algorithm 1 with personalised FL methods. In\nall our experiments, we use overdamped Langevin dynamics to sample locally and call this speci\ufb01c\ninstance of Algorithm 1, FedSOUL. In addition, we set p(z(i) | \u03b2) = N(\u00b5, \u03c32Id) with \u03b2 = {\u00b5, \u03c3} for\nsimplicity. To be comparable with existing personalised FL approaches that only consider periodic\ncommunication via multiple local steps, we do not resort to the proposed compression mechanism\nalthough the latter could be of interest for real-world applications. Additional experiments and details\nabout experimental design are provided in the supplement.\n\nSynthetic Data. We start by showcasing the bene\ufb01ts of FedSOUL for clients having small and highly\nheterogeneous data sets as pointed out in Section 1 and Section 2. To this end, we consider a similar\nexperimental setting as in Collins et al. (2021) where synthetic observations {y(i)\nj }j\u2208[Ni] \u2208 Di\n\n8\n\n\ftrue\u03c6(cid:62)\n\ntruex(i)\n\nj \u223c N(z(i)\n\nj \u223c N(0k, Ik) and y(i)\n\nare generated via the following procedure: x(i)\nj , 0.1).\nThe ground-truth parameters z(i)\ntrue \u2208 Rd and \u03c6true \u2208 Rk\u00d7d have been randomly generated\nbeforehand with (d, k) = (2, 20). Compared to Collins et al. (2021), we use heterogeneous\ndata partitions across clients so that 90% of the b = 100 clients have small data sets of size\n5 and the remaining 10% have data sets of size 10. We compare our results with FedRep\n(Collins et al., 2021) and FedAvg (McMahan et al., 2017) since they stand for two limiting in-\nstances of the proposed methodology, see Section 4 and Gelman and Hill (2007, Section 12).\nFigure 2 compares the different ap-\nproaches by computing the princi-\nple angle distance1 (respectively the\n(cid:96)2 norm) between \u03c6true (respectively\nz(i)\nthe\ntrue) and its estimated value;\nlesser the better. In contrast to its main\ncompetitors and based on both met-\nrics, FedSOUL provides an impressive\nimprovement. This illustrates the ben-\ne\ufb01ts of the introduction of a common\nprior p(z(i) | \u03b2) which allows to pre-\nvent from over\ufb01tting on clients with small data sets while performing personalisation. Additional\nresults with other choices for (b, d, k) and data partitioning strategies are available in the supplement.\n\nFigure 2: Small data sets - synthetic data.\n\nMoreover, to compare our algorithm with a non-FL setting, we perform a non-distributed and non-\nfederated stochastic approximation algorithm to \ufb01nd \u03b8\u2217 using a large number of iterations to get an\naccurate approximation of the optimal parameter \u03b8\u2217. Then, we use FedPop to obtain an estimate \u02dc\u03b8\u2217\nand measure the relative error in l2- distance between \u03b8\u2217 and \u02dc\u03b8\u2217. For some outer iterations T = 100,\nthe relative error was less than 10\u22123, which illustrates the relevance of our theoretical results. We\nalso test the performances of the proposed approach when the warm-start strategy is not used. In this\ncase, we have to set M = 50 to achieve the same performances as in the stateful variant of FedSOUL.\n\nReal Data. We consider now real image data sets, namely CIFAR-10 and CIFAR-100 (Krizhevsky,\n2009). For our likelihood model de\ufb01ned by p(Di | \u03c6, z(i)), we use 5-layer convolutional neural\nnetworks and perform personalisation for the last layer. We set b = 100 for convenience and control\ndata heterogeneity by assigning to each client Ni images belonging to only S different classes.\n\nSmall data sets. Under this setting,\nwe \ufb01rst consider (10%, 50%, 90%)\nof clients having small data sets of\nsize either Ni = 5 or Ni = 10;\nwhile remaining clients have larger\ndata sets of size Ni = 25. We com-\npare our approach with FedRep since\nit stands for the state-of-the-art person-\nalised FL approach. The algorithms\nare trained ful\ufb01lling the same com-\nputational budget. Figure 3 shows\nthe average accuracy across clients for\nthe two approaches on both CIFAR-\n10 and CIFAR-100. We can see that FedSOUL is consistently better than FedRep over different\ncon\ufb01gurations.\n\nFigure 3: (right) CIFAR-10 with S = 5 and (left) CIFAR-\n100 with S = 20. The x-axis refers to the percentage of\nclients having Ni \u2208 {5, 10} images.\n\nFull data sets. In addition to show that the proposed approach achieves state-of-the-art performances\non small data sets (which is common in the cross-device scenario), we now illustrate that FedSOUL\nis also competitive on larger data sets. To this end, we use all training images in CIFAR-10 and\nCIFAR-100 image data sets and consider the same data partitioning as in Collins et al. (2021). More\nprecisely, in this case the number of observations and the number of classes per client are uniformly\nshared over the clients. Table 2 shows our results in comparison with state-of-the-art personalised\nFL approaches. We can see that that our model outperforms other methods on both CIFAR-10\n\n1de\ufb01ned in (Collins et al., 2021, De\ufb01nition 1)\n\n9\n\nFedRepFedAvgFedSOUL05101520253035l2 norm of the estimation error010020030040050010\u22121100Principal Angle DistanceFedRepFedAvgFedSOUL1050900.00.10.20.30.4AccuracyFedSOUL,5FedRep,5FedSOUL,10FedRep,101050900.000.020.040.060.080.100.120.14AccuracyFedSOUL,5FedRep,5FedSOUL,10FedRep,10\fTable 2: Real data - Full data sets. Accuracy (in %) on test samples. FedAvg and SCAFFOLD are not\npersonalised FL approaches but stand for well-known FL benchmarks.\n\n(# clients b, # classes per client S)\n\n(100, 2)\n\n(100, 5)\n\n(100, 5)\n\n(100, 20)\n\nCIFAR-10\n\nCIFAR-100\n\nLocal learning only\n\nFedAvg (McMahan et al., 2017)\nSCAFFOLD (Karimireddy et al., 2020)\n\nLG-FedAvg (Liang et al., 2019)\nPer-FedAvg (Fallah et al., 2020)\nL2GD (Hanzely and Richt\u00e1rik, 2020)\nAPFL (Deng et al., 2021b)\nDITTO (Li et al., 2021)\nFedRep (Collins et al., 2021)\nFedAvg + fine-tuning (FT)\n\nFedSOUL (this paper)\n\n89.79\n\n42.65\n37.72\n\n84.14\n82.27\n81.04\n83.77\n85.39\n87.70\n85.63\n\n91.12\n\n70.68\n\n51.78\n47.33\n\n63.02\n67.20\n59.98\n72.29\n70.34\n75.68\n71.32\n\n79.48\n\n75.29\n\n23.94\n20.32\n\n72.44\n72.05\n72.13\n78.20\n78.91\n79.15\n79.03\n\n79.56\n\n41.29\n\n31.97\n22.52\n\n38.76\n52.49\n42.84\n55.44\n56.34\n56.10\n56.19\n\n59.73\n\nand CIFAR-100 by a large margin. Additional results with other personalised FL algorithms are\npostponed to the supplement.\n\nUncertainty Quanti\ufb01cation on Real Data. As highlighted in Table 1, one advantage of the\nproposed approach compared to existing personalised FL methods is the ability to perform uncertainty\nquanti\ufb01cation by sampling locally from the posterior p(z(i) | Di, \u03c6K, \u03b2K), see Algorithm 1. We\nillustrate this feature by computing on CIFAR-10 calibration curves and scores (e.g. expected\ncalibration error aka ECE) on a speci\ufb01c client; and by performing an out-of-distribution analysis\non MNIST/FashionMNIST data sets. Figure 4 shows that the proposed approach provides relevant\nuncertainty diagnosis. Additional results on uncertainty quanti\ufb01cation can be found in the supplement.\n\n higher diameter and value norm.\n\nBased on the section(s), please answer the following question with yes, no, or n/a and provide a brief justification for your answer.\nQuestion: If the authors ran experiments, do the authors report error bars (e.g., with respect to the random seed after running experiments multiple times)?"}, "3d": {"role": "user", "content": "The following is the experiments section of the paper you are reviewing:\n\n\nIn this section, we illustrate the bene\ufb01ts of our methodology on several FL benchmarks associated to\nboth synthetic and real data. Since existing Bayesian FL approaches are not suited for personalisation\n(see Table 1), we only compare the performances of Algorithm 1 with personalised FL methods. In\nall our experiments, we use overdamped Langevin dynamics to sample locally and call this speci\ufb01c\ninstance of Algorithm 1, FedSOUL. In addition, we set p(z(i) | \u03b2) = N(\u00b5, \u03c32Id) with \u03b2 = {\u00b5, \u03c3} for\nsimplicity. To be comparable with existing personalised FL approaches that only consider periodic\ncommunication via multiple local steps, we do not resort to the proposed compression mechanism\nalthough the latter could be of interest for real-world applications. Additional experiments and details\nabout experimental design are provided in the supplement.\n\nSynthetic Data. We start by showcasing the bene\ufb01ts of FedSOUL for clients having small and highly\nheterogeneous data sets as pointed out in Section 1 and Section 2. To this end, we consider a similar\nexperimental setting as in Collins et al. (2021) where synthetic observations {y(i)\nj }j\u2208[Ni] \u2208 Di\n\n8\n\n\ftrue\u03c6(cid:62)\n\ntruex(i)\n\nj \u223c N(z(i)\n\nj \u223c N(0k, Ik) and y(i)\n\nare generated via the following procedure: x(i)\nj , 0.1).\nThe ground-truth parameters z(i)\ntrue \u2208 Rd and \u03c6true \u2208 Rk\u00d7d have been randomly generated\nbeforehand with (d, k) = (2, 20). Compared to Collins et al. (2021), we use heterogeneous\ndata partitions across clients so that 90% of the b = 100 clients have small data sets of size\n5 and the remaining 10% have data sets of size 10. We compare our results with FedRep\n(Collins et al., 2021) and FedAvg (McMahan et al., 2017) since they stand for two limiting in-\nstances of the proposed methodology, see Section 4 and Gelman and Hill (2007, Section 12).\nFigure 2 compares the different ap-\nproaches by computing the princi-\nple angle distance1 (respectively the\n(cid:96)2 norm) between \u03c6true (respectively\nz(i)\nthe\ntrue) and its estimated value;\nlesser the better. In contrast to its main\ncompetitors and based on both met-\nrics, FedSOUL provides an impressive\nimprovement. This illustrates the ben-\ne\ufb01ts of the introduction of a common\nprior p(z(i) | \u03b2) which allows to pre-\nvent from over\ufb01tting on clients with small data sets while performing personalisation. Additional\nresults with other choices for (b, d, k) and data partitioning strategies are available in the supplement.\n\nFigure 2: Small data sets - synthetic data.\n\nMoreover, to compare our algorithm with a non-FL setting, we perform a non-distributed and non-\nfederated stochastic approximation algorithm to \ufb01nd \u03b8\u2217 using a large number of iterations to get an\naccurate approximation of the optimal parameter \u03b8\u2217. Then, we use FedPop to obtain an estimate \u02dc\u03b8\u2217\nand measure the relative error in l2- distance between \u03b8\u2217 and \u02dc\u03b8\u2217. For some outer iterations T = 100,\nthe relative error was less than 10\u22123, which illustrates the relevance of our theoretical results. We\nalso test the performances of the proposed approach when the warm-start strategy is not used. In this\ncase, we have to set M = 50 to achieve the same performances as in the stateful variant of FedSOUL.\n\nReal Data. We consider now real image data sets, namely CIFAR-10 and CIFAR-100 (Krizhevsky,\n2009). For our likelihood model de\ufb01ned by p(Di | \u03c6, z(i)), we use 5-layer convolutional neural\nnetworks and perform personalisation for the last layer. We set b = 100 for convenience and control\ndata heterogeneity by assigning to each client Ni images belonging to only S different classes.\n\nSmall data sets. Under this setting,\nwe \ufb01rst consider (10%, 50%, 90%)\nof clients having small data sets of\nsize either Ni = 5 or Ni = 10;\nwhile remaining clients have larger\ndata sets of size Ni = 25. We com-\npare our approach with FedRep since\nit stands for the state-of-the-art person-\nalised FL approach. The algorithms\nare trained ful\ufb01lling the same com-\nputational budget. Figure 3 shows\nthe average accuracy across clients for\nthe two approaches on both CIFAR-\n10 and CIFAR-100. We can see that FedSOUL is consistently better than FedRep over different\ncon\ufb01gurations.\n\nFigure 3: (right) CIFAR-10 with S = 5 and (left) CIFAR-\n100 with S = 20. The x-axis refers to the percentage of\nclients having Ni \u2208 {5, 10} images.\n\nFull data sets. In addition to show that the proposed approach achieves state-of-the-art performances\non small data sets (which is common in the cross-device scenario), we now illustrate that FedSOUL\nis also competitive on larger data sets. To this end, we use all training images in CIFAR-10 and\nCIFAR-100 image data sets and consider the same data partitioning as in Collins et al. (2021). More\nprecisely, in this case the number of observations and the number of classes per client are uniformly\nshared over the clients. Table 2 shows our results in comparison with state-of-the-art personalised\nFL approaches. We can see that that our model outperforms other methods on both CIFAR-10\n\n1de\ufb01ned in (Collins et al., 2021, De\ufb01nition 1)\n\n9\n\nFedRepFedAvgFedSOUL05101520253035l2 norm of the estimation error010020030040050010\u22121100Principal Angle DistanceFedRepFedAvgFedSOUL1050900.00.10.20.30.4AccuracyFedSOUL,5FedRep,5FedSOUL,10FedRep,101050900.000.020.040.060.080.100.120.14AccuracyFedSOUL,5FedRep,5FedSOUL,10FedRep,10\fTable 2: Real data - Full data sets. Accuracy (in %) on test samples. FedAvg and SCAFFOLD are not\npersonalised FL approaches but stand for well-known FL benchmarks.\n\n(# clients b, # classes per client S)\n\n(100, 2)\n\n(100, 5)\n\n(100, 5)\n\n(100, 20)\n\nCIFAR-10\n\nCIFAR-100\n\nLocal learning only\n\nFedAvg (McMahan et al., 2017)\nSCAFFOLD (Karimireddy et al., 2020)\n\nLG-FedAvg (Liang et al., 2019)\nPer-FedAvg (Fallah et al., 2020)\nL2GD (Hanzely and Richt\u00e1rik, 2020)\nAPFL (Deng et al., 2021b)\nDITTO (Li et al., 2021)\nFedRep (Collins et al., 2021)\nFedAvg + fine-tuning (FT)\n\nFedSOUL (this paper)\n\n89.79\n\n42.65\n37.72\n\n84.14\n82.27\n81.04\n83.77\n85.39\n87.70\n85.63\n\n91.12\n\n70.68\n\n51.78\n47.33\n\n63.02\n67.20\n59.98\n72.29\n70.34\n75.68\n71.32\n\n79.48\n\n75.29\n\n23.94\n20.32\n\n72.44\n72.05\n72.13\n78.20\n78.91\n79.15\n79.03\n\n79.56\n\n41.29\n\n31.97\n22.52\n\n38.76\n52.49\n42.84\n55.44\n56.34\n56.10\n56.19\n\n59.73\n\nand CIFAR-100 by a large margin. Additional results with other personalised FL algorithms are\npostponed to the supplement.\n\nUncertainty Quanti\ufb01cation on Real Data. As highlighted in Table 1, one advantage of the\nproposed approach compared to existing personalised FL methods is the ability to perform uncertainty\nquanti\ufb01cation by sampling locally from the posterior p(z(i) | Di, \u03c6K, \u03b2K), see Algorithm 1. We\nillustrate this feature by computing on CIFAR-10 calibration curves and scores (e.g. expected\ncalibration error aka ECE) on a speci\ufb01c client; and by performing an out-of-distribution analysis\non MNIST/FashionMNIST data sets. Figure 4 shows that the proposed approach provides relevant\nuncertainty diagnosis. Additional results on uncertainty quanti\ufb01cation can be found in the supplement.\n\n higher diameter and value norm.\n\nThe following is the appendix_3 section of the paper you are reviewing:\n\n\nS3.1 Synthetic datasets\n\nIn this section, following the experiments from the main paper, we will show additional con\ufb01gurations\nof the toy example. We still use the same model (see Section 5 and Singhal et al. [47], Collins et al.\n[8]), but we choose different values of (d, k, b). First, let us test, how the total number of clients b\nimpacts the performances of the different approaches. Figure S1 and Figure S2 depict our results for\nb\n, with the size of the minimal dataset being 5 and the share of clients with the minimal\n}\ndataset 90%. We can see that in both cases, FedSOUL outperforms its competitors.\n\n50, 200\n\n2{\n\nSecond, we test, how the dimensionality of raw data impacts the result. Figure S3 and Figure S4\nshow our results with k\n\n. All others parameters are the same as before.\n}\n\nOne more experiment we conducted is the dependence on latent dimensionality d. We test two\noptions d = 2 (as in original experiments) and d = 5 in Figure S5 and Figure S6. Again, the more\n\n5, 50\n\n2{\n\n13\n\n\f\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure S5: Small data sets - synthetic data. Latent space dimensionality is d = 5.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure S6: Small data sets - synthetic data. Latent space dimensionality is d = 2.\n\ny\nc\na\nr\nu\nc\nc\nA\n\n0.7\n\n0.6\n\n0.5\n\n0.4\n\n0.3\n\n0.2\n\n0.1\n\n0.0\n\ny\nc\na\nr\nu\nc\nc\nA\n\n0.4\n\n0.3\n\n0.2\n\n0.1\n\n0.0\n\n10\n\n50\n\nFedSOUL\nFedRep\nAPFL\n\n90\n\n10\n\n50\n\nFedSOUL\nFedRep\nAPFL\n\n90\n\nFigure S7: Small image datasets. The minimal local dataset size is 2 (top) or 5 (bottom).\n\nparameters we have to learn (given the same small data budget), the better Bayesian methods (i.e.\nFedSOUL) are better.\n\nS3.2 Image datasets classi\ufb01cation\n\nIn this section, we provide an additional baseline for the experiments with personalization, in case\nwe have only a few heterogeneous data. Speci\ufb01cally, we consider APFL [13] which is another\npersonalized federated learning approach. We consider the CIFAR-10 dataset with 100 clients.\nAmong these clients, there are 10, 50, or 90 which have a local dataset of either 5 (one setup) or 10\n(another setup). Else of size 25.\n\nWe see in Figure S7 that FedSOUL typically performs better than FedRep, but on par with APFL. It\nis surprising, that APFL is a very good baseline in this type of problem, which it was not specially\ndesigned for.\n\nS3.3 Image datasets uncertainty quanti\ufb01cation\n\nIn this section, we provide additional experiments on image uncertainty with CIFAR-10 (in distribu-\ntion) and SVHN (out of distribution) datasets. As a measure of uncertainty, we will use predictive\nentropy. On \n\nBased on the section(s), please answer the following question with yes, no, or n/a and provide a brief justification for your answer.\nQuestion: If the authors ran experiments, do the authors include the total amount of compute and the type of resources used (e.g., type of GPUs, internal cluster, or cloud provider)?"}, "4a": {"role": "user", "content": "The following is the experiments section of the paper you are reviewing:\n\n\nIn this section, we illustrate the bene\ufb01ts of our methodology on several FL benchmarks associated to\nboth synthetic and real data. Since existing Bayesian FL approaches are not suited for personalisation\n(see Table 1), we only compare the performances of Algorithm 1 with personalised FL methods. In\nall our experiments, we use overdamped Langevin dynamics to sample locally and call this speci\ufb01c\ninstance of Algorithm 1, FedSOUL. In addition, we set p(z(i) | \u03b2) = N(\u00b5, \u03c32Id) with \u03b2 = {\u00b5, \u03c3} for\nsimplicity. To be comparable with existing personalised FL approaches that only consider periodic\ncommunication via multiple local steps, we do not resort to the proposed compression mechanism\nalthough the latter could be of interest for real-world applications. Additional experiments and details\nabout experimental design are provided in the supplement.\n\nSynthetic Data. We start by showcasing the bene\ufb01ts of FedSOUL for clients having small and highly\nheterogeneous data sets as pointed out in Section 1 and Section 2. To this end, we consider a similar\nexperimental setting as in Collins et al. (2021) where synthetic observations {y(i)\nj }j\u2208[Ni] \u2208 Di\n\n8\n\n\ftrue\u03c6(cid:62)\n\ntruex(i)\n\nj \u223c N(z(i)\n\nj \u223c N(0k, Ik) and y(i)\n\nare generated via the following procedure: x(i)\nj , 0.1).\nThe ground-truth parameters z(i)\ntrue \u2208 Rd and \u03c6true \u2208 Rk\u00d7d have been randomly generated\nbeforehand with (d, k) = (2, 20). Compared to Collins et al. (2021), we use heterogeneous\ndata partitions across clients so that 90% of the b = 100 clients have small data sets of size\n5 and the remaining 10% have data sets of size 10. We compare our results with FedRep\n(Collins et al., 2021) and FedAvg (McMahan et al., 2017) since they stand for two limiting in-\nstances of the proposed methodology, see Section 4 and Gelman and Hill (2007, Section 12).\nFigure 2 compares the different ap-\nproaches by computing the princi-\nple angle distance1 (respectively the\n(cid:96)2 norm) between \u03c6true (respectively\nz(i)\nthe\ntrue) and its estimated value;\nlesser the better. In contrast to its main\ncompetitors and based on both met-\nrics, FedSOUL provides an impressive\nimprovement. This illustrates the ben-\ne\ufb01ts of the introduction of a common\nprior p(z(i) | \u03b2) which allows to pre-\nvent from over\ufb01tting on clients with small data sets while performing personalisation. Additional\nresults with other choices for (b, d, k) and data partitioning strategies are available in the supplement.\n\nFigure 2: Small data sets - synthetic data.\n\nMoreover, to compare our algorithm with a non-FL setting, we perform a non-distributed and non-\nfederated stochastic approximation algorithm to \ufb01nd \u03b8\u2217 using a large number of iterations to get an\naccurate approximation of the optimal parameter \u03b8\u2217. Then, we use FedPop to obtain an estimate \u02dc\u03b8\u2217\nand measure the relative error in l2- distance between \u03b8\u2217 and \u02dc\u03b8\u2217. For some outer iterations T = 100,\nthe relative error was less than 10\u22123, which illustrates the relevance of our theoretical results. We\nalso test the performances of the proposed approach when the warm-start strategy is not used. In this\ncase, we have to set M = 50 to achieve the same performances as in the stateful variant of FedSOUL.\n\nReal Data. We consider now real image data sets, namely CIFAR-10 and CIFAR-100 (Krizhevsky,\n2009). For our likelihood model de\ufb01ned by p(Di | \u03c6, z(i)), we use 5-layer convolutional neural\nnetworks and perform personalisation for the last layer. We set b = 100 for convenience and control\ndata heterogeneity by assigning to each client Ni images belonging to only S different classes.\n\nSmall data sets. Under this setting,\nwe \ufb01rst consider (10%, 50%, 90%)\nof clients having small data sets of\nsize either Ni = 5 or Ni = 10;\nwhile remaining clients have larger\ndata sets of size Ni = 25. We com-\npare our approach with FedRep since\nit stands for the state-of-the-art person-\nalised FL approach. The algorithms\nare trained ful\ufb01lling the same com-\nputational budget. Figure 3 shows\nthe average accuracy across clients for\nthe two approaches on both CIFAR-\n10 and CIFAR-100. We can see that FedSOUL is consistently better than FedRep over different\ncon\ufb01gurations.\n\nFigure 3: (right) CIFAR-10 with S = 5 and (left) CIFAR-\n100 with S = 20. The x-axis refers to the percentage of\nclients having Ni \u2208 {5, 10} images.\n\nFull data sets. In addition to show that the proposed approach achieves state-of-the-art performances\non small data sets (which is common in the cross-device scenario), we now illustrate that FedSOUL\nis also competitive on larger data sets. To this end, we use all training images in CIFAR-10 and\nCIFAR-100 image data sets and consider the same data partitioning as in Collins et al. (2021). More\nprecisely, in this case the number of observations and the number of classes per client are uniformly\nshared over the clients. Table 2 shows our results in comparison with state-of-the-art personalised\nFL approaches. We can see that that our model outperforms other methods on both CIFAR-10\n\n1de\ufb01ned in (Collins et al., 2021, De\ufb01nition 1)\n\n9\n\nFedRepFedAvgFedSOUL05101520253035l2 norm of the estimation error010020030040050010\u22121100Principal Angle DistanceFedRepFedAvgFedSOUL1050900.00.10.20.30.4AccuracyFedSOUL,5FedRep,5FedSOUL,10FedRep,101050900.000.020.040.060.080.100.120.14AccuracyFedSOUL,5FedRep,5FedSOUL,10FedRep,10\fTable 2: Real data - Full data sets. Accuracy (in %) on test samples. FedAvg and SCAFFOLD are not\npersonalised FL approaches but stand for well-known FL benchmarks.\n\n(# clients b, # classes per client S)\n\n(100, 2)\n\n(100, 5)\n\n(100, 5)\n\n(100, 20)\n\nCIFAR-10\n\nCIFAR-100\n\nLocal learning only\n\nFedAvg (McMahan et al., 2017)\nSCAFFOLD (Karimireddy et al., 2020)\n\nLG-FedAvg (Liang et al., 2019)\nPer-FedAvg (Fallah et al., 2020)\nL2GD (Hanzely and Richt\u00e1rik, 2020)\nAPFL (Deng et al., 2021b)\nDITTO (Li et al., 2021)\nFedRep (Collins et al., 2021)\nFedAvg + fine-tuning (FT)\n\nFedSOUL (this paper)\n\n89.79\n\n42.65\n37.72\n\n84.14\n82.27\n81.04\n83.77\n85.39\n87.70\n85.63\n\n91.12\n\n70.68\n\n51.78\n47.33\n\n63.02\n67.20\n59.98\n72.29\n70.34\n75.68\n71.32\n\n79.48\n\n75.29\n\n23.94\n20.32\n\n72.44\n72.05\n72.13\n78.20\n78.91\n79.15\n79.03\n\n79.56\n\n41.29\n\n31.97\n22.52\n\n38.76\n52.49\n42.84\n55.44\n56.34\n56.10\n56.19\n\n59.73\n\nand CIFAR-100 by a large margin. Additional results with other personalised FL algorithms are\npostponed to the supplement.\n\nUncertainty Quanti\ufb01cation on Real Data. As highlighted in Table 1, one advantage of the\nproposed approach compared to existing personalised FL methods is the ability to perform uncertainty\nquanti\ufb01cation by sampling locally from the posterior p(z(i) | Di, \u03c6K, \u03b2K), see Algorithm 1. We\nillustrate this feature by computing on CIFAR-10 calibration curves and scores (e.g. expected\ncalibration error aka ECE) on a speci\ufb01c client; and by performing an out-of-distribution analysis\non MNIST/FashionMNIST data sets. Figure 4 shows that the proposed approach provides relevant\nuncertainty diagnosis. Additional results on uncertainty quanti\ufb01cation can be found in the supplement.\n\n higher diameter and value norm.\n\nBased on the section(s), please answer the following question with yes, no, or n/a and provide a brief justification for your answer.\nQuestion: If the authors use existing assets (e.g., code, data, models), do the authors cite the creators?"}}}