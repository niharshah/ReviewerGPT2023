{"paper_index": 8, "title": "Fast Bayesian Inference with Batch Bayesian Quadrature via Kernel Recombination", "abstract": "\n\nCalculation of Bayesian posteriors and model evidences typically requires numer-\nical integration. Bayesian quadrature (BQ), a surrogate-model-based approach to\nnumerical integration, is capable of superb sample efficiency, but its lack of paralleli-\nsation has hindered its practical applications. In this work, we propose a parallelised\n(batch) BQ method, employing techniques from kernel quadrature, that possesses\nan empirically exponential convergence rate. Additionally, just as with Nested Sam-\npling, our method permits simultaneous inference of both posteriors and model evi-\ndence. Samples from our BQ surrogate model are re-selected to give a sparse set of\nsamples, via a kernel recombination algorithm, requiring negligible additional time\nto increase the batch size. Empirically, we find that our approach significantly out-\nperforms the sampling efficiency of both state-of-the-art BQ techniques and Nested\nSampling in various real-world datasets, including lithium-ion battery analytics.2\n\n1\n\n", "introduction": "\n\nMany applications in science, engineering, and economics involve complex simulations to explain the\nstructure and dynamics of the process. Such models are derived from knowledge of the mechanisms\nand principles underlying the data-generating process, and are critical for scientific hypothesis-\nbuilding and testing. However, dozens of plausible simulators describing the same phenomena often\nexist, owing to differing assumptions or levels of approximation. Similar situations can be found in\nselection of simulator-based control models, selection of machine learning models on large-scale\ndatasets, and in many data assimilation applications [28].\n\nIn such settings, with multiple competing models, choosing the best model for the dataset is crucial.\nBayesian model evidence gives a clear criteria for such model selection. However, computing\nmodel evidence requires integration over the likelihood, which is challenging, particularly when\nthe likelihood is non-closed-form and/or expensive. The ascertained model is often applied to\n\n\u2217Equal contribution\n2Code: https://github.com/ma921/BASQ\n\n36th Conference on Neural Information Processing Systems (NeurIPS 2022).\n\n\fproduce posteriors for prediction and parameter estimation afterwards. There are many algorithms\nspecialised for the calculation of model evidences or posteriors, although only a limited number of\nBayesian inference solvers estimate both model evidence and posteriors in one go. As such, costly\ncomputations are often repeated (at least) twice. Addressing this concern, nested sampling (NS)\n[71, 46] was developed to estimate both model evidence and posteriors simultaneously, and has been\nbroadly applied, especially amongst astrophysicists for cosmological model selection [63]. However,\nNS is based on a Monte Carlo (MC) sampler, and its slow convergence rate is a practical hindrance.\n\nTo aid NS, and other approaches, parallel computing is widely applied to improve the speed of\nwall-clock computation. Modern computer clusters and graphical processing units enable scientists\nto query the likelihood in large batches. However, parallelisation can, at best, linearly accelerate NS,\ndoing little to counter NS\u2019s inherently slow convergence rate as a MC sampler.\n\nThis paper investigates batch Bayesian quadrature (BQ) [65] for fast Bayesian inference. BQ solves\nthe integral as an inference problem, modelling the likelihood function with a probabilistic model\n(typically a Gaussian process (GP)). Gunter et al. [37] proposed Warped sequential active Bayesian\nintegration (WSABI), which adopts active learning to select samples upon uncertainty over the\nintegrand. WSABI showed that BQ with expensive GP calculations could achieve faster convergence\nin wall time than cheap MC samplers. Wagstaff et al. [78] introduced batch WSABI, achieving even\nfaster calculation via parallel computing and became the fastest BQ model to date. We improve upon\nthese existing works for a large-scale batch case.\n\n", "methods": "\n\nWe analysed the convergence over single iteration on a simplified version of BASQ, which assumes\nthe BQ is modelled with vanilla BQ, without batch and hyperparameter updates. Note that the kernel\nK on Rd in this section refers to the given covariance kernel of GP at each step. We discuss the\nconvergence of BASQ in one iteration. We consider the importance sampling: Let f be a probability\ndensity on Rd and g be another density such that f = \u03bbg with a nonnegative function \u03bb. Let us call\nsuch a pair, (f, g), a density pair with weight \u03bb.\nWe approximate the kernel K with K0 = (cid:80)n\u22121\ni=1 ci\u03c6i(x)\u03c6i(y). In general, we can apply the ker-\nnel recombination algorithm [59, 75] with the weighted sample (wrec, Xrec) to obtain a weighted\npoint set (wquad, Xquad) of size n satisfying w\u22a4\nquad\u03c6i(Xquad) = w\u22a4\nrec\u03c6i(Xrec) (i = 1, . . . , n \u2212 1)\nand w\u22a4\nrec1. By modifying the kernel recombination algorithm, we can require\n(x) := (cid:112)K(x, x) \u2212 K0(x, x) [44]. We call such\nquadk1/2\nw\u22a4\n(wquad, Xquad) a proper kernel recombination of (wrec, Xrec) with K0.11 We have the following\nguarantee (proved in Supplementary):\nTheorem 1. Suppose (cid:82) (cid:112)K(x, x)f (x) dx < \u221e, \u2113 \u223c GP(m, K), and we are given an (n \u2212 1)-\ndimensional kernel K0 such that K1 := K \u2212 K0 is also a kernel. Let (f, g) be a density pair\nwith weight \u03bb. Let Xrec be an N -point independent sample from g and wrec := \u03bb(Xrec). Then, if\n(wquad, Xquad) is a proper kernel recombination of (wrec, Xrec) for K0, it satisfies\n\n(Xrec), where k1/2\n\nquad1 = w\u22a4\n\n(Xquad) \u2264 w\u22a4\n\nreck1/2\n\n1\n\n1\n\n1\n\n(cid:20)(cid:113)\n\n(cid:21)\nvar[Zf | xquad]\n\nExrec\n\n\u2264 2\n\n(cid:18)(cid:90)\n\nK1(x, x)f (x) dx\n\n+\n\n(cid:19)1/2\n\n(cid:114)\n\nCK,f,g\nN\n\n,\n\n(12)\n\nwhere Zf := (cid:82) \u2113(x)f (x) dx and CK,f,g := (cid:82) K(x, x)\u03bb(x)f (x) dx \u2212 (cid:82)(cid:82) K(x, y)f (x)f (y) dx dy.\n\nThe above approximation has one source of randomness which stems from sampling N points xrec\nfrom g. One can also apply this estimate with a random kernel and thereby introduce another source\nof randomness. In particular, when we use the Nystr\u00f6m approximation for K0 (that ensures K1 is a\nkernel [44]), then one can show that (cid:82) K1(x, x)f (x) dx can be bounded by\n(cid:18) nKmax\u221a\nM\n\nK1(x, x)f (x) dx \u2264 n\u03c3n +\n\n\u03c3m + Op\n\n\u221e\n(cid:88)\n\n(13)\n\n(cid:19)\n\n(cid:90)\n\n,\n\nm=n+1\n\nwhere \u03c3n is the n-th eigenvalue of the integral operator L2(f ) \u220b h (cid:55)\u2192 (cid:82) K(\u00b7, y)h(y)f (y) dx,\nKmax := supx K(x, x). However, note that unlike Eq. (12), this inequality only applies with high\nprobability due to the randomness of K0; see Supplementary for details.\n\n11Note that the inequality constraint on the diagonal value here is only needed for theoretical guarantee, and\n\nskipping it does not reduce the empirical performance [44].\n\n9\n\nLog KL dinvergenceLog MAE of integralBranin-Hoo FunctionAckley FunctionOscillatory Functionwall time (s)wall time (s)wall time (s)Log RMSE of posteriorNegative Log evidencewall time (s)wall time (s)Battery simulatorPhase-field modelBASQ-IVRBASQ-IGBBASQ-UBbatch WSABINS (MultiNest)NS (dynamic)NS (MLFriends)Hierarchical GPwall time (s)90502041.502250450050201.41.90225045001020.40.6022504500\fIf, for example, K is a Gaussian kernel on Rd and f is a Gaussian distribution, we have \u03c3n =\nO(exp(\u2212cn1/d)) for some constant c > 0 (see Supplementary). So in (12) we also achieve an\nempirically exponential rate when N \u226b CK,f,g . RCHQ works well with a moderate M in practise.\nNote that unlike the previous analysis [50], we do not have to assume that the space is compact. 12\n\n", "experiments": "\n\nGiven our new model BASQ, with three variants of the proposal distribution, IVR, IGB, and UB, we\nnow test for speed against MC samplers and batch WSABI. We compared with three NS methods\n[71, 29, 46, 14, 15], coded with [72, 17]. According to the review [16], MLFriends is the state-of-the-\nart NS sampler to date. The code is implemented based on [77, 34, 42, 76, 38, 31, 10, 7], and code\naround kernel recombination [24, 44] with additional modification. All experiments on synthetic\ndatasets were averaged over 10 repeats, computed in parallel with multicore CPUs, without GPU for\nfair comparison.8 The posterior distribution of NS was estimated via KDE with weighted samples\n[33]. For maximum speed performance, batch size was optimised for each method in each dataset,\nin fairness to the competitors. Batch WSABI needs to optimise batch size to balance the likelihood\nquery cost and sampling cost, because sampling cost increases rapidly with batch size, as shown\nin Figure 1(a). Therefore, it has an optimal batch size for faster convergence. By wall time cost,\nwe exclude the cost of integrand evaluation; that is, the wall time cost is the overhead cost of batch\nevaluation. Details can be found in the Supplementary.\n\n7Negative elements in the matrices only exist in K(X, X)\u22121, which can be drawn from the memory of the\nGP regression model without additional calculation. The number of positive components is half of the matrix on\naverage, resulting in O(n2/2). Then, taking the threshold via the inverse of the recombination sample size N ,\nthe number of components becomes ncomp \u226a n2, resulting in sampling complexity O(n2/2 + ncompN ).\n\n8Performed on MacBook Pro 2019, 2.4 GHz 8-Core Intel Core i9, 64 GB 2667 MHz DDR4\n\n7\n\n\f5.1 Synthetic problems\n\nWe evaluate all methods on three synthetic problems. The goal is to estimate the integral and posterior\nof the likelihood modelled with the highly multimodal functions. Prior was set to a two-dimensional\nmultivariate normal distribution, with a zero mean vector, and covariance whose diagonal elements\nare 2. The optimised batch sizes for each methods are BASQ: 100, batch WSABI: 16. The synthetic\nlikelihood functions are cheap (0.5 ms on average). This is advantageous setting for NS: Within 10\nseconds, the batch WSABI, BASQ, and NS collected 32, 600, 23225 samples, respectively. As for\nthe metrics, posterior estimation was tested with Kullback-Leibler (KL) upon random 10,000 samples\nfrom true posterior. Evidence was evaluated with MAE, and ground truth was derived analytically.\n\nLikelihood functions Branin-Hoo [49] is 8 modal function in two-dimensional space. Ackley [73]\nis a highly multimodal function with point symmetric periodical peaks in two-dimensional space.\nOscillatory function [32] is a highly multimodal function with reflection symmetric periodical peaks\nof highly-correlated ellipsoids in two-dimensional space.\n\n5.2 Real-world dataset\n\nWe consider three real-world applications with expensive likelihoods, which are simulator-based\nand hierarchical GP. We adopted the empirical metric due to no ground truth. For the posterior, we\ncan calculate the true conditional posterior distribution along the line passing through ground truth\nparameter points. Then, evaluate the posterior with root mean squared error (RMSE) against 50\ntest samples for each dimension. For integral, we compare the model evidence itself. Expensive\nlikelihoods makes the sample size per wall time amongst the methods no significant difference,\nwhereas rejection sampling based NS dismiss more than 50% of queried samples. The batch sizes are\nBASQ: 32, batch WSABI: 8. (see Supplementary)\n\n: The simulator is the SPMe [61], 9\nParameter estimation of the lithium-ion battery simulator\nestimating 3 simulation parameters at a given time-series voltage-current signal (the diffusivity of\nlithium-ion on the anode and cathode, and the experimental noise variance). Prior is modified to log\nmultivariate normal distribution from [4]. Each query takes 1.2 seconds on average.\n\nParameter estimation of the phase-field model\n: The simulator is the phase-field model [61],\n10 estimating 4 simulation parameters at given time-series two-dimensional morphological image\n(temperature, interaction parameter, Bohr magneton coefficient, and gradient energy coefficient).\nPrior is a log multivariate normal distribution. Each query takes 7.4 seconds on average.\n\nHyperparameter marginalisation of hierarchical GP model The hierarchical GP model was\ndesigned for analysing the large-scale battery time-series dataset from solar off-grid system field data\n[3].8 For fast estimation of parameters in each GP, the recursive technique [74] is adopted. The task\nis to marginalise 5 GP hyperparameters at given hyperprior, which is modified to log multivariate\nnormal distribution from [3]. Each query takes 1.1 seconds on average.\n\n5.3 Results\n\nWe find BASQ consistently delivers strong empirical performance, as shown in Figure 2. On all\nbenchmark problems, BASQ-IVR, IGB, or UB outperform baseline methods except in the battery\nsimulator evidence estimation. The very low-dimensional and sharp unimodal nature of this likelihood\ncould be advantageous for biased greedy batch WSABI, as IGB superiority supports this viewpoint.\nThis suggests that BASQ could be a generally fast Bayesian solver as far as we investigated. In the\nmultimodal setting of the synthetic dataset, BASQ-UB outperforms, whereas IVR does in a simulator-\nbased likelihoods. When comparing each proposal distribution, BASQ-IVR was the performant. Our\nresults support the general use of IVR, or UB if the likelihood is known to be highly multimodal.\n\n9SPMe code used was translated into Python from MATLAB [11, 12]. This open-source code is published\n\nunder the BSD 3-clause Licence. See more information on [11]\n\n10Code used was from [54, 3]. All rights of the code are reserved by the authors. Thus, we do not redistribute\n\nthe original code.\n\n8\n\n\fFigure 2: Time in seconds vs. KL divergence for posterior, and MAE for evidence in the synthetic\ndatasets. Time in seconds vs. RMSE for posterior and evidence itself in the real-world dataset.\n\n", "conclusion": "\n\nWe introduced a batch BQ approach, BASQ, capable of simultaneous calculation of both model\nevidence and posteriors. BASQ demonstrated faster convergence (in wall-clock time) on both\nsynthetic and real-world datasets, when compared against existing BQ approaches and state-of-the-art\nNS. Further, mathematical analysis shows the possibility to converge exponentially-fast under natural\nassumptions. As the BASQ framework is general-purpose, this can be applied to other active learning\nGP-based applications, such as Bayesian optimisation [52], dynamic optimisation like control [26],\nand probabilistic numerics like ODE solvers [45]. Although it scales to the number of data seen in\nlarge-scale GP experiments, practical BASQ usage is limited to fewer than 16 dimensions (similar to\nmany GP-based algorithms). However, RCHQ is agnostic to the input space, allowing quadrature in\nmanifold space. An appropriate latent variable warped GP modelling, such as GPLVM [58], could\npave the way to high dimensional quadrature in future work. In addition, while WSABI modelling\nlimits the kernel to a squared exponential kernel, RCHQ allows to adopt other kernels or priors\nwithout a bespoke modelling BQ models. (See Supplementary). As for the mathematical proof, we\ndo not incorporate batch and hyperparameter updates, which should be addressed in future work. The\ngenerality of our theoretical guarantee with respect to kernel and distribution should be useful for\nextending the analysis to the whole algorithm.\n\n", "appendix": "Supplementary: Fast Bayesian Inference with Batch\nBayesian Quadrature via Kernel Recombination\n\n1 Convergence analysis\n\n1.1 Proof of Theorem 1\n\nWe provide the proof of the following theorem given in the main text.\nTheorem 1. Suppose  K(x, x)f (x) dx < \u221e, \u2113 \u223c GP(m, K), and we are given an (n \u2212 1)-\ndimensional kernel K0 such that K1 := K \u2212 K0 is also a kernel. Let (f, g) be a density pair\nwith weight \u03bb. Let xrec be an N -point independent sample from g and wrec := \u03bb(xrec). Then, if\n(wquad, xquad) is a proper recombination of (wrec, xrec) for K0, it satisfies\n\n\n\nExrec\n\n\n\nvar[Zf | xquad]\n\n\u2264 2\n\n\n\nK1(x, x)f (x) dx\n\n+\n\n1/2\n\n\n\nCK,f,g\nN\n\n(1)\n\nwhere Zf :=  \u2113(x)f (x) dx and CK,f,g :=  K(x, x)\u03bb(x)f (x) dx \u2212  K(x, y)f (x)f (y) dx dy.\nRecall H is the RKHS given by the kernel K. As the kernel satisfies  K(x, x)f (x) dx < \u221e, the\nmean embedding\n\n\n\n\u00b5K(f ) :=\n\nf (x)K(x, \u00b7) dx\n\n(2)\n\nis a well-defined element of H. We first discuss its approximation via importance sampling.\nLemma 1. Let f be a probability density on Rd and g be another density such that f = \u03bbg with a\nnonnegative function \u03bb. Let xrec be an N -point independent sample from g and wrec = \u03bb(xrec) be the\nweights. If we define \u00b5r := 1\n\nN w\u22a4\n\nrecK(xrec, \u00b7) then it satisfies\nE[\u2225\u00b5K(f ) \u2212 \u00b5r\u22252\nH] = 1\n\nN CK,f,g\n\nwhere CK,f,g =  K(x, x)\u03bb(x)f (x) dx \u2212  K(x, y)f (x)f (y) dx dy\nFurthermore, the choice g(x) \u221d K(x, x)f (x) minimises CK,f,g, if \u03bb = K(x, x)\u22121/2 is well-\ndefined.\n\nProof. Let xrec = (X1, . . . , XN ), so \u00b5r = 1\nN\n\nN\n\ni=1 \u03bb(Xi)K(Xi, \u00b7). From (2), we have\n\n\u2225\u00b5K(f ) \u2212 \u00b5r\u22252\n\nH = \u2225\u00b5K(f )\u22252\n \n\nH \u2212 2\u27e8\u00b5K(f ), \u00b5r\u27e9H + \u2225\u00b5r\u22252\nH\nN\n\n\n\nK(x, y)f (x)f (y) dx dy \u2212\n\n=\n\n(4)\n\nK(x, Xi)f (x)\u03bb(Xi) dx\n\n(5)\n\n2\nN\n\ni=1\n\n+\n\n1\nN 2\n\nN\n\n\ni,j=1\n\nK(Xi, Xj)\u03bb(Xi)\u03bb(Xj).\n\n(6)\n\nWe have\n\n\nE\n\nK(x, Xi)f (x)\u03bb(Xi) dx\n\n \n\n\n\n=\n\nK(x, y)f (x)\u03bb(y)g(y) dx dy =\n\nand for i \u0338= j\n\nE[K(Xi, Xj)\u03bb(Xi)\u03bb(Xj)] =\n\n \n\nK(x, y)\u03bb(x)\u03bb(y)g(x)g(y) dx dy =\n\n1\n\n \n\n \n\nK(x, y)f (x)f (y) dx dy\n\n(7)\n\nK(x, y)f (x)f (y) dx dy,\n\n(8)\n\n\fso we in total have\n\nE[\u2225\u00b5K(f ) \u2212 \u00b5r\u22252\n\nH] =\n\n1\nN 2\n\nN\n\n\ni=1\n\n\n=\n\n1\nN\n\nE[K(Xi, Xi)\u03bb(Xi)2] \u2212\n\nK(x, x)\u03bb(x)f (x) dx \u2212\n\n1\nN\n \n\n \n\nK(x, y)f (x)f (y) dx dy\n\n(9)\n\nK(x, y)f (x)f (y) dx dy\n\n=\n\n\n\nCK,f,g\nN\n\n.\n\n(10)\n\nWe next show the optimality of g(x) \u2248 K(x, x)f (x).\nIt suffices to consider when\n K(x, x)\u03bb(x)f (x) dx is minimised as the second term is independent of g. From the Cauchy-\nSchwarz, we have\n\n\n\nK(x, x)\u03bb(x)f (x) dx =\n\n\n\nK(x, x)\u03bb(x)f (x) dx\n\n f (x)\n\u03bb(x)\n\n K(x, x)f (x) dx\n\n2\n\n,\n\ndx \u2265\n\nand the equality is satisfied if g(x) = f (x)\n\n\u03bb(x) \u221d K(x, x)f (x).\n\n(11)\n\nProof of Theorem 1. Let (wquad, xquad) be a proper recombination of (wrec, xrec), and let Qn be the\nquadrature formula given by points xquad and weights 1\nquadh(xquad). We\nalso define \u00b5n := 1\nquadK(xquad, \u00b7).\n\nN wquad, i.e, Q(h) := 1\n\nN w\u22a4\n\nN w\u22a4\n\nA well-known fact is that the worst-case error of Qn (with respect to f here) wce(Qn) =\nsup\u2225h\u2225\u22641|Qn(h) \u2212  h(x)f (x) dx| satisfies wce(Qn) = \u2225\u00b5K(f ) \u2212 \u00b5n\u2225H for a kernel satisfying\n K(x, x)f (x) dx < \u221e [9, 18]. By using this and the relation between Bayesian quadrature and\nkernel quadrature in the main text, we have\n\n\n\nvar[Zf | xquad] \u2264 wce(Qn) \u2264 \u2225\u00b5K(f ) \u2212 \u00b5r\u2225H + \u2225\u00b5r \u2212 \u00b5n\u2225H\n\nFrom Lemma 1 we have E[\u2225\u00b5K(f ) \u2212 \u00b5r\u2225H] \u2264 E[\u2225\u00b5K(f ) \u2212 \u00b5r\u22252\nsuffices to show\n\nExrec[\u2225\u00b5r \u2212 \u00b5n\u2225H] \u2264 2\n\nK1(x, x)f (x) dx\n\n.\n\n(13)\n\n\n\n1/2\n\n(12)\nH]1/2 = CK,f,g/N , so it now\n\nWe first have\n\n\u2225\u00b5r \u2212 \u00b5n\u22252\n\nH =\n\n1\nN 2\n\nw\u22a4\n\nrecK(xrec, xrec)wrec \u2212 2 w\u22a4\n\nrecK(xrec, xquad)wquad + w\u22a4\n\n ,\nquadK(xquad, xquad)wquad\n(14)\n\nand from the recombination property we also have\n\nw\u22a4\n\nrecK0(xrec, xrec)wrec \u2212 2 w\u22a4\n\nrecK0(xrec, xquad)wquad + w\u22a4\n\nquadK0(xquad, xquad)wquad = 0,\n\n(15)\n\nwhich follows from the fact that (wrec, xrec) and (wquad, xquad) give the same kernel embedding for\nthe RKHS given by K0 as the latter is a recombination of the former (see e.g. [11, Eq. 14]). By\nsubtracting, we obtain\n\n\u2225\u00b5r \u2212 \u00b5n\u22252\nH\n1\nw\u22a4\nN 2\n= \u2225\u00b5(1)\n\n=\n\nr \u2212 \u00b5(1)\n\nn \u22252\nH1\nwhere H1 is the RKHS given by K1 and\n\n,\n\nrecK1(xrec, xrec)wrec \u2212 2 w\u22a4\n\nrecK1(xrec, xquad)wquad + w\u22a4\n\nquadK1(xquad, xquad)wquad\n\n\u00b5(1)\nr\n\n:=\n\n1\nN\n\nw\u22a4\n\nrecK1(xrec, \u00b7),\n\n\u00b5(1)\n\nn :=\n\n1\nN\n\nw\u22a4\n\nquadK1(xquad, \u00b7).\n\n\n\n(16)\n\n(17)\n\n(18)\n\n(19)\n\nNow, by letting k1/2\n\n(x) := K(x, x), we have \u2225K1(x, \u00b7)\u2225H1 = k1/2\n\n1\n\n(x) for a point x. So we have\n\n\u2225\u00b5(1)\n\nr \u2225H1 \u2264\n\n1\n1\nN\n\nreck1/2\nw\u22a4\n\n1\n\n(xrec),\n\n\u2225\u00b5(1)\n\nn \u2225H1 \u2264\n\n1\nN\n\nquadk1/2\nw\u22a4\n\n1\n\n(xquad) \u2264\n\n1\nN\n\nreck1/2\nw\u22a4\n\n1\n\n(xrec),\n\n(20)\n\n2\n\n\fwhere the last inequality follows from the assumption that (wquad, xquad) is a proper recombination of\n(wrec, xrec). Therefore, we have the estimate\n\n\u2225\u00b5r \u2212 \u00b5n\u2225H = \u2225\u00b5(1)\n\nr \u2212 \u00b5(1)\n\nn \u2225H1 \u2264 \u2225\u00b5(1)\n\nr \u2225H1 + \u2225\u00b5(1)\n\nn \u2225H1 \u2264\n\n2\nN\n\nreck1/2\nw\u22a4\n\n1\n\n(xrec).\n\n(21)\n\nFinally, to prove (13), we recall that xrec is an N -point independent sample from g and wrec = \u03bb(xrec),\nso we obtain\n\nExrec [\u2225\u00b5r \u2212 \u00b5n\u2225H] \u2264 2 Exrec\n\n 1\nN\n\n\n(xrec)\n\n1\n\nreck1/2\nw\u22a4\n K1(x, x)f (x) dx \u2264 2\n\n= 2\n\n= 2\n\n\n\n\u03bb(x)k1/2\n\n1\n\n(x)g(x) dx\n\n(22)\n\n\n\nK1(x, x)f (x) dx\n\n,\n\n(23)\n\n1/2\n\nwhere we have used Cauchy\u2013Schwarz in the last inequality.\n\n1.2 Eigenvalue dacay of integral operators\n\nLet us consider the integral operator\n\n\n\nh \u2192\n\nK(\u00b7, y)h(y)f (y) dy\n\n(24)\n\nL2(f ) :=  \u02dch(x)2f (x) dx < \u221e}, and let \u03c31 \u2265 \u03c32 \u2265\nwhere h \u2208 L2(f ) := {\u02dch | measurable, \u2225\u02dch\u22252\n\u00b7 \u00b7 \u00b7 \u2265 0 be eigenvalues of this operator. This sequence of eigenvalues is known to be closely related\nto the convergence rate of kernel quadrature [3].\n\nFor the Nystr\u00f6m approximation, we have the following estimate represented by the eigenvalues:\nTheorem 2 ([11]). For a probability density function f on Rd, let xnys be an M -point independent\nsample from f . Let K0 be the rank-(n \u2212 1) approximate kernel using xnys given by Eq. (10) in the\nmain text. Then, K1 := K \u2212 K0 satisfies\n\n\n\nK1(x, x)f (x) dx \u2264 n\u03c3n +\n\n\u221e\n\n\nm=n+1\n\n\u03c3n +\n\n2(n \u2212 1)Kmax\n\u221a\nM\n\n\n\n\n\n1 +\n\n2 log\n\n\n\n1\n\u03b4\n\n(25)\n\nwith probability at least 1 \u2212 \u03b4.\n\nThis gives a theoretical guarantee for one step of our algorithm, combined with Theorem 1.\n\nAlthough the sequence of eigenvalues \u03c3n does not have an obvious expression when K is the kernel\nin the middle of our algorithms BASQ, when K is a multivariate Gaussian (RBF) kernel and f is also\na Gaussian density, we have a concrete expression of eigenvalues [8].\nIndeed, if K(x, y) = exp(\u2212\u03f52|x \u2212 y|2) and f (x) \u221d exp(\u2212\u03b12|x|2), in the case d = 1, we have\n\u03c3n = abn for some constants a > 0 and 0 < b < 1 depending on \u03f5 and \u03b1. Thus, for the d-dimensional\ncase, we can roughly estimate that \u03c3n \u2264 adbm+1 if n > md. So, by only using n, we have\n\n\u03c3n \u2264 adb\u2308n1/d\u2309 \u2264 adb(n1/d) = ad exp(\u2212cn1/d),\n\n(26)\n\nfor c = log(1/b).\n\n2 Model Analysis\n\n2.1 Ablation study\n\n2.1.1 Ablation study of sampling methods\n\nWe investigated the influence of each component using 10-dimensional Gaussian mixture likelihood.\nThe performance is evaluated by taking the mean and standard deviation of five metrics when each\nmodel gathered 1,000 observations with n = 100 batch size. LogMAE is the natural logarithmic\nMAE between the estimated integral value and true one, and the logKL is the natural logarithmic of\nthe KL divergence between the estimated posterior and true one. Wall time is the overhead time until\n\n3\n\n\fTable 1: Ablation study of sampling methods\n\nSampling\n\nProp. dist.\n\nAlternate update\n\nPerformance metric\n\nUnc.\n\nFactor.\n\nLinear\n\nOptimal\n\nKernel\n\nsampl.\n\ntrick\n\nIVR\n\nIVR\n\nUpdate\n\nRCHQ logMAE\n\nlogKL\n\nwall\n\nlogMAE\n\nlogKL\n\ntime (s)\n\nper time\n\nper time\n\n\u2714\n\n\u2714\n\n\u2714\n\n\u2714\n\n\u2714\n\n\u2714\n\n\u2714\n\n\u2714\n\n\u2714\n\n\u2714\n\n\u2714\n\n\u2714\n\n\u2714\n\n\u2714\n\n\u2714\n\n\u2714\n\n-1.8750\n\n-8.1366\n\n407.42\n\n-0.0046\n\n-0.0200\n\n\u00b10.0435\n\n\u00b10.2049\n\n\u00b110.139\n\n\u00b10.0002\n\n\u00b10.0010\n\n-3.3310\n\n-9.5934\n\n50.065\n\n-0.0702\n\n-0.1976\n\n\u00b10.5265\n\n\u00b10.1697\n\n\u00b18.3153\n\n\u00b10.0222\n\n\u00b10.0362\n\n-3.6743\n\n-9.6108\n\n367.63\n\n-0.0100\n\n-0.0263\n\n\u00b10.0449\n\n\u00b10.1363\n\n\u00b126.565\n\n\u00b10.0008\n\n\u00b10.0023\n\n-1.5936\n\n-7.9967\n\n47.499\n\n-0.0346\n\n-0.1735\n\n\u00b10.0016\n\n\u00b10.0025\n\n\u00b18.1966\n\n\u00b10.0060\n\n\u00b10.0300\n\n-3.4379\n\n-9.7877\n\n810.45\n\n-0.0042\n\n-0.0121\n\n\u00b10.2345\n\n\u00b10.4589\n\n\u00b114.468\n\n\u00b10.0003\n\n\u00b10.0008\n\n-4.0138\n\n-9.6222\n\n48.75\n\n-0.0848\n\n-0.2038\n\n\u00b10.0078\n\n\u00b10.17147\n\n\u00b18.2176\n\n\u00b10.0144\n\n\u00b10.0379\n\n\u2714\n\n\u2714\n\n\u2714\n\n\u2714\n\n\u2714\n\ngathering 1,000 observations in seconds. LogMAE per time refers to the value that logMAE divided\nby the wall time. LogKL per time is the same.\n\nUncertainty sampling (Unc. sampl.) and factorisation trick (Factor. trick) refers to the technique\nexplained in the section 4.2 in the main paper. Linearised IVR proposal distribution (Linear IVR)\nis the ones with Equations (8)-(10) in the main paper, whereas the optimal IVR is the square-root\nkernel IVR g(x) =\ny (x, x)\u03c0(x) derived from the Lemma 1. Kernel update refers to the type-II\nMLE to optimise the hyperparameters. RCHQ means whether or not to adopt RCHQ, if not, it means\nmulti-start optimisation (that is, the same with batch WSABI.)\n\nC L\n\n\n\n\n\nSampling from optimal IVR of the square root is intractable, so we adopted the SMC sampling\nscheme. Firstly, supersamples Xsuper are generated from prior \u03c0(x), then we calculate the weights\nC L\ny (Xi, Xi)\u03c0(xi)/\u03c0(xi) =\nwi =\n,\ni wi\nwhere \ni wn\ni = 1. At last, we resample subsamples Xquad from the supersamples Xsuper with the\nweights wi. By removing the identical samples from Xquad, we can construct Xquad. This removal\nreduces the size of subsamples to approximately 100 times smaller, so we need to supersample at\nleast 100 times larger than the size of the subsample Xquad. As the size of subsamples is already large,\nthis SMC procedure is computationally demanding.\n\ny (Xi, Xi), then we normalise them via wn\ni\n\n:= wi\n\n\nC L\n\n\n\nAll components were examined by removing each. The ablation study of the sampling scheme\nin table 1 shows that all components are essential for reducing overhead or faster convergence.\nAlternate update and uncertainty sampling contribute to the fast convergence, and factorisation trick\nand linearised proposal distribution reduce overhead with a negligible effect on convergence.\n\n2.1.2 Ablation study of BQ modelling\n\nWe investigated BQ modelling influence with the same procedure of the ablation study in the previous\nsection. The compared models are WSABI-L, WSABI-M, Vanilla BQ (VBA), and log-warp BQ\n(BBQ). For the details of VBQ and BBQ modelling, see sections 3.3 and 3.4. With regard to the\nWSABI-M modelling, it has a disadvantageous formula in the mean posterior predictive distribution\n\n4\n\n\fTable 2: Ablation study of BQ modelling\n\nBQ modelling\n\nPerformance metric\n\nWSABI\n\nWSABI\n\nVBQ\n\nBBQ\n\nlogMAE\n\nlogKL\n\nwall\n\nlogMAE\n\nlogKL\n\n-L\n\n\u2714\n\n-M\n\n(no warp)\n\n(log warp)\n\ntime (s)\n\nper time\n\nper time\n\n\u2714\n\n\u2714\n\n-4.0138\n\n-9.6222\n\n48.75\n\n-0.0848\n\n-0.2038\n\n\u00b10.0078\n\n\u00b10.1715\n\n8.2176\n\n0.0144\n\n0.0379\n\n-4.0418\n\n-10.083\n\n814.13\n\n-0.0050\n\n-0.0123\n\n0.0532\n\n0.2088\n\n\u00b119.813\n\n\u00b10.0002\n\n\u00b10.0006\n\n-2.5842\n\n12.307\n\n50.901\n\n0.0534\n\n0.2954\n\n\u00b10.9978\n\n\u00b10.0158\n\n\u00b15.3634\n\n\u00b10.0252\n\n\u00b10.0254\n\n\u2714\n\n-3.1278\n\n9.0425\n\n54.092\n\n0.0617\n\n-0.2038\n\n\u00b11.7428\n\n\u00b10.3634\n\n\u00b15.4892\n\n\u00b10.0285\n\n\u00b10.0379\n\nmL\n\ny (x). The acquisition function is expressed as:\n\nmL\n\ny (x) := \u03b1 +\n\n1\n2\n\nmy(x)2 + Cy(x, x) .\n\nThen, the expectation of the WSABI-M is no more single term;\n\nE[mL\n\ny (x)] := \u03b1 +\n\n1\n2\n\nE[my(x)2] +\n\n1\n2\n\nE[Cy(x, x)]\n\n(27)\n\n(28)\n\nAs such, we cannot apply the factorisation trick for speed. Thus, we should adopt the SMC sampling\nscheme to sample from WSABI-M as the same procedure with the square root kernel IVR (Optimal\nIVR) explained in the previous section. This significantly slows down the computation with WSABI-\nM.\n\nThe ablation study result is shown in table 2. While WSABI-M achieves slightly better accuracy than\nWSABI-L, it also records the slowest computation. The WSABI-M intractable expression hinders to\napply the quick sampling schemes we adopted. Vanilla BQ and BBQ (log warped BQ) shows larger\nerrors. Therefore, the WSABI-L adoption is reasonable in this setting.\n\n2.1.3 Ablation study of kernel modelling\n\nWe investigated kernel modelling influence with the same procedure of the ablation study in the\nprevious section. The compared kernels are RBF (Radial Basis Function, as known as squared\nexponential, or Gaussian), Mat\u00e9rn32, Mat\u00e9rn52, Polynomial, Exponential, Rational quadratic, and\nexponentiated quadratic. The quadrature was performed via RCHQ with the weighted sum of the\nmean predictive distribution of the optimised GP. Exponential kernel marked the best accuracy in\nthe evidence inference, whereas the KL divergence of posterior is embarrassingly erroneous. This is\nbecause all kernels examined in this section is not warped; thus, the GP-modelled likelihood is not\nnon-negative.\n\n2.2 Hyperparameter sensitivity analysis\n\n2.2.1 Analysis results\n\nThe hyperparameter sensitivity analysis is performed in the same setting as the previous section that\nadopts a ten-dimensional Gaussian mixture. The analysis was performed with functional Analysis\nof Variance (ANOVA) [12]. The functional ANOVA is the statistical method to decompose the\nvariance V of a black box function f into additive components V U associated with each subset of\nhyperparameters. [12] adopts random forest for efficient decomposition and marginal prediction\n\n5\n\n\fTable 3: Ablation study of kernel modelling\n\nKenel\n\nlogMAE\n\nlogKL\n\nwall\n\nlogMAE\n\nlogKL\n\ntime (s)\n\nper time\n\nper time\n\nRBF\n\nMat\u00e9rn32\n\nMat\u00e9rn52\n\nPolynomial\n\nExponential\n\n-2.9598\n\n12.321\n\n54.730\n\n-0.0543\n\n0.2349\n\n\u00b1 0.3679\n\n\u00b1 0.0288\n\n\u00b1 1.2979\n\n\u00b1 0.0080\n\n\u00b1 0.0048\n\n-3.7328\n\n12.312\n\n53.502\n\n-0.0701\n\n0.2355\n\n\u00b1 0.9608\n\n\u00b1 0.0577\n\n\u00b1 0.8661\n\n\u00b1 0.0191\n\n\u00b1 0.0026\n\n-4.3773\n\n12.332\n\n53.925\n\n-0.0817\n\n0.2341\n\n\u00b1 1.4593\n\n\u00b1 0.0765\n\n\u00b1 0.9662\n\n\u00b1 0.0285\n\n\u00b1 0.0027\n\n-3.7828\n\n12.208\n\n52.281\n\n-0.0728\n\n0.2449\n\n\u00b1 0.6803\n\n\u00b1 0.0673\n\n\u00b1 1.5491\n\n\u00b1 0.0152\n\n\u00b1 0.0056\n\n-4.6094\n\n12.268\n\n54.891\n\n-0.0841\n\n0.2252\n\n\u00b1 1.3440\n\n\u00b1 0.0291\n\n\u00b1 0.3428\n\n\u00b1 0.0250\n\n\u00b1 0.0009\n\nRational\n\n-3.2004\n\n12.309\n\n62.645\n\n-0.0514\n\n0.2059\n\nquadratic\n\n\u00b1 0.6515\n\n\u00b1 0.0596\n\n\u00b1 1.7786\n\n\u00b1 0.0119\n\n\u00b1 0.0046\n\nExponentiated\n\n-3.3361\n\n11.046\n\n53.306\n\n-0.0627\n\n0.2072\n\nquadratic\n\n\u00b1 1.4484\n\n\u00b1 0.4465\n\n\u00b1 0.1992\n\n\u00b1 0.0274\n\n\u00b1 0.0076\n\nTable 4: Sensitivity analysis with functional ANOVA\n\nhyperparameters\n\nlogMAE logKL\n\nN\nM\nr\nN,M\nN,r\nM,r\nN,M,r\n\n0.0835\n0.1041\n0.0041\n0.0710\n0.1021\n0.4598\n0.1754\n\n0.0465\n0.0824\n0.0226\n0.1134\n0.1299\n0.4381\n0.3116\n\nwall\ntime\n\n0.6347\n0.2497\n0.0071\n0.0903\n0.0058\n0.0059\n0.1671\n\nlogMAE\nper time\n\nlogKL\nper time\n\n0.1729\n0.6401\n0.0040\n0.1077\n0.0062\n0.0604\n0.0065\n\n0.1811\n0.6789\n0.0024\n0.1206\n0.0024\n0.0123\n0.0023\n\nover each hyperparameter. The hyperparameters to be analysed are the number of subsamples for\nrecombination N , the number of samples for the Nystr\u00f6m method, and the partition ratio in the IVR\nproposal distribution r. They need to satisfy the following relationship; N \u226b M > n, where n is the\nbatch size. We typically take n = 100, so M should be larger than at least 200, and N should be\nlarger than at least 20,000. Grid search was adopted for the hyperparameter space, with the range of\nN = 20,000, 50,000, 100,000, 500,000, 1,000,000, M = 200, 500, 1,000, 5,000, 10,000, and r = 0.0,\n0.25, 0.5, 0.75, 1.0, resulting in 125 data points. To compensate for the dynamic range difference, a\nnatural logarithmic log N and log M were used as the input.\n\nThe result is shown in Table 4. Each value refers to the fraction of the decomposed variance,\ncorresponding to the importance of each hyperparameter over the performance metric. Functional\nANOVA evaluates the main effect and the pairwise interaction effect. Figure 1 shows the marginal\npredictions on the important two hyperparameters for each performance metric.\n\nAs the most obvious case, we will look into the wall time. The most important hyperparameter\nwas N , and the second was M . This is well correlated to the theoretical aspect. The overhead\ncomputation time of BASQ can be decomposed into two components; subsampling and RCHQ. The\nN subsampling is dependent on N as O(n2/2 + ncompN ). ncomp is way less than M or n2 and is\ninsensitive to the hyperparameter variation or GP updates as we designed it to be sparse. The SMC\n\n6\n\n\fFigure 1: Sensitivity analysis of the hyperparameters over the metrics\n\nsampling for M is negligible as O(N + M ). The RCHQ is O(N M + M 2 log n + M n2 log(N/n)).\nComparing the complexity, the RCHQ stands out. Therefore, the whole BASQ algorithm complexity\nis dominated by RCHQ. While M has the squared component, N itself is as large as M 2. Therefore,\nthe selected two hyperparameters N and M align with the theory. Figure 1 agrees the above analysis.\n\nThe logMAE and logKL metrics have a similar trend to each other. In fact, their correlation coefficient\nwas 0.6494. This makes sense because both metrics are determined by the functional approximation\naccuracy of likelihood \u2113(x). While increasing M is always beneficial in any r, varying r is effective\nin larger M . At last, the metrics of performance per wall time is the combination of these effects.\nObviously, time is the dominant factor, so we should limit the N and M as small as possible. The\nmost important hyperparameter was M . This is a natural consequence because M affects the overhead\nincrement less than N but contributes to reducing errors more.\n\n2.2.2 A guideline to select hyperparameters\n\nThe main takeaway from the functional ANOVA analysis is that the accuracy with and without\nthe time constraint has an opposite trend. Therefore, the best hyperparameter set is dependent on\nthe overhead time allowance. The expensiveness of likelihood evaluation determines this. Per the\nlikelihood query time per iteration, we should increase M and N for faster convergence.\n\nIn the cheap likelihood case, the most relevant metric is logMAE per time and logKL per time. As we\nshould minimise the time, we choose the minimal size for N and M to minimise the overhead. As\nthe typical hyperparameter set is (n, N, M, r) = (100, 200, 20, 000, 0.5), and these are the minimum\nvalues for N and M at given n. The remained choice is the selection of r. As shown in Figure 1,\nr = 1, namely, UB proposal distribution, was the best selection in the Gaussian mixture likelihood\ncase. A similar trend is observed in the synthetic dataset. However, as we observed in the real-world\n\n7\n\ntime1086logM10.011.513.0logNlogMAElogKLlogM1086logM1086r0.01.0r0.01.0logMAE per timelogKL per time1086logM1086logM10.011.513.0logN10.011.513.0logNhighlow\fdataset cases, some likelihoods showed that r = 0.5, namely IVR proposal distribution outperformed\nthe UB. Therefore, the r = 1 might be the first choice., and r = 0.5 is the second choice.\n\nIn the expensive likelihood case, we can increase the N and M because the overhead time is less\nsignificant than the likelihood query time. The logMAE and logKL without time constraints are good\nguidelines for tuning the hyperparameters. As M is the most significant hyperparameter, we wish to\nincrease M first. However, we have to increase N under the constraint N \u226b M , necessary for RCHQ\nfast convergence. Empirically, we recommend increasing the M three times larger than N from the\nminimum set because the importance factor ratio in Table 4 is roughly three times. And the increment\nof M should be corresponded to the likelihood query time tlikelihood over the RCHQ computation time\ntRCHQ. We increase the M in accordance with the ratio rcomp := tlikelihood/tRCHQ. Thus, the M and\nN is determined as M = rcompMmin, N = rcomp\n\n3 Nmin, where Mmin = 200, Nmin = 20, 000.\n\n3 Analytical form of integrals\n\n3.1 Gaussian identities\n\nN (x; m1, \u03a31)N (x; m2, \u03a32) = CcN (x; mc, \u03a3c)\n\n\n\nN (x; m1, \u03a31)N (x; m2, \u03a32)dx = Cc\n\n(29)\n\n(30)\n\nN (Ax+b; m, \u03a3) =\n\n\n\n|2\u03c0(A\u22a4\u03a3\u22121A)\u22121|\n|2\u03c0\u03a3|\n\n\n\nx; A\u22121m \u2212 b, (A\u22a4\u03a3\u22121A)\u22121\n\nN\n\nwhere\n\n\u22121\n\n\u03a3c = \u03a3\u22121\n1 + \u03a3\u22121\n2\nmc = \u03a3\u22121\n\u22121 \u03a3\u22121\n1 + \u03a3\u22121\n2\nCc = N (m1; m2, \u03a31 + \u03a32)\n\n1 m1 + \u03a3\u22121\n\n2 m2\n\n\n\nIn the finite number of product case:\n\nn\n\n\ni=1\n\nN (x; mi, \u03a3i) = CmN (x; mm, \u03a3m)\n\nN (x; mi, \u03a3i)dx = Cm\n\n\n\nN (x; mm, \u03a3m)dx\n\n n\n\n\ni=1\n\n= Cm\n\n\u22121\n\n\u03a3\u22121\n\ni\n\n\n\nwhere\n\n\u03a3m =\n\nmm =\n\n n\n\n\ni=1\n n\n\n\n\u03a3\u22121\n\ni\n\n\n\n\u22121 n\n\n\ni=1\n\n\u03a3\u22121\n\ni mi\n\n\n\ni=1\n\n\nCm = exp\n\n\u2212\n\n\n\n1\n2\n\n(n \u2212 1)d log 2\u03c0 \u2212\n\nn\n\n\ni=1\n\nlog \n\n\u03a3\u22121\ni\n\n\n + log\n\n+\n\nn\n\n\ni=1\n\n(\u03a3\u22121\n\ni mi)\u22a4\u03a3i(\u03a3\u22121\n\ni mi) \u2212\n\n\n\n\n\n\n\nn\n\n\ni=1\n\ni\n\n\n\n\u03a3\u22121\n\n\n\n\n\n\n\u22a4  n\n\n\n n\n\n\ni=1\n\n\u03a3\u22121\n\ni mi\n\n\n\n8\n\n\u22121  n\n\n\n\u03a3\u22121\n\ni\n\n\n\n\u03a3\u22121\n\ni mi\n\n\n\ni=1\n\ni=1\n\n(41)\n\n\uf8f9\n\n\uf8fb\n\n\uf8fc\n\uf8fd\n\n\uf8fe\n\n(31)\n\n(32)\n\n(33)\n\n(34)\n\n(35)\n\n(36)\n\n(37)\n\n(38)\n\n(39)\n\n(40)\n\n\f3.2 Definitions\n\n\u2032\n\nx\u2217: predictive data points, x\u2217 \u2208 Rd\nX: the observed data points, X \u2208 Rn\u00d7d\ny = \u2113true(X): the observed likelihood, y \u2208 Rn\n\u03c0(x\u2217) = N (x\u2217; \u00b5\u03c0, \u03a3\u03c0): prior distribution,\nv\n: a kernel variance,\nl: a kernel lengthscale,\nK(x\u2217, X) = vN (x\u2217; X, W): a RBF kernel,\nv = v\nW: a diagonal covariance matrix whose diagonal elements are the lengthscales of each dimension,\n\n\u2032|2\u03c0W|: a normalised kernel variance,\n\n\n\nl2\n0\n\nW =\n\n0\nl2\nI: The identity matrix,\nKXX = K(X, X): a kernel over the observed data points.\n\n(42)\n\n(43)\n\n(44)\n\n(45)\n\n\n\n(46)\n\n3.3 Warped GPs as Gaussian Mixture\n\n3.3.1 Mean\n\nmL\n\ny (x\u2217) = \u03b1 +\n\n= \u03b1 +\n\n= \u03b1 +\n\n1\n2\n1\n2\n1\n2\n\n\u02dcmy(x\u2217)2\n\nyT K\u22121\n\nXX K(X, x\u2217)K(x\u2217, X)K\u22121\n\nXX y\n\n\n\ni,j\n\n\u03c9i\u03c9jK(Xi, x\u2217)K(x\u2217, Xj)\n\n= \u03b1 +\n\n\n\ni,j\n\n\n\nx\u2217;\n\nwm\n\nij N\n\n\n\nXi + Xj\n2\n\n,\n\nW\n2\n\nwhere\nWoodbury vector: \u03c9 = K\u22121\nmean weight: wm\n\nij = 1\n\nXX y,\n\n2 v2\u03c9i\u03c9jN (Xi; Xj, 2W)\n\n3.3.2 Variance\n\nC L\n\ny (x\u2217, x\n\n\u2032\n\n\u2217) = \u02dcmy(x) \u02dcCy(x, x\u2032) \u02dcmy(x\u2032)\n= [K(x\u2217, X)\u03c9]\u22a4 \n= \u03c9\u22a4K(X, x\u2217)K(x\u2217, x\n\nK(x\u2217, x\n\n\u2032\n\n\u2032\n\n\u2032\n\n\u2217)K(x\n\u2212 \u03c9\u22a4K(X, x\u2217)K(x\u2217, X)K\u22121\n\n\n\u03c9i\u03c9jK(Xi, x\u2217)K(x\u2217, x\n\n\u2217, X)\u03c9\nXX K(X, x\n\u2217)K(x\n\n\u2032\n\n\u2032\n\n=\n\n\u2217, Xj)\n\n\u2032\n\n\u2217)K(x\n\n\u2032\n\n\u2217, X)\u03c9\n\n\u2217) \u2212 K(x\u2217, X)K\u22121\n\nXX K(X, x\n\n \n\n\u2032\nK(x\n\n\u2217, X)\u03c9\n\n\u2032\n\n\u2217)\n\ni,j\n\n\n\n\u2212\n\ni,j\n\n\u03c9i\u03c9j\n\n\n\nk,l\n\n\u2126klK(Xj, x\u2217)K(x\u2217, Xi)K(Xk, x\n\n\u2032\n\n\u2217)K(x\n\n\u2032\n\n\u2217, Xl)\n\n\n\n=\n\ni,j\n\nwv\n\nijCv(i, j) \u2212\n\n\n\n\n\ni,j\n\nk,l\n\nwvv\n\nijklCvv(i, j, k, l)\n\nwhere\nThe inverse kernel weight: \u2126kl = K\u22121\nthe first variance weight: wv\nij = v3\u03c9i\u03c9j\nthe second variance weight: wvv\nthe first variance Gaussian variable\n\nXX (k, l)\n\nijkl = v4\u03c9i\u03c9j\u2126kl\n\n9\n\n\fCv(i, j) = N\n\n\uf8eb\n\n\uf8ee\n\n\uf8ed\n\n\uf8f0\n\nx\u2217\nx\nx\n\n\u2032\n\u2217\n\u2032\n\u2217\n\n\uf8f9\n\n\uf8fb ;\n\n\n\nXi\nXj\nx\u2217\n\nW 0\n\n0\n0 W 0\n0 W\n0\n\n,\n\n\uf8f6\n\uf8f8\n\nthe second variance Gaussian variable\n\uf8ee\n\n\uf8eb\n\n\uf8ee\n\n\uf8f9\n\n\uf8ee\n\n\uf8f9\n\nCvv(i, j, k, l) = N\n\n\uf8ec\n\uf8ec\n\uf8ed\n\n\uf8ef\n\uf8ef\n\uf8f0\n\n;\n\n\uf8fa\n\uf8fa\n\uf8fb\n\n\uf8ef\n\uf8f0\n\n\uf8fa\n\uf8fb ,\n\n\uf8ef\n\uf8f0\n\nXi\nXj\nXk\nXl\n\nx\u2217\nx\u2217\nx\nx\n\n\u2032\n\u2217\n\u2032\n\u2217\n\nW 0\n0\n0 W 0\n0\n0\n\n0\n0\n0 W 0\n0 W\n0\n\n\uf8f6\n\n\uf8f9\n\n\uf8fa\n\uf8fb\n\n\uf8f7\n\uf8f7\n\uf8f8\n\n10\n\n\f3.3.3 Symmetric variance\n\nHere we consider x\u2217 = x\u2032\n\n\u2217 and x\u2217 is a sample with dimension d:\n\nC L\n\ny (x\u2217, x\u2217) = C L\n\ny (x\u2217)\n\n= \u03c9\u22a4K(X, x\u2217)K(x\u2217, x\u2217)K(x\u2217, X)\u03c9\n\u2212 \u03c9\u22a4K(X, x\u2217)K(x\u2217, X)K\u22121\n\n\n=\n\n\u03c9i\u03c9jK(Xi, x\u2217)K(x\u2217, x\u2217)K(x\u2217, Xj)\n\nXX K(X, x\u2217)K(x\u2217, X)\u03c9\n\ni,j\n\n\n\n\u2212\n\n\u03c9i\u03c9j\n\ni,j\nv\n|2\u03c0W|\n\n\n\u2212\n\n\u03c9i\u03c9j\n\ni,j\nv3\n|2\u03c0W|\n\uf8ee\n\n\n\ni,j\n\n=\n\n=\n\n\n\nk,l\n\n\u2126klK(Xj, x\u2217)K(x\u2217, Xi)K(Xk, x\u2217)K(x\u2217, Xl)\n\n\n\ni,j\n\n\u03c9i\u03c9jK(Xi, x\u2217)K(x\u2217, Xj)\n\n\n\nk,l\n\n\u2126klK(Xj, x\u2217)K(x\u2217, Xi)K(Xk, x\u2217)K(x\u2217, Xl)\n\n\u03c9i\u03c9jN (Xi; Xj, 2W)N\n\nx\u2217;\n\n\n\n\n\nXi + Xj\n2\n\n,\n\nW\n2\n\n\u2212 v4 \n\n\uf8f0\u03c9i\u03c9j\n\n\n\n\n\n\u2126klN (Xl; Xi, 2W)N\n\nx\u2217;\n\n\n\ni,j\n\nk,l\n\n\u00b7N (Xk; Xj, 2W)N\n\nx\u2217;\n\n\n\n\n\nXl + Xi\n2\n\n,\n\nW\n2\n\n\n\n,\n\nW\n2\n\nXk + Xj\n2\n\n\n\n\n=\n\nw\n\n\u2032v\nij N\n\n\n\nx\u2217;\n\ni,j\n\nXi + Xj\n2\n\n,\n\nW\n2\n\n\n\n\u2212\n\n\n\n\n\ni,j\n\nk,l\n\nw\n\n\u2032vv\nijklN\n\nx\u2217;\n\nXi + Xj + Xk + Xl\n4\n\nwhere\nij = h3\u221a\n\u2032v\nw\n\u2032vv\nijkl = h4\u03c9i\u03c9j\u2126klN (Xl; Xi, 2W)N (Xk; Xj, 2W)N\n\n\u03c9i\u03c9jN (Xi; Xj, 2W)\n\n|2\u03c0W|\n\nw\n\n Xk+Xj\n2\n\n; Xi+Xl\n2\n\n, W\n\n\n\n3.4 Model evidence\n\nThe distribution over integral Z is given by:\n\n\n\np(Z|y) =\n\npZ|\u2113(x\u2217)p\u2113(x\u2217)|ydx\n\n= pZ|\u2113(x\u2217)N \u2113(x\u2217); mL\nZ; EZ|y, varZ|y\n\n= N\n\n\n\ny (x\u2217), C L\n\ny (x\u2217)\n\n11\n\n\n\n,\n\nW\n4\n\n(47)\n\n(48)\n\n(49)\n\n(50)\n\n\f3.4.1 Mean of the integral\n\nEZ|y = EmL\n\ny\n\n\n\n\n\n=\n\nmL\n\ny (x\u2217)\u03c0(x\u2217)dx\u2217\n\n= \u03b1 +\n\n= \u03b1 +\n\n= \u03b1 +\n\n\n\n1\n2\n\n\ni,j\n\n\n\ni,j\n\n3.4.2 Variance of the integral\n\n\u02dcm2\n\ny(x\u2217)\u03c0(x\u2217)dx\u2217\n\n\n\nwm\nij\n\nN\n\nx\u2217;\n\nXi + Xj\n2\n\n\n\n,\n\nW\n2\n\nN (x\u2217; \u00b5\u03c0, \u03a3\u03c0)dx\u2217\n\nwm\n\nij N\n\n Xi + Xj\n2\n\n; \u00b5\u03c0,\n\nW\n2\n\n\n\n+ \u03a3\u03c0\n\n(51)\n\n(52)\n\n(53)\n\n(54)\n\n(55)\n\nvarZ|y = varC L\n\n\n\ny\n\n \n\n=\n\n=\n\n\u03c0(x\u2217)C L\n\ny (x\u2217, x\n\n\u2217)\u03c0(x\n\n\u2032\n\n\u2032\n\n\u2032\n\u2217)dx\u2217dx\n\u2217\n\n  \n\n\u03c9\u22a4K(X, x\u2217)K(x\u2217, x\n\n\u2032\n\n\u2032\n\n\u2217)K(x\n\n\u2217, X)\u03c9\u03c0(x\u2217)\u03c0(x\n\n\u2217)\n\n\u2032\n\n\u2212\u03c9\u22a4K(X, x\u2217)K(x\u2217, X)K\u22121\n\nXX K(X, x\n\n\u2217)K(x\n\n\u2032\n\n\u2032\n\n\u2217, X)\u03c9\u03c0(x\u2217)\u03c0(x\n\n\n\n\u2032\n\n\u2217)\n\ndx\u2217dx\n\n\u2032\n\u2217\n\n\n\n=\n\n\u03c9i\u03c9j\n\n \n\ni,j\n\nK(Xi, x\u2217)K(x\u2217, x\n\n\u2032\n\n\u2217)K(x\n\n\u2032\n\n\u2217, Xj)\u03c0(x\u2217)\u03c0(x\n\n\u2032\n\n\u2032\n\u2217)dx\u2217dx\n\u2217\n\n\n\n\u2212\n\n\u03c9i\u03c9j\n\n\n\n\u2126kl\n\n \n\nK(Xj, x\u2217)K(x\u2217, Xi)K(Xk, x\n\n\u2032\n\n\u2217)K(x\n\n\u2032\n\n\u2217, Xl)\u03c0(x\u2217)\u03c0(x\n\n\u2032\n\n\u2032\n\u2217)dx\u2217dx\n\u2217\n\ni,j\n\n\n\n=\n\n\u03c9i\u03c9jh3\n\ni,j\n\nk,l\n \n\nN (x\n\n\u2032\n\n\u2217; \u00b5\u03c0, \u03a3\u03c0)N (x\n\n\u2032\n\n\u2217; Xj, W)\n\n\n\nN (x\u2217; Xi, W)N (x\u2217; x\n\n\u2032\n\n\u2217, W)N (x\u2217; \u00b5\u03c0, \u03a3\u03c0)dx\u2217\n\n\n\ndx\n\n\u2032\n\u2217\n\n\n\n\n\n\u2212\n\nk, l\u03c9i\u03c9j\u2126klh4\n\n\n\ni,j\n\nN (x\u2217; Xj, W)N (x\u2217; Xi, W)N (x\u2217; \u00b5\u03c0, \u03a3\u03c0)dx\u2217\n\n\n\n\u00b7\n\nN (x\n\n\u2032\n\n\u2217; Xk, W)N (x\n\n\u2032\n\n\u2217; xl, W)N (x\n\n\u2032\n\n\u2032\n\u2217; \u00b5\u03c0, \u03a3\u03c0)dx\n\u2217\n\n\n\n=\n\n\u03c9i\u03c9jh3\n\n\n\nN (x\n\ni,j\n\n\u2032\n\n\u2217; \u00b5\u03c0, \u03a3\u03c0)N (x\n\n\u2217; Xj, W)N (x\n\n\u2032\n\n\u2032\n\n\u2217; Xi, W)N\n\n\n\n\u2032\n\u2217\n\nXi + x\n2\n\n; \u00b5\u03c0,\n\nW\n2\n\n\n\ndx\n\n\u2032\n\u2217\n\n+ \u03a3\u03c0\n\n\n\n\n\n\u2212\n\nk, l\u03c9i\u03c9j\u2126klh4\n\n\nN (Xj; Xi, 2W )N\n\ni,j\n\n Xi + Xj\n2\n\n; \u00b5\u03c0,\n\nW\n2\n\n\n\n+ \u03a3\u03c0\n\n\nN (Xk; xl, 2W )N\n\n\u00b7\n\n Xk + Xl\n2\n\n; \u00b5\u03c0,\n\nW\n2\n\n+ \u03a3\u03c0\n\n\n\n=\n\n\u03c9i\u03c9jh3\n\n\n\nN (x\n\n\u2032\n\n\u2217; \u00b5\u03c0, \u03a3\u03c0)N (x\n\n\u2032\n\n\u2217; Xi, W)N (x\n\n\u2032\n\n\u2217; Xj, W)2dN\n\n\n\n\n\nx\n\n\u2032\n\n\u2217; 2\u00b5\u03c0 \u2212 Xi/2, 2W + 4\u03a3\u03c0\n\n\n\n\u2032\ndx\n\u2217\n\ni,j\n\n\n\n\n\n\u2212\n\nwvv\n\nijklKvv(i, j, k, l)\n\ni,j\n\nk,l\n2dwv\n\n\n\n=\n\ni,j\n\nijKv(i, j) \u2212\n\n\n\n\n\ni,j\n\nk,l\n\nwvv\n\nijklKvv(i, j, k, l)\n\n12\n\n(56)\n\n\fwhere\n\nKv(i, j) = N (Xi; Xj, 2W)N\n\n Xi + Xj\n2\n\n; \u00b5\u03c0,\n\nW\n2\n\n\n\n+ \u03a3\u03c0\n\n(57)\n\n\n2W\u22121 + \u03a3\u22121\n\n\u03c0\n\nN\n\n\u22121 W\u22121(Xi + Xj) + \u03a3\u22121\n\n\u03c0 \u00b5\u03c0\n\n ; 2\u00b5\u03c0 \u2212\n\n, 2W\u22121 + \u03a3\u22121\n\n\u03c0\n\n\u22121\n\n+ 2W + 2\u03a3\u03c0\n\n\n\nXi\n2\n\nKvv(i, j, k, l) =\n\n\nN (Xj; Xi, 2W)N\n\n Xi + Xj\n2\n\n; \u00b5\u03c0,\n\nW\n2\n\n \n\n+ \u03a3\u03c0\n\nN (Xk; Xl, 2W)N\n\n(58)\n Xk + Xl\n2\n\n; \u00b5\u03c0,\n\n(59)\n\n\n\nW\n2\n\n+ \u03a3\u03c0\n\n3.5 Posterior inference\n\n3.5.1 Joint posterior\n\n3.5.2 Marginal posterior\n\np(x) =\n\nmL\n\ny (x)\u03c0(x)\nE[Z|y]\n\n(60)\n\nThe marginal posterior can be on obtained from Gaussian mixture form of joint posterior. Thanks to\nthe Gaussianity, marginal posterior can be easily derived by extracting the d-th element of matrices in\nthe following mixture of Gaussians.\n\np(x) =\n\n\u03b1\nE[Z|y]\n\n\n\n+\n\ni,j\n\nwp\n\nijN (x\u2217; \u00b5p, \u03a3p)\n\n(61)\n\nwhere\n Xi+Xj\nwm\nwp\nij\nE[Z|y] N\nij =\n2\n\u03a3p = (2W\u22121 + \u03a3\u22121\n\u03c0 )\u22121\n\u00b5p = \u03a3p(W\u22121(Xi + Xj) + \u03a3\u22121\n\n; \u00b5\u03c0, W\n\n\n\n2 + \u03a3\u03c0\n\n\u03c0 \u00b5\u03c0)\n\n3.5.3 Conditional posterior\n\nThe conditional posterior p (x; d = d | d = D \\ D(\u2265 d)) can be derived from the Gaussian mixture\nform of joint posterior. We can obtain the conditional posterior via applying the following relationship\nto each Gaussian: Assume x \u223c N (x; \u00b5, \u03a3) where\n\n\u00b5a\n\u00b5b\n\n\u03a3a \u03a3c\n\u03a3\u22a4\nc \u03a3b\n\nxa\nxb\n\n\u03a3 =\n\n\u00b5 =\n\n(62)\n\nx =\n\n\n\n\n\nThen\n\np(xa)|p(xb) = N (xa; \u02c6\u00b5a, \u02c6\u03a3a)\n\np(xb)|p(xa) = N (xb; \u02c6\u00b5b, \u02c6\u03a3b)\n\n \u02c6\u00b5a = \u00b5a + \u03a3c\u03a3\u22121\n\u02c6\u03a3a = \u03a3a \u2212 \u03a3c\u03a3\u22121\n \u02c6\u00b5b = \u00b5b + \u03a3c\u03a3\u22121\n\u02c6\u03a3b = \u03a3b \u2212 \u03a3c\u03a3\u22121\n\nb (xb \u2212 \u00b5b)\nb \u03a3\u22a4\nc\na (xa \u2212 \u00b5a)\na \u03a3\u22a4\nc\n\n(63)\n\n(64)\n\n4 Uncertainty sampling\n\n4.1 Analytical form of acquisiton function\n\n4.1.1 Acquisiton function as Gaussian Mixture\n\nWe set the acquisition function A(x) as the product of the variance and the prior. As is shown in Eq.\n(47), When we provide the predictive samples x\u2217:\n\n13\n\n\fA(x\u2217) = C L\n= C L\n\uf8eb\n\ny (x\u2217, x\u2217)\u03c0(x\u2217)\ny (x\u2217)\u03c0(x\u2217)\n\n=\n\n\uf8ed\n\n\n\ni,j\n\nw\n\n\u2032v\nij N\n\n\n\nx\u2217;\n\nXi + Xj\n2\n\n,\n\nW\n2\n\n\n\n\u2212\n\n\n\n\n\ni,j\n\nk,l\n\nw\n\n\u2032vv\nijklN\n\n\n\nx\u2217;\n\nXi + Xj + Xk + Xl\n4\n\n,\n\nW\n4\n\n\u00b7 N (x\u2217; \u00b5\u03c0, \u03a3\u03c0)\n\nij N x\u2217; \u00b5A\n\u2032A\n\nw\n\nij, \u03a3A\n\n=\n\n\n\n\n\n \u2212\n\nijkl N x\u2217; \u00b5AA\n\u2032AA\nw\n\nijkl, \u03a3AA\n\n\n\ni,j\n\ni,j\n\nk,l\n\nwhere\n\u03a3A = (2W\u22121 + \u03a3\u22121\n\u03c0 )\u22121\nW\u22121(Xi + Xj) + \u03a3\u22121\n\u00b5A\nij = \u03a3A\n\u2032v\n\u2032A\n2 + \u03a3\u03c0\nij N\nij = w\n\n Xi+Xj\n2\n\n; \u00b5\u03c0, W\n\nw\n\n\u03c0 \u00b5\u03c0\n\n\n\n\n\u03c0 )\u22121\n\n\u03a3AA = (4W \u22121 + \u03a3\u22121\n\u00b5AA\nijkl = \u03a3A\n\u2032AA\nijkl = w\nw\n\nW\u22121(Xi + Xj + Xk + Xl) + \u03a3\u22121\n\u03c0 \u00b5\u03c0\n\n\u2032vv\nijklN\n\n Xi+Xj +Xk+Xl\n4\n\n4 + \u03a3\u03c0\n\n; \u00b5\u03c0, W\n\n\n\n4.1.2 Normalising constant and PDF\n\nThe normalising constant can be obtained via the integral:\n\n\n\nZA =\n\nA(x\u2217)dx\u2217\n\n\n\n=\n\nw\n\n\u2032A\nij \u2212\n\n\n\n\n\nw\n\n\u2032AA\nijkl\n\ni,j\n\ni,j\n\nk,l\n\nThus, the normalised acquisition function as PDF pA is as follows:\n\npA(x\u2217) = \u02dcA(x\u2217)\n\nC L\n\ny (x\u2217)\u03c0(x\u2217)\nZA\nijN (x\u2217; \u00b5A\n\nwA\n\n\n\nij, \u03a3A) \u2212\n\n=\n\n=\n\ni,j\n\nwhere wA\nwAA\nijkl = w\n\nij = w\n\u2032AA\nijkl /ZA\n\n\u2032A\nij /ZA\n\n\n\n\n\ni,j\n\nk,l\n\nwAA\n\nijklN (x\u2217; \u00b5AA\n\nijkl, \u03a3AA)\n\n(65)\n\n(66)\n\uf8f6\n\n\n\n\uf8f8\n\n(67)\n(68)\n\n(69)\n\n(70)\n\n(71)\n\n(72)\n\n(73)\n\n4.1.3 Factorisation trick\nThe factorisation trick is set in the conditions where the likelihood is |\u02dc\u2113(x)|, the distribution of interest\nf (x) is |\u02dc\u2113(x)|\u03c0(x), and the acquiition function is \u02dcCy(x, x)\u03c0(x). We will derive the Gaussian mixture\nform of this acquisition function.\n\nA(x) = \u02dcCy(x, x)\u03c0(x)\n\n=\n\nv\n|2\u03c0W|\n\nN (x; \u00b5\u03c0, \u03a3\u03c0) \u2212 v2 \n\n\u2126ijN (x; \u00b5f , \u03a3f )\n\nij\n\n(74)\n\n(75)\n\nwhere\n\u03a3f = 2W\u22121 + \u03a3\u22121\n\u00b5f = \u03a3f W\u22121(Xi + Xj) + \u03a3\u22121\n\n\u22121\n\n\u03c0\n\n\u03c0 \u00b5\u03c0\n\n\n\n14\n\n\fThen, normalising constant is:\n\n\n\nZ f\n\nA =\n\nA(x)dx =\n\nv\n|2\u03c0W|\n\n\u2212 v2 \n\n\u2126ij\n\nij\n\n(76)\n\nTherefore, the acquisition function as a probability distribution function pA(x) is:\n\npA(x) =\n\nv\n|2\u03c0W|\n\nZ f\nA\n\nN (x; \u00b5\u03c0, \u03a3\u03c0) \u2212\n\nv2\nZ f\nA\n\n\n\nij\n\n\u2126ijN (x; \u00b5f , \u03a3f )\n\n(77)\n\n4.2 Efficient sampler\n\n4.2.1 Acquisition function as sparse Gaussian mixture sampler\n\nEq. (77) clearly explains the acquisition function can be written as a Gaussian mixture, but it also\ncontains negative components. The first term is obviously positive, and the second term is a mixture\nof positive and negative components. The condition where the second term becomes positive is\n\u2126ij < 0. By checking the negativity of the element \u2126ij, we can reduce the number of components\nby half on average. Then, when we consider sampling from this non-negative acquisition function,\nthe following steps will be performed: First, we sample the index of the component from weighted\ncategorical distribution \u03a0(x), and the weights are the one in Eq. (77). Then, we sample from the\nnormal distribution that has the same index identified in the first process. These sampling will be\nrepeated until the accumulated number of the sample reaches the same as the recombination sample\nsize N . This means the component whose weight is lower than 1/N is unlikely to be sampled even\nonce. Therefore, we can dismiss these components with the threshold of 1/N . Interestingly, the\nweights of Gaussians vary exponentially. The reduced number of Gaussians is much lower than n2.\nAs such, we can construct the efficient sparse Gaussian mixture sampler of the acquisition function\np\u2032\nA(x).\n\n4.2.2 Sequential Monte Carlo\n\nRecall from the Eqs (8) - (10) in the main paper, we wish to sample from g(x) = (1\u2212r)\u03c0(x)+rpA(x).\nWe have the efficient sampler p\u2032\nA(x) is the function which is\nconstructed from only positive components of pA(x). Thus, we need to correct this difference via\nsequential Monte Carlo (SMC). The idea of SMC is simple:\n\nA(x) \u0338= pA(x) because p\u2032\n\nA(x), but p\u2032\n\nA(x), x \u2208 RrN\n\n1. sample x \u223c p\u2032\n2. calculate weights wsmc = pA(x)/p\u2032\n3. resample from the categorical distribution of the index of x based on wsmc\n\nA(x)\n\nA(x), the rejected samples in the procedure 3 is minimised. As we formulate p\u2032\n\nIf pA(x) \u2248 p\u2032\nA(x)\ncan approximate pA(x) well, the number of samples to be rejected is negligibly small. Thus, the\nnumber of samples from pA(x) is slightly smaller than rN . The number of samples for \u03c0(x) in g(x)\nis adjusted to this fluctuation to keep the partition ratio r.\n\n5 Other BQ modelling\n\n5.1 Non-Gaussian Prior\n\nNon-Gaussian prior distributions can be applied via importance sampling.\n\n\n\n\u2113(x)\u03c0(x) =\n\n=\n\n\n\n\n\n\u2113(x)\n\n\u03c0(x)\ng(x)\n\ng(x)dx\n\n\u2113\u2032(x)g(x)dx\n\n(78)\n\n(79)\n\nwhere \u03c0(x) is the arbitrary prior distribution of interest, g(x) is the proposal distribution of Gaussian\n(mixture), \u2113\u2032(x) = \u2113(x)\u03c0(x)/g(x) is the modified likelihood. Then, we set the two independent GPs\non each of \u2113(x) and \u2113\u2032(x). Then, both the model evidence Z =  \u2113\u2032(x)g(x)dx, and the posterior\np(x) = \u2113(x)\u03c0(x)/Z becomes analytical.\n\n15\n\n\f5.2 Non-Gaussian kernel\n\nWSABI-BQ methods are limited to the squared exponential kernel in the likelihood modelling.\nHowever, other BQ modelling permits the selection of different kernels. For instance, there are the\nexisting works on tractable BQ modelling with kernels of Mat\u00e9rn [7], Wendland [16], Gegenbauer\n[7], Trigonometric (Integration by parts), splines [20] polynomial [6], and gradient-based kernel [15].\nSee details in [7].\n\n5.3 RCHQ for Non-Gaussian prior and kernel\n\nRCHQ permits the integral estimation via non-Gaussian prior and/or kernel without bespoke mod-\nelling like the above techniques.\n\nXquad, wquad = RCHQ(BQmodel, sampler)\nE[\u2113(x)\u03c0(x)] = wquadmL\nrecC L\n\ny (Xquad)\ny (xrec, xrec)wrec \u2212 2 w\u22a4\n\nVar[\u2113(x)\u03c0(x)] = w\u22a4\n\nrecC L\n\ny (xrec, xquad)wquad + w\u22a4\n\nquadC L\n\ny (xquad, xquad)wquad\n(82)\n\n(80)\n\n(81)\n\n5.4 Vanilla BQ model (VBQ)\n\n5.4.1 Expectation\n\n\n\nm\u21130(x)\u03c0(x)dx = v\n\n\n\nN (x; X, W)N (x; \u00b5\u03c0, \u03a3\u03c0)dx\u03c9\n\n= vN (X; \u00b5\u03c0, W + \u03a3\u03c0)\u03c9\n\n5.4.2 Acquisition function\n\nAunnormalised(x) = C(x, x)\u03c0(x)\n\n(83)\n\n(84)\n(85)\n\n(86)\n\n= K(x, x)\u03c0(x) \u2212 K(x, X)K(X, X)\u22121K(X, x)\u03c0(x)\n= N (x; x, W)N (x; \u00b5\u03c0, \u03a3\u03c0) \u2212 v2N (x; \u00b5\u03c0, \u03a3\u03c0)N (x; X, W)K(X, X)\u22121N (x; X, W)\u22a4\n\n(87)\n\n(88)\n\n=\n\nv\n|2\u03c0W|\n\n=\n\nv\n|2\u03c0W|\n\n=\n\nv\n|2\u03c0W|\n\nwhere\n\nN (x; \u00b5\u03c0, \u03a3\u03c0) \u2212 v2 \n\n\u2126ijN (\u00b5\u03c0; Xi, W + \u03a3\u03c0)N (x; X \u2032\n\ni, W\u2032)N (x; Xj, W)\n\ni,j\n\nN (x; \u00b5\u03c0, \u03a3\u03c0) \u2212 v2 \n\ni,j\n\n\u2126ijN (\u00b5\u03c0; Xi, W + \u03a3\u03c0)N (Xj; X \u2032\n\n(89)\ni, W + W\u2032)N (x; X \u2032\u2032\n\nij, W\u2032\u2032)\n\nN (x; \u00b5\u03c0, \u03a3\u03c0) \u2212\n\nwijN (x; X \u2032\u2032\n\nij, W\u2032\u2032)\n\n\n\ni,j\n\n\u2126ij := K(X, X)\u22121\nwi := v2\u2126ijN (\u00b5\u03c0; Xi, W + \u03a3\u03c0)N (Xj; X \u2032\nW\u2032 = (W\u22121 + \u03a3\u22121\ni = W\u2032(W\u22121Xi + \u03a3\u22121\nX \u2032\nW\u2032\u2032 = W\u2032\u22121 + W\u22121\u22121\nij = W\u2032\u2032 W\u2032\u22121X \u2032\nX \u2032\u2032\n\ni + W\u22121Xj\n\n\u03c0 )\u22121\n\n\u03c0 \u00b5\u03c0)\n\n\n\ni, W + W\u2032)\n\n16\n\n(90)\n\n(91)\n\n(92)\n\n(93)\n\n(94)\n\n(95)\n\n(96)\n\n(97)\n\n(98)\n\n(99)\n(100)\n\n\fThen, the normalised acquisition function pA(x) as a probability distribution is as follows:\n\nPA(x) := Aunnormalised(x)/ZA\n\n=\n\nv\n|2\u03c0W|\n\nZA\n\nN (x; \u00b5\u03c0, \u03a3\u03c0) \u2212\n\n\n\ni,j\n\nwi\nZA\n\nN (x; X \u2032\u2032\n\nij, W\u2032\u2032)\n\nwhere\n\nZA =\n\n\n\nAunnormalised(x)dx\n\n(101)\n\n(102)\n\n(103)\n\n(104)\n\n\n\n=\n\nv\n|2\u03c0W|\n\nN (x; \u00b5\u03c0, \u03a3\u03c0)dx \u2212 v2 \n\n\u2126ijN (\u00b5\u03c0; Xi, W + \u03a3\u03c0)\n\n\n\nN (x; X \u2032\n\ni, W\u2032)N (x; Xj, W)dx\n\ni,j\n\n=\n\nv\n|2\u03c0W|\n\n\u2212 v2 \n\ni,j\n\n\u2126ijN (\u00b5\u03c0; Xi, W + \u03a3\u03c0)N (Xj; X \u2032\n\ni, W + W\u2032)\n\n(105)\n\n(106)\n\n(107)\n\n5.5 Log-GP BQ modelling (BBQ)\n\n5.5.1 BBQ modelling\n\nThe doubly-Bayesian quadrature (BBQ) is modelled with log-warped GPs as follows (see details in\nthe paper [17]):\n\nSet three GPs\n\np(\u21130|D) \u223c GP(\u21130; m\u21130 (x), C\u21130 (x, x\u2032))\n\np(log \u21130|D) \u223c GP(log \u21130; mlog \u21130(x), Clog \u21130(x, x\u2032))\np(\u2206log \u21130|D) \u223c GP(\u2206log \u21130 ; m\u2206(x), C\u2206(x, x\u2032))\n\nDefinitions\n\nExpectation\n\nexp(log \u2113(x)) \u2248 exp(log \u21130(x)) + exp(log \u21130(x))(log \u2113(x) \u2212 log \u21130(x))\n\n\u21130 := m\u21130\n\n\u2206log \u21130 := mlog \u21130 \u2212 log \u21130 = mlog \u21130 \u2212 log(m\u21130)\n\nm\u2113 = m\u21130 + m\u21130m\u2206(x)\n\n\n\nE[Z|D] =\n\nm\u21130(x)\u03c0(x)dx +\n\n\n\nm\u21130(x)m\u2206(x)\u03c0(x)dx\n\nThe first term is as follows:\n\n\n\nm\u21130(x)\u03c0(x)dx = v\n\n\n\nN (x; X, W)N (x; \u00b5\u03c0, \u03a3\u03c0)dx\u03c9\n\n= vN (X; \u00b5\u03c0, W + \u03a3\u03c0)\u03c9\n\nwhere\n\n\u03c9 = K(X, X)\u22121\u21130(X)\n\nK(x, X) = vN (x; X, W)\n\n17\n\n(108)\n\n(109)\n\n(110)\n(111)\n\n(112)\n(113)\n(114)\n(115)\n\n(116)\n\n(117)\n\n(118)\n\n(119)\n(120)\n\n(121)\n(122)\n\n\fThe second term is as follows:\n\n\n\nm\u21130(x)\u2206log \u21130(x)p(x)dx\n\n\n\n= vv\u2206\u03c9\u22a4\n\nN (x; X, W)\u22a4N (x; X\u2206, W\u2206)N (x; \u00b5\u03c0, \u03a3\u03c0)dx\u03c9\u2206\n\n\n= vv\u2206\u03c9\u22a4N (X\u22a4 \u2212 X\u2206, 0, W + W\u2206)\n\nN (x; \u00b5\u2206, \u03a3\u2206)N (x; \u00b5\u03c0, \u03a3\u03c0)dx\u03c9\u2206\n\n= vv\u2206\u03c9\u22a4N (X\u22a4 \u2212 X\u2206, 0, W + W\u2206)N (\u00b5\u2206; \u00b5\u03c0, \u03a3\u03c0 + \u03a3\u2206)\u03c9\u2206\n\nwhere\n\n\u03c9\u2206 = K(X\u2206, X\u2206)\u22121\u2206log \u21130(X\u2206)\n\u00b5\u2206 = [W\u22121 + W\u2206,\u22121]\u22121(W\u22121X + W\u2206,\u22121X\u2206)\n\u03a3\u2206 = [W\u22121 + W\u2206,\u22121]\u22121\n\n(123)\n\n(124)\n\n(125)\n\n(126)\n(127)\n\n(128)\n\n(129)\n\n(130)\n(131)\n\nX\u2206 is the observed data for the correlation factor \u2206log \u21130, which includes not only X but also the\nadditional data points via mlog \u21130 \u2212 log(m\u21130 ), with GPs calculation.\n\n5.5.2 Sampling for BBQ\n\nWe apply BASQ-VBQ sampling scheme for log-GP log\u21130, then calculate the others as post-process.\nTherefore, the sampling cost is similar to the VBQ, whereas the integral estimation as post-process is\nmore expensive than VBQ.\n\n6 Experimental details\n\n6.1 Synthetic problems\n\n6.1.1 Quadrature hyperparameters\n\nThe initial quadrature hyperparameters are as follows:\nA kernel length scale l = 2\nA kernel variance v\u2032 = 2\nRecombination sample size N = 20, 000\nNystr\u00f6m sample size M = N/100\nSupersample ratio rsuper = 100\nProposal distribution g(x) partition ratio r = 0.5\n\nThe supersample ratio rsuper is the ratio of supersamples for SMC sampling of acquisition function\nagainst the recombination sample size N .\n\nA kernel length scale and a kernel variance are important for selecting the samples in the first batch.\nNevertheless, these parameters are updated via type-II MLE optimisation after the second round.\nNystr\u00f6m sample size must be larger than the batch size n, and the recombination sample size is\npreferred to satisfy N \u226b M . Larger N and M give more accurate sample selection via kernel\nquadrature. However, larger subsamples result in a longer wall-time. We do not need to change the\nvalues as long as the integral converged to the designated criterion. When longer computational time\nis allowed, or likelihood is expensive enough to regard recombination time as negligible, larger N ,\nM will give us a faster convergence rate.\n\nThe partition ratio r is the only hyperparamter that affects the convergence sensitively. The optimal\nvalue depends the integrand and it is challenging to know the optimal value before running. As we\ny \u03c0(x) gives the optimal upper bound. r = 0.5 is a good approximation of\nderived in Lemma 1,\n \u03c0(x). Here,\nthis optimal proposal distribution: g(x) = (1 \u2212 r)\u03c0(x) + rC L\n\ny \u03c0(x) = (1 \u2212 r) + rC L\n\nC L\n\n\n\ny\n\n18\n\n\fthe linearisation gives the approximation\n\n\n\nC L\n\ny =\n\n\n\n1 + (C L\n\ny \u2212 1) \u2248 1 +\n\nCL\n\ny \u22121\n2 = 0.5 + 0.5C L\ny .\n\n\n\nC L\n\ny \u03c0(x). Thus, r = 0.5 is a safe choice.\n\nTherefore, (0.5 + 0.5C L\n\ny )\u03c0(x) \u2248\n\n6.1.2 Gaussian mixture\n\nThe likelihood function of the Gaussian mixture used in Figure 1 in the main paper is expressed as:\n\n\u2113true(x) =\n\nn\n\n\ni=1\n\nwiN (x; \u00b5i, \u03a3i)\n\nwi = N (\u00b5i; \u00b5\u03c0, \u03a3i + \u03a3\u03c0)\u22121\n\n\n\nZtrue =\n\n\u2113true(x)\u03c0(x)dx,\n\n\n\nwi\n\nN (x; \u00b5i, \u03a3i)N (x; \u00b5\u03c0, \u03a3\u03c0)dx\n\nwiN (\u00b5i; \u00b5\u03c0, \u03a3i + \u03a3\u03c0)\n\n=\n\n=\n\nn\n\n\ni=1\nn\n\n\ni=1\n\n= 1\n\nwhere\n\u00b5\u03c0 = 0\n\u03a3\u03c0 = 2I\n\u03c0(x) = N (x; \u00b5\u03c0, \u03a3\u03c0)\n\nThe prior is the same throughout the synthetic problems.\n\n6.1.3 Branin-Hoo function\n\nThe Branin-Hoo function in Figure 2 in the main paper is expressed as:\n\n2\n\n\n\u2113true(x) =\n\nsin(xi) + 1\n2 xi)2 + 3\n( 1\n\n2 cos(3xi)2\n\n10\n\n,\n\nx \u2208 R2\n\ni=1\n\n\nZtrue =\n\n\u2113true(x)\u03c0(x)dx\n\n= 0.9557282\n\u2248 0.913416\n\n(132)\n\n(133)\n\n(134)\n\n(135)\n\n(136)\n\n(137)\n\n(138)\n\n(139)\n\n(140)\n(141)\n\n6.1.4 Ackley function\n\nThe Ackley function in Figure 2 in the main paper is expressed as:\n\n\uf8eb\n\n\u2113true(x) = \u221220 exp\n\n\uf8ed\u2212\n\n\n\n\n\n\n1\n5\n\n1\n2\n\n2\n\n\ni=1\n\n\uf8f6\n\nx2\ni\n\n\uf8f8 + exp\n\n\n\n1\n2\n\n2\n\n\ni=1\n\n\n\ncos(2\u03c0xi)\n\n+ 20,\n\nx \u2208 R2\n\n(142)\n\n\n\nZtrue =\n\n\u2113true(x)\u03c0(x)dx\n\n\u2248 5.43478\n\n19\n\n(143)\n\n(144)\n\n\fFigure 2: Performance comparison with N-dimensional Gaussian mixture likelihood function. (a)\ndimension study, (b) convergence rate, and (c) wall time vs MAE of integral. (a) varies from 2 to 16\ndimensions, (b) and (c) are 10 dimensional Gaussian mixture.\n\n6.1.5 Oscillatory function\n\nThe Oscillatory function in Figure 2 in the main paper is expressed as:\n\n\n\n\u2113true(x) = cos\n\n2\u03c0 + 5\n\n\n\nxi\n\n+ 1,\n\nx \u2208 R2\n\n2\n\n\ni=1\n\n\n\nZtrue =\n\n= 1\n\n\u2113true(x)\u03c0(x)dx\n\n(145)\n\n(146)\n\n(147)\n\n6.1.6 Additional experiments\n\nDimensional study in Gaussian mixture likelihood Figure 2(a) shows the dimension study of\nGaussian mixture likelihood. The BASQ and BQ are conditioned at the same time budget (200\nseconds). The higher dimension gives a more inaccurate estimation. From this result, we recommend\nusing BASQ with fewer than 16 dimensions.\n\nAblation study We investigated the influence of the approximation we adopted using 10 dimen-\nsional Gaussian mixture likelihood. The compared models are as follows:\n\n1. Exact sampler (without factorisation trick)\n\n2. Provable recombination (without LP solver)\n\nThe exact sampler without the factorisation trick is the one that exactly follows the Eqs. (8) - (10) of\nthe main paper. That is, the distribution of interest f (x) is the prior \u03c0(x). In addition, the kernel for\nthe acquisition function is an unwarped C L\ny , which is computationally expensive. Next, the provable\nrecombination algorithm is the one introduced in [19] with the best known computational complexity.\nAs explained in the Background section of the main paper, our BASQ implementation is based on an\n\n20\n\nBranin-HooTrue PosteriorBASQ-UB inferenceAckleyOscillatory\fFigure 3: Qualitative evaluation of posterior inference in synthetic problems\n\nLP solver (Gurobi [31] for this time) with empirically faster computational time. We compared the\ninfluence of these approximations.\n\nFigure 2(b) illustrates that these approximations are not affecting the convergence rate in the sample\nefficiency. However, when compared to the wall-clock time (Figure 2(c)), the exact sampler without\nthe factorisation trick is apparently slow to converge. Moreover, the provable recombination algorithm\nis slower than an LP solver implementation. Thus, the number of samples the provable recombination\nalgorithm per wall time is much smaller than the LP solver. Therefore, our BASQ standard solver\ndelivers solid empirical performance.\n\nQualitative evaluation of posterior inference Figure 3 shows the qualitative evaluation of joint\nposterior inference after 200 seconds passed against the analytical true posterior. The estimated\nposterior shape is exactly the same as the ground truth.\n\n6.2 Real-world problems\n\n6.2.1 Battery simulator\n\nBackground Single Particle Model with electrolyte dynamics (SPMe) is a commonly-used lithium-\nion battery simulator to predict the voltage response at given excitation current time-series data.\nEstimating SPMe parameters from observations are well known for ill-conditioned problem because\nthis model is overparameterised [5]. In the physical model, we need to separate the anode and cathode\ninternal states to represent actual cell components. However, when it comes to predicting the voltage\nresponse, this separation into two components is redundant. Except for extreme conditions such\nas low temperature, most voltage responses can be expressed with a single component. Therefore,\nthe parameters of cathode and anode often have a perfect negative correlation, meaning an arbitrary\ncombination of cathode and anode parameters can reconstruct the exactly same voltage profile. As\nsuch, point estimation means nothing in these cases. Bayesian inference can capture this negative\ncorrelation as covariance. Therefore, Bayesian inference is a natural choice for parameter estimation\nin the battery simulator. Moreover, there are many plausible battery simulators with differing levels\nof approximation. Selecting the model satisfying both predictability and a minimal number of\nparameters is crucial for faster calculation, particularly in setting up the control simulator. Therefore,\nBayesian model selection with model evidence is essential. The experimental setup is basically\nfollowing [2].\n\nProblem setting We wish to infer the posterior distribution of 3 simulation parameters\n(Dn, Dp, \u03c3n), where Dn is the diffusivity on anode, Dp is the diffusivity on cathode, \u03c3n is the\n\n21\n\ndimensionLog MAE of integralBASQ-UBbatch WSABI\fnoise variance of the observed data. We have the observed time-series voltage y and exciation profiles\ni as training dataset.\n\nThe parameter inference is modelled as follows:\n\ny\u2217 = Sim(x\u2217, i\u2217)\n\n\u03c0(x\u2217) = Lognormal(x\u2217; \u00b5\u03c0, \u03a3\u03c0)\n\n\u2113true(x\u2217) = N [Sim(x\u2217, i); y, \u03c3nI]\n\n(148)\n(149)\n(150)\n\nwhere\n\u00b5\u03c0 = [1.38, 0, \u221220.25]\n\u03a3\u03c0 = diag([0.03, 0.001, 0.001])\nin the logarithmic space.\n\nParameters The observed data y and i are generated by the simulator with multiharmonic sinusoidal\nexcitation current defined as:\n\ni = 0.132671 [sin(1/5\u03c0t) + sin(2\u03c0t) + sin(20\u03c0t) + sin(200\u03c0t)]\ny = Sim(xtrue, i) +\n\n\u03c3nU[0, 1]\n\n\u221a\n\n(151)\n\n(152)\n\nwhere\nt is discretised for 10 seconds with the sampling rate of 0.00025 seconds, resulting in 40,000 data\npoints.\nxtrue = [ exp(1.361) \u00d7 10\u221214, exp(0) \u00d7 10\u221213, exp(\u221220.25) \u00d7 10\u221210 ]\n\nMetric The posterior distribution is evaluated via RMSE between true and inferred conditional\nposterior on each parameter. The RMSE is calculated on 50 grid samples for each dimension so as to\nslice the maximum value of the joint posterior. Each 50 grid samples are equally-spaced and bounded\nwith the following boundaries:\nbounds = [1.1, 1.7], [\u22120.075, 0.08], [\u221220.3, \u221220.2]\nwhere the boundaries are given by [lower, upper] in the logarithmic space.\n\n6.2.2 Phase-field model\n\nBackground The PFM is a flexible time-evolving interfacial physical model that can easily in-\ncorporate the multi-physical energy [13]. In this dataset, the PFM is applied to the simulation of\nspinodal decomposition, which is the self-organised nanostructure in the bistable Fe-Cr alloy at high\ntemperatures. Spinodal decomposition is an inherently stochastic process, making characterisation\nchallenging [14]. Therefore, Bayesian model selection is promising for estimating its parameter and\ndetermining the model physics component.\n\nProblem setting We wish to infer the posterior distribution of 4 simulation parameters\n(T, LcT , nB, Lg), where T is the temperature, LcT is the interaction parameter that defines the\ninteraction between composition and temperature, nB is the number of Bohr magnetons per atom,\nand Lg is the gradient energy coefficient. We have the observed time-series 2-dimensional images y.\n\nThe parameter inference is modelled as follows:\n\ny\u2217 = Sim(x\u2217)\n\n\u03c0(x\u2217) = Lognormal(x\u2217; \u00b5\u03c0, \u03a3\u03c0)\n\n\u2113true(x\u2217) = N [Sim(x\u2217); y, \u03c3nI]\n\n(153)\n(154)\n(155)\n\nwhere\n\u03c3n = 10\u22124\n\u00b5\u03c0 = [1.91, 0.718, 0.798, 0.693]\n\u03a3\u03c0 = diag([0.0003, 0.00006, 0.0001, 0.0001])\nin the logarithmic space.\n\n22\n\n\fParameters The observed data y is generated by the simulator defined as:\n\ny = Sim(xtrue) +\n\n\u221a\n\n\u03c3nU[0, 1]\n\n(156)\n\nwhere\ny is discretised in both spatially and time-domain. Time domain is discretised for 5000 seconds with\nthe sampling rate of 1 seconds, resulting in 5,000 data points. 2-dimensional space is discretised for\n64 \u00d7 64 nm2, with 64 \u00d7 64 nm2 pixels. The total data points are 64 \u00d7 64 \u00d7 5, 000 = 20, 480, 000.\nxtrue = [ exp(1.90657514) \u00d7 102, exp(0.71783979) \u00d7 104, exp(0.7975072), exp(0.69314718) \u00d7\n10\u221215 ]\n\nMetric The posterior distribution is evaluated via RMSE between true and inferred conditional\nposterior on each parameter. The RMSE is calculated on 50 grid samples for each dimension so as to\nslice the maximum value of the joint posterior. Each 50 grid samples are equally-spaced and bounded\nwith the following boundaries:\nbounds = [1.87, 1.94], [0.69, 0.73], [0.77, 0.83], [0.68, 0.73]\nwhere the boundaries are given by [lower, upper] in the logarithmic space.\n\n6.2.3 Hyperparameter marginalisation of hierarchical GP\n\nBackground The hierarchical GP model was designed for analysing the large-scale battery time-\nseries dataset from solar off-grid system field data all over the African continent [1]. The field data\ncontains the information of time-series operating conditions (I, T, V ), where I is the excitation\ncurrent, T is the temperature, and V is the voltage. We wish to estimate the state of health (SoH)\nfrom these field data, achieving the preventive battery replacement before it fails for the convenience\nof those who rely on the power system for their living. However, estimating the state of health is\nchallenging because the raw data (I, T, V ) is not correlated to the battery health. There are several\ndefinitions of SoH, but the internal resistance of a battery R is adopted in [1]. In the usual circuit\nelement, resistance can be easily calculated from R = V /I via Ohm\u2019s law. However, the battery\ninternal resistance R is way more complex. Battery internal resistance R is a function of (t, I, T, c),\nwhere t is time, c is the acid concentration. Furthermore, there are two factors of resistance variation;\nionic polarisation and aging. To incorporate these physical insights to the machine learning model, [1]\nis adopted the hierarchical GP model. First, they adopted the additive kernel of a squared exponential\nkernel and a Wiener velocity kernel to divide the ionic polarisation effect and aging effect. Second,\nthey adopted the hierarchical GPs to model V to divide into R-dependent GP and non-R-dependent\nGP to incorporate the Open Circuit Voltage-State of Charge (OCV-SOC) relationship.\n\nProblem setting We wish to infer the hyperposterior distribution of 5 GP hyperparameters\n(lT , lI , lc, \u03c30, \u03c31), where lT , lI , lc are the a squared exponential kernel lengthscale of temperature T ,\ncurrent I, and acid concentration c, respectively, and \u03c30, \u03c31 are the kernel variances of a squared\nexponential kernel and a Wiener velocity kernel, respectively. We have the observed time-series\ndataset of (I, T, V ) as y.\n\nThe hyperposterior inference is based on the energy function \u03a6(x) (The details can be found in [1],\nEquation (15) in the Appendix information).\n\n\u03a6x = \u2212 log p(y|x) \u2212 log p(x)\n\n= \u2212 log p(x) +\n\n1\n2\n\n\n\nt\n\nlog |St(x)| +\n\n1\n2\n\n\n\nt\n\nt St(x)\u22121et +\neT\n\n\n\nt\n\nnt\n2\n\nlog 2\u03c0\n\n(157)\n\n(158)\n\nwhere\np(x) = Lognormal(x\u2217; \u00b5\u03c0, \u03a3\u03c0) is a hyperprior.\net is the error vector for each charging segment.\nnt is the number of observations in the charging segment.\nSt(x) is the innovation covariance for the segment.\n\u00b5\u03c0 = [3.96, 1.94, 2.79, 2.26, 0.34]\n\u03a3\u03c0 = diag([1, 1, 1, 1, 1])\nin the logarithmic space.\n\n23\n\n\fMetric The posterior distribution is evaluated via RMSE between true and inferred conditional\nposterior on each parameter. The RMSE is calculated on 50 grid samples for each dimension so as to\nslice the maximum value of the joint posterior. Each 50 grid samples are equally-spaced and bounded\nwith the following boundaries:\nbounds = [\u221210, 10], [\u221210, 10], [\u221210, 10], [\u221210, 10], [\u221210, 10]\nwhere the boundaries are given by [lower, upper] in the logarithmic space.\n\n7 Technical details; Q & A\n\nQ1: How does BASQ enable RCHQ to perform the batch selection? A1: The trick that achieves\nparallelisation is the alternately subsampling in section 4.1, not RCHQ itself. While BQ aims to\ncalculate the target integral Z =  \u2113true(x)\u03c0(x)dx, RCHQ over a single iteration aims to calculate\nthe empirical integral Z =  \u2113(x)\u03c0emp(x)dx over empirical measure defined by N subsamples\nXrec. At each iteration, we greedily select the batch candidates via RCHQ that can minimise the\nintegral variance over the current empirical measure. As we gather more observation data points and\nupdate the kernel (GP), the above two integrals approach the same. In other words, any KQ method,\nincluding kernel herding, can be applied to the batch selection via this alternately subsampling\nscheme. Secondly, such a dual quadrature scheme tends to be computationally demanding, but\ntractable computation and superb sample efficiency of RCHQ permit scalable batch parallelisation.\n\nQ2: Why does RCHQ outperform the kernel herding? A2: The reason why RCHQ converges\nfaster than herding is that RCHQ exploits more information than herding. While herding greedily\noptimises sequentially, RCHQ explicitly exploits the information of the spectral decay of the kernel\nand the probability distribution, both of which herding neglects. Exploiting the spectral decay\ncorresponds to capturing the approximately finite dimensionality of the kernel. RCHQ adopts the\nNystr\u00f6m method for its approximation. This convergence rate superiority can be confirmed in figure\n2(a) in [11]. (\"N. + emp + opt\" refers to RCHQ.) While RCHQ exponentially decays, herding does\nnot show such fast decay in the Gaussian setting. Therefore, BQ with RCHQ can converge faster\nthan BQ with kernel herding, allowing scalable and sample-efficient batch selection.\n\nQ3: Are there some potential areas, if any, where the proposed method performs worse than\nexisting ones? A3: Probably yes, there is. The advantage of herding over RCHQ is the computation\ncost. In the small batch size setting, the difference in the level of convergence between herding and\nRCHQ is much smaller than in the large batch size n. Therefore, herding might perform better than\nRCHQ in the small batch with a very cheap likelihood case as herding might earn more samples than\nRCHQ. The comparison against other KQ methods is summarised in table 1 in [11]. RCHQ gives a\nsmall theoretical bound of the worst-case error with tractable computation cost compared to herding,\nDPP, CVS, and vanilla BQ.\n\nQ4: What are the pros and cons of RCHQ over the Determinantal Point Process (DPP)? DPP\nconsiders the correlation correctly, whereas RCHQ assumes i.i.d. However, DPP requires prohibitive\ncomputation. Table 1 in [11] compares DPP-based KQ [4] and RCHQ (\"N. + empirical\" refers to\nRCHQ), which clearly shows that RCHQ provides not only tractable computation but also competitive\ntheoretical bound of worst-case error with mathematical proof.\n\nQ5: Why Nystr\u00f6m? Other low-rank approximation possibilities? A5: Because Nystr\u00f6m is\nadvantageous to derive convergence based on spectral decay asymptotically and theoretically. The\nonly requirement for the RCHQ is a finite sequence of good test functions, so finite dimensional\napproximations such as random Fourier features can also be used.\n\nQ6: Theorem 1 does not apply to WSABI-transformed BASQ but to a variant which uses vanilla\nBQ. Is that correct? A6: Yes. Theorem 1 is under the assumptions which the BQ is modelled\nwith vanilla BQ, without batch and hyperparameter updates. However, if we accept the linearisation\nof WSABI-L and assume that the \u2113(x) is (approximately) in the GP over the current iteration, the\ntheoretical analysis is correct.\n\n24\n\n\fFigure 4: Performance comparison with 1-dimensional Gaussian mixture likelihood function.\n\nQ7: Why is the coefficient 0.8 used in the definition of alpha? A7: We inherit the coefficient of\n0.8 from the original paper [10]. However, they said the performance is insensitive to the choice of\nthis coefficient, so it is not limited to 0.8 in general.\n\nQ8: Can we apply WSABI for negative integral? A8: Yes. \u03b1 can take negative value, so WSABI\ntransformation can be applied to negative integral case when we apply WSABI to general function\nintegration.\n\n7.1 Detailed description of the difference between RCHQ and batch WSABI\n\nFigure 4 shows the performance comparison between the kernel recombination (RCHQ) and the\nuncertainty sampling (batch WSABI). Firstly, the true likelihood \u2113true was modelled with the mixture\nof one-dimensional Gaussians, which was generated under the procedure described in footnote 4 in\nthe main paper. Prior \u03c0(x) is a broader one-dimensional Gaussian, and the posterior p(x) is also\nthe mixture of Gaussians, thanks to the Gaussianity. While we can access the information of the\nprior distribution function \u03c0(x), we cannot access the ones of the posterior p(x) or true likelihood\nfunction \u2113true. However, we can query the true likelihood value at a given location \u2113true(Xquad) with\na large batch (n = 16 in this case). Now, we have four observations n = 4 with black dots. We\nhave constructed the WSABI-L GPs with the given four observations (X, y). The mean dotted line\nrepresents the mean of posterior predictive distributions mL\ny (x), the blue shaded area shows the mean\n\u00b1 corrected variance C L\ny (x, x)\u03c0(x). A myriad of blue lines represents the functions sampled from\ny (x), C L\nGP \u2113 \u223c GP(\u2113; mL\ny (x, x)). The above problem setting is shared with both algorithms.\n\nOn the one hand, batch WSABI adopts local penalisation with multi-start optimisation. The\nacquisition function A(x) for batch WSABI is the uncertainty sampling Var[\u2113(x)\u03c0(x)] =\n\u03c0(x)2Cy(x)my(x)2] as shown in purple dotted line. We can see four peaks corresponding to the\npositions of larger variance in WSABI-L GP. Multi-start optimisation generates 100 random samples\nfrom prior as multi-starting points, then run a gradient-based optimisation algorithm (L-BFGS) to\nfind the maxima. Then, we take the largest point amongst the solutions. After taking the largest point,\n\n25\n\ntrue likelihoodpriorposteriorGP-modelledlikelihoodsamplingfunctionupdatedGPsubsamplesx  ~ g(x)RecombinationLog MAEsparse-1.82-5.42truerLog MAEacquisitionfunction A(x)denseUncertainty Sampling\fwe locally penalise the point with Lipschitz Cone. Then the largest peak is split into two peaks with\nsmaller heights. Then, the multi-start optimisation will be applied again to find the next maxima. We\nwill iterate this greedy selection for n = 16. The selected points are depicted with red dots. The\nWSABI-L GPs are updated with the given 16 observations. As we can see, the selected 16 batch\ncandidates are concentrated around the largest peak in the acquisition function, resulting in a higher\nintegration error. This is mainly because the local penalisation tends to aggregate around the large\npeak, where the newly generated penalised peaks still have significant heights. When compared the\nacquisition function A(x) with the true posterior p(x), we can find that the peak positions between\nA(x) and p(x) are not so correlated. In other words, local penalisation trusts the acquisition function\ntoo much, although the early-stage acquisition function A(x) is not such a reliable information source.\nMoreover, the multi-start optimisation requires the optimisation loop per the number of seeds, which\nis computationally demanding. In addition, the possibility of finding the global optima becomes\nexponentially lower when the dimension is scaled up. Thus, the multi-start optimisation requires\nexponentially increasing the number of random seeds, although there are still no guarantees to find\nthe global maxima of acquisition function A(x). Therefore, batch WSABI is slow and inefficient in\nselecting the batch candidates.\n\nOn the other hand, RCHQ using the linearised IVR proposal distribution g(x) = (1\u2212r)\u03c0(x)+rA(x).\nThis is mixed with the prior and GP variance, so it is less dependent on the early-stage A(x). The\nsubsampled histogram depicted with blue bars has similar peaks with A(x), but still, there is room\nfor the possibility of selecting other regions. Then, RCHQ constructs the empirical measure based\non these N subsamples and resamples M samples for the Nystr\u00f6m method to construct the finite\ntest functions. The test functions are applied to construct the metric to evaluate the worst-case error.\nThen, RCHQ selects n batch candidates to minimise the worst-case error over the empirical measure\nwith the kernel recombination. As we can see, the selected 16 batch candidates are sparser and\nwell-captured the true likelihood peaks than local penalisation, resulting in a smaller integration error.\nSuch subsampling is done faster than multi-start optimisation thanks to the efficient sampler, and\nrecombination is also tractable with single LP solver iteration. As such, the RCHQ can select sparser\ncandidates than local penalisation within more tractable computation time.\n\nReferences\n\n[1] A. Aitio and D. A. Howey. Predicting battery end of life from solar off-grid system field data\n\nusing machine learning. Joule, 5(12):3204\u20133220, 2021.\n\n[2] A. Aitio, S. G. Marquis, P. Ascencio, and D. A. Howey. Bayesian parameter estimation applied\nto the Li-ion battery single particle model with electrolyte dynamics. IFAC-PapersOnLine,\n53(2):12497\u201312504, 2020.\n\n[3] F. Bach. On the equivalence between kernel quadrature rules and random feature expansions.\n\nJournal of Machine Learning Research, 18:714, 2017.\n\n[4] A. Belhadji, R. Bardenet, and P. Chainais.\n\nInternational Conference on Neural\n2019.\n7012ef0335aa2adbab58bd6d0702ba41-Paper.pdf.\n\nURL:\n\nIn\n(NeurIPS),\nInformation Processing Systems\nhttps://proceedings.neurips.cc/paper/2019/file/\n\nKernel quadrature with DPPs.\n\n[5] A. M. Bizeray, J. H. Kim, S. R. Duncan, and D. A. Howey. Identifiability and parameter estima-\ntion of the single particle lithium-ion battery model. IEEE Transactions on Control Systems Tech-\nnology, 27(5):1862\u20131877, 2019. URL: https://doi.org/10.1109/TCST.2018.2838097.\n\n[6] F.-X. Briol, C. J. Oates, M. Girolami, and M. A. Osborne. Frank-Wolfe Bayesian quadrature:\nProbabilistic integration with theoretical guarantees. In International Conference on Neural\nInformation Processing Systems (NeurIPS), 2015. URL: https://proceedings.neurips.\ncc/paper/2015/file/ba3866600c3540f67c1e9575e213be0a-Paper.pdf.\n\n[7] F.-X. Briol, C. J. Oates, M. Girolami, M. A. Osborne, and D. Sejdinovic. Probabilistic integra-\n\ntion: a role in statistical computation? Statistical Science, 34(1):1\u201322, 2019.\n\n[8] Gregory E. Fasshauer and Michael J. McCourt. Stable evaluation of Gaussian radial basis\nfunction interpolants. SIAM Journal on Scientific Computing, 34(2):A737\u2013A762, 2012. URL:\nhttps://doi.org/10.1137/110824784.\n\n26\n\n\f[9] Arthur Gretton, Karsten Borgwardt, Malte Rasch, Bernhard Sch\u00f6lkopf, and Alex Smola. A\nkernel method for the two-sample-problem. Advances in neural information processing systems,\n19, 2006.\n\n[10] T. Gunter, M. A. Osborne, R. Garnett, P. Hennig, and S. J. Roberts. Sampling for inference\nin probabilistic models with fast Bayesian quadrature. In International Conference on Neural\nInformation Processing Systems (NeurIPS), 2014. URL: https://proceedings.neurips.\ncc/paper/2014/file/e94f63f579e05cb49c05c2d050ead9c0-Paper.pdf.\n\n[11] S. Hayakawa, H. Oberhauser, and T. Lyons. Positively weighted kernel quadrature via subsam-\npling. In International Conference on Neural Information Processing Systems (NeurIPS), 2022.\ndoi:10.48550/arXiv.2107.09597.\n\n[12] F. Hutter, H. Hoos, and K. Leyton-Brown. An efficient approach for assessing hyperparameter\nimportance. In International Conference on Machine Learning (ICML), pages 754\u2013762, 2014.\n\n[13] S. G. Kim, W. T. Kim, and T. Suzuki. Phase-field model for binary alloys. Physical review e,\n\n60(6):7186, 1999.\n\n[14] Y. Matsuura, Y. Tsukada, and T. Koyama. Adjoint model for estimating material parameters\nbased on microstructure evolution during spinodal decomposition. Physical Review Materials,\n5(11):113801, 2021.\n\n[15] C. J. Oates, M. Girolami, and N. Chopin. Control functionals for Monte Carlo integration.\nJournal of the Royal Statistical Society: Series B (Statistical Methodology), 79(3):695\u2013718,\n2017.\n\n[16] C. J. Oates, T. Papamarkou, and M. Girolami. The controlled thermodynamic integral\nfor bayesian model evidence evaluation. Journal of the American Statistical Association,\n111(514):634\u2013645, 2016.\n\n[17] M. A. Osborne, D. Duvenaud, R. Garnett, C. Rasmussen, S. Roberts, and Z. Ghahramani. Active\nlearning of model evidence using Bayesian quadrature. In International Conference on Neural\nInformation Processing Systems (NeurIPS), 2012. URL: https://proceedings.neurips.\ncc/paper/2012/file/6364d3f0f495b6ab9dcf8d3b5c6e0b01-Paper.pdf.\n\n[18] Bharath K Sriperumbudur, Arthur Gretton, Kenji Fukumizu, Bernhard Sch\u00f6lkopf, and Gert RG\nLanckriet. Hilbert space embeddings and metrics on probability measures. The Journal of\nMachine Learning Research, 11:1517\u20131561, 2010.\n\n[19] M. Tchernychova. Carath\u00e9odory cubature measures. PhD thesis, University of Oxford, 2015.\n\n[20] G. Wahba. Spline models for observational data. SIAM, 1990.\n\n27\n\n\f", "appendix 1": "\n\n1.1 Proof of Theorem 1\n\nWe provide the proof of the following theorem given in the main text.\nTheorem 1. Suppose (cid:82) (cid:112)K(x, x)f (x) dx < \u221e, \u2113 \u223c GP(m, K), and we are given an (n \u2212 1)-\ndimensional kernel K0 such that K1 := K \u2212 K0 is also a kernel. Let (f, g) be a density pair\nwith weight \u03bb. Let xrec be an N -point independent sample from g and wrec := \u03bb(xrec). Then, if\n(wquad, xquad) is a proper recombination of (wrec, xrec) for K0, it satisfies\n\n(cid:20)(cid:113)\n\nExrec\n\n(cid:21)\n\nvar[Zf | xquad]\n\n\u2264 2\n\n(cid:18)(cid:90)\n\nK1(x, x)f (x) dx\n\n+\n\n(cid:19)1/2\n\n(cid:114)\n\nCK,f,g\nN\n\n(1)\n\nwhere Zf := (cid:82) \u2113(x)f (x) dx and CK,f,g := (cid:82) K(x, x)\u03bb(x)f (x) dx \u2212 (cid:82)(cid:82) K(x, y)f (x)f (y) dx dy.\nRecall H is the RKHS given by the kernel K. As the kernel satisfies (cid:82) (cid:112)K(x, x)f (x) dx < \u221e, the\nmean embedding\n\n(cid:90)\n\n\u00b5K(f ) :=\n\nf (x)K(x, \u00b7) dx\n\n(2)\n\nis a well-defined element of H. We first discuss its approximation via importance sampling.\nLemma 1. Let f be a probability density on Rd and g be another density such that f = \u03bbg with a\nnonnegative function \u03bb. Let xrec be an N -point independent sample from g and wrec = \u03bb(xrec) be the\nweights. If we define \u00b5r := 1\n\nN w\u22a4\n\nrecK(xrec, \u00b7) then it satisfies\nE[\u2225\u00b5K(f ) \u2212 \u00b5r\u22252\nH] = 1\n\nN CK,f,g\n\nwhere CK,f,g = (cid:82) K(x, x)\u03bb(x)f (x) dx \u2212 (cid:82)(cid:82) K(x, y)f (x)f (y) dx dy\nFurthermore, the choice g(x) \u221d (cid:112)K(x, x)f (x) minimises CK,f,g, if \u03bb = K(x, x)\u22121/2 is well-\ndefined.\n\nProof. Let xrec = (X1, . . . , XN ), so \u00b5r = 1\nN\n\n(cid:80)N\n\ni=1 \u03bb(Xi)K(Xi, \u00b7). From (2), we have\n\n\u2225\u00b5K(f ) \u2212 \u00b5r\u22252\n\nH = \u2225\u00b5K(f )\u22252\n(cid:90) (cid:90)\n\nH \u2212 2\u27e8\u00b5K(f ), \u00b5r\u27e9H + \u2225\u00b5r\u22252\nH\nN\n(cid:90)\n(cid:88)\n\nK(x, y)f (x)f (y) dx dy \u2212\n\n=\n\n(4)\n\nK(x, Xi)f (x)\u03bb(Xi) dx\n\n(5)\n\n2\nN\n\ni=1\n\n+\n\n1\nN 2\n\nN\n(cid:88)\n\ni,j=1\n\nK(Xi, Xj)\u03bb(Xi)\u03bb(Xj).\n\n(6)\n\nWe have\n(cid:20)(cid:90)\n\nE\n\nK(x, Xi)f (x)\u03bb(Xi) dx\n\n(cid:90) (cid:90)\n\n(cid:21)\n\n=\n\nK(x, y)f (x)\u03bb(y)g(y) dx dy =\n\nand for i \u0338= j\n\nE[K(Xi, Xj)\u03bb(Xi)\u03bb(Xj)] =\n\n(cid:90) (cid:90)\n\nK(x, y)\u03bb(x)\u03bb(y)g(x)g(y) dx dy =\n\n1\n\n(cid:90) (cid:90)\n\n(cid:90) (cid:90)\n\nK(x, y)f (x)f (y) dx dy\n\n(7)\n\nK(x, y)f (x)f (y) dx dy,\n\n(8)\n\n\fso we in total have\n\nE[\u2225\u00b5K(f ) \u2212 \u00b5r\u22252\n\nH] =\n\n1\nN 2\n\nN\n(cid:88)\n\ni=1\n(cid:18)(cid:90)\n\n=\n\n1\nN\n\nE[K(Xi, Xi)\u03bb(Xi)2] \u2212\n\nK(x, x)\u03bb(x)f (x) dx \u2212\n\n1\nN\n(cid:90) (cid:90)\n\n(cid:90) (cid:90)\n\nK(x, y)f (x)f (y) dx dy\n\n(9)\n\nK(x, y)f (x)f (y) dx dy\n\n=\n\n(cid:19)\n\nCK,f,g\nN\n\n.\n\n(10)\n\nWe next show the optimality of g(x) \u2248 (cid:112)K(x, x)f (x).\nIt suffices to consider when\n(cid:82) K(x, x)\u03bb(x)f (x) dx is minimised as the second term is independent of g. From the Cauchy-\nSchwarz, we have\n\n(cid:90)\n\nK(x, x)\u03bb(x)f (x) dx =\n\n(cid:90)\n\nK(x, x)\u03bb(x)f (x) dx\n\n(cid:90) f (x)\n\u03bb(x)\n\n(cid:18)(cid:90) (cid:112)K(x, x)f (x) dx\n\n(cid:19)2\n\n,\n\ndx \u2265\n\nand the equality is satisfied if g(x) = f (x)\n\n\u03bb(x) \u221d (cid:112)K(x, x)f (x).\n\n(11)\n\nProof of Theorem 1. Let (wquad, xquad) be a proper recombination of (wrec, xrec), and let Qn be the\nquadrature formula given by points xquad and weights 1\nquadh(xquad). We\nalso define \u00b5n := 1\nquadK(xquad, \u00b7).\n\nN wquad, i.e, Q(h) := 1\n\nN w\u22a4\n\nN w\u22a4\n\nA well-known fact is that the worst-case error of Qn (with respect to f here) wce(Qn) =\nsup\u2225h\u2225\u22641|Qn(h) \u2212 (cid:82) h(x)f (x) dx| satisfies wce(Qn) = \u2225\u00b5K(f ) \u2212 \u00b5n\u2225H for a kernel satisfying\n(cid:82) (cid:112)K(x, x)f (x) dx < \u221e [9, 18]. By using this and the relation between Bayesian quadrature and\nkernel quadrature in the main text, we have\n\n(cid:113)\n\nvar[Zf | xquad] \u2264 wce(Qn) \u2264 \u2225\u00b5K(f ) \u2212 \u00b5r\u2225H + \u2225\u00b5r \u2212 \u00b5n\u2225H\n\nFrom Lemma 1 we have E[\u2225\u00b5K(f ) \u2212 \u00b5r\u2225H] \u2264 E[\u2225\u00b5K(f ) \u2212 \u00b5r\u22252\nsuffices to show\n\nExrec[\u2225\u00b5r \u2212 \u00b5n\u2225H] \u2264 2\n\nK1(x, x)f (x) dx\n\n.\n\n(13)\n\n(cid:18)(cid:90)\n\n(cid:19)1/2\n\n(12)\nH]1/2 = (cid:112)CK,f,g/N , so it now\n\nWe first have\n\n\u2225\u00b5r \u2212 \u00b5n\u22252\n\nH =\n\n1\nN 2\n\n(cid:0)w\u22a4\n\nrecK(xrec, xrec)wrec \u2212 2 w\u22a4\n\nrecK(xrec, xquad)wquad + w\u22a4\n\n(cid:1) ,\nquadK(xquad, xquad)wquad\n(14)\n\nand from the recombination property we also have\n\nw\u22a4\n\nrecK0(xrec, xrec)wrec \u2212 2 w\u22a4\n\nrecK0(xrec, xquad)wquad + w\u22a4\n\nquadK0(xquad, xquad)wquad = 0,\n\n(15)\n\nwhich follows from the fact that (wrec, xrec) and (wquad, xquad) give the same kernel embedding for\nthe RKHS given by K0 as the latter is a recombination of the former (see e.g. [11, Eq. 14]). By\nsubtracting, we obtain\n\n\u2225\u00b5r \u2212 \u00b5n\u22252\nH\n1\n(cid:0)w\u22a4\nN 2\n= \u2225\u00b5(1)\n\n=\n\nr \u2212 \u00b5(1)\n\nn \u22252\nH1\nwhere H1 is the RKHS given by K1 and\n\n,\n\nrecK1(xrec, xrec)wrec \u2212 2 w\u22a4\n\nrecK1(xrec, xquad)wquad + w\u22a4\n\nquadK1(xquad, xquad)wquad\n\n\u00b5(1)\nr\n\n:=\n\n1\nN\n\nw\u22a4\n\nrecK1(xrec, \u00b7),\n\n\u00b5(1)\n\nn :=\n\n1\nN\n\nw\u22a4\n\nquadK1(xquad, \u00b7).\n\n(cid:1)\n\n(16)\n\n(17)\n\n(18)\n\n(19)\n\nNow, by letting k1/2\n\n(x) := (cid:112)K(x, x), we have \u2225K1(x, \u00b7)\u2225H1 = k1/2\n\n1\n\n(x) for a point x. So we have\n\n\u2225\u00b5(1)\n\nr \u2225H1 \u2264\n\n1\n1\nN\n\nreck1/2\nw\u22a4\n\n1\n\n(xrec),\n\n\u2225\u00b5(1)\n\nn \u2225H1 \u2264\n\n1\nN\n\nquadk1/2\nw\u22a4\n\n1\n\n(xquad) \u2264\n\n1\nN\n\nreck1/2\nw\u22a4\n\n1\n\n(xrec),\n\n(20)\n\n2\n\n\fwhere the last inequality follows from the assumption that (wquad, xquad) is a proper recombination of\n(wrec, xrec). Therefore, we have the estimate\n\n\u2225\u00b5r \u2212 \u00b5n\u2225H = \u2225\u00b5(1)\n\nr \u2212 \u00b5(1)\n\nn \u2225H1 \u2264 \u2225\u00b5(1)\n\nr \u2225H1 + \u2225\u00b5(1)\n\nn \u2225H1 \u2264\n\n2\nN\n\nreck1/2\nw\u22a4\n\n1\n\n(xrec).\n\n(21)\n\nFinally, to prove (13), we recall that xrec is an N -point independent sample from g and wrec = \u03bb(xrec),\nso we obtain\n\nExrec [\u2225\u00b5r \u2212 \u00b5n\u2225H] \u2264 2 Exrec\n\n(cid:20) 1\nN\n\n(cid:21)\n(xrec)\n\n1\n\nreck1/2\nw\u22a4\n(cid:90) (cid:112)K1(x, x)f (x) dx \u2264 2\n\n= 2\n\n= 2\n\n(cid:90)\n\n\u03bb(x)k1/2\n\n1\n\n(x)g(x) dx\n\n(22)\n\n(cid:18)(cid:90)\n\nK1(x, x)f (x) dx\n\n,\n\n(23)\n\n(cid:19)1/2\n\nwhere we have used Cauchy\u2013Schwarz in the last inequality.\n\n1.2 Eigenvalue dacay of integral operators\n\nLet us consider the integral operator\n\n(cid:90)\n\nh (cid:55)\u2192\n\nK(\u00b7, y)h(y)f (y) dy\n\n(24)\n\nL2(f ) := (cid:82) \u02dch(x)2f (x) dx < \u221e}, and let \u03c31 \u2265 \u03c32 \u2265\nwhere h \u2208 L2(f ) := {\u02dch | measurable, \u2225\u02dch\u22252\n\u00b7 \u00b7 \u00b7 \u2265 0 be eigenvalues of this operator. This sequence of eigenvalues is known to be closely related\nto the convergence rate of kernel quadrature [3].\n\nFor the Nystr\u00f6m approximation, we have the following estimate represented by the eigenvalues:\nTheorem 2 ([11]). For a probability density function f on Rd, let xnys be an M -point independent\nsample from f . Let K0 be the rank-(n \u2212 1) approximate kernel using xnys given by Eq. (10) in the\nmain text. Then, K1 := K \u2212 K0 satisfies\n\n(cid:90)\n\nK1(x, x)f (x) dx \u2264 n\u03c3n +\n\n\u221e\n(cid:88)\n\nm=n+1\n\n\u03c3n +\n\n2(n \u2212 1)Kmax\n\u221a\nM\n\n(cid:32)\n\n(cid:114)\n\n1 +\n\n2 log\n\n(cid:33)\n\n1\n\u03b4\n\n(25)\n\nwith probability at least 1 \u2212 \u03b4.\n\nThis gives a theoretical guarantee for one step of our algorithm, combined with Theorem 1.\n\nAlthough the sequence of eigenvalues \u03c3n does not have an obvious expression when K is the kernel\nin the middle of our algorithms BASQ, when K is a multivariate Gaussian (RBF) kernel and f is also\na Gaussian density, we have a concrete expression of eigenvalues [8].\nIndeed, if K(x, y) = exp(\u2212\u03f52|x \u2212 y|2) and f (x) \u221d exp(\u2212\u03b12|x|2), in the case d = 1, we have\n\u03c3n = abn for some constants a > 0 and 0 < b < 1 depending on \u03f5 and \u03b1. Thus, for the d-dimensional\ncase, we can roughly estimate that \u03c3n \u2264 adbm+1 if n > md. So, by only using n, we have\n\n\u03c3n \u2264 adb\u2308n1/d\u2309 \u2264 adb(n1/d) = ad exp(\u2212cn1/d),\n\n(26)\n\nfor c = log(1/b).\n\n", "appendix 6": "\n\n6.1 Synthetic problems\n\n6.1.1 Quadrature hyperparameters\n\nThe initial quadrature hyperparameters are as follows:\nA kernel length scale l = 2\nA kernel variance v\u2032 = 2\nRecombination sample size N = 20, 000\nNystr\u00f6m sample size M = N/100\nSupersample ratio rsuper = 100\nProposal distribution g(x) partition ratio r = 0.5\n\nThe supersample ratio rsuper is the ratio of supersamples for SMC sampling of acquisition function\nagainst the recombination sample size N .\n\nA kernel length scale and a kernel variance are important for selecting the samples in the first batch.\nNevertheless, these parameters are updated via type-II MLE optimisation after the second round.\nNystr\u00f6m sample size must be larger than the batch size n, and the recombination sample size is\npreferred to satisfy N \u226b M . Larger N and M give more accurate sample selection via kernel\nquadrature. However, larger subsamples result in a longer wall-time. We do not need to change the\nvalues as long as the integral converged to the designated criterion. When longer computational time\nis allowed, or likelihood is expensive enough to regard recombination time as negligible, larger N ,\nM will give us a faster convergence rate.\n\nThe partition ratio r is the only hyperparamter that affects the convergence sensitively. The optimal\nvalue depends the integrand and it is challenging to know the optimal value before running. As we\ny \u03c0(x) gives the optimal upper bound. r = 0.5 is a good approximation of\nderived in Lemma 1,\n(cid:9) \u03c0(x). Here,\nthis optimal proposal distribution: g(x) = (1 \u2212 r)\u03c0(x) + rC L\n\ny \u03c0(x) = (cid:8)(1 \u2212 r) + rC L\n\nC L\n\n(cid:113)\n\ny\n\n18\n\n\fthe linearisation gives the approximation\n\n(cid:113)\n\nC L\n\ny =\n\n(cid:113)\n\n1 + (C L\n\ny \u2212 1) \u2248 1 +\n\nCL\n\ny \u22121\n2 = 0.5 + 0.5C L\ny .\n\n(cid:113)\n\nC L\n\ny \u03c0(x). Thus, r = 0.5 is a safe choice.\n\nTherefore, (0.5 + 0.5C L\n\ny )\u03c0(x) \u2248\n\n6.1.2 Gaussian mixture\n\nThe likelihood function of the Gaussian mixture used in Figure 1 in the main paper is expressed as:\n\n\u2113true(x) =\n\nn\n(cid:88)\n\ni=1\n\nwiN (x; \u00b5i, \u03a3i)\n\nwi = N (\u00b5i; \u00b5\u03c0, \u03a3i + \u03a3\u03c0)\u22121\n\n(cid:90)\n\nZtrue =\n\n\u2113true(x)\u03c0(x)dx,\n\n(cid:90)\n\nwi\n\nN (x; \u00b5i, \u03a3i)N (x; \u00b5\u03c0, \u03a3\u03c0)dx\n\nwiN (\u00b5i; \u00b5\u03c0, \u03a3i + \u03a3\u03c0)\n\n=\n\n=\n\nn\n(cid:88)\n\ni=1\nn\n(cid:88)\n\ni=1\n\n= 1\n\nwhere\n\u00b5\u03c0 = 0\n\u03a3\u03c0 = 2I\n\u03c0(x) = N (x; \u00b5\u03c0, \u03a3\u03c0)\n\nThe prior is the same throughout the synthetic problems.\n\n6.1.3 Branin-Hoo function\n\nThe Branin-Hoo function in Figure 2 in the main paper is expressed as:\n\n2\n(cid:89)\n\n\u2113true(x) =\n\n(cid:2)sin(xi) + 1\n2 xi)2 + 3\n( 1\n\n2 cos(3xi)(cid:3)2\n\n10\n\n,\n\nx \u2208 R2\n\ni=1\n(cid:90)\n\nZtrue =\n\n\u2113true(x)\u03c0(x)dx\n\n= 0.9557282\n\u2248 0.913416\n\n(132)\n\n(133)\n\n(134)\n\n(135)\n\n(136)\n\n(137)\n\n(138)\n\n(139)\n\n(140)\n(141)\n\n6.1.4 Ackley function\n\nThe Ackley function in Figure 2 in the main paper is expressed as:\n\n\uf8eb\n\n\u2113true(x) = \u221220 exp\n\n\uf8ed\u2212\n\n(cid:118)\n(cid:117)\n(cid:117)\n(cid:116)\n\n1\n5\n\n1\n2\n\n2\n(cid:88)\n\ni=1\n\n\uf8f6\n\nx2\ni\n\n\uf8f8 + exp\n\n(cid:32)\n\n1\n2\n\n2\n(cid:88)\n\ni=1\n\n(cid:33)\n\ncos(2\u03c0xi)\n\n+ 20,\n\nx \u2208 R2\n\n(142)\n\n(cid:90)\n\nZtrue =\n\n\u2113true(x)\u03c0(x)dx\n\n\u2248 5.43478\n\n19\n\n(143)\n\n(144)\n\n\fFigure 2: Performance comparison with N-dimensional Gaussian mixture likelihood function. (a)\ndimension study, (b) convergence rate, and (c) wall time vs MAE of integral. (a) varies from 2 to 16\ndimensions, (b) and (c) are 10 dimensional Gaussian mixture.\n\n6.1.5 Oscillatory function\n\nThe Oscillatory function in Figure 2 in the main paper is expressed as:\n\n(cid:32)\n\n\u2113true(x) = cos\n\n2\u03c0 + 5\n\n(cid:33)\n\nxi\n\n+ 1,\n\nx \u2208 R2\n\n2\n(cid:88)\n\ni=1\n\n(cid:90)\n\nZtrue =\n\n= 1\n\n\u2113true(x)\u03c0(x)dx\n\n(145)\n\n(146)\n\n(147)\n\n6.1.6 Additional experiments\n\nDimensional study in Gaussian mixture likelihood Figure 2(a) shows the dimension study of\nGaussian mixture likelihood. The BASQ and BQ are conditioned at the same time budget (200\nseconds). The higher dimension gives a more inaccurate estimation. From this result, we recommend\nusing BASQ with fewer than 16 dimensions.\n\nAblation study We investigated the influence of the approximation we adopted using 10 dimen-\nsional Gaussian mixture likelihood. The compared models are as follows:\n\n1. Exact sampler (without factorisation trick)\n\n2. Provable recombination (without LP solver)\n\nThe exact sampler without the factorisation trick is the one that exactly follows the Eqs. (8) - (10) of\nthe main paper. That is, the distribution of interest f (x) is the prior \u03c0(x). In addition, the kernel for\nthe acquisition function is an unwarped C L\ny , which is computationally expensive. Next, the provable\nrecombination algorithm is the one introduced in [19] with the best known computational complexity.\nAs explained in the Background section of the main paper, our BASQ implementation is based on an\n\n20\n\nBranin-HooTrue PosteriorBASQ-UB inferenceAckleyOscillatory\fFigure 3: Qualitative evaluation of posterior inference in synthetic problems\n\nLP solver (Gurobi [31] for this time) with empirically faster computational time. We compared the\ninfluence of these approximations.\n\nFigure 2(b) illustrates that these approximations are not affecting the convergence rate in the sample\nefficiency. However, when compared to the wall-clock time (Figure 2(c)), the exact sampler without\nthe factorisation trick is apparently slow to converge. Moreover, the provable recombination algorithm\nis slower than an LP solver implementation. Thus, the number of samples the provable recombination\nalgorithm per wall time is much smaller than the LP solver. Therefore, our BASQ standard solver\ndelivers solid empirical performance.\n\nQualitative evaluation of posterior inference Figure 3 shows the qualitative evaluation of joint\nposterior inference after 200 seconds passed against the analytical true posterior. The estimated\nposterior shape is exactly the same as the ground truth.\n\n6.2 Real-world problems\n\n6.2.1 Battery simulator\n\nBackground Single Particle Model with electrolyte dynamics (SPMe) is a commonly-used lithium-\nion battery simulator to predict the voltage response at given excitation current time-series data.\nEstimating SPMe parameters from observations are well known for ill-conditioned problem because\nthis model is overparameterised [5]. In the physical model, we need to separate the anode and cathode\ninternal states to represent actual cell components. However, when it comes to predicting the voltage\nresponse, this separation into two components is redundant. Except for extreme conditions such\nas low temperature, most voltage responses can be expressed with a single component. Therefore,\nthe parameters of cathode and anode often have a perfect negative correlation, meaning an arbitrary\ncombination of cathode and anode parameters can reconstruct the exactly same voltage profile. As\nsuch, point estimation means nothing in these cases. Bayesian inference can capture this negative\ncorrelation as covariance. Therefore, Bayesian inference is a natural choice for parameter estimation\nin the battery simulator. Moreover, there are many plausible battery simulators with differing levels\nof approximation. Selecting the model satisfying both predictability and a minimal number of\nparameters is crucial for faster calculation, particularly in setting up the control simulator. Therefore,\nBayesian model selection with model evidence is essential. The experimental setup is basically\nfollowing [2].\n\nProblem setting We wish to infer the posterior distribution of 3 simulation parameters\n(Dn, Dp, \u03c3n), where Dn is the diffusivity on anode, Dp is the diffusivity on cathode, \u03c3n is the\n\n21\n\ndimensionLog MAE of integralBASQ-UBbatch WSABI\fnoise variance of the observed data. We have the observed time-series voltage y and exciation profiles\ni as training dataset.\n\nThe parameter inference is modelled as follows:\n\ny\u2217 = Sim(x\u2217, i\u2217)\n\n\u03c0(x\u2217) = Lognormal(x\u2217; \u00b5\u03c0, \u03a3\u03c0)\n\n\u2113true(x\u2217) = N [Sim(x\u2217, i); y, \u03c3nI]\n\n(148)\n(149)\n(150)\n\nwhere\n\u00b5\u03c0 = [1.38, 0, \u221220.25]\n\u03a3\u03c0 = diag([0.03, 0.001, 0.001])\nin the logarithmic space.\n\nParameters The observed data y and i are generated by the simulator with multiharmonic sinusoidal\nexcitation current defined as:\n\ni = 0.132671 [sin(1/5\u03c0t) + sin(2\u03c0t) + sin(20\u03c0t) + sin(200\u03c0t)]\ny = Sim(xtrue, i) +\n\n\u03c3nU[0, 1]\n\n\u221a\n\n(151)\n\n(152)\n\nwhere\nt is discretised for 10 seconds with the sampling rate of 0.00025 seconds, resulting in 40,000 data\npoints.\nxtrue = [ exp(1.361) \u00d7 10\u221214, exp(0) \u00d7 10\u221213, exp(\u221220.25) \u00d7 10\u221210 ]\n\nMetric The posterior distribution is evaluated via RMSE between true and inferred conditional\nposterior on each parameter. The RMSE is calculated on 50 grid samples for each dimension so as to\nslice the maximum value of the joint posterior. Each 50 grid samples are equally-spaced and bounded\nwith the following boundaries:\nbounds = [1.1, 1.7], [\u22120.075, 0.08], [\u221220.3, \u221220.2]\nwhere the boundaries are given by [lower, upper] in the logarithmic space.\n\n6.2.2 Phase-field model\n\nBackground The PFM is a flexible time-evolving interfacial physical model that can easily in-\ncorporate the multi-physical energy [13]. In this dataset, the PFM is applied to the simulation of\nspinodal decomposition, which is the self-organised nanostructure in the bistable Fe-Cr alloy at high\ntemperatures. Spinodal decomposition is an inherently stochastic process, making characterisation\nchallenging [14]. Therefore, Bayesian model selection is promising for estimating its parameter and\ndetermining the model physics component.\n\nProblem setting We wish to infer the posterior distribution of 4 simulation parameters\n(T, LcT , nB, Lg), where T is the temperature, LcT is the interaction parameter that defines the\ninteraction between composition and temperature, nB is the number of Bohr magnetons per atom,\nand Lg is the gradient energy coefficient. We have the observed time-series 2-dimensional images y.\n\nThe parameter inference is modelled as follows:\n\ny\u2217 = Sim(x\u2217)\n\n\u03c0(x\u2217) = Lognormal(x\u2217; \u00b5\u03c0, \u03a3\u03c0)\n\n\u2113true(x\u2217) = N [Sim(x\u2217); y, \u03c3nI]\n\n(153)\n(154)\n(155)\n\nwhere\n\u03c3n = 10\u22124\n\u00b5\u03c0 = [1.91, 0.718, 0.798, 0.693]\n\u03a3\u03c0 = diag([0.0003, 0.00006, 0.0001, 0.0001])\nin the logarithmic space.\n\n22\n\n\fParameters The observed data y is generated by the simulator defined as:\n\ny = Sim(xtrue) +\n\n\u221a\n\n\u03c3nU[0, 1]\n\n(156)\n\nwhere\ny is discretised in both spatially and time-domain. Time domain is discretised for 5000 seconds with\nthe sampling rate of 1 seconds, resulting in 5,000 data points. 2-dimensional space is discretised for\n64 \u00d7 64 nm2, with 64 \u00d7 64 nm2 pixels. The total data points are 64 \u00d7 64 \u00d7 5, 000 = 20, 480, 000.\nxtrue = [ exp(1.90657514) \u00d7 102, exp(0.71783979) \u00d7 104, exp(0.7975072), exp(0.69314718) \u00d7\n10\u221215 ]\n\nMetric The posterior distribution is evaluated via RMSE between true and inferred conditional\nposterior on each parameter. The RMSE is calculated on 50 grid samples for each dimension so as to\nslice the maximum value of the joint posterior. Each 50 grid samples are equally-spaced and bounded\nwith the following boundaries:\nbounds = [1.87, 1.94], [0.69, 0.73], [0.77, 0.83], [0.68, 0.73]\nwhere the boundaries are given by [lower, upper] in the logarithmic space.\n\n6.2.3 Hyperparameter marginalisation of hierarchical GP\n\nBackground The hierarchical GP model was designed for analysing the large-scale battery time-\nseries dataset from solar off-grid system field data all over the African continent [1]. The field data\ncontains the information of time-series operating conditions (I, T, V ), where I is the excitation\ncurrent, T is the temperature, and V is the voltage. We wish to estimate the state of health (SoH)\nfrom these field data, achieving the preventive battery replacement before it fails for the convenience\nof those who rely on the power system for their living. However, estimating the state of health is\nchallenging because the raw data (I, T, V ) is not correlated to the battery health. There are several\ndefinitions of SoH, but the internal resistance of a battery R is adopted in [1]. In the usual circuit\nelement, resistance can be easily calculated from R = V /I via Ohm\u2019s law. However, the battery\ninternal resistance R is way more complex. Battery internal resistance R is a function of (t, I, T, c),\nwhere t is time, c is the acid concentration. Furthermore, there are two factors of resistance variation;\nionic polarisation and aging. To incorporate these physical insights to the machine learning model, [1]\nis adopted the hierarchical GP model. First, they adopted the additive kernel of a squared exponential\nkernel and a Wiener velocity kernel to divide the ionic polarisation effect and aging effect. Second,\nthey adopted the hierarchical GPs to model V to divide into R-dependent GP and non-R-dependent\nGP to incorporate the Open Circuit Voltage-State of Charge (OCV-SOC) relationship.\n\nProblem setting We wish to infer the hyperposterior distribution of 5 GP hyperparameters\n(lT , lI , lc, \u03c30, \u03c31), where lT , lI , lc are the a squared exponential kernel lengthscale of temperature T ,\ncurrent I, and acid concentration c, respectively, and \u03c30, \u03c31 are the kernel variances of a squared\nexponential kernel and a Wiener velocity kernel, respectively. We have the observed time-series\ndataset of (I, T, V ) as y.\n\nThe hyperposterior inference is based on the energy function \u03a6(x) (The details can be found in [1],\nEquation (15) in the Appendix information).\n\n\u03a6x = \u2212 log p(y|x) \u2212 log p(x)\n\n= \u2212 log p(x) +\n\n1\n2\n\n(cid:88)\n\nt\n\nlog |St(x)| +\n\n1\n2\n\n(cid:88)\n\nt\n\nt St(x)\u22121et +\neT\n\n(cid:88)\n\nt\n\nnt\n2\n\nlog 2\u03c0\n\n(157)\n\n(158)\n\nwhere\np(x) = Lognormal(x\u2217; \u00b5\u03c0, \u03a3\u03c0) is a hyperprior.\net is the error vector for each charging segment.\nnt is the number of observations in the charging segment.\nSt(x) is the innovation covariance for the segment.\n\u00b5\u03c0 = [3.96, 1.94, 2.79, 2.26, 0.34]\n\u03a3\u03c0 = diag([1, 1, 1, 1, 1])\nin the logarithmic space.\n\n23\n\n\fMetric The posterior distribution is evaluated via RMSE between true and inferred conditional\nposterior on each parameter. The RMSE is calculated on 50 grid samples for each dimension so as to\nslice the maximum value of the joint posterior. Each 50 grid samples are equally-spaced and bounded\nwith the following boundaries:\nbounds = [\u221210, 10], [\u221210, 10], [\u221210, 10], [\u221210, 10], [\u221210, 10]\nwhere the boundaries are given by [lower, upper] in the logarithmic space.\n\n", "system_prompts": [{"role": "system", "content": "You are a computer science researcher currently reviewing a paper titled \"Fast Bayesian Inference with Batch Bayesian Quadrature via Kernel Recombination\" for the NeurIPS computer science conference. Your goal is to try to be as objective and truthful as possible in your answers about the paper provided. Your reviews will be used for causal reasoning in determining the quality of the paper."}], "prompts": {"1b": {"role": "user", "content": "The following is the conclusion section of the paper you are reviewing:\n\n\nWe introduced a batch BQ approach, BASQ, capable of simultaneous calculation of both model\nevidence and posteriors. BASQ demonstrated faster convergence (in wall-clock time) on both\nsynthetic and real-world datasets, when compared against existing BQ approaches and state-of-the-art\nNS. Further, mathematical analysis shows the possibility to converge exponentially-fast under natural\nassumptions. As the BASQ framework is general-purpose, this can be applied to other active learning\nGP-based applications, such as Bayesian optimisation [52], dynamic optimisation like control [26],\nand probabilistic numerics like ODE solvers [45]. Although it scales to the number of data seen in\nlarge-scale GP experiments, practical BASQ usage is limited to fewer than 16 dimensions (similar to\nmany GP-based algorithms). However, RCHQ is agnostic to the input space, allowing quadrature in\nmanifold space. An appropriate latent variable warped GP modelling, such as GPLVM [58], could\npave the way to high dimensional quadrature in future work. In addition, while WSABI modelling\nlimits the kernel to a squared exponential kernel, RCHQ allows to adopt other kernels or priors\nwithout a bespoke modelling BQ models. (See Supplementary). As for the mathematical proof, we\ndo not incorporate batch and hyperparameter updates, which should be addressed in future work. The\ngenerality of our theoretical guarantee with respect to kernel and distribution should be useful for\nextending the analysis to the whole algorithm.\n\n\n\nBased on the section(s), please answer the following question with yes, no, or n/a and provide a brief justification for your answer.\nQuestion: Do the authors describe the limitations of their work?"}, "1c": {"role": "user", "content": "The following is the conclusion section of the paper you are reviewing:\n\n\nWe introduced a batch BQ approach, BASQ, capable of simultaneous calculation of both model\nevidence and posteriors. BASQ demonstrated faster convergence (in wall-clock time) on both\nsynthetic and real-world datasets, when compared against existing BQ approaches and state-of-the-art\nNS. Further, mathematical analysis shows the possibility to converge exponentially-fast under natural\nassumptions. As the BASQ framework is general-purpose, this can be applied to other active learning\nGP-based applications, such as Bayesian optimisation [52], dynamic optimisation like control [26],\nand probabilistic numerics like ODE solvers [45]. Although it scales to the number of data seen in\nlarge-scale GP experiments, practical BASQ usage is limited to fewer than 16 dimensions (similar to\nmany GP-based algorithms). However, RCHQ is agnostic to the input space, allowing quadrature in\nmanifold space. An appropriate latent variable warped GP modelling, such as GPLVM [58], could\npave the way to high dimensional quadrature in future work. In addition, while WSABI modelling\nlimits the kernel to a squared exponential kernel, RCHQ allows to adopt other kernels or priors\nwithout a bespoke modelling BQ models. (See Supplementary). As for the mathematical proof, we\ndo not incorporate batch and hyperparameter updates, which should be addressed in future work. The\ngenerality of our theoretical guarantee with respect to kernel and distribution should be useful for\nextending the analysis to the whole algorithm.\n\n\n\nBased on the section(s), please answer the following question with yes, no, or n/a and provide a brief justification for your answer.\nQuestion: Do the authors discuss any potential negative societal impacts of their work?"}, "2a": {"role": "user", "content": "The following is the methods section of the paper you are reviewing:\n\n\nWe analysed the convergence over single iteration on a simplified version of BASQ, which assumes\nthe BQ is modelled with vanilla BQ, without batch and hyperparameter updates. Note that the kernel\nK on Rd in this section refers to the given covariance kernel of GP at each step. We discuss the\nconvergence of BASQ in one iteration. We consider the importance sampling: Let f be a probability\ndensity on Rd and g be another density such that f = \u03bbg with a nonnegative function \u03bb. Let us call\nsuch a pair, (f, g), a density pair with weight \u03bb.\nWe approximate the kernel K with K0 = (cid:80)n\u22121\ni=1 ci\u03c6i(x)\u03c6i(y). In general, we can apply the ker-\nnel recombination algorithm [59, 75] with the weighted sample (wrec, Xrec) to obtain a weighted\npoint set (wquad, Xquad) of size n satisfying w\u22a4\nquad\u03c6i(Xquad) = w\u22a4\nrec\u03c6i(Xrec) (i = 1, . . . , n \u2212 1)\nand w\u22a4\nrec1. By modifying the kernel recombination algorithm, we can require\n(x) := (cid:112)K(x, x) \u2212 K0(x, x) [44]. We call such\nquadk1/2\nw\u22a4\n(wquad, Xquad) a proper kernel recombination of (wrec, Xrec) with K0.11 We have the following\nguarantee (proved in Supplementary):\nTheorem 1. Suppose (cid:82) (cid:112)K(x, x)f (x) dx < \u221e, \u2113 \u223c GP(m, K), and we are given an (n \u2212 1)-\ndimensional kernel K0 such that K1 := K \u2212 K0 is also a kernel. Let (f, g) be a density pair\nwith weight \u03bb. Let Xrec be an N -point independent sample from g and wrec := \u03bb(Xrec). Then, if\n(wquad, Xquad) is a proper kernel recombination of (wrec, Xrec) for K0, it satisfies\n\n(Xrec), where k1/2\n\nquad1 = w\u22a4\n\n(Xquad) \u2264 w\u22a4\n\nreck1/2\n\n1\n\n1\n\n1\n\n(cid:20)(cid:113)\n\n(cid:21)\nvar[Zf | xquad]\n\nExrec\n\n\u2264 2\n\n(cid:18)(cid:90)\n\nK1(x, x)f (x) dx\n\n+\n\n(cid:19)1/2\n\n(cid:114)\n\nCK,f,g\nN\n\n,\n\n(12)\n\nwhere Zf := (cid:82) \u2113(x)f (x) dx and CK,f,g := (cid:82) K(x, x)\u03bb(x)f (x) dx \u2212 (cid:82)(cid:82) K(x, y)f (x)f (y) dx dy.\n\nThe above approximation has one source of randomness which stems from sampling N points xrec\nfrom g. One can also apply this estimate with a random kernel and thereby introduce another source\nof randomness. In particular, when we use the Nystr\u00f6m approximation for K0 (that ensures K1 is a\nkernel [44]), then one can show that (cid:82) K1(x, x)f (x) dx can be bounded by\n(cid:18) nKmax\u221a\nM\n\nK1(x, x)f (x) dx \u2264 n\u03c3n +\n\n\u03c3m + Op\n\n\u221e\n(cid:88)\n\n(13)\n\n(cid:19)\n\n(cid:90)\n\n,\n\nm=n+1\n\nwhere \u03c3n is the n-th eigenvalue of the integral operator L2(f ) \u220b h (cid:55)\u2192 (cid:82) K(\u00b7, y)h(y)f (y) dx,\nKmax := supx K(x, x). However, note that unlike Eq. (12), this inequality only applies with high\nprobability due to the randomness of K0; see Supplementary for details.\n\n11Note that the inequality constraint on the diagonal value here is only needed for theoretical guarantee, and\n\nskipping it does not reduce the empirical performance [44].\n\n9\n\nLog KL dinvergenceLog MAE of integralBranin-Hoo FunctionAckley FunctionOscillatory Functionwall time (s)wall time (s)wall time (s)Log RMSE of posteriorNegative Log evidencewall time (s)wall time (s)Battery simulatorPhase-field modelBASQ-IVRBASQ-IGBBASQ-UBbatch WSABINS (MultiNest)NS (dynamic)NS (MLFriends)Hierarchical GPwall time (s)90502041.502250450050201.41.90225045001020.40.6022504500\fIf, for example, K is a Gaussian kernel on Rd and f is a Gaussian distribution, we have \u03c3n =\nO(exp(\u2212cn1/d)) for some constant c > 0 (see Supplementary). So in (12) we also achieve an\nempirically exponential rate when N \u226b CK,f,g . RCHQ works well with a moderate M in practise.\nNote that unlike the previous analysis [50], we do not have to assume that the space is compact. 12\n\n\n\nThe following is the appendix 1 section of the paper you are reviewing:\n\n\n1.1 Proof of Theorem 1\n\nWe provide the proof of the following theorem given in the main text.\nTheorem 1. Suppose (cid:82) (cid:112)K(x, x)f (x) dx < \u221e, \u2113 \u223c GP(m, K), and we are given an (n \u2212 1)-\ndimensional kernel K0 such that K1 := K \u2212 K0 is also a kernel. Let (f, g) be a density pair\nwith weight \u03bb. Let xrec be an N -point independent sample from g and wrec := \u03bb(xrec). Then, if\n(wquad, xquad) is a proper recombination of (wrec, xrec) for K0, it satisfies\n\n(cid:20)(cid:113)\n\nExrec\n\n(cid:21)\n\nvar[Zf | xquad]\n\n\u2264 2\n\n(cid:18)(cid:90)\n\nK1(x, x)f (x) dx\n\n+\n\n(cid:19)1/2\n\n(cid:114)\n\nCK,f,g\nN\n\n(1)\n\nwhere Zf := (cid:82) \u2113(x)f (x) dx and CK,f,g := (cid:82) K(x, x)\u03bb(x)f (x) dx \u2212 (cid:82)(cid:82) K(x, y)f (x)f (y) dx dy.\nRecall H is the RKHS given by the kernel K. As the kernel satisfies (cid:82) (cid:112)K(x, x)f (x) dx < \u221e, the\nmean embedding\n\n(cid:90)\n\n\u00b5K(f ) :=\n\nf (x)K(x, \u00b7) dx\n\n(2)\n\nis a well-defined element of H. We first discuss its approximation via importance sampling.\nLemma 1. Let f be a probability density on Rd and g be another density such that f = \u03bbg with a\nnonnegative function \u03bb. Let xrec be an N -point independent sample from g and wrec = \u03bb(xrec) be the\nweights. If we define \u00b5r := 1\n\nN w\u22a4\n\nrecK(xrec, \u00b7) then it satisfies\nE[\u2225\u00b5K(f ) \u2212 \u00b5r\u22252\nH] = 1\n\nN CK,f,g\n\nwhere CK,f,g = (cid:82) K(x, x)\u03bb(x)f (x) dx \u2212 (cid:82)(cid:82) K(x, y)f (x)f (y) dx dy\nFurthermore, the choice g(x) \u221d (cid:112)K(x, x)f (x) minimises CK,f,g, if \u03bb = K(x, x)\u22121/2 is well-\ndefined.\n\nProof. Let xrec = (X1, . . . , XN ), so \u00b5r = 1\nN\n\n(cid:80)N\n\ni=1 \u03bb(Xi)K(Xi, \u00b7). From (2), we have\n\n\u2225\u00b5K(f ) \u2212 \u00b5r\u22252\n\nH = \u2225\u00b5K(f )\u22252\n(cid:90) (cid:90)\n\nH \u2212 2\u27e8\u00b5K(f ), \u00b5r\u27e9H + \u2225\u00b5r\u22252\nH\nN\n(cid:90)\n(cid:88)\n\nK(x, y)f (x)f (y) dx dy \u2212\n\n=\n\n(4)\n\nK(x, Xi)f (x)\u03bb(Xi) dx\n\n(5)\n\n2\nN\n\ni=1\n\n+\n\n1\nN 2\n\nN\n(cid:88)\n\ni,j=1\n\nK(Xi, Xj)\u03bb(Xi)\u03bb(Xj).\n\n(6)\n\nWe have\n(cid:20)(cid:90)\n\nE\n\nK(x, Xi)f (x)\u03bb(Xi) dx\n\n(cid:90) (cid:90)\n\n(cid:21)\n\n=\n\nK(x, y)f (x)\u03bb(y)g(y) dx dy =\n\nand for i \u0338= j\n\nE[K(Xi, Xj)\u03bb(Xi)\u03bb(Xj)] =\n\n(cid:90) (cid:90)\n\nK(x, y)\u03bb(x)\u03bb(y)g(x)g(y) dx dy =\n\n1\n\n(cid:90) (cid:90)\n\n(cid:90) (cid:90)\n\nK(x, y)f (x)f (y) dx dy\n\n(7)\n\nK(x, y)f (x)f (y) dx dy,\n\n(8)\n\n\fso we in total have\n\nE[\u2225\u00b5K(f ) \u2212 \u00b5r\u22252\n\nH] =\n\n1\nN 2\n\nN\n(cid:88)\n\ni=1\n(cid:18)(cid:90)\n\n=\n\n1\nN\n\nE[K(Xi, Xi)\u03bb(Xi)2] \u2212\n\nK(x, x)\u03bb(x)f (x) dx \u2212\n\n1\nN\n(cid:90) (cid:90)\n\n(cid:90) (cid:90)\n\nK(x, y)f (x)f (y) dx dy\n\n(9)\n\nK(x, y)f (x)f (y) dx dy\n\n=\n\n(cid:19)\n\nCK,f,g\nN\n\n.\n\n(10)\n\nWe next show the optimality of g(x) \u2248 (cid:112)K(x, x)f (x).\nIt suffices to consider when\n(cid:82) K(x, x)\u03bb(x)f (x) dx is minimised as the second term is independent of g. From the Cauchy-\nSchwarz, we have\n\n(cid:90)\n\nK(x, x)\u03bb(x)f (x) dx =\n\n(cid:90)\n\nK(x, x)\u03bb(x)f (x) dx\n\n(cid:90) f (x)\n\u03bb(x)\n\n(cid:18)(cid:90) (cid:112)K(x, x)f (x) dx\n\n(cid:19)2\n\n,\n\ndx \u2265\n\nand the equality is satisfied if g(x) = f (x)\n\n\u03bb(x) \u221d (cid:112)K(x, x)f (x).\n\n(11)\n\nProof of Theorem 1. Let (wquad, xquad) be a proper recombination of (wrec, xrec), and let Qn be the\nquadrature formula given by points xquad and weights 1\nquadh(xquad). We\nalso define \u00b5n := 1\nquadK(xquad, \u00b7).\n\nN wquad, i.e, Q(h) := 1\n\nN w\u22a4\n\nN w\u22a4\n\nA well-known fact is that the worst-case error of Qn (with respect to f here) wce(Qn) =\nsup\u2225h\u2225\u22641|Qn(h) \u2212 (cid:82) h(x)f (x) dx| satisfies wce(Qn) = \u2225\u00b5K(f ) \u2212 \u00b5n\u2225H for a kernel satisfying\n(cid:82) (cid:112)K(x, x)f (x) dx < \u221e [9, 18]. By using this and the relation between Bayesian quadrature and\nkernel quadrature in the main text, we have\n\n(cid:113)\n\nvar[Zf | xquad] \u2264 wce(Qn) \u2264 \u2225\u00b5K(f ) \u2212 \u00b5r\u2225H + \u2225\u00b5r \u2212 \u00b5n\u2225H\n\nFrom Lemma 1 we have E[\u2225\u00b5K(f ) \u2212 \u00b5r\u2225H] \u2264 E[\u2225\u00b5K(f ) \u2212 \u00b5r\u22252\nsuffices to show\n\nExrec[\u2225\u00b5r \u2212 \u00b5n\u2225H] \u2264 2\n\nK1(x, x)f (x) dx\n\n.\n\n(13)\n\n(cid:18)(cid:90)\n\n(cid:19)1/2\n\n(12)\nH]1/2 = (cid:112)CK,f,g/N , so it now\n\nWe first have\n\n\u2225\u00b5r \u2212 \u00b5n\u22252\n\nH =\n\n1\nN 2\n\n(cid:0)w\u22a4\n\nrecK(xrec, xrec)wrec \u2212 2 w\u22a4\n\nrecK(xrec, xquad)wquad + w\u22a4\n\n(cid:1) ,\nquadK(xquad, xquad)wquad\n(14)\n\nand from the recombination property we also have\n\nw\u22a4\n\nrecK0(xrec, xrec)wrec \u2212 2 w\u22a4\n\nrecK0(xrec, xquad)wquad + w\u22a4\n\nquadK0(xquad, xquad)wquad = 0,\n\n(15)\n\nwhich follows from the fact that (wrec, xrec) and (wquad, xquad) give the same kernel embedding for\nthe RKHS given by K0 as the latter is a recombination of the former (see e.g. [11, Eq. 14]). By\nsubtracting, we obtain\n\n\u2225\u00b5r \u2212 \u00b5n\u22252\nH\n1\n(cid:0)w\u22a4\nN 2\n= \u2225\u00b5(1)\n\n=\n\nr \u2212 \u00b5(1)\n\nn \u22252\nH1\nwhere H1 is the RKHS given by K1 and\n\n,\n\nrecK1(xrec, xrec)wrec \u2212 2 w\u22a4\n\nrecK1(xrec, xquad)wquad + w\u22a4\n\nquadK1(xquad, xquad)wquad\n\n\u00b5(1)\nr\n\n:=\n\n1\nN\n\nw\u22a4\n\nrecK1(xrec, \u00b7),\n\n\u00b5(1)\n\nn :=\n\n1\nN\n\nw\u22a4\n\nquadK1(xquad, \u00b7).\n\n(cid:1)\n\n(16)\n\n(17)\n\n(18)\n\n(19)\n\nNow, by letting k1/2\n\n(x) := (cid:112)K(x, x), we have \u2225K1(x, \u00b7)\u2225H1 = k1/2\n\n1\n\n(x) for a point x. So we have\n\n\u2225\u00b5(1)\n\nr \u2225H1 \u2264\n\n1\n1\nN\n\nreck1/2\nw\u22a4\n\n1\n\n(xrec),\n\n\u2225\u00b5(1)\n\nn \u2225H1 \u2264\n\n1\nN\n\nquadk1/2\nw\u22a4\n\n1\n\n(xquad) \u2264\n\n1\nN\n\nreck1/2\nw\u22a4\n\n1\n\n(xrec),\n\n(20)\n\n2\n\n\fwhere the last inequality follows from the assumption that (wquad, xquad) is a proper recombination of\n(wrec, xrec). Therefore, we have the estimate\n\n\u2225\u00b5r \u2212 \u00b5n\u2225H = \u2225\u00b5(1)\n\nr \u2212 \u00b5(1)\n\nn \u2225H1 \u2264 \u2225\u00b5(1)\n\nr \u2225H1 + \u2225\u00b5(1)\n\nn \u2225H1 \u2264\n\n2\nN\n\nreck1/2\nw\u22a4\n\n1\n\n(xrec).\n\n(21)\n\nFinally, to prove (13), we recall that xrec is an N -point independent sample from g and wrec = \u03bb(xrec),\nso we obtain\n\nExrec [\u2225\u00b5r \u2212 \u00b5n\u2225H] \u2264 2 Exrec\n\n(cid:20) 1\nN\n\n(cid:21)\n(xrec)\n\n1\n\nreck1/2\nw\u22a4\n(cid:90) (cid:112)K1(x, x)f (x) dx \u2264 2\n\n= 2\n\n= 2\n\n(cid:90)\n\n\u03bb(x)k1/2\n\n1\n\n(x)g(x) dx\n\n(22)\n\n(cid:18)(cid:90)\n\nK1(x, x)f (x) dx\n\n,\n\n(23)\n\n(cid:19)1/2\n\nwhere we have used Cauchy\u2013Schwarz in the last inequality.\n\n1.2 Eigenvalue dacay of integral operators\n\nLet us consider the integral operator\n\n(cid:90)\n\nh (cid:55)\u2192\n\nK(\u00b7, y)h(y)f (y) dy\n\n(24)\n\nL2(f ) := (cid:82) \u02dch(x)2f (x) dx < \u221e}, and let \u03c31 \u2265 \u03c32 \u2265\nwhere h \u2208 L2(f ) := {\u02dch | measurable, \u2225\u02dch\u22252\n\u00b7 \u00b7 \u00b7 \u2265 0 be eigenvalues of this operator. This sequence of eigenvalues is known to be closely related\nto the convergence rate of kernel quadrature [3].\n\nFor the Nystr\u00f6m approximation, we have the following estimate represented by the eigenvalues:\nTheorem 2 ([11]). For a probability density function f on Rd, let xnys be an M -point independent\nsample from f . Let K0 be the rank-(n \u2212 1) approximate kernel using xnys given by Eq. (10) in the\nmain text. Then, K1 := K \u2212 K0 satisfies\n\n(cid:90)\n\nK1(x, x)f (x) dx \u2264 n\u03c3n +\n\n\u221e\n(cid:88)\n\nm=n+1\n\n\u03c3n +\n\n2(n \u2212 1)Kmax\n\u221a\nM\n\n(cid:32)\n\n(cid:114)\n\n1 +\n\n2 log\n\n(cid:33)\n\n1\n\u03b4\n\n(25)\n\nwith probability at least 1 \u2212 \u03b4.\n\nThis gives a theoretical guarantee for one step of our algorithm, combined with Theorem 1.\n\nAlthough the sequence of eigenvalues \u03c3n does not have an obvious expression when K is the kernel\nin the middle of our algorithms BASQ, when K is a multivariate Gaussian (RBF) kernel and f is also\na Gaussian density, we have a concrete expression of eigenvalues [8].\nIndeed, if K(x, y) = exp(\u2212\u03f52|x \u2212 y|2) and f (x) \u221d exp(\u2212\u03b12|x|2), in the case d = 1, we have\n\u03c3n = abn for some constants a > 0 and 0 < b < 1 depending on \u03f5 and \u03b1. Thus, for the d-dimensional\ncase, we can roughly estimate that \u03c3n \u2264 adbm+1 if n > md. So, by only using n, we have\n\n\u03c3n \u2264 adb\u2308n1/d\u2309 \u2264 adb(n1/d) = ad exp(\u2212cn1/d),\n\n(26)\n\nfor c = log(1/b).\n\n\n\nBased on the section(s), please answer the following question with yes, no, or n/a and provide a brief justification for your answer.\nQuestion: If the authors include theoretical results, do the authors state the full set of assumptions of all theoretical results?"}, "2b": {"role": "user", "content": "The following is the methods section of the paper you are reviewing:\n\n\nWe analysed the convergence over single iteration on a simplified version of BASQ, which assumes\nthe BQ is modelled with vanilla BQ, without batch and hyperparameter updates. Note that the kernel\nK on Rd in this section refers to the given covariance kernel of GP at each step. We discuss the\nconvergence of BASQ in one iteration. We consider the importance sampling: Let f be a probability\ndensity on Rd and g be another density such that f = \u03bbg with a nonnegative function \u03bb. Let us call\nsuch a pair, (f, g), a density pair with weight \u03bb.\nWe approximate the kernel K with K0 = (cid:80)n\u22121\ni=1 ci\u03c6i(x)\u03c6i(y). In general, we can apply the ker-\nnel recombination algorithm [59, 75] with the weighted sample (wrec, Xrec) to obtain a weighted\npoint set (wquad, Xquad) of size n satisfying w\u22a4\nquad\u03c6i(Xquad) = w\u22a4\nrec\u03c6i(Xrec) (i = 1, . . . , n \u2212 1)\nand w\u22a4\nrec1. By modifying the kernel recombination algorithm, we can require\n(x) := (cid:112)K(x, x) \u2212 K0(x, x) [44]. We call such\nquadk1/2\nw\u22a4\n(wquad, Xquad) a proper kernel recombination of (wrec, Xrec) with K0.11 We have the following\nguarantee (proved in Supplementary):\nTheorem 1. Suppose (cid:82) (cid:112)K(x, x)f (x) dx < \u221e, \u2113 \u223c GP(m, K), and we are given an (n \u2212 1)-\ndimensional kernel K0 such that K1 := K \u2212 K0 is also a kernel. Let (f, g) be a density pair\nwith weight \u03bb. Let Xrec be an N -point independent sample from g and wrec := \u03bb(Xrec). Then, if\n(wquad, Xquad) is a proper kernel recombination of (wrec, Xrec) for K0, it satisfies\n\n(Xrec), where k1/2\n\nquad1 = w\u22a4\n\n(Xquad) \u2264 w\u22a4\n\nreck1/2\n\n1\n\n1\n\n1\n\n(cid:20)(cid:113)\n\n(cid:21)\nvar[Zf | xquad]\n\nExrec\n\n\u2264 2\n\n(cid:18)(cid:90)\n\nK1(x, x)f (x) dx\n\n+\n\n(cid:19)1/2\n\n(cid:114)\n\nCK,f,g\nN\n\n,\n\n(12)\n\nwhere Zf := (cid:82) \u2113(x)f (x) dx and CK,f,g := (cid:82) K(x, x)\u03bb(x)f (x) dx \u2212 (cid:82)(cid:82) K(x, y)f (x)f (y) dx dy.\n\nThe above approximation has one source of randomness which stems from sampling N points xrec\nfrom g. One can also apply this estimate with a random kernel and thereby introduce another source\nof randomness. In particular, when we use the Nystr\u00f6m approximation for K0 (that ensures K1 is a\nkernel [44]), then one can show that (cid:82) K1(x, x)f (x) dx can be bounded by\n(cid:18) nKmax\u221a\nM\n\nK1(x, x)f (x) dx \u2264 n\u03c3n +\n\n\u03c3m + Op\n\n\u221e\n(cid:88)\n\n(13)\n\n(cid:19)\n\n(cid:90)\n\n,\n\nm=n+1\n\nwhere \u03c3n is the n-th eigenvalue of the integral operator L2(f ) \u220b h (cid:55)\u2192 (cid:82) K(\u00b7, y)h(y)f (y) dx,\nKmax := supx K(x, x). However, note that unlike Eq. (12), this inequality only applies with high\nprobability due to the randomness of K0; see Supplementary for details.\n\n11Note that the inequality constraint on the diagonal value here is only needed for theoretical guarantee, and\n\nskipping it does not reduce the empirical performance [44].\n\n9\n\nLog KL dinvergenceLog MAE of integralBranin-Hoo FunctionAckley FunctionOscillatory Functionwall time (s)wall time (s)wall time (s)Log RMSE of posteriorNegative Log evidencewall time (s)wall time (s)Battery simulatorPhase-field modelBASQ-IVRBASQ-IGBBASQ-UBbatch WSABINS (MultiNest)NS (dynamic)NS (MLFriends)Hierarchical GPwall time (s)90502041.502250450050201.41.90225045001020.40.6022504500\fIf, for example, K is a Gaussian kernel on Rd and f is a Gaussian distribution, we have \u03c3n =\nO(exp(\u2212cn1/d)) for some constant c > 0 (see Supplementary). So in (12) we also achieve an\nempirically exponential rate when N \u226b CK,f,g . RCHQ works well with a moderate M in practise.\nNote that unlike the previous analysis [50], we do not have to assume that the space is compact. 12\n\n\n\nThe following is the appendix 1 section of the paper you are reviewing:\n\n\n1.1 Proof of Theorem 1\n\nWe provide the proof of the following theorem given in the main text.\nTheorem 1. Suppose (cid:82) (cid:112)K(x, x)f (x) dx < \u221e, \u2113 \u223c GP(m, K), and we are given an (n \u2212 1)-\ndimensional kernel K0 such that K1 := K \u2212 K0 is also a kernel. Let (f, g) be a density pair\nwith weight \u03bb. Let xrec be an N -point independent sample from g and wrec := \u03bb(xrec). Then, if\n(wquad, xquad) is a proper recombination of (wrec, xrec) for K0, it satisfies\n\n(cid:20)(cid:113)\n\nExrec\n\n(cid:21)\n\nvar[Zf | xquad]\n\n\u2264 2\n\n(cid:18)(cid:90)\n\nK1(x, x)f (x) dx\n\n+\n\n(cid:19)1/2\n\n(cid:114)\n\nCK,f,g\nN\n\n(1)\n\nwhere Zf := (cid:82) \u2113(x)f (x) dx and CK,f,g := (cid:82) K(x, x)\u03bb(x)f (x) dx \u2212 (cid:82)(cid:82) K(x, y)f (x)f (y) dx dy.\nRecall H is the RKHS given by the kernel K. As the kernel satisfies (cid:82) (cid:112)K(x, x)f (x) dx < \u221e, the\nmean embedding\n\n(cid:90)\n\n\u00b5K(f ) :=\n\nf (x)K(x, \u00b7) dx\n\n(2)\n\nis a well-defined element of H. We first discuss its approximation via importance sampling.\nLemma 1. Let f be a probability density on Rd and g be another density such that f = \u03bbg with a\nnonnegative function \u03bb. Let xrec be an N -point independent sample from g and wrec = \u03bb(xrec) be the\nweights. If we define \u00b5r := 1\n\nN w\u22a4\n\nrecK(xrec, \u00b7) then it satisfies\nE[\u2225\u00b5K(f ) \u2212 \u00b5r\u22252\nH] = 1\n\nN CK,f,g\n\nwhere CK,f,g = (cid:82) K(x, x)\u03bb(x)f (x) dx \u2212 (cid:82)(cid:82) K(x, y)f (x)f (y) dx dy\nFurthermore, the choice g(x) \u221d (cid:112)K(x, x)f (x) minimises CK,f,g, if \u03bb = K(x, x)\u22121/2 is well-\ndefined.\n\nProof. Let xrec = (X1, . . . , XN ), so \u00b5r = 1\nN\n\n(cid:80)N\n\ni=1 \u03bb(Xi)K(Xi, \u00b7). From (2), we have\n\n\u2225\u00b5K(f ) \u2212 \u00b5r\u22252\n\nH = \u2225\u00b5K(f )\u22252\n(cid:90) (cid:90)\n\nH \u2212 2\u27e8\u00b5K(f ), \u00b5r\u27e9H + \u2225\u00b5r\u22252\nH\nN\n(cid:90)\n(cid:88)\n\nK(x, y)f (x)f (y) dx dy \u2212\n\n=\n\n(4)\n\nK(x, Xi)f (x)\u03bb(Xi) dx\n\n(5)\n\n2\nN\n\ni=1\n\n+\n\n1\nN 2\n\nN\n(cid:88)\n\ni,j=1\n\nK(Xi, Xj)\u03bb(Xi)\u03bb(Xj).\n\n(6)\n\nWe have\n(cid:20)(cid:90)\n\nE\n\nK(x, Xi)f (x)\u03bb(Xi) dx\n\n(cid:90) (cid:90)\n\n(cid:21)\n\n=\n\nK(x, y)f (x)\u03bb(y)g(y) dx dy =\n\nand for i \u0338= j\n\nE[K(Xi, Xj)\u03bb(Xi)\u03bb(Xj)] =\n\n(cid:90) (cid:90)\n\nK(x, y)\u03bb(x)\u03bb(y)g(x)g(y) dx dy =\n\n1\n\n(cid:90) (cid:90)\n\n(cid:90) (cid:90)\n\nK(x, y)f (x)f (y) dx dy\n\n(7)\n\nK(x, y)f (x)f (y) dx dy,\n\n(8)\n\n\fso we in total have\n\nE[\u2225\u00b5K(f ) \u2212 \u00b5r\u22252\n\nH] =\n\n1\nN 2\n\nN\n(cid:88)\n\ni=1\n(cid:18)(cid:90)\n\n=\n\n1\nN\n\nE[K(Xi, Xi)\u03bb(Xi)2] \u2212\n\nK(x, x)\u03bb(x)f (x) dx \u2212\n\n1\nN\n(cid:90) (cid:90)\n\n(cid:90) (cid:90)\n\nK(x, y)f (x)f (y) dx dy\n\n(9)\n\nK(x, y)f (x)f (y) dx dy\n\n=\n\n(cid:19)\n\nCK,f,g\nN\n\n.\n\n(10)\n\nWe next show the optimality of g(x) \u2248 (cid:112)K(x, x)f (x).\nIt suffices to consider when\n(cid:82) K(x, x)\u03bb(x)f (x) dx is minimised as the second term is independent of g. From the Cauchy-\nSchwarz, we have\n\n(cid:90)\n\nK(x, x)\u03bb(x)f (x) dx =\n\n(cid:90)\n\nK(x, x)\u03bb(x)f (x) dx\n\n(cid:90) f (x)\n\u03bb(x)\n\n(cid:18)(cid:90) (cid:112)K(x, x)f (x) dx\n\n(cid:19)2\n\n,\n\ndx \u2265\n\nand the equality is satisfied if g(x) = f (x)\n\n\u03bb(x) \u221d (cid:112)K(x, x)f (x).\n\n(11)\n\nProof of Theorem 1. Let (wquad, xquad) be a proper recombination of (wrec, xrec), and let Qn be the\nquadrature formula given by points xquad and weights 1\nquadh(xquad). We\nalso define \u00b5n := 1\nquadK(xquad, \u00b7).\n\nN wquad, i.e, Q(h) := 1\n\nN w\u22a4\n\nN w\u22a4\n\nA well-known fact is that the worst-case error of Qn (with respect to f here) wce(Qn) =\nsup\u2225h\u2225\u22641|Qn(h) \u2212 (cid:82) h(x)f (x) dx| satisfies wce(Qn) = \u2225\u00b5K(f ) \u2212 \u00b5n\u2225H for a kernel satisfying\n(cid:82) (cid:112)K(x, x)f (x) dx < \u221e [9, 18]. By using this and the relation between Bayesian quadrature and\nkernel quadrature in the main text, we have\n\n(cid:113)\n\nvar[Zf | xquad] \u2264 wce(Qn) \u2264 \u2225\u00b5K(f ) \u2212 \u00b5r\u2225H + \u2225\u00b5r \u2212 \u00b5n\u2225H\n\nFrom Lemma 1 we have E[\u2225\u00b5K(f ) \u2212 \u00b5r\u2225H] \u2264 E[\u2225\u00b5K(f ) \u2212 \u00b5r\u22252\nsuffices to show\n\nExrec[\u2225\u00b5r \u2212 \u00b5n\u2225H] \u2264 2\n\nK1(x, x)f (x) dx\n\n.\n\n(13)\n\n(cid:18)(cid:90)\n\n(cid:19)1/2\n\n(12)\nH]1/2 = (cid:112)CK,f,g/N , so it now\n\nWe first have\n\n\u2225\u00b5r \u2212 \u00b5n\u22252\n\nH =\n\n1\nN 2\n\n(cid:0)w\u22a4\n\nrecK(xrec, xrec)wrec \u2212 2 w\u22a4\n\nrecK(xrec, xquad)wquad + w\u22a4\n\n(cid:1) ,\nquadK(xquad, xquad)wquad\n(14)\n\nand from the recombination property we also have\n\nw\u22a4\n\nrecK0(xrec, xrec)wrec \u2212 2 w\u22a4\n\nrecK0(xrec, xquad)wquad + w\u22a4\n\nquadK0(xquad, xquad)wquad = 0,\n\n(15)\n\nwhich follows from the fact that (wrec, xrec) and (wquad, xquad) give the same kernel embedding for\nthe RKHS given by K0 as the latter is a recombination of the former (see e.g. [11, Eq. 14]). By\nsubtracting, we obtain\n\n\u2225\u00b5r \u2212 \u00b5n\u22252\nH\n1\n(cid:0)w\u22a4\nN 2\n= \u2225\u00b5(1)\n\n=\n\nr \u2212 \u00b5(1)\n\nn \u22252\nH1\nwhere H1 is the RKHS given by K1 and\n\n,\n\nrecK1(xrec, xrec)wrec \u2212 2 w\u22a4\n\nrecK1(xrec, xquad)wquad + w\u22a4\n\nquadK1(xquad, xquad)wquad\n\n\u00b5(1)\nr\n\n:=\n\n1\nN\n\nw\u22a4\n\nrecK1(xrec, \u00b7),\n\n\u00b5(1)\n\nn :=\n\n1\nN\n\nw\u22a4\n\nquadK1(xquad, \u00b7).\n\n(cid:1)\n\n(16)\n\n(17)\n\n(18)\n\n(19)\n\nNow, by letting k1/2\n\n(x) := (cid:112)K(x, x), we have \u2225K1(x, \u00b7)\u2225H1 = k1/2\n\n1\n\n(x) for a point x. So we have\n\n\u2225\u00b5(1)\n\nr \u2225H1 \u2264\n\n1\n1\nN\n\nreck1/2\nw\u22a4\n\n1\n\n(xrec),\n\n\u2225\u00b5(1)\n\nn \u2225H1 \u2264\n\n1\nN\n\nquadk1/2\nw\u22a4\n\n1\n\n(xquad) \u2264\n\n1\nN\n\nreck1/2\nw\u22a4\n\n1\n\n(xrec),\n\n(20)\n\n2\n\n\fwhere the last inequality follows from the assumption that (wquad, xquad) is a proper recombination of\n(wrec, xrec). Therefore, we have the estimate\n\n\u2225\u00b5r \u2212 \u00b5n\u2225H = \u2225\u00b5(1)\n\nr \u2212 \u00b5(1)\n\nn \u2225H1 \u2264 \u2225\u00b5(1)\n\nr \u2225H1 + \u2225\u00b5(1)\n\nn \u2225H1 \u2264\n\n2\nN\n\nreck1/2\nw\u22a4\n\n1\n\n(xrec).\n\n(21)\n\nFinally, to prove (13), we recall that xrec is an N -point independent sample from g and wrec = \u03bb(xrec),\nso we obtain\n\nExrec [\u2225\u00b5r \u2212 \u00b5n\u2225H] \u2264 2 Exrec\n\n(cid:20) 1\nN\n\n(cid:21)\n(xrec)\n\n1\n\nreck1/2\nw\u22a4\n(cid:90) (cid:112)K1(x, x)f (x) dx \u2264 2\n\n= 2\n\n= 2\n\n(cid:90)\n\n\u03bb(x)k1/2\n\n1\n\n(x)g(x) dx\n\n(22)\n\n(cid:18)(cid:90)\n\nK1(x, x)f (x) dx\n\n,\n\n(23)\n\n(cid:19)1/2\n\nwhere we have used Cauchy\u2013Schwarz in the last inequality.\n\n1.2 Eigenvalue dacay of integral operators\n\nLet us consider the integral operator\n\n(cid:90)\n\nh (cid:55)\u2192\n\nK(\u00b7, y)h(y)f (y) dy\n\n(24)\n\nL2(f ) := (cid:82) \u02dch(x)2f (x) dx < \u221e}, and let \u03c31 \u2265 \u03c32 \u2265\nwhere h \u2208 L2(f ) := {\u02dch | measurable, \u2225\u02dch\u22252\n\u00b7 \u00b7 \u00b7 \u2265 0 be eigenvalues of this operator. This sequence of eigenvalues is known to be closely related\nto the convergence rate of kernel quadrature [3].\n\nFor the Nystr\u00f6m approximation, we have the following estimate represented by the eigenvalues:\nTheorem 2 ([11]). For a probability density function f on Rd, let xnys be an M -point independent\nsample from f . Let K0 be the rank-(n \u2212 1) approximate kernel using xnys given by Eq. (10) in the\nmain text. Then, K1 := K \u2212 K0 satisfies\n\n(cid:90)\n\nK1(x, x)f (x) dx \u2264 n\u03c3n +\n\n\u221e\n(cid:88)\n\nm=n+1\n\n\u03c3n +\n\n2(n \u2212 1)Kmax\n\u221a\nM\n\n(cid:32)\n\n(cid:114)\n\n1 +\n\n2 log\n\n(cid:33)\n\n1\n\u03b4\n\n(25)\n\nwith probability at least 1 \u2212 \u03b4.\n\nThis gives a theoretical guarantee for one step of our algorithm, combined with Theorem 1.\n\nAlthough the sequence of eigenvalues \u03c3n does not have an obvious expression when K is the kernel\nin the middle of our algorithms BASQ, when K is a multivariate Gaussian (RBF) kernel and f is also\na Gaussian density, we have a concrete expression of eigenvalues [8].\nIndeed, if K(x, y) = exp(\u2212\u03f52|x \u2212 y|2) and f (x) \u221d exp(\u2212\u03b12|x|2), in the case d = 1, we have\n\u03c3n = abn for some constants a > 0 and 0 < b < 1 depending on \u03f5 and \u03b1. Thus, for the d-dimensional\ncase, we can roughly estimate that \u03c3n \u2264 adbm+1 if n > md. So, by only using n, we have\n\n\u03c3n \u2264 adb\u2308n1/d\u2309 \u2264 adb(n1/d) = ad exp(\u2212cn1/d),\n\n(26)\n\nfor c = log(1/b).\n\n\n\nBased on the section(s), please answer the following question with yes, no, or n/a and provide a brief justification for your answer.\nQuestion: If the authors include theoretical results, do the authors include complete proofs of all theoretical results?"}, "3a": {"role": "user", "content": "The following is the introduction section of the paper you are reviewing:\n\n\nMany applications in science, engineering, and economics involve complex simulations to explain the\nstructure and dynamics of the process. Such models are derived from knowledge of the mechanisms\nand principles underlying the data-generating process, and are critical for scientific hypothesis-\nbuilding and testing. However, dozens of plausible simulators describing the same phenomena often\nexist, owing to differing assumptions or levels of approximation. Similar situations can be found in\nselection of simulator-based control models, selection of machine learning models on large-scale\ndatasets, and in many data assimilation applications [28].\n\nIn such settings, with multiple competing models, choosing the best model for the dataset is crucial.\nBayesian model evidence gives a clear criteria for such model selection. However, computing\nmodel evidence requires integration over the likelihood, which is challenging, particularly when\nthe likelihood is non-closed-form and/or expensive. The ascertained model is often applied to\n\n\u2217Equal contribution\n2Code: https://github.com/ma921/BASQ\n\n36th Conference on Neural Information Processing Systems (NeurIPS 2022).\n\n\fproduce posteriors for prediction and parameter estimation afterwards. There are many algorithms\nspecialised for the calculation of model evidences or posteriors, although only a limited number of\nBayesian inference solvers estimate both model evidence and posteriors in one go. As such, costly\ncomputations are often repeated (at least) twice. Addressing this concern, nested sampling (NS)\n[71, 46] was developed to estimate both model evidence and posteriors simultaneously, and has been\nbroadly applied, especially amongst astrophysicists for cosmological model selection [63]. However,\nNS is based on a Monte Carlo (MC) sampler, and its slow convergence rate is a practical hindrance.\n\nTo aid NS, and other approaches, parallel computing is widely applied to improve the speed of\nwall-clock computation. Modern computer clusters and graphical processing units enable scientists\nto query the likelihood in large batches. However, parallelisation can, at best, linearly accelerate NS,\ndoing little to counter NS\u2019s inherently slow convergence rate as a MC sampler.\n\nThis paper investigates batch Bayesian quadrature (BQ) [65] for fast Bayesian inference. BQ solves\nthe integral as an inference problem, modelling the likelihood function with a probabilistic model\n(typically a Gaussian process (GP)). Gunter et al. [37] proposed Warped sequential active Bayesian\nintegration (WSABI), which adopts active learning to select samples upon uncertainty over the\nintegrand. WSABI showed that BQ with expensive GP calculations could achieve faster convergence\nin wall time than cheap MC samplers. Wagstaff et al. [78] introduced batch WSABI, achieving even\nfaster calculation via parallel computing and became the fastest BQ model to date. We improve upon\nthese existing works for a large-scale batch case.\n\n\n\nThe following is the appendix 6 section of the paper you are reviewing:\n\n\n6.1 Synthetic problems\n\n6.1.1 Quadrature hyperparameters\n\nThe initial quadrature hyperparameters are as follows:\nA kernel length scale l = 2\nA kernel variance v\u2032 = 2\nRecombination sample size N = 20, 000\nNystr\u00f6m sample size M = N/100\nSupersample ratio rsuper = 100\nProposal distribution g(x) partition ratio r = 0.5\n\nThe supersample ratio rsuper is the ratio of supersamples for SMC sampling of acquisition function\nagainst the recombination sample size N .\n\nA kernel length scale and a kernel variance are important for selecting the samples in the first batch.\nNevertheless, these parameters are updated via type-II MLE optimisation after the second round.\nNystr\u00f6m sample size must be larger than the batch size n, and the recombination sample size is\npreferred to satisfy N \u226b M . Larger N and M give more accurate sample selection via kernel\nquadrature. However, larger subsamples result in a longer wall-time. We do not need to change the\nvalues as long as the integral converged to the designated criterion. When longer computational time\nis allowed, or likelihood is expensive enough to regard recombination time as negligible, larger N ,\nM will give us a faster convergence rate.\n\nThe partition ratio r is the only hyperparamter that affects the convergence sensitively. The optimal\nvalue depends the integrand and it is challenging to know the optimal value before running. As we\ny \u03c0(x) gives the optimal upper bound. r = 0.5 is a good approximation of\nderived in Lemma 1,\n(cid:9) \u03c0(x). Here,\nthis optimal proposal distribution: g(x) = (1 \u2212 r)\u03c0(x) + rC L\n\ny \u03c0(x) = (cid:8)(1 \u2212 r) + rC L\n\nC L\n\n(cid:113)\n\ny\n\n18\n\n\fthe linearisation gives the approximation\n\n(cid:113)\n\nC L\n\ny =\n\n(cid:113)\n\n1 + (C L\n\ny \u2212 1) \u2248 1 +\n\nCL\n\ny \u22121\n2 = 0.5 + 0.5C L\ny .\n\n(cid:113)\n\nC L\n\ny \u03c0(x). Thus, r = 0.5 is a safe choice.\n\nTherefore, (0.5 + 0.5C L\n\ny )\u03c0(x) \u2248\n\n6.1.2 Gaussian mixture\n\nThe likelihood function of the Gaussian mixture used in Figure 1 in the main paper is expressed as:\n\n\u2113true(x) =\n\nn\n(cid:88)\n\ni=1\n\nwiN (x; \u00b5i, \u03a3i)\n\nwi = N (\u00b5i; \u00b5\u03c0, \u03a3i + \u03a3\u03c0)\u22121\n\n(cid:90)\n\nZtrue =\n\n\u2113true(x)\u03c0(x)dx,\n\n(cid:90)\n\nwi\n\nN (x; \u00b5i, \u03a3i)N (x; \u00b5\u03c0, \u03a3\u03c0)dx\n\nwiN (\u00b5i; \u00b5\u03c0, \u03a3i + \u03a3\u03c0)\n\n=\n\n=\n\nn\n(cid:88)\n\ni=1\nn\n(cid:88)\n\ni=1\n\n= 1\n\nwhere\n\u00b5\u03c0 = 0\n\u03a3\u03c0 = 2I\n\u03c0(x) = N (x; \u00b5\u03c0, \u03a3\u03c0)\n\nThe prior is the same throughout the synthetic problems.\n\n6.1.3 Branin-Hoo function\n\nThe Branin-Hoo function in Figure 2 in the main paper is expressed as:\n\n2\n(cid:89)\n\n\u2113true(x) =\n\n(cid:2)sin(xi) + 1\n2 xi)2 + 3\n( 1\n\n2 cos(3xi)(cid:3)2\n\n10\n\n,\n\nx \u2208 R2\n\ni=1\n(cid:90)\n\nZtrue =\n\n\u2113true(x)\u03c0(x)dx\n\n= 0.9557282\n\u2248 0.913416\n\n(132)\n\n(133)\n\n(134)\n\n(135)\n\n(136)\n\n(137)\n\n(138)\n\n(139)\n\n(140)\n(141)\n\n6.1.4 Ackley function\n\nThe Ackley function in Figure 2 in the main paper is expressed as:\n\n\uf8eb\n\n\u2113true(x) = \u221220 exp\n\n\uf8ed\u2212\n\n(cid:118)\n(cid:117)\n(cid:117)\n(cid:116)\n\n1\n5\n\n1\n2\n\n2\n(cid:88)\n\ni=1\n\n\uf8f6\n\nx2\ni\n\n\uf8f8 + exp\n\n(cid:32)\n\n1\n2\n\n2\n(cid:88)\n\ni=1\n\n(cid:33)\n\ncos(2\u03c0xi)\n\n+ 20,\n\nx \u2208 R2\n\n(142)\n\n(cid:90)\n\nZtrue =\n\n\u2113true(x)\u03c0(x)dx\n\n\u2248 5.43478\n\n19\n\n(143)\n\n(144)\n\n\fFigure 2: Performance comparison with N-dimensional Gaussian mixture likelihood function. (a)\ndimension study, (b) convergence rate, and (c) wall time vs MAE of integral. (a) varies from 2 to 16\ndimensions, (b) and (c) are 10 dimensional Gaussian mixture.\n\n6.1.5 Oscillatory function\n\nThe Oscillatory function in Figure 2 in the main paper is expressed as:\n\n(cid:32)\n\n\u2113true(x) = cos\n\n2\u03c0 + 5\n\n(cid:33)\n\nxi\n\n+ 1,\n\nx \u2208 R2\n\n2\n(cid:88)\n\ni=1\n\n(cid:90)\n\nZtrue =\n\n= 1\n\n\u2113true(x)\u03c0(x)dx\n\n(145)\n\n(146)\n\n(147)\n\n6.1.6 Additional experiments\n\nDimensional study in Gaussian mixture likelihood Figure 2(a) shows the dimension study of\nGaussian mixture likelihood. The BASQ and BQ are conditioned at the same time budget (200\nseconds). The higher dimension gives a more inaccurate estimation. From this result, we recommend\nusing BASQ with fewer than 16 dimensions.\n\nAblation study We investigated the influence of the approximation we adopted using 10 dimen-\nsional Gaussian mixture likelihood. The compared models are as follows:\n\n1. Exact sampler (without factorisation trick)\n\n2. Provable recombination (without LP solver)\n\nThe exact sampler without the factorisation trick is the one that exactly follows the Eqs. (8) - (10) of\nthe main paper. That is, the distribution of interest f (x) is the prior \u03c0(x). In addition, the kernel for\nthe acquisition function is an unwarped C L\ny , which is computationally expensive. Next, the provable\nrecombination algorithm is the one introduced in [19] with the best known computational complexity.\nAs explained in the Background section of the main paper, our BASQ implementation is based on an\n\n20\n\nBranin-HooTrue PosteriorBASQ-UB inferenceAckleyOscillatory\fFigure 3: Qualitative evaluation of posterior inference in synthetic problems\n\nLP solver (Gurobi [31] for this time) with empirically faster computational time. We compared the\ninfluence of these approximations.\n\nFigure 2(b) illustrates that these approximations are not affecting the convergence rate in the sample\nefficiency. However, when compared to the wall-clock time (Figure 2(c)), the exact sampler without\nthe factorisation trick is apparently slow to converge. Moreover, the provable recombination algorithm\nis slower than an LP solver implementation. Thus, the number of samples the provable recombination\nalgorithm per wall time is much smaller than the LP solver. Therefore, our BASQ standard solver\ndelivers solid empirical performance.\n\nQualitative evaluation of posterior inference Figure 3 shows the qualitative evaluation of joint\nposterior inference after 200 seconds passed against the analytical true posterior. The estimated\nposterior shape is exactly the same as the ground truth.\n\n6.2 Real-world problems\n\n6.2.1 Battery simulator\n\nBackground Single Particle Model with electrolyte dynamics (SPMe) is a commonly-used lithium-\nion battery simulator to predict the voltage response at given excitation current time-series data.\nEstimating SPMe parameters from observations are well known for ill-conditioned problem because\nthis model is overparameterised [5]. In the physical model, we need to separate the anode and cathode\ninternal states to represent actual cell components. However, when it comes to predicting the voltage\nresponse, this separation into two components is redundant. Except for extreme conditions such\nas low temperature, most voltage responses can be expressed with a single component. Therefore,\nthe parameters of cathode and anode often have a perfect negative correlation, meaning an arbitrary\ncombination of cathode and anode parameters can reconstruct the exactly same voltage profile. As\nsuch, point estimation means nothing in these cases. Bayesian inference can capture this negative\ncorrelation as covariance. Therefore, Bayesian inference is a natural choice for parameter estimation\nin the battery simulator. Moreover, there are many plausible battery simulators with differing levels\nof approximation. Selecting the model satisfying both predictability and a minimal number of\nparameters is crucial for faster calculation, particularly in setting up the control simulator. Therefore,\nBayesian model selection with model evidence is essential. The experimental setup is basically\nfollowing [2].\n\nProblem setting We wish to infer the posterior distribution of 3 simulation parameters\n(Dn, Dp, \u03c3n), where Dn is the diffusivity on anode, Dp is the diffusivity on cathode, \u03c3n is the\n\n21\n\ndimensionLog MAE of integralBASQ-UBbatch WSABI\fnoise variance of the observed data. We have the observed time-series voltage y and exciation profiles\ni as training dataset.\n\nThe parameter inference is modelled as follows:\n\ny\u2217 = Sim(x\u2217, i\u2217)\n\n\u03c0(x\u2217) = Lognormal(x\u2217; \u00b5\u03c0, \u03a3\u03c0)\n\n\u2113true(x\u2217) = N [Sim(x\u2217, i); y, \u03c3nI]\n\n(148)\n(149)\n(150)\n\nwhere\n\u00b5\u03c0 = [1.38, 0, \u221220.25]\n\u03a3\u03c0 = diag([0.03, 0.001, 0.001])\nin the logarithmic space.\n\nParameters The observed data y and i are generated by the simulator with multiharmonic sinusoidal\nexcitation current defined as:\n\ni = 0.132671 [sin(1/5\u03c0t) + sin(2\u03c0t) + sin(20\u03c0t) + sin(200\u03c0t)]\ny = Sim(xtrue, i) +\n\n\u03c3nU[0, 1]\n\n\u221a\n\n(151)\n\n(152)\n\nwhere\nt is discretised for 10 seconds with the sampling rate of 0.00025 seconds, resulting in 40,000 data\npoints.\nxtrue = [ exp(1.361) \u00d7 10\u221214, exp(0) \u00d7 10\u221213, exp(\u221220.25) \u00d7 10\u221210 ]\n\nMetric The posterior distribution is evaluated via RMSE between true and inferred conditional\nposterior on each parameter. The RMSE is calculated on 50 grid samples for each dimension so as to\nslice the maximum value of the joint posterior. Each 50 grid samples are equally-spaced and bounded\nwith the following boundaries:\nbounds = [1.1, 1.7], [\u22120.075, 0.08], [\u221220.3, \u221220.2]\nwhere the boundaries are given by [lower, upper] in the logarithmic space.\n\n6.2.2 Phase-field model\n\nBackground The PFM is a flexible time-evolving interfacial physical model that can easily in-\ncorporate the multi-physical energy [13]. In this dataset, the PFM is applied to the simulation of\nspinodal decomposition, which is the self-organised nanostructure in the bistable Fe-Cr alloy at high\ntemperatures. Spinodal decomposition is an inherently stochastic process, making characterisation\nchallenging [14]. Therefore, Bayesian model selection is promising for estimating its parameter and\ndetermining the model physics component.\n\nProblem setting We wish to infer the posterior distribution of 4 simulation parameters\n(T, LcT , nB, Lg), where T is the temperature, LcT is the interaction parameter that defines the\ninteraction between composition and temperature, nB is the number of Bohr magnetons per atom,\nand Lg is the gradient energy coefficient. We have the observed time-series 2-dimensional images y.\n\nThe parameter inference is modelled as follows:\n\ny\u2217 = Sim(x\u2217)\n\n\u03c0(x\u2217) = Lognormal(x\u2217; \u00b5\u03c0, \u03a3\u03c0)\n\n\u2113true(x\u2217) = N [Sim(x\u2217); y, \u03c3nI]\n\n(153)\n(154)\n(155)\n\nwhere\n\u03c3n = 10\u22124\n\u00b5\u03c0 = [1.91, 0.718, 0.798, 0.693]\n\u03a3\u03c0 = diag([0.0003, 0.00006, 0.0001, 0.0001])\nin the logarithmic space.\n\n22\n\n\fParameters The observed data y is generated by the simulator defined as:\n\ny = Sim(xtrue) +\n\n\u221a\n\n\u03c3nU[0, 1]\n\n(156)\n\nwhere\ny is discretised in both spatially and time-domain. Time domain is discretised for 5000 seconds with\nthe sampling rate of 1 seconds, resulting in 5,000 data points. 2-dimensional space is discretised for\n64 \u00d7 64 nm2, with 64 \u00d7 64 nm2 pixels. The total data points are 64 \u00d7 64 \u00d7 5, 000 = 20, 480, 000.\nxtrue = [ exp(1.90657514) \u00d7 102, exp(0.71783979) \u00d7 104, exp(0.7975072), exp(0.69314718) \u00d7\n10\u221215 ]\n\nMetric The posterior distribution is evaluated via RMSE between true and inferred conditional\nposterior on each parameter. The RMSE is calculated on 50 grid samples for each dimension so as to\nslice the maximum value of the joint posterior. Each 50 grid samples are equally-spaced and bounded\nwith the following boundaries:\nbounds = [1.87, 1.94], [0.69, 0.73], [0.77, 0.83], [0.68, 0.73]\nwhere the boundaries are given by [lower, upper] in the logarithmic space.\n\n6.2.3 Hyperparameter marginalisation of hierarchical GP\n\nBackground The hierarchical GP model was designed for analysing the large-scale battery time-\nseries dataset from solar off-grid system field data all over the African continent [1]. The field data\ncontains the information of time-series operating conditions (I, T, V ), where I is the excitation\ncurrent, T is the temperature, and V is the voltage. We wish to estimate the state of health (SoH)\nfrom these field data, achieving the preventive battery replacement before it fails for the convenience\nof those who rely on the power system for their living. However, estimating the state of health is\nchallenging because the raw data (I, T, V ) is not correlated to the battery health. There are several\ndefinitions of SoH, but the internal resistance of a battery R is adopted in [1]. In the usual circuit\nelement, resistance can be easily calculated from R = V /I via Ohm\u2019s law. However, the battery\ninternal resistance R is way more complex. Battery internal resistance R is a function of (t, I, T, c),\nwhere t is time, c is the acid concentration. Furthermore, there are two factors of resistance variation;\nionic polarisation and aging. To incorporate these physical insights to the machine learning model, [1]\nis adopted the hierarchical GP model. First, they adopted the additive kernel of a squared exponential\nkernel and a Wiener velocity kernel to divide the ionic polarisation effect and aging effect. Second,\nthey adopted the hierarchical GPs to model V to divide into R-dependent GP and non-R-dependent\nGP to incorporate the Open Circuit Voltage-State of Charge (OCV-SOC) relationship.\n\nProblem setting We wish to infer the hyperposterior distribution of 5 GP hyperparameters\n(lT , lI , lc, \u03c30, \u03c31), where lT , lI , lc are the a squared exponential kernel lengthscale of temperature T ,\ncurrent I, and acid concentration c, respectively, and \u03c30, \u03c31 are the kernel variances of a squared\nexponential kernel and a Wiener velocity kernel, respectively. We have the observed time-series\ndataset of (I, T, V ) as y.\n\nThe hyperposterior inference is based on the energy function \u03a6(x) (The details can be found in [1],\nEquation (15) in the Appendix information).\n\n\u03a6x = \u2212 log p(y|x) \u2212 log p(x)\n\n= \u2212 log p(x) +\n\n1\n2\n\n(cid:88)\n\nt\n\nlog |St(x)| +\n\n1\n2\n\n(cid:88)\n\nt\n\nt St(x)\u22121et +\neT\n\n(cid:88)\n\nt\n\nnt\n2\n\nlog 2\u03c0\n\n(157)\n\n(158)\n\nwhere\np(x) = Lognormal(x\u2217; \u00b5\u03c0, \u03a3\u03c0) is a hyperprior.\net is the error vector for each charging segment.\nnt is the number of observations in the charging segment.\nSt(x) is the innovation covariance for the segment.\n\u00b5\u03c0 = [3.96, 1.94, 2.79, 2.26, 0.34]\n\u03a3\u03c0 = diag([1, 1, 1, 1, 1])\nin the logarithmic space.\n\n23\n\n\fMetric The posterior distribution is evaluated via RMSE between true and inferred conditional\nposterior on each parameter. The RMSE is calculated on 50 grid samples for each dimension so as to\nslice the maximum value of the joint posterior. Each 50 grid samples are equally-spaced and bounded\nwith the following boundaries:\nbounds = [\u221210, 10], [\u221210, 10], [\u221210, 10], [\u221210, 10], [\u221210, 10]\nwhere the boundaries are given by [lower, upper] in the logarithmic space.\n\n\n\nBased on the section(s), please answer the following question with yes, no, or n/a and provide a brief justification for your answer.\nQuestion: If the authors ran experiments, do the authors include the code, data, and instructions needed to reproduce the main experimental results (either in the supplemental material or as a URL)?"}, "3b": {"role": "user", "content": "The following is the experiments section of the paper you are reviewing:\n\n\nGiven our new model BASQ, with three variants of the proposal distribution, IVR, IGB, and UB, we\nnow test for speed against MC samplers and batch WSABI. We compared with three NS methods\n[71, 29, 46, 14, 15], coded with [72, 17]. According to the review [16], MLFriends is the state-of-the-\nart NS sampler to date. The code is implemented based on [77, 34, 42, 76, 38, 31, 10, 7], and code\naround kernel recombination [24, 44] with additional modification. All experiments on synthetic\ndatasets were averaged over 10 repeats, computed in parallel with multicore CPUs, without GPU for\nfair comparison.8 The posterior distribution of NS was estimated via KDE with weighted samples\n[33]. For maximum speed performance, batch size was optimised for each method in each dataset,\nin fairness to the competitors. Batch WSABI needs to optimise batch size to balance the likelihood\nquery cost and sampling cost, because sampling cost increases rapidly with batch size, as shown\nin Figure 1(a). Therefore, it has an optimal batch size for faster convergence. By wall time cost,\nwe exclude the cost of integrand evaluation; that is, the wall time cost is the overhead cost of batch\nevaluation. Details can be found in the Supplementary.\n\n7Negative elements in the matrices only exist in K(X, X)\u22121, which can be drawn from the memory of the\nGP regression model without additional calculation. The number of positive components is half of the matrix on\naverage, resulting in O(n2/2). Then, taking the threshold via the inverse of the recombination sample size N ,\nthe number of components becomes ncomp \u226a n2, resulting in sampling complexity O(n2/2 + ncompN ).\n\n8Performed on MacBook Pro 2019, 2.4 GHz 8-Core Intel Core i9, 64 GB 2667 MHz DDR4\n\n7\n\n\f5.1 Synthetic problems\n\nWe evaluate all methods on three synthetic problems. The goal is to estimate the integral and posterior\nof the likelihood modelled with the highly multimodal functions. Prior was set to a two-dimensional\nmultivariate normal distribution, with a zero mean vector, and covariance whose diagonal elements\nare 2. The optimised batch sizes for each methods are BASQ: 100, batch WSABI: 16. The synthetic\nlikelihood functions are cheap (0.5 ms on average). This is advantageous setting for NS: Within 10\nseconds, the batch WSABI, BASQ, and NS collected 32, 600, 23225 samples, respectively. As for\nthe metrics, posterior estimation was tested with Kullback-Leibler (KL) upon random 10,000 samples\nfrom true posterior. Evidence was evaluated with MAE, and ground truth was derived analytically.\n\nLikelihood functions Branin-Hoo [49] is 8 modal function in two-dimensional space. Ackley [73]\nis a highly multimodal function with point symmetric periodical peaks in two-dimensional space.\nOscillatory function [32] is a highly multimodal function with reflection symmetric periodical peaks\nof highly-correlated ellipsoids in two-dimensional space.\n\n5.2 Real-world dataset\n\nWe consider three real-world applications with expensive likelihoods, which are simulator-based\nand hierarchical GP. We adopted the empirical metric due to no ground truth. For the posterior, we\ncan calculate the true conditional posterior distribution along the line passing through ground truth\nparameter points. Then, evaluate the posterior with root mean squared error (RMSE) against 50\ntest samples for each dimension. For integral, we compare the model evidence itself. Expensive\nlikelihoods makes the sample size per wall time amongst the methods no significant difference,\nwhereas rejection sampling based NS dismiss more than 50% of queried samples. The batch sizes are\nBASQ: 32, batch WSABI: 8. (see Supplementary)\n\n: The simulator is the SPMe [61], 9\nParameter estimation of the lithium-ion battery simulator\nestimating 3 simulation parameters at a given time-series voltage-current signal (the diffusivity of\nlithium-ion on the anode and cathode, and the experimental noise variance). Prior is modified to log\nmultivariate normal distribution from [4]. Each query takes 1.2 seconds on average.\n\nParameter estimation of the phase-field model\n: The simulator is the phase-field model [61],\n10 estimating 4 simulation parameters at given time-series two-dimensional morphological image\n(temperature, interaction parameter, Bohr magneton coefficient, and gradient energy coefficient).\nPrior is a log multivariate normal distribution. Each query takes 7.4 seconds on average.\n\nHyperparameter marginalisation of hierarchical GP model The hierarchical GP model was\ndesigned for analysing the large-scale battery time-series dataset from solar off-grid system field data\n[3].8 For fast estimation of parameters in each GP, the recursive technique [74] is adopted. The task\nis to marginalise 5 GP hyperparameters at given hyperprior, which is modified to log multivariate\nnormal distribution from [3]. Each query takes 1.1 seconds on average.\n\n5.3 Results\n\nWe find BASQ consistently delivers strong empirical performance, as shown in Figure 2. On all\nbenchmark problems, BASQ-IVR, IGB, or UB outperform baseline methods except in the battery\nsimulator evidence estimation. The very low-dimensional and sharp unimodal nature of this likelihood\ncould be advantageous for biased greedy batch WSABI, as IGB superiority supports this viewpoint.\nThis suggests that BASQ could be a generally fast Bayesian solver as far as we investigated. In the\nmultimodal setting of the synthetic dataset, BASQ-UB outperforms, whereas IVR does in a simulator-\nbased likelihoods. When comparing each proposal distribution, BASQ-IVR was the performant. Our\nresults support the general use of IVR, or UB if the likelihood is known to be highly multimodal.\n\n9SPMe code used was translated into Python from MATLAB [11, 12]. This open-source code is published\n\nunder the BSD 3-clause Licence. See more information on [11]\n\n10Code used was from [54, 3]. All rights of the code are reserved by the authors. Thus, we do not redistribute\n\nthe original code.\n\n8\n\n\fFigure 2: Time in seconds vs. KL divergence for posterior, and MAE for evidence in the synthetic\ndatasets. Time in seconds vs. RMSE for posterior and evidence itself in the real-world dataset.\n\n\n\nThe following is the appendix 6 section of the paper you are reviewing:\n\n\n6.1 Synthetic problems\n\n6.1.1 Quadrature hyperparameters\n\nThe initial quadrature hyperparameters are as follows:\nA kernel length scale l = 2\nA kernel variance v\u2032 = 2\nRecombination sample size N = 20, 000\nNystr\u00f6m sample size M = N/100\nSupersample ratio rsuper = 100\nProposal distribution g(x) partition ratio r = 0.5\n\nThe supersample ratio rsuper is the ratio of supersamples for SMC sampling of acquisition function\nagainst the recombination sample size N .\n\nA kernel length scale and a kernel variance are important for selecting the samples in the first batch.\nNevertheless, these parameters are updated via type-II MLE optimisation after the second round.\nNystr\u00f6m sample size must be larger than the batch size n, and the recombination sample size is\npreferred to satisfy N \u226b M . Larger N and M give more accurate sample selection via kernel\nquadrature. However, larger subsamples result in a longer wall-time. We do not need to change the\nvalues as long as the integral converged to the designated criterion. When longer computational time\nis allowed, or likelihood is expensive enough to regard recombination time as negligible, larger N ,\nM will give us a faster convergence rate.\n\nThe partition ratio r is the only hyperparamter that affects the convergence sensitively. The optimal\nvalue depends the integrand and it is challenging to know the optimal value before running. As we\ny \u03c0(x) gives the optimal upper bound. r = 0.5 is a good approximation of\nderived in Lemma 1,\n(cid:9) \u03c0(x). Here,\nthis optimal proposal distribution: g(x) = (1 \u2212 r)\u03c0(x) + rC L\n\ny \u03c0(x) = (cid:8)(1 \u2212 r) + rC L\n\nC L\n\n(cid:113)\n\ny\n\n18\n\n\fthe linearisation gives the approximation\n\n(cid:113)\n\nC L\n\ny =\n\n(cid:113)\n\n1 + (C L\n\ny \u2212 1) \u2248 1 +\n\nCL\n\ny \u22121\n2 = 0.5 + 0.5C L\ny .\n\n(cid:113)\n\nC L\n\ny \u03c0(x). Thus, r = 0.5 is a safe choice.\n\nTherefore, (0.5 + 0.5C L\n\ny )\u03c0(x) \u2248\n\n6.1.2 Gaussian mixture\n\nThe likelihood function of the Gaussian mixture used in Figure 1 in the main paper is expressed as:\n\n\u2113true(x) =\n\nn\n(cid:88)\n\ni=1\n\nwiN (x; \u00b5i, \u03a3i)\n\nwi = N (\u00b5i; \u00b5\u03c0, \u03a3i + \u03a3\u03c0)\u22121\n\n(cid:90)\n\nZtrue =\n\n\u2113true(x)\u03c0(x)dx,\n\n(cid:90)\n\nwi\n\nN (x; \u00b5i, \u03a3i)N (x; \u00b5\u03c0, \u03a3\u03c0)dx\n\nwiN (\u00b5i; \u00b5\u03c0, \u03a3i + \u03a3\u03c0)\n\n=\n\n=\n\nn\n(cid:88)\n\ni=1\nn\n(cid:88)\n\ni=1\n\n= 1\n\nwhere\n\u00b5\u03c0 = 0\n\u03a3\u03c0 = 2I\n\u03c0(x) = N (x; \u00b5\u03c0, \u03a3\u03c0)\n\nThe prior is the same throughout the synthetic problems.\n\n6.1.3 Branin-Hoo function\n\nThe Branin-Hoo function in Figure 2 in the main paper is expressed as:\n\n2\n(cid:89)\n\n\u2113true(x) =\n\n(cid:2)sin(xi) + 1\n2 xi)2 + 3\n( 1\n\n2 cos(3xi)(cid:3)2\n\n10\n\n,\n\nx \u2208 R2\n\ni=1\n(cid:90)\n\nZtrue =\n\n\u2113true(x)\u03c0(x)dx\n\n= 0.9557282\n\u2248 0.913416\n\n(132)\n\n(133)\n\n(134)\n\n(135)\n\n(136)\n\n(137)\n\n(138)\n\n(139)\n\n(140)\n(141)\n\n6.1.4 Ackley function\n\nThe Ackley function in Figure 2 in the main paper is expressed as:\n\n\uf8eb\n\n\u2113true(x) = \u221220 exp\n\n\uf8ed\u2212\n\n(cid:118)\n(cid:117)\n(cid:117)\n(cid:116)\n\n1\n5\n\n1\n2\n\n2\n(cid:88)\n\ni=1\n\n\uf8f6\n\nx2\ni\n\n\uf8f8 + exp\n\n(cid:32)\n\n1\n2\n\n2\n(cid:88)\n\ni=1\n\n(cid:33)\n\ncos(2\u03c0xi)\n\n+ 20,\n\nx \u2208 R2\n\n(142)\n\n(cid:90)\n\nZtrue =\n\n\u2113true(x)\u03c0(x)dx\n\n\u2248 5.43478\n\n19\n\n(143)\n\n(144)\n\n\fFigure 2: Performance comparison with N-dimensional Gaussian mixture likelihood function. (a)\ndimension study, (b) convergence rate, and (c) wall time vs MAE of integral. (a) varies from 2 to 16\ndimensions, (b) and (c) are 10 dimensional Gaussian mixture.\n\n6.1.5 Oscillatory function\n\nThe Oscillatory function in Figure 2 in the main paper is expressed as:\n\n(cid:32)\n\n\u2113true(x) = cos\n\n2\u03c0 + 5\n\n(cid:33)\n\nxi\n\n+ 1,\n\nx \u2208 R2\n\n2\n(cid:88)\n\ni=1\n\n(cid:90)\n\nZtrue =\n\n= 1\n\n\u2113true(x)\u03c0(x)dx\n\n(145)\n\n(146)\n\n(147)\n\n6.1.6 Additional experiments\n\nDimensional study in Gaussian mixture likelihood Figure 2(a) shows the dimension study of\nGaussian mixture likelihood. The BASQ and BQ are conditioned at the same time budget (200\nseconds). The higher dimension gives a more inaccurate estimation. From this result, we recommend\nusing BASQ with fewer than 16 dimensions.\n\nAblation study We investigated the influence of the approximation we adopted using 10 dimen-\nsional Gaussian mixture likelihood. The compared models are as follows:\n\n1. Exact sampler (without factorisation trick)\n\n2. Provable recombination (without LP solver)\n\nThe exact sampler without the factorisation trick is the one that exactly follows the Eqs. (8) - (10) of\nthe main paper. That is, the distribution of interest f (x) is the prior \u03c0(x). In addition, the kernel for\nthe acquisition function is an unwarped C L\ny , which is computationally expensive. Next, the provable\nrecombination algorithm is the one introduced in [19] with the best known computational complexity.\nAs explained in the Background section of the main paper, our BASQ implementation is based on an\n\n20\n\nBranin-HooTrue PosteriorBASQ-UB inferenceAckleyOscillatory\fFigure 3: Qualitative evaluation of posterior inference in synthetic problems\n\nLP solver (Gurobi [31] for this time) with empirically faster computational time. We compared the\ninfluence of these approximations.\n\nFigure 2(b) illustrates that these approximations are not affecting the convergence rate in the sample\nefficiency. However, when compared to the wall-clock time (Figure 2(c)), the exact sampler without\nthe factorisation trick is apparently slow to converge. Moreover, the provable recombination algorithm\nis slower than an LP solver implementation. Thus, the number of samples the provable recombination\nalgorithm per wall time is much smaller than the LP solver. Therefore, our BASQ standard solver\ndelivers solid empirical performance.\n\nQualitative evaluation of posterior inference Figure 3 shows the qualitative evaluation of joint\nposterior inference after 200 seconds passed against the analytical true posterior. The estimated\nposterior shape is exactly the same as the ground truth.\n\n6.2 Real-world problems\n\n6.2.1 Battery simulator\n\nBackground Single Particle Model with electrolyte dynamics (SPMe) is a commonly-used lithium-\nion battery simulator to predict the voltage response at given excitation current time-series data.\nEstimating SPMe parameters from observations are well known for ill-conditioned problem because\nthis model is overparameterised [5]. In the physical model, we need to separate the anode and cathode\ninternal states to represent actual cell components. However, when it comes to predicting the voltage\nresponse, this separation into two components is redundant. Except for extreme conditions such\nas low temperature, most voltage responses can be expressed with a single component. Therefore,\nthe parameters of cathode and anode often have a perfect negative correlation, meaning an arbitrary\ncombination of cathode and anode parameters can reconstruct the exactly same voltage profile. As\nsuch, point estimation means nothing in these cases. Bayesian inference can capture this negative\ncorrelation as covariance. Therefore, Bayesian inference is a natural choice for parameter estimation\nin the battery simulator. Moreover, there are many plausible battery simulators with differing levels\nof approximation. Selecting the model satisfying both predictability and a minimal number of\nparameters is crucial for faster calculation, particularly in setting up the control simulator. Therefore,\nBayesian model selection with model evidence is essential. The experimental setup is basically\nfollowing [2].\n\nProblem setting We wish to infer the posterior distribution of 3 simulation parameters\n(Dn, Dp, \u03c3n), where Dn is the diffusivity on anode, Dp is the diffusivity on cathode, \u03c3n is the\n\n21\n\ndimensionLog MAE of integralBASQ-UBbatch WSABI\fnoise variance of the observed data. We have the observed time-series voltage y and exciation profiles\ni as training dataset.\n\nThe parameter inference is modelled as follows:\n\ny\u2217 = Sim(x\u2217, i\u2217)\n\n\u03c0(x\u2217) = Lognormal(x\u2217; \u00b5\u03c0, \u03a3\u03c0)\n\n\u2113true(x\u2217) = N [Sim(x\u2217, i); y, \u03c3nI]\n\n(148)\n(149)\n(150)\n\nwhere\n\u00b5\u03c0 = [1.38, 0, \u221220.25]\n\u03a3\u03c0 = diag([0.03, 0.001, 0.001])\nin the logarithmic space.\n\nParameters The observed data y and i are generated by the simulator with multiharmonic sinusoidal\nexcitation current defined as:\n\ni = 0.132671 [sin(1/5\u03c0t) + sin(2\u03c0t) + sin(20\u03c0t) + sin(200\u03c0t)]\ny = Sim(xtrue, i) +\n\n\u03c3nU[0, 1]\n\n\u221a\n\n(151)\n\n(152)\n\nwhere\nt is discretised for 10 seconds with the sampling rate of 0.00025 seconds, resulting in 40,000 data\npoints.\nxtrue = [ exp(1.361) \u00d7 10\u221214, exp(0) \u00d7 10\u221213, exp(\u221220.25) \u00d7 10\u221210 ]\n\nMetric The posterior distribution is evaluated via RMSE between true and inferred conditional\nposterior on each parameter. The RMSE is calculated on 50 grid samples for each dimension so as to\nslice the maximum value of the joint posterior. Each 50 grid samples are equally-spaced and bounded\nwith the following boundaries:\nbounds = [1.1, 1.7], [\u22120.075, 0.08], [\u221220.3, \u221220.2]\nwhere the boundaries are given by [lower, upper] in the logarithmic space.\n\n6.2.2 Phase-field model\n\nBackground The PFM is a flexible time-evolving interfacial physical model that can easily in-\ncorporate the multi-physical energy [13]. In this dataset, the PFM is applied to the simulation of\nspinodal decomposition, which is the self-organised nanostructure in the bistable Fe-Cr alloy at high\ntemperatures. Spinodal decomposition is an inherently stochastic process, making characterisation\nchallenging [14]. Therefore, Bayesian model selection is promising for estimating its parameter and\ndetermining the model physics component.\n\nProblem setting We wish to infer the posterior distribution of 4 simulation parameters\n(T, LcT , nB, Lg), where T is the temperature, LcT is the interaction parameter that defines the\ninteraction between composition and temperature, nB is the number of Bohr magnetons per atom,\nand Lg is the gradient energy coefficient. We have the observed time-series 2-dimensional images y.\n\nThe parameter inference is modelled as follows:\n\ny\u2217 = Sim(x\u2217)\n\n\u03c0(x\u2217) = Lognormal(x\u2217; \u00b5\u03c0, \u03a3\u03c0)\n\n\u2113true(x\u2217) = N [Sim(x\u2217); y, \u03c3nI]\n\n(153)\n(154)\n(155)\n\nwhere\n\u03c3n = 10\u22124\n\u00b5\u03c0 = [1.91, 0.718, 0.798, 0.693]\n\u03a3\u03c0 = diag([0.0003, 0.00006, 0.0001, 0.0001])\nin the logarithmic space.\n\n22\n\n\fParameters The observed data y is generated by the simulator defined as:\n\ny = Sim(xtrue) +\n\n\u221a\n\n\u03c3nU[0, 1]\n\n(156)\n\nwhere\ny is discretised in both spatially and time-domain. Time domain is discretised for 5000 seconds with\nthe sampling rate of 1 seconds, resulting in 5,000 data points. 2-dimensional space is discretised for\n64 \u00d7 64 nm2, with 64 \u00d7 64 nm2 pixels. The total data points are 64 \u00d7 64 \u00d7 5, 000 = 20, 480, 000.\nxtrue = [ exp(1.90657514) \u00d7 102, exp(0.71783979) \u00d7 104, exp(0.7975072), exp(0.69314718) \u00d7\n10\u221215 ]\n\nMetric The posterior distribution is evaluated via RMSE between true and inferred conditional\nposterior on each parameter. The RMSE is calculated on 50 grid samples for each dimension so as to\nslice the maximum value of the joint posterior. Each 50 grid samples are equally-spaced and bounded\nwith the following boundaries:\nbounds = [1.87, 1.94], [0.69, 0.73], [0.77, 0.83], [0.68, 0.73]\nwhere the boundaries are given by [lower, upper] in the logarithmic space.\n\n6.2.3 Hyperparameter marginalisation of hierarchical GP\n\nBackground The hierarchical GP model was designed for analysing the large-scale battery time-\nseries dataset from solar off-grid system field data all over the African continent [1]. The field data\ncontains the information of time-series operating conditions (I, T, V ), where I is the excitation\ncurrent, T is the temperature, and V is the voltage. We wish to estimate the state of health (SoH)\nfrom these field data, achieving the preventive battery replacement before it fails for the convenience\nof those who rely on the power system for their living. However, estimating the state of health is\nchallenging because the raw data (I, T, V ) is not correlated to the battery health. There are several\ndefinitions of SoH, but the internal resistance of a battery R is adopted in [1]. In the usual circuit\nelement, resistance can be easily calculated from R = V /I via Ohm\u2019s law. However, the battery\ninternal resistance R is way more complex. Battery internal resistance R is a function of (t, I, T, c),\nwhere t is time, c is the acid concentration. Furthermore, there are two factors of resistance variation;\nionic polarisation and aging. To incorporate these physical insights to the machine learning model, [1]\nis adopted the hierarchical GP model. First, they adopted the additive kernel of a squared exponential\nkernel and a Wiener velocity kernel to divide the ionic polarisation effect and aging effect. Second,\nthey adopted the hierarchical GPs to model V to divide into R-dependent GP and non-R-dependent\nGP to incorporate the Open Circuit Voltage-State of Charge (OCV-SOC) relationship.\n\nProblem setting We wish to infer the hyperposterior distribution of 5 GP hyperparameters\n(lT , lI , lc, \u03c30, \u03c31), where lT , lI , lc are the a squared exponential kernel lengthscale of temperature T ,\ncurrent I, and acid concentration c, respectively, and \u03c30, \u03c31 are the kernel variances of a squared\nexponential kernel and a Wiener velocity kernel, respectively. We have the observed time-series\ndataset of (I, T, V ) as y.\n\nThe hyperposterior inference is based on the energy function \u03a6(x) (The details can be found in [1],\nEquation (15) in the Appendix information).\n\n\u03a6x = \u2212 log p(y|x) \u2212 log p(x)\n\n= \u2212 log p(x) +\n\n1\n2\n\n(cid:88)\n\nt\n\nlog |St(x)| +\n\n1\n2\n\n(cid:88)\n\nt\n\nt St(x)\u22121et +\neT\n\n(cid:88)\n\nt\n\nnt\n2\n\nlog 2\u03c0\n\n(157)\n\n(158)\n\nwhere\np(x) = Lognormal(x\u2217; \u00b5\u03c0, \u03a3\u03c0) is a hyperprior.\net is the error vector for each charging segment.\nnt is the number of observations in the charging segment.\nSt(x) is the innovation covariance for the segment.\n\u00b5\u03c0 = [3.96, 1.94, 2.79, 2.26, 0.34]\n\u03a3\u03c0 = diag([1, 1, 1, 1, 1])\nin the logarithmic space.\n\n23\n\n\fMetric The posterior distribution is evaluated via RMSE between true and inferred conditional\nposterior on each parameter. The RMSE is calculated on 50 grid samples for each dimension so as to\nslice the maximum value of the joint posterior. Each 50 grid samples are equally-spaced and bounded\nwith the following boundaries:\nbounds = [\u221210, 10], [\u221210, 10], [\u221210, 10], [\u221210, 10], [\u221210, 10]\nwhere the boundaries are given by [lower, upper] in the logarithmic space.\n\n\n\nBased on the section(s), please answer the following question with yes, no, or n/a and provide a brief justification for your answer.\nQuestion: If the authors ran experiments, do the authors specify all the training details (e.g., data splits, hyperparameters, how they were chosen)?"}, "3c": {"role": "user", "content": "The following is the experiments section of the paper you are reviewing:\n\n\nGiven our new model BASQ, with three variants of the proposal distribution, IVR, IGB, and UB, we\nnow test for speed against MC samplers and batch WSABI. We compared with three NS methods\n[71, 29, 46, 14, 15], coded with [72, 17]. According to the review [16], MLFriends is the state-of-the-\nart NS sampler to date. The code is implemented based on [77, 34, 42, 76, 38, 31, 10, 7], and code\naround kernel recombination [24, 44] with additional modification. All experiments on synthetic\ndatasets were averaged over 10 repeats, computed in parallel with multicore CPUs, without GPU for\nfair comparison.8 The posterior distribution of NS was estimated via KDE with weighted samples\n[33]. For maximum speed performance, batch size was optimised for each method in each dataset,\nin fairness to the competitors. Batch WSABI needs to optimise batch size to balance the likelihood\nquery cost and sampling cost, because sampling cost increases rapidly with batch size, as shown\nin Figure 1(a). Therefore, it has an optimal batch size for faster convergence. By wall time cost,\nwe exclude the cost of integrand evaluation; that is, the wall time cost is the overhead cost of batch\nevaluation. Details can be found in the Supplementary.\n\n7Negative elements in the matrices only exist in K(X, X)\u22121, which can be drawn from the memory of the\nGP regression model without additional calculation. The number of positive components is half of the matrix on\naverage, resulting in O(n2/2). Then, taking the threshold via the inverse of the recombination sample size N ,\nthe number of components becomes ncomp \u226a n2, resulting in sampling complexity O(n2/2 + ncompN ).\n\n8Performed on MacBook Pro 2019, 2.4 GHz 8-Core Intel Core i9, 64 GB 2667 MHz DDR4\n\n7\n\n\f5.1 Synthetic problems\n\nWe evaluate all methods on three synthetic problems. The goal is to estimate the integral and posterior\nof the likelihood modelled with the highly multimodal functions. Prior was set to a two-dimensional\nmultivariate normal distribution, with a zero mean vector, and covariance whose diagonal elements\nare 2. The optimised batch sizes for each methods are BASQ: 100, batch WSABI: 16. The synthetic\nlikelihood functions are cheap (0.5 ms on average). This is advantageous setting for NS: Within 10\nseconds, the batch WSABI, BASQ, and NS collected 32, 600, 23225 samples, respectively. As for\nthe metrics, posterior estimation was tested with Kullback-Leibler (KL) upon random 10,000 samples\nfrom true posterior. Evidence was evaluated with MAE, and ground truth was derived analytically.\n\nLikelihood functions Branin-Hoo [49] is 8 modal function in two-dimensional space. Ackley [73]\nis a highly multimodal function with point symmetric periodical peaks in two-dimensional space.\nOscillatory function [32] is a highly multimodal function with reflection symmetric periodical peaks\nof highly-correlated ellipsoids in two-dimensional space.\n\n5.2 Real-world dataset\n\nWe consider three real-world applications with expensive likelihoods, which are simulator-based\nand hierarchical GP. We adopted the empirical metric due to no ground truth. For the posterior, we\ncan calculate the true conditional posterior distribution along the line passing through ground truth\nparameter points. Then, evaluate the posterior with root mean squared error (RMSE) against 50\ntest samples for each dimension. For integral, we compare the model evidence itself. Expensive\nlikelihoods makes the sample size per wall time amongst the methods no significant difference,\nwhereas rejection sampling based NS dismiss more than 50% of queried samples. The batch sizes are\nBASQ: 32, batch WSABI: 8. (see Supplementary)\n\n: The simulator is the SPMe [61], 9\nParameter estimation of the lithium-ion battery simulator\nestimating 3 simulation parameters at a given time-series voltage-current signal (the diffusivity of\nlithium-ion on the anode and cathode, and the experimental noise variance). Prior is modified to log\nmultivariate normal distribution from [4]. Each query takes 1.2 seconds on average.\n\nParameter estimation of the phase-field model\n: The simulator is the phase-field model [61],\n10 estimating 4 simulation parameters at given time-series two-dimensional morphological image\n(temperature, interaction parameter, Bohr magneton coefficient, and gradient energy coefficient).\nPrior is a log multivariate normal distribution. Each query takes 7.4 seconds on average.\n\nHyperparameter marginalisation of hierarchical GP model The hierarchical GP model was\ndesigned for analysing the large-scale battery time-series dataset from solar off-grid system field data\n[3].8 For fast estimation of parameters in each GP, the recursive technique [74] is adopted. The task\nis to marginalise 5 GP hyperparameters at given hyperprior, which is modified to log multivariate\nnormal distribution from [3]. Each query takes 1.1 seconds on average.\n\n5.3 Results\n\nWe find BASQ consistently delivers strong empirical performance, as shown in Figure 2. On all\nbenchmark problems, BASQ-IVR, IGB, or UB outperform baseline methods except in the battery\nsimulator evidence estimation. The very low-dimensional and sharp unimodal nature of this likelihood\ncould be advantageous for biased greedy batch WSABI, as IGB superiority supports this viewpoint.\nThis suggests that BASQ could be a generally fast Bayesian solver as far as we investigated. In the\nmultimodal setting of the synthetic dataset, BASQ-UB outperforms, whereas IVR does in a simulator-\nbased likelihoods. When comparing each proposal distribution, BASQ-IVR was the performant. Our\nresults support the general use of IVR, or UB if the likelihood is known to be highly multimodal.\n\n9SPMe code used was translated into Python from MATLAB [11, 12]. This open-source code is published\n\nunder the BSD 3-clause Licence. See more information on [11]\n\n10Code used was from [54, 3]. All rights of the code are reserved by the authors. Thus, we do not redistribute\n\nthe original code.\n\n8\n\n\fFigure 2: Time in seconds vs. KL divergence for posterior, and MAE for evidence in the synthetic\ndatasets. Time in seconds vs. RMSE for posterior and evidence itself in the real-world dataset.\n\n\n\nBased on the section(s), please answer the following question with yes, no, or n/a and provide a brief justification for your answer.\nQuestion: If the authors ran experiments, do the authors report error bars (e.g., with respect to the random seed after running experiments multiple times)?"}, "3d": {"role": "user", "content": "The following is the experiments section of the paper you are reviewing:\n\n\nGiven our new model BASQ, with three variants of the proposal distribution, IVR, IGB, and UB, we\nnow test for speed against MC samplers and batch WSABI. We compared with three NS methods\n[71, 29, 46, 14, 15], coded with [72, 17]. According to the review [16], MLFriends is the state-of-the-\nart NS sampler to date. The code is implemented based on [77, 34, 42, 76, 38, 31, 10, 7], and code\naround kernel recombination [24, 44] with additional modification. All experiments on synthetic\ndatasets were averaged over 10 repeats, computed in parallel with multicore CPUs, without GPU for\nfair comparison.8 The posterior distribution of NS was estimated via KDE with weighted samples\n[33]. For maximum speed performance, batch size was optimised for each method in each dataset,\nin fairness to the competitors. Batch WSABI needs to optimise batch size to balance the likelihood\nquery cost and sampling cost, because sampling cost increases rapidly with batch size, as shown\nin Figure 1(a). Therefore, it has an optimal batch size for faster convergence. By wall time cost,\nwe exclude the cost of integrand evaluation; that is, the wall time cost is the overhead cost of batch\nevaluation. Details can be found in the Supplementary.\n\n7Negative elements in the matrices only exist in K(X, X)\u22121, which can be drawn from the memory of the\nGP regression model without additional calculation. The number of positive components is half of the matrix on\naverage, resulting in O(n2/2). Then, taking the threshold via the inverse of the recombination sample size N ,\nthe number of components becomes ncomp \u226a n2, resulting in sampling complexity O(n2/2 + ncompN ).\n\n8Performed on MacBook Pro 2019, 2.4 GHz 8-Core Intel Core i9, 64 GB 2667 MHz DDR4\n\n7\n\n\f5.1 Synthetic problems\n\nWe evaluate all methods on three synthetic problems. The goal is to estimate the integral and posterior\nof the likelihood modelled with the highly multimodal functions. Prior was set to a two-dimensional\nmultivariate normal distribution, with a zero mean vector, and covariance whose diagonal elements\nare 2. The optimised batch sizes for each methods are BASQ: 100, batch WSABI: 16. The synthetic\nlikelihood functions are cheap (0.5 ms on average). This is advantageous setting for NS: Within 10\nseconds, the batch WSABI, BASQ, and NS collected 32, 600, 23225 samples, respectively. As for\nthe metrics, posterior estimation was tested with Kullback-Leibler (KL) upon random 10,000 samples\nfrom true posterior. Evidence was evaluated with MAE, and ground truth was derived analytically.\n\nLikelihood functions Branin-Hoo [49] is 8 modal function in two-dimensional space. Ackley [73]\nis a highly multimodal function with point symmetric periodical peaks in two-dimensional space.\nOscillatory function [32] is a highly multimodal function with reflection symmetric periodical peaks\nof highly-correlated ellipsoids in two-dimensional space.\n\n5.2 Real-world dataset\n\nWe consider three real-world applications with expensive likelihoods, which are simulator-based\nand hierarchical GP. We adopted the empirical metric due to no ground truth. For the posterior, we\ncan calculate the true conditional posterior distribution along the line passing through ground truth\nparameter points. Then, evaluate the posterior with root mean squared error (RMSE) against 50\ntest samples for each dimension. For integral, we compare the model evidence itself. Expensive\nlikelihoods makes the sample size per wall time amongst the methods no significant difference,\nwhereas rejection sampling based NS dismiss more than 50% of queried samples. The batch sizes are\nBASQ: 32, batch WSABI: 8. (see Supplementary)\n\n: The simulator is the SPMe [61], 9\nParameter estimation of the lithium-ion battery simulator\nestimating 3 simulation parameters at a given time-series voltage-current signal (the diffusivity of\nlithium-ion on the anode and cathode, and the experimental noise variance). Prior is modified to log\nmultivariate normal distribution from [4]. Each query takes 1.2 seconds on average.\n\nParameter estimation of the phase-field model\n: The simulator is the phase-field model [61],\n10 estimating 4 simulation parameters at given time-series two-dimensional morphological image\n(temperature, interaction parameter, Bohr magneton coefficient, and gradient energy coefficient).\nPrior is a log multivariate normal distribution. Each query takes 7.4 seconds on average.\n\nHyperparameter marginalisation of hierarchical GP model The hierarchical GP model was\ndesigned for analysing the large-scale battery time-series dataset from solar off-grid system field data\n[3].8 For fast estimation of parameters in each GP, the recursive technique [74] is adopted. The task\nis to marginalise 5 GP hyperparameters at given hyperprior, which is modified to log multivariate\nnormal distribution from [3]. Each query takes 1.1 seconds on average.\n\n5.3 Results\n\nWe find BASQ consistently delivers strong empirical performance, as shown in Figure 2. On all\nbenchmark problems, BASQ-IVR, IGB, or UB outperform baseline methods except in the battery\nsimulator evidence estimation. The very low-dimensional and sharp unimodal nature of this likelihood\ncould be advantageous for biased greedy batch WSABI, as IGB superiority supports this viewpoint.\nThis suggests that BASQ could be a generally fast Bayesian solver as far as we investigated. In the\nmultimodal setting of the synthetic dataset, BASQ-UB outperforms, whereas IVR does in a simulator-\nbased likelihoods. When comparing each proposal distribution, BASQ-IVR was the performant. Our\nresults support the general use of IVR, or UB if the likelihood is known to be highly multimodal.\n\n9SPMe code used was translated into Python from MATLAB [11, 12]. This open-source code is published\n\nunder the BSD 3-clause Licence. See more information on [11]\n\n10Code used was from [54, 3]. All rights of the code are reserved by the authors. Thus, we do not redistribute\n\nthe original code.\n\n8\n\n\fFigure 2: Time in seconds vs. KL divergence for posterior, and MAE for evidence in the synthetic\ndatasets. Time in seconds vs. RMSE for posterior and evidence itself in the real-world dataset.\n\n\n\nBased on the section(s), please answer the following question with yes, no, or n/a and provide a brief justification for your answer.\nQuestion: If the authors ran experiments, do the authors include the total amount of compute and the type of resources used (e.g., type of GPUs, internal cluster, or cloud provider)?"}, "4a": {"role": "user", "content": "The following is the experiments section of the paper you are reviewing:\n\n\nGiven our new model BASQ, with three variants of the proposal distribution, IVR, IGB, and UB, we\nnow test for speed against MC samplers and batch WSABI. We compared with three NS methods\n[71, 29, 46, 14, 15], coded with [72, 17]. According to the review [16], MLFriends is the state-of-the-\nart NS sampler to date. The code is implemented based on [77, 34, 42, 76, 38, 31, 10, 7], and code\naround kernel recombination [24, 44] with additional modification. All experiments on synthetic\ndatasets were averaged over 10 repeats, computed in parallel with multicore CPUs, without GPU for\nfair comparison.8 The posterior distribution of NS was estimated via KDE with weighted samples\n[33]. For maximum speed performance, batch size was optimised for each method in each dataset,\nin fairness to the competitors. Batch WSABI needs to optimise batch size to balance the likelihood\nquery cost and sampling cost, because sampling cost increases rapidly with batch size, as shown\nin Figure 1(a). Therefore, it has an optimal batch size for faster convergence. By wall time cost,\nwe exclude the cost of integrand evaluation; that is, the wall time cost is the overhead cost of batch\nevaluation. Details can be found in the Supplementary.\n\n7Negative elements in the matrices only exist in K(X, X)\u22121, which can be drawn from the memory of the\nGP regression model without additional calculation. The number of positive components is half of the matrix on\naverage, resulting in O(n2/2). Then, taking the threshold via the inverse of the recombination sample size N ,\nthe number of components becomes ncomp \u226a n2, resulting in sampling complexity O(n2/2 + ncompN ).\n\n8Performed on MacBook Pro 2019, 2.4 GHz 8-Core Intel Core i9, 64 GB 2667 MHz DDR4\n\n7\n\n\f5.1 Synthetic problems\n\nWe evaluate all methods on three synthetic problems. The goal is to estimate the integral and posterior\nof the likelihood modelled with the highly multimodal functions. Prior was set to a two-dimensional\nmultivariate normal distribution, with a zero mean vector, and covariance whose diagonal elements\nare 2. The optimised batch sizes for each methods are BASQ: 100, batch WSABI: 16. The synthetic\nlikelihood functions are cheap (0.5 ms on average). This is advantageous setting for NS: Within 10\nseconds, the batch WSABI, BASQ, and NS collected 32, 600, 23225 samples, respectively. As for\nthe metrics, posterior estimation was tested with Kullback-Leibler (KL) upon random 10,000 samples\nfrom true posterior. Evidence was evaluated with MAE, and ground truth was derived analytically.\n\nLikelihood functions Branin-Hoo [49] is 8 modal function in two-dimensional space. Ackley [73]\nis a highly multimodal function with point symmetric periodical peaks in two-dimensional space.\nOscillatory function [32] is a highly multimodal function with reflection symmetric periodical peaks\nof highly-correlated ellipsoids in two-dimensional space.\n\n5.2 Real-world dataset\n\nWe consider three real-world applications with expensive likelihoods, which are simulator-based\nand hierarchical GP. We adopted the empirical metric due to no ground truth. For the posterior, we\ncan calculate the true conditional posterior distribution along the line passing through ground truth\nparameter points. Then, evaluate the posterior with root mean squared error (RMSE) against 50\ntest samples for each dimension. For integral, we compare the model evidence itself. Expensive\nlikelihoods makes the sample size per wall time amongst the methods no significant difference,\nwhereas rejection sampling based NS dismiss more than 50% of queried samples. The batch sizes are\nBASQ: 32, batch WSABI: 8. (see Supplementary)\n\n: The simulator is the SPMe [61], 9\nParameter estimation of the lithium-ion battery simulator\nestimating 3 simulation parameters at a given time-series voltage-current signal (the diffusivity of\nlithium-ion on the anode and cathode, and the experimental noise variance). Prior is modified to log\nmultivariate normal distribution from [4]. Each query takes 1.2 seconds on average.\n\nParameter estimation of the phase-field model\n: The simulator is the phase-field model [61],\n10 estimating 4 simulation parameters at given time-series two-dimensional morphological image\n(temperature, interaction parameter, Bohr magneton coefficient, and gradient energy coefficient).\nPrior is a log multivariate normal distribution. Each query takes 7.4 seconds on average.\n\nHyperparameter marginalisation of hierarchical GP model The hierarchical GP model was\ndesigned for analysing the large-scale battery time-series dataset from solar off-grid system field data\n[3].8 For fast estimation of parameters in each GP, the recursive technique [74] is adopted. The task\nis to marginalise 5 GP hyperparameters at given hyperprior, which is modified to log multivariate\nnormal distribution from [3]. Each query takes 1.1 seconds on average.\n\n5.3 Results\n\nWe find BASQ consistently delivers strong empirical performance, as shown in Figure 2. On all\nbenchmark problems, BASQ-IVR, IGB, or UB outperform baseline methods except in the battery\nsimulator evidence estimation. The very low-dimensional and sharp unimodal nature of this likelihood\ncould be advantageous for biased greedy batch WSABI, as IGB superiority supports this viewpoint.\nThis suggests that BASQ could be a generally fast Bayesian solver as far as we investigated. In the\nmultimodal setting of the synthetic dataset, BASQ-UB outperforms, whereas IVR does in a simulator-\nbased likelihoods. When comparing each proposal distribution, BASQ-IVR was the performant. Our\nresults support the general use of IVR, or UB if the likelihood is known to be highly multimodal.\n\n9SPMe code used was translated into Python from MATLAB [11, 12]. This open-source code is published\n\nunder the BSD 3-clause Licence. See more information on [11]\n\n10Code used was from [54, 3]. All rights of the code are reserved by the authors. Thus, we do not redistribute\n\nthe original code.\n\n8\n\n\fFigure 2: Time in seconds vs. KL divergence for posterior, and MAE for evidence in the synthetic\ndatasets. Time in seconds vs. RMSE for posterior and evidence itself in the real-world dataset.\n\n\n\nBased on the section(s), please answer the following question with yes, no, or n/a and provide a brief justification for your answer.\nQuestion: If the authors use existing assets (e.g., code, data, models), do the authors cite the creators?"}, "4b": {"role": "user", "content": "The following is the experiments section of the paper you are reviewing:\n\n\nGiven our new model BASQ, with three variants of the proposal distribution, IVR, IGB, and UB, we\nnow test for speed against MC samplers and batch WSABI. We compared with three NS methods\n[71, 29, 46, 14, 15], coded with [72, 17]. According to the review [16], MLFriends is the state-of-the-\nart NS sampler to date. The code is implemented based on [77, 34, 42, 76, 38, 31, 10, 7], and code\naround kernel recombination [24, 44] with additional modification. All experiments on synthetic\ndatasets were averaged over 10 repeats, computed in parallel with multicore CPUs, without GPU for\nfair comparison.8 The posterior distribution of NS was estimated via KDE with weighted samples\n[33]. For maximum speed performance, batch size was optimised for each method in each dataset,\nin fairness to the competitors. Batch WSABI needs to optimise batch size to balance the likelihood\nquery cost and sampling cost, because sampling cost increases rapidly with batch size, as shown\nin Figure 1(a). Therefore, it has an optimal batch size for faster convergence. By wall time cost,\nwe exclude the cost of integrand evaluation; that is, the wall time cost is the overhead cost of batch\nevaluation. Details can be found in the Supplementary.\n\n7Negative elements in the matrices only exist in K(X, X)\u22121, which can be drawn from the memory of the\nGP regression model without additional calculation. The number of positive components is half of the matrix on\naverage, resulting in O(n2/2). Then, taking the threshold via the inverse of the recombination sample size N ,\nthe number of components becomes ncomp \u226a n2, resulting in sampling complexity O(n2/2 + ncompN ).\n\n8Performed on MacBook Pro 2019, 2.4 GHz 8-Core Intel Core i9, 64 GB 2667 MHz DDR4\n\n7\n\n\f5.1 Synthetic problems\n\nWe evaluate all methods on three synthetic problems. The goal is to estimate the integral and posterior\nof the likelihood modelled with the highly multimodal functions. Prior was set to a two-dimensional\nmultivariate normal distribution, with a zero mean vector, and covariance whose diagonal elements\nare 2. The optimised batch sizes for each methods are BASQ: 100, batch WSABI: 16. The synthetic\nlikelihood functions are cheap (0.5 ms on average). This is advantageous setting for NS: Within 10\nseconds, the batch WSABI, BASQ, and NS collected 32, 600, 23225 samples, respectively. As for\nthe metrics, posterior estimation was tested with Kullback-Leibler (KL) upon random 10,000 samples\nfrom true posterior. Evidence was evaluated with MAE, and ground truth was derived analytically.\n\nLikelihood functions Branin-Hoo [49] is 8 modal function in two-dimensional space. Ackley [73]\nis a highly multimodal function with point symmetric periodical peaks in two-dimensional space.\nOscillatory function [32] is a highly multimodal function with reflection symmetric periodical peaks\nof highly-correlated ellipsoids in two-dimensional space.\n\n5.2 Real-world dataset\n\nWe consider three real-world applications with expensive likelihoods, which are simulator-based\nand hierarchical GP. We adopted the empirical metric due to no ground truth. For the posterior, we\ncan calculate the true conditional posterior distribution along the line passing through ground truth\nparameter points. Then, evaluate the posterior with root mean squared error (RMSE) against 50\ntest samples for each dimension. For integral, we compare the model evidence itself. Expensive\nlikelihoods makes the sample size per wall time amongst the methods no significant difference,\nwhereas rejection sampling based NS dismiss more than 50% of queried samples. The batch sizes are\nBASQ: 32, batch WSABI: 8. (see Supplementary)\n\n: The simulator is the SPMe [61], 9\nParameter estimation of the lithium-ion battery simulator\nestimating 3 simulation parameters at a given time-series voltage-current signal (the diffusivity of\nlithium-ion on the anode and cathode, and the experimental noise variance). Prior is modified to log\nmultivariate normal distribution from [4]. Each query takes 1.2 seconds on average.\n\nParameter estimation of the phase-field model\n: The simulator is the phase-field model [61],\n10 estimating 4 simulation parameters at given time-series two-dimensional morphological image\n(temperature, interaction parameter, Bohr magneton coefficient, and gradient energy coefficient).\nPrior is a log multivariate normal distribution. Each query takes 7.4 seconds on average.\n\nHyperparameter marginalisation of hierarchical GP model The hierarchical GP model was\ndesigned for analysing the large-scale battery time-series dataset from solar off-grid system field data\n[3].8 For fast estimation of parameters in each GP, the recursive technique [74] is adopted. The task\nis to marginalise 5 GP hyperparameters at given hyperprior, which is modified to log multivariate\nnormal distribution from [3]. Each query takes 1.1 seconds on average.\n\n5.3 Results\n\nWe find BASQ consistently delivers strong empirical performance, as shown in Figure 2. On all\nbenchmark problems, BASQ-IVR, IGB, or UB outperform baseline methods except in the battery\nsimulator evidence estimation. The very low-dimensional and sharp unimodal nature of this likelihood\ncould be advantageous for biased greedy batch WSABI, as IGB superiority supports this viewpoint.\nThis suggests that BASQ could be a generally fast Bayesian solver as far as we investigated. In the\nmultimodal setting of the synthetic dataset, BASQ-UB outperforms, whereas IVR does in a simulator-\nbased likelihoods. When comparing each proposal distribution, BASQ-IVR was the performant. Our\nresults support the general use of IVR, or UB if the likelihood is known to be highly multimodal.\n\n9SPMe code used was translated into Python from MATLAB [11, 12]. This open-source code is published\n\nunder the BSD 3-clause Licence. See more information on [11]\n\n10Code used was from [54, 3]. All rights of the code are reserved by the authors. Thus, we do not redistribute\n\nthe original code.\n\n8\n\n\fFigure 2: Time in seconds vs. KL divergence for posterior, and MAE for evidence in the synthetic\ndatasets. Time in seconds vs. RMSE for posterior and evidence itself in the real-world dataset.\n\n\n\nBased on the section(s), please answer the following question with yes, no, or n/a and provide a brief justification for your answer.\nQuestion: If the authors use existing assets (e.g., code, data, models) or curate/release new assets, do the authors mention the license of the assets?"}, "4c": {"role": "user", "content": "The following is the introduction section of the paper you are reviewing:\n\n\nMany applications in science, engineering, and economics involve complex simulations to explain the\nstructure and dynamics of the process. Such models are derived from knowledge of the mechanisms\nand principles underlying the data-generating process, and are critical for scientific hypothesis-\nbuilding and testing. However, dozens of plausible simulators describing the same phenomena often\nexist, owing to differing assumptions or levels of approximation. Similar situations can be found in\nselection of simulator-based control models, selection of machine learning models on large-scale\ndatasets, and in many data assimilation applications [28].\n\nIn such settings, with multiple competing models, choosing the best model for the dataset is crucial.\nBayesian model evidence gives a clear criteria for such model selection. However, computing\nmodel evidence requires integration over the likelihood, which is challenging, particularly when\nthe likelihood is non-closed-form and/or expensive. The ascertained model is often applied to\n\n\u2217Equal contribution\n2Code: https://github.com/ma921/BASQ\n\n36th Conference on Neural Information Processing Systems (NeurIPS 2022).\n\n\fproduce posteriors for prediction and parameter estimation afterwards. There are many algorithms\nspecialised for the calculation of model evidences or posteriors, although only a limited number of\nBayesian inference solvers estimate both model evidence and posteriors in one go. As such, costly\ncomputations are often repeated (at least) twice. Addressing this concern, nested sampling (NS)\n[71, 46] was developed to estimate both model evidence and posteriors simultaneously, and has been\nbroadly applied, especially amongst astrophysicists for cosmological model selection [63]. However,\nNS is based on a Monte Carlo (MC) sampler, and its slow convergence rate is a practical hindrance.\n\nTo aid NS, and other approaches, parallel computing is widely applied to improve the speed of\nwall-clock computation. Modern computer clusters and graphical processing units enable scientists\nto query the likelihood in large batches. However, parallelisation can, at best, linearly accelerate NS,\ndoing little to counter NS\u2019s inherently slow convergence rate as a MC sampler.\n\nThis paper investigates batch Bayesian quadrature (BQ) [65] for fast Bayesian inference. BQ solves\nthe integral as an inference problem, modelling the likelihood function with a probabilistic model\n(typically a Gaussian process (GP)). Gunter et al. [37] proposed Warped sequential active Bayesian\nintegration (WSABI), which adopts active learning to select samples upon uncertainty over the\nintegrand. WSABI showed that BQ with expensive GP calculations could achieve faster convergence\nin wall time than cheap MC samplers. Wagstaff et al. [78] introduced batch WSABI, achieving even\nfaster calculation via parallel computing and became the fastest BQ model to date. We improve upon\nthese existing works for a large-scale batch case.\n\n\n\nBased on the section(s), please answer the following question with yes, no, or n/a and provide a brief justification for your answer.\nQuestion: If the authors curate/release new assets (e.g., code, data, models), do the authors include any new assets either in the supplemental material or as a URL?"}, "4d": {"role": "user", "content": "The following is the experiments section of the paper you are reviewing:\n\n\nGiven our new model BASQ, with three variants of the proposal distribution, IVR, IGB, and UB, we\nnow test for speed against MC samplers and batch WSABI. We compared with three NS methods\n[71, 29, 46, 14, 15], coded with [72, 17]. According to the review [16], MLFriends is the state-of-the-\nart NS sampler to date. The code is implemented based on [77, 34, 42, 76, 38, 31, 10, 7], and code\naround kernel recombination [24, 44] with additional modification. All experiments on synthetic\ndatasets were averaged over 10 repeats, computed in parallel with multicore CPUs, without GPU for\nfair comparison.8 The posterior distribution of NS was estimated via KDE with weighted samples\n[33]. For maximum speed performance, batch size was optimised for each method in each dataset,\nin fairness to the competitors. Batch WSABI needs to optimise batch size to balance the likelihood\nquery cost and sampling cost, because sampling cost increases rapidly with batch size, as shown\nin Figure 1(a). Therefore, it has an optimal batch size for faster convergence. By wall time cost,\nwe exclude the cost of integrand evaluation; that is, the wall time cost is the overhead cost of batch\nevaluation. Details can be found in the Supplementary.\n\n7Negative elements in the matrices only exist in K(X, X)\u22121, which can be drawn from the memory of the\nGP regression model without additional calculation. The number of positive components is half of the matrix on\naverage, resulting in O(n2/2). Then, taking the threshold via the inverse of the recombination sample size N ,\nthe number of components becomes ncomp \u226a n2, resulting in sampling complexity O(n2/2 + ncompN ).\n\n8Performed on MacBook Pro 2019, 2.4 GHz 8-Core Intel Core i9, 64 GB 2667 MHz DDR4\n\n7\n\n\f5.1 Synthetic problems\n\nWe evaluate all methods on three synthetic problems. The goal is to estimate the integral and posterior\nof the likelihood modelled with the highly multimodal functions. Prior was set to a two-dimensional\nmultivariate normal distribution, with a zero mean vector, and covariance whose diagonal elements\nare 2. The optimised batch sizes for each methods are BASQ: 100, batch WSABI: 16. The synthetic\nlikelihood functions are cheap (0.5 ms on average). This is advantageous setting for NS: Within 10\nseconds, the batch WSABI, BASQ, and NS collected 32, 600, 23225 samples, respectively. As for\nthe metrics, posterior estimation was tested with Kullback-Leibler (KL) upon random 10,000 samples\nfrom true posterior. Evidence was evaluated with MAE, and ground truth was derived analytically.\n\nLikelihood functions Branin-Hoo [49] is 8 modal function in two-dimensional space. Ackley [73]\nis a highly multimodal function with point symmetric periodical peaks in two-dimensional space.\nOscillatory function [32] is a highly multimodal function with reflection symmetric periodical peaks\nof highly-correlated ellipsoids in two-dimensional space.\n\n5.2 Real-world dataset\n\nWe consider three real-world applications with expensive likelihoods, which are simulator-based\nand hierarchical GP. We adopted the empirical metric due to no ground truth. For the posterior, we\ncan calculate the true conditional posterior distribution along the line passing through ground truth\nparameter points. Then, evaluate the posterior with root mean squared error (RMSE) against 50\ntest samples for each dimension. For integral, we compare the model evidence itself. Expensive\nlikelihoods makes the sample size per wall time amongst the methods no significant difference,\nwhereas rejection sampling based NS dismiss more than 50% of queried samples. The batch sizes are\nBASQ: 32, batch WSABI: 8. (see Supplementary)\n\n: The simulator is the SPMe [61], 9\nParameter estimation of the lithium-ion battery simulator\nestimating 3 simulation parameters at a given time-series voltage-current signal (the diffusivity of\nlithium-ion on the anode and cathode, and the experimental noise variance). Prior is modified to log\nmultivariate normal distribution from [4]. Each query takes 1.2 seconds on average.\n\nParameter estimation of the phase-field model\n: The simulator is the phase-field model [61],\n10 estimating 4 simulation parameters at given time-series two-dimensional morphological image\n(temperature, interaction parameter, Bohr magneton coefficient, and gradient energy coefficient).\nPrior is a log multivariate normal distribution. Each query takes 7.4 seconds on average.\n\nHyperparameter marginalisation of hierarchical GP model The hierarchical GP model was\ndesigned for analysing the large-scale battery time-series dataset from solar off-grid system field data\n[3].8 For fast estimation of parameters in each GP, the recursive technique [74] is adopted. The task\nis to marginalise 5 GP hyperparameters at given hyperprior, which is modified to log multivariate\nnormal distribution from [3]. Each query takes 1.1 seconds on average.\n\n5.3 Results\n\nWe find BASQ consistently delivers strong empirical performance, as shown in Figure 2. On all\nbenchmark problems, BASQ-IVR, IGB, or UB outperform baseline methods except in the battery\nsimulator evidence estimation. The very low-dimensional and sharp unimodal nature of this likelihood\ncould be advantageous for biased greedy batch WSABI, as IGB superiority supports this viewpoint.\nThis suggests that BASQ could be a generally fast Bayesian solver as far as we investigated. In the\nmultimodal setting of the synthetic dataset, BASQ-UB outperforms, whereas IVR does in a simulator-\nbased likelihoods. When comparing each proposal distribution, BASQ-IVR was the performant. Our\nresults support the general use of IVR, or UB if the likelihood is known to be highly multimodal.\n\n9SPMe code used was translated into Python from MATLAB [11, 12]. This open-source code is published\n\nunder the BSD 3-clause Licence. See more information on [11]\n\n10Code used was from [54, 3]. All rights of the code are reserved by the authors. Thus, we do not redistribute\n\nthe original code.\n\n8\n\n\fFigure 2: Time in seconds vs. KL divergence for posterior, and MAE for evidence in the synthetic\ndatasets. Time in seconds vs. RMSE for posterior and evidence itself in the real-world dataset.\n\n\n\nBased on the section(s), please answer the following question with yes, no, or n/a and provide a brief justification for your answer.\nQuestion: If the authors curate/release new assets (e.g., code, data, models), do the authors discuss whether and how consent was obtained from people whose data they are using/curating?"}, "4e": {"role": "user", "content": "The following is the experiments section of the paper you are reviewing:\n\n\nGiven our new model BASQ, with three variants of the proposal distribution, IVR, IGB, and UB, we\nnow test for speed against MC samplers and batch WSABI. We compared with three NS methods\n[71, 29, 46, 14, 15], coded with [72, 17]. According to the review [16], MLFriends is the state-of-the-\nart NS sampler to date. The code is implemented based on [77, 34, 42, 76, 38, 31, 10, 7], and code\naround kernel recombination [24, 44] with additional modification. All experiments on synthetic\ndatasets were averaged over 10 repeats, computed in parallel with multicore CPUs, without GPU for\nfair comparison.8 The posterior distribution of NS was estimated via KDE with weighted samples\n[33]. For maximum speed performance, batch size was optimised for each method in each dataset,\nin fairness to the competitors. Batch WSABI needs to optimise batch size to balance the likelihood\nquery cost and sampling cost, because sampling cost increases rapidly with batch size, as shown\nin Figure 1(a). Therefore, it has an optimal batch size for faster convergence. By wall time cost,\nwe exclude the cost of integrand evaluation; that is, the wall time cost is the overhead cost of batch\nevaluation. Details can be found in the Supplementary.\n\n7Negative elements in the matrices only exist in K(X, X)\u22121, which can be drawn from the memory of the\nGP regression model without additional calculation. The number of positive components is half of the matrix on\naverage, resulting in O(n2/2). Then, taking the threshold via the inverse of the recombination sample size N ,\nthe number of components becomes ncomp \u226a n2, resulting in sampling complexity O(n2/2 + ncompN ).\n\n8Performed on MacBook Pro 2019, 2.4 GHz 8-Core Intel Core i9, 64 GB 2667 MHz DDR4\n\n7\n\n\f5.1 Synthetic problems\n\nWe evaluate all methods on three synthetic problems. The goal is to estimate the integral and posterior\nof the likelihood modelled with the highly multimodal functions. Prior was set to a two-dimensional\nmultivariate normal distribution, with a zero mean vector, and covariance whose diagonal elements\nare 2. The optimised batch sizes for each methods are BASQ: 100, batch WSABI: 16. The synthetic\nlikelihood functions are cheap (0.5 ms on average). This is advantageous setting for NS: Within 10\nseconds, the batch WSABI, BASQ, and NS collected 32, 600, 23225 samples, respectively. As for\nthe metrics, posterior estimation was tested with Kullback-Leibler (KL) upon random 10,000 samples\nfrom true posterior. Evidence was evaluated with MAE, and ground truth was derived analytically.\n\nLikelihood functions Branin-Hoo [49] is 8 modal function in two-dimensional space. Ackley [73]\nis a highly multimodal function with point symmetric periodical peaks in two-dimensional space.\nOscillatory function [32] is a highly multimodal function with reflection symmetric periodical peaks\nof highly-correlated ellipsoids in two-dimensional space.\n\n5.2 Real-world dataset\n\nWe consider three real-world applications with expensive likelihoods, which are simulator-based\nand hierarchical GP. We adopted the empirical metric due to no ground truth. For the posterior, we\ncan calculate the true conditional posterior distribution along the line passing through ground truth\nparameter points. Then, evaluate the posterior with root mean squared error (RMSE) against 50\ntest samples for each dimension. For integral, we compare the model evidence itself. Expensive\nlikelihoods makes the sample size per wall time amongst the methods no significant difference,\nwhereas rejection sampling based NS dismiss more than 50% of queried samples. The batch sizes are\nBASQ: 32, batch WSABI: 8. (see Supplementary)\n\n: The simulator is the SPMe [61], 9\nParameter estimation of the lithium-ion battery simulator\nestimating 3 simulation parameters at a given time-series voltage-current signal (the diffusivity of\nlithium-ion on the anode and cathode, and the experimental noise variance). Prior is modified to log\nmultivariate normal distribution from [4]. Each query takes 1.2 seconds on average.\n\nParameter estimation of the phase-field model\n: The simulator is the phase-field model [61],\n10 estimating 4 simulation parameters at given time-series two-dimensional morphological image\n(temperature, interaction parameter, Bohr magneton coefficient, and gradient energy coefficient).\nPrior is a log multivariate normal distribution. Each query takes 7.4 seconds on average.\n\nHyperparameter marginalisation of hierarchical GP model The hierarchical GP model was\ndesigned for analysing the large-scale battery time-series dataset from solar off-grid system field data\n[3].8 For fast estimation of parameters in each GP, the recursive technique [74] is adopted. The task\nis to marginalise 5 GP hyperparameters at given hyperprior, which is modified to log multivariate\nnormal distribution from [3]. Each query takes 1.1 seconds on average.\n\n5.3 Results\n\nWe find BASQ consistently delivers strong empirical performance, as shown in Figure 2. On all\nbenchmark problems, BASQ-IVR, IGB, or UB outperform baseline methods except in the battery\nsimulator evidence estimation. The very low-dimensional and sharp unimodal nature of this likelihood\ncould be advantageous for biased greedy batch WSABI, as IGB superiority supports this viewpoint.\nThis suggests that BASQ could be a generally fast Bayesian solver as far as we investigated. In the\nmultimodal setting of the synthetic dataset, BASQ-UB outperforms, whereas IVR does in a simulator-\nbased likelihoods. When comparing each proposal distribution, BASQ-IVR was the performant. Our\nresults support the general use of IVR, or UB if the likelihood is known to be highly multimodal.\n\n9SPMe code used was translated into Python from MATLAB [11, 12]. This open-source code is published\n\nunder the BSD 3-clause Licence. See more information on [11]\n\n10Code used was from [54, 3]. All rights of the code are reserved by the authors. Thus, we do not redistribute\n\nthe original code.\n\n8\n\n\fFigure 2: Time in seconds vs. KL divergence for posterior, and MAE for evidence in the synthetic\ndatasets. Time in seconds vs. RMSE for posterior and evidence itself in the real-world dataset.\n\n\n\nBased on the section(s), please answer the following question with yes, no, or n/a and provide a brief justification for your answer.\nQuestion: If the authors curate/release new assets (e.g., code, data, models), do the authors discuss whether the data they are using/curating contains personally identifiable information or offensive content?"}, "5a": {"role": "user", "content": "The following is the experiments section of the paper you are reviewing:\n\n\nGiven our new model BASQ, with three variants of the proposal distribution, IVR, IGB, and UB, we\nnow test for speed against MC samplers and batch WSABI. We compared with three NS methods\n[71, 29, 46, 14, 15], coded with [72, 17]. According to the review [16], MLFriends is the state-of-the-\nart NS sampler to date. The code is implemented based on [77, 34, 42, 76, 38, 31, 10, 7], and code\naround kernel recombination [24, 44] with additional modification. All experiments on synthetic\ndatasets were averaged over 10 repeats, computed in parallel with multicore CPUs, without GPU for\nfair comparison.8 The posterior distribution of NS was estimated via KDE with weighted samples\n[33]. For maximum speed performance, batch size was optimised for each method in each dataset,\nin fairness to the competitors. Batch WSABI needs to optimise batch size to balance the likelihood\nquery cost and sampling cost, because sampling cost increases rapidly with batch size, as shown\nin Figure 1(a). Therefore, it has an optimal batch size for faster convergence. By wall time cost,\nwe exclude the cost of integrand evaluation; that is, the wall time cost is the overhead cost of batch\nevaluation. Details can be found in the Supplementary.\n\n7Negative elements in the matrices only exist in K(X, X)\u22121, which can be drawn from the memory of the\nGP regression model without additional calculation. The number of positive components is half of the matrix on\naverage, resulting in O(n2/2). Then, taking the threshold via the inverse of the recombination sample size N ,\nthe number of components becomes ncomp \u226a n2, resulting in sampling complexity O(n2/2 + ncompN ).\n\n8Performed on MacBook Pro 2019, 2.4 GHz 8-Core Intel Core i9, 64 GB 2667 MHz DDR4\n\n7\n\n\f5.1 Synthetic problems\n\nWe evaluate all methods on three synthetic problems. The goal is to estimate the integral and posterior\nof the likelihood modelled with the highly multimodal functions. Prior was set to a two-dimensional\nmultivariate normal distribution, with a zero mean vector, and covariance whose diagonal elements\nare 2. The optimised batch sizes for each methods are BASQ: 100, batch WSABI: 16. The synthetic\nlikelihood functions are cheap (0.5 ms on average). This is advantageous setting for NS: Within 10\nseconds, the batch WSABI, BASQ, and NS collected 32, 600, 23225 samples, respectively. As for\nthe metrics, posterior estimation was tested with Kullback-Leibler (KL) upon random 10,000 samples\nfrom true posterior. Evidence was evaluated with MAE, and ground truth was derived analytically.\n\nLikelihood functions Branin-Hoo [49] is 8 modal function in two-dimensional space. Ackley [73]\nis a highly multimodal function with point symmetric periodical peaks in two-dimensional space.\nOscillatory function [32] is a highly multimodal function with reflection symmetric periodical peaks\nof highly-correlated ellipsoids in two-dimensional space.\n\n5.2 Real-world dataset\n\nWe consider three real-world applications with expensive likelihoods, which are simulator-based\nand hierarchical GP. We adopted the empirical metric due to no ground truth. For the posterior, we\ncan calculate the true conditional posterior distribution along the line passing through ground truth\nparameter points. Then, evaluate the posterior with root mean squared error (RMSE) against 50\ntest samples for each dimension. For integral, we compare the model evidence itself. Expensive\nlikelihoods makes the sample size per wall time amongst the methods no significant difference,\nwhereas rejection sampling based NS dismiss more than 50% of queried samples. The batch sizes are\nBASQ: 32, batch WSABI: 8. (see Supplementary)\n\n: The simulator is the SPMe [61], 9\nParameter estimation of the lithium-ion battery simulator\nestimating 3 simulation parameters at a given time-series voltage-current signal (the diffusivity of\nlithium-ion on the anode and cathode, and the experimental noise variance). Prior is modified to log\nmultivariate normal distribution from [4]. Each query takes 1.2 seconds on average.\n\nParameter estimation of the phase-field model\n: The simulator is the phase-field model [61],\n10 estimating 4 simulation parameters at given time-series two-dimensional morphological image\n(temperature, interaction parameter, Bohr magneton coefficient, and gradient energy coefficient).\nPrior is a log multivariate normal distribution. Each query takes 7.4 seconds on average.\n\nHyperparameter marginalisation of hierarchical GP model The hierarchical GP model was\ndesigned for analysing the large-scale battery time-series dataset from solar off-grid system field data\n[3].8 For fast estimation of parameters in each GP, the recursive technique [74] is adopted. The task\nis to marginalise 5 GP hyperparameters at given hyperprior, which is modified to log multivariate\nnormal distribution from [3]. Each query takes 1.1 seconds on average.\n\n5.3 Results\n\nWe find BASQ consistently delivers strong empirical performance, as shown in Figure 2. On all\nbenchmark problems, BASQ-IVR, IGB, or UB outperform baseline methods except in the battery\nsimulator evidence estimation. The very low-dimensional and sharp unimodal nature of this likelihood\ncould be advantageous for biased greedy batch WSABI, as IGB superiority supports this viewpoint.\nThis suggests that BASQ could be a generally fast Bayesian solver as far as we investigated. In the\nmultimodal setting of the synthetic dataset, BASQ-UB outperforms, whereas IVR does in a simulator-\nbased likelihoods. When comparing each proposal distribution, BASQ-IVR was the performant. Our\nresults support the general use of IVR, or UB if the likelihood is known to be highly multimodal.\n\n9SPMe code used was translated into Python from MATLAB [11, 12]. This open-source code is published\n\nunder the BSD 3-clause Licence. See more information on [11]\n\n10Code used was from [54, 3]. All rights of the code are reserved by the authors. Thus, we do not redistribute\n\nthe original code.\n\n8\n\n\fFigure 2: Time in seconds vs. KL divergence for posterior, and MAE for evidence in the synthetic\ndatasets. Time in seconds vs. RMSE for posterior and evidence itself in the real-world dataset.\n\n\n\nBased on the section(s), please answer the following question with yes, no, or n/a and provide a brief justification for your answer.\nQuestion: If the authors used crowdsourcing or conducted research with human subjects, do the authors include the full text of instructions given to participants and screenshots, if applicable?"}, "5b": {"role": "user", "content": "The following is the experiments section of the paper you are reviewing:\n\n\nGiven our new model BASQ, with three variants of the proposal distribution, IVR, IGB, and UB, we\nnow test for speed against MC samplers and batch WSABI. We compared with three NS methods\n[71, 29, 46, 14, 15], coded with [72, 17]. According to the review [16], MLFriends is the state-of-the-\nart NS sampler to date. The code is implemented based on [77, 34, 42, 76, 38, 31, 10, 7], and code\naround kernel recombination [24, 44] with additional modification. All experiments on synthetic\ndatasets were averaged over 10 repeats, computed in parallel with multicore CPUs, without GPU for\nfair comparison.8 The posterior distribution of NS was estimated via KDE with weighted samples\n[33]. For maximum speed performance, batch size was optimised for each method in each dataset,\nin fairness to the competitors. Batch WSABI needs to optimise batch size to balance the likelihood\nquery cost and sampling cost, because sampling cost increases rapidly with batch size, as shown\nin Figure 1(a). Therefore, it has an optimal batch size for faster convergence. By wall time cost,\nwe exclude the cost of integrand evaluation; that is, the wall time cost is the overhead cost of batch\nevaluation. Details can be found in the Supplementary.\n\n7Negative elements in the matrices only exist in K(X, X)\u22121, which can be drawn from the memory of the\nGP regression model without additional calculation. The number of positive components is half of the matrix on\naverage, resulting in O(n2/2). Then, taking the threshold via the inverse of the recombination sample size N ,\nthe number of components becomes ncomp \u226a n2, resulting in sampling complexity O(n2/2 + ncompN ).\n\n8Performed on MacBook Pro 2019, 2.4 GHz 8-Core Intel Core i9, 64 GB 2667 MHz DDR4\n\n7\n\n\f5.1 Synthetic problems\n\nWe evaluate all methods on three synthetic problems. The goal is to estimate the integral and posterior\nof the likelihood modelled with the highly multimodal functions. Prior was set to a two-dimensional\nmultivariate normal distribution, with a zero mean vector, and covariance whose diagonal elements\nare 2. The optimised batch sizes for each methods are BASQ: 100, batch WSABI: 16. The synthetic\nlikelihood functions are cheap (0.5 ms on average). This is advantageous setting for NS: Within 10\nseconds, the batch WSABI, BASQ, and NS collected 32, 600, 23225 samples, respectively. As for\nthe metrics, posterior estimation was tested with Kullback-Leibler (KL) upon random 10,000 samples\nfrom true posterior. Evidence was evaluated with MAE, and ground truth was derived analytically.\n\nLikelihood functions Branin-Hoo [49] is 8 modal function in two-dimensional space. Ackley [73]\nis a highly multimodal function with point symmetric periodical peaks in two-dimensional space.\nOscillatory function [32] is a highly multimodal function with reflection symmetric periodical peaks\nof highly-correlated ellipsoids in two-dimensional space.\n\n5.2 Real-world dataset\n\nWe consider three real-world applications with expensive likelihoods, which are simulator-based\nand hierarchical GP. We adopted the empirical metric due to no ground truth. For the posterior, we\ncan calculate the true conditional posterior distribution along the line passing through ground truth\nparameter points. Then, evaluate the posterior with root mean squared error (RMSE) against 50\ntest samples for each dimension. For integral, we compare the model evidence itself. Expensive\nlikelihoods makes the sample size per wall time amongst the methods no significant difference,\nwhereas rejection sampling based NS dismiss more than 50% of queried samples. The batch sizes are\nBASQ: 32, batch WSABI: 8. (see Supplementary)\n\n: The simulator is the SPMe [61], 9\nParameter estimation of the lithium-ion battery simulator\nestimating 3 simulation parameters at a given time-series voltage-current signal (the diffusivity of\nlithium-ion on the anode and cathode, and the experimental noise variance). Prior is modified to log\nmultivariate normal distribution from [4]. Each query takes 1.2 seconds on average.\n\nParameter estimation of the phase-field model\n: The simulator is the phase-field model [61],\n10 estimating 4 simulation parameters at given time-series two-dimensional morphological image\n(temperature, interaction parameter, Bohr magneton coefficient, and gradient energy coefficient).\nPrior is a log multivariate normal distribution. Each query takes 7.4 seconds on average.\n\nHyperparameter marginalisation of hierarchical GP model The hierarchical GP model was\ndesigned for analysing the large-scale battery time-series dataset from solar off-grid system field data\n[3].8 For fast estimation of parameters in each GP, the recursive technique [74] is adopted. The task\nis to marginalise 5 GP hyperparameters at given hyperprior, which is modified to log multivariate\nnormal distribution from [3]. Each query takes 1.1 seconds on average.\n\n5.3 Results\n\nWe find BASQ consistently delivers strong empirical performance, as shown in Figure 2. On all\nbenchmark problems, BASQ-IVR, IGB, or UB outperform baseline methods except in the battery\nsimulator evidence estimation. The very low-dimensional and sharp unimodal nature of this likelihood\ncould be advantageous for biased greedy batch WSABI, as IGB superiority supports this viewpoint.\nThis suggests that BASQ could be a generally fast Bayesian solver as far as we investigated. In the\nmultimodal setting of the synthetic dataset, BASQ-UB outperforms, whereas IVR does in a simulator-\nbased likelihoods. When comparing each proposal distribution, BASQ-IVR was the performant. Our\nresults support the general use of IVR, or UB if the likelihood is known to be highly multimodal.\n\n9SPMe code used was translated into Python from MATLAB [11, 12]. This open-source code is published\n\nunder the BSD 3-clause Licence. See more information on [11]\n\n10Code used was from [54, 3]. All rights of the code are reserved by the authors. Thus, we do not redistribute\n\nthe original code.\n\n8\n\n\fFigure 2: Time in seconds vs. KL divergence for posterior, and MAE for evidence in the synthetic\ndatasets. Time in seconds vs. RMSE for posterior and evidence itself in the real-world dataset.\n\n\n\nBased on the section(s), please answer the following question with yes, no, or n/a and provide a brief justification for your answer.\nQuestion: If the authors used crowdsourcing or conducted research with human subjects, do the authors describe any potential participant risks, with links to Institutional Review Board (IRB) approvals, if applicable?"}, "5c": {"role": "user", "content": "The following is the experiments section of the paper you are reviewing:\n\n\nGiven our new model BASQ, with three variants of the proposal distribution, IVR, IGB, and UB, we\nnow test for speed against MC samplers and batch WSABI. We compared with three NS methods\n[71, 29, 46, 14, 15], coded with [72, 17]. According to the review [16], MLFriends is the state-of-the-\nart NS sampler to date. The code is implemented based on [77, 34, 42, 76, 38, 31, 10, 7], and code\naround kernel recombination [24, 44] with additional modification. All experiments on synthetic\ndatasets were averaged over 10 repeats, computed in parallel with multicore CPUs, without GPU for\nfair comparison.8 The posterior distribution of NS was estimated via KDE with weighted samples\n[33]. For maximum speed performance, batch size was optimised for each method in each dataset,\nin fairness to the competitors. Batch WSABI needs to optimise batch size to balance the likelihood\nquery cost and sampling cost, because sampling cost increases rapidly with batch size, as shown\nin Figure 1(a). Therefore, it has an optimal batch size for faster convergence. By wall time cost,\nwe exclude the cost of integrand evaluation; that is, the wall time cost is the overhead cost of batch\nevaluation. Details can be found in the Supplementary.\n\n7Negative elements in the matrices only exist in K(X, X)\u22121, which can be drawn from the memory of the\nGP regression model without additional calculation. The number of positive components is half of the matrix on\naverage, resulting in O(n2/2). Then, taking the threshold via the inverse of the recombination sample size N ,\nthe number of components becomes ncomp \u226a n2, resulting in sampling complexity O(n2/2 + ncompN ).\n\n8Performed on MacBook Pro 2019, 2.4 GHz 8-Core Intel Core i9, 64 GB 2667 MHz DDR4\n\n7\n\n\f5.1 Synthetic problems\n\nWe evaluate all methods on three synthetic problems. The goal is to estimate the integral and posterior\nof the likelihood modelled with the highly multimodal functions. Prior was set to a two-dimensional\nmultivariate normal distribution, with a zero mean vector, and covariance whose diagonal elements\nare 2. The optimised batch sizes for each methods are BASQ: 100, batch WSABI: 16. The synthetic\nlikelihood functions are cheap (0.5 ms on average). This is advantageous setting for NS: Within 10\nseconds, the batch WSABI, BASQ, and NS collected 32, 600, 23225 samples, respectively. As for\nthe metrics, posterior estimation was tested with Kullback-Leibler (KL) upon random 10,000 samples\nfrom true posterior. Evidence was evaluated with MAE, and ground truth was derived analytically.\n\nLikelihood functions Branin-Hoo [49] is 8 modal function in two-dimensional space. Ackley [73]\nis a highly multimodal function with point symmetric periodical peaks in two-dimensional space.\nOscillatory function [32] is a highly multimodal function with reflection symmetric periodical peaks\nof highly-correlated ellipsoids in two-dimensional space.\n\n5.2 Real-world dataset\n\nWe consider three real-world applications with expensive likelihoods, which are simulator-based\nand hierarchical GP. We adopted the empirical metric due to no ground truth. For the posterior, we\ncan calculate the true conditional posterior distribution along the line passing through ground truth\nparameter points. Then, evaluate the posterior with root mean squared error (RMSE) against 50\ntest samples for each dimension. For integral, we compare the model evidence itself. Expensive\nlikelihoods makes the sample size per wall time amongst the methods no significant difference,\nwhereas rejection sampling based NS dismiss more than 50% of queried samples. The batch sizes are\nBASQ: 32, batch WSABI: 8. (see Supplementary)\n\n: The simulator is the SPMe [61], 9\nParameter estimation of the lithium-ion battery simulator\nestimating 3 simulation parameters at a given time-series voltage-current signal (the diffusivity of\nlithium-ion on the anode and cathode, and the experimental noise variance). Prior is modified to log\nmultivariate normal distribution from [4]. Each query takes 1.2 seconds on average.\n\nParameter estimation of the phase-field model\n: The simulator is the phase-field model [61],\n10 estimating 4 simulation parameters at given time-series two-dimensional morphological image\n(temperature, interaction parameter, Bohr magneton coefficient, and gradient energy coefficient).\nPrior is a log multivariate normal distribution. Each query takes 7.4 seconds on average.\n\nHyperparameter marginalisation of hierarchical GP model The hierarchical GP model was\ndesigned for analysing the large-scale battery time-series dataset from solar off-grid system field data\n[3].8 For fast estimation of parameters in each GP, the recursive technique [74] is adopted. The task\nis to marginalise 5 GP hyperparameters at given hyperprior, which is modified to log multivariate\nnormal distribution from [3]. Each query takes 1.1 seconds on average.\n\n5.3 Results\n\nWe find BASQ consistently delivers strong empirical performance, as shown in Figure 2. On all\nbenchmark problems, BASQ-IVR, IGB, or UB outperform baseline methods except in the battery\nsimulator evidence estimation. The very low-dimensional and sharp unimodal nature of this likelihood\ncould be advantageous for biased greedy batch WSABI, as IGB superiority supports this viewpoint.\nThis suggests that BASQ could be a generally fast Bayesian solver as far as we investigated. In the\nmultimodal setting of the synthetic dataset, BASQ-UB outperforms, whereas IVR does in a simulator-\nbased likelihoods. When comparing each proposal distribution, BASQ-IVR was the performant. Our\nresults support the general use of IVR, or UB if the likelihood is known to be highly multimodal.\n\n9SPMe code used was translated into Python from MATLAB [11, 12]. This open-source code is published\n\nunder the BSD 3-clause Licence. See more information on [11]\n\n10Code used was from [54, 3]. All rights of the code are reserved by the authors. Thus, we do not redistribute\n\nthe original code.\n\n8\n\n\fFigure 2: Time in seconds vs. KL divergence for posterior, and MAE for evidence in the synthetic\ndatasets. Time in seconds vs. RMSE for posterior and evidence itself in the real-world dataset.\n\n\n\nBased on the section(s), please answer the following question with yes, no, or n/a and provide a brief justification for your answer.\nQuestion: If the authors used crowdsourcing or conducted research with human subjects, do the authors include the estimated hourly wage paid to participants and the total amount spent on participant compensation?"}}}