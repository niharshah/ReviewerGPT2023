{"paper_index": 7, "title": "Laplacian Autoencoders for Learning Stochastic Representations", "abstract": "\n\nEstablished methods for unsupervised representation learning such as variational\nautoencoders produce none or poorly calibrated uncertainty estimates making it\ndif\ufb01cult to evaluate if learned representations are stable and reliable. In this work,\nwe present a Bayesian autoencoder for unsupervised representation learning, which\nis trained using a novel variational lower bound of the autoencoder evidence. This\nis maximized using Monte Carlo EM with a variational distribution that takes\nthe shape of a Laplace approximation. We develop a new Hessian approximation\nthat scales linearly with data size allowing us to model high-dimensional data.\nEmpirically, we show that our Laplacian autoencoder estimates well-calibrated\nuncertainties in both latent and output space. We demonstrate that this results in\nimproved performance across a multitude of downstream tasks.\n\n1\n\n", "introduction": "\n\nUnsupervised representation learning is a brittle matter. Consider the classic autoencoder (AE)\n(Rumelhart et al., 1986), which compresses data x to a low-dimensional representation z from which\ndata is approximately reconstructed. The nonlinearity of the model implies that sometimes small\nchanges to data x give a large change in the latent representation z (and sometimes not). Likewise,\nfor some data, reconstructions are of low quality, while for others it is near perfect. Unfortunately,\nthe model does not have a built-in quanti\ufb01cation of its uncertainty, and we cannot easily answer when\nthe representation is reliable and accurately re\ufb02ects data.\n\nThe celebrated variational autoencoder (VAE) (Kingma and Welling, 2014; Rezende et al., 2014)\naddress this concern directly through an explicit likelihood model p(x\nz) and a variational approxi-\n|\nx). Both these distributions have parameters predicted by\nmation of the representation posterior p(z\nneural networks that act similarly to the encoder\u2013decoder pair of the classic autoencoder.\n\n|\n\nBut is the VAE\u2019s quanti\ufb01cation of reliability reliable? To investigate, we \ufb01t a VAE with a two-\ndimensional latent representation to the MNIST dataset (Lecun et al., 1998), and illustrate the predicted\nz) in Fig. 1a. The model learns to assign high uncertainty to low-level image\nuncertainty of p(x\n|\nfeatures such as edges but predicts its smallest values far away from the data distribution. Not only\nis such behavior counter-intuitive, but it is also suboptimal in terms of data likelihood (Sec. 1.1).\nRetrospectively, this should not be surprising as the uncertainty levels away from the data are governed\nby the extrapolatory behavior of the neural network determining p(x\nz). This suggests that perhaps\nuncertainty should be a derived quantity rather than a predicted one.\n\n|\n\n1 Denotes equal contribution; author order determined by a simulated coin toss.\n\n36th Conference on Neural Information Processing Systems (NeurIPS 2022).\n\n\f<latexit sha1_base64=\"I+0UOx9m5Mg8vkDHEjZJgZ5mbJ4=\">AAAB9XicbVBNS0JBFJ1nX2ZfVss2gxIYgbwnWc+dFUFLg/wAn8m8cdTBeR/M3FfJw3/Rok2LItr2X9r5bxo1orIDFw7n3Mu997ih4ApMc2wkFhaXlleSq6m19Y3NrfT2Tk0FkaSsSgMRyIZLFBPcZ1XgIFgjlIx4rmB1d3A+8eu3TCoe+NcwDFnLIz2fdzkloKUbB9g9xDlygGunF6N2OmvmS6Uju2hjTexj2ypgK29O8U2y5Yxz+DAuDyvt9IfTCWjkMR+oIEo1LTOEVkwkcCrYKOVEioWEDkiPNTX1icdUK55ePcL7WungbiB1+YCn6s+JmHhKDT1Xd3oE+uqvNxH/85oRdO1WzP0wAubT2aJuJDAEeBIB7nDJKIihJoRKrm/FtE8koaCDSukQ5l6eJ7VC3irmzSudxhmaIYn2UAblkIVOUBldogqqIookekTP6MW4M56MV+Nt1powvmZ20S8Y75/+rZUq</latexit>\n\n(a) VAE\n\n<latexit sha1_base64=\"KnTu97m0JcXB3xIDlkbq5VJDXf4=\">AAAB7nicbVBNS8NAEJ34WetX1aOXxUbwFJKiNd6KXjxWsB/QhrLZbtulu0nY3Qgl9Ed48aCIV3+PN/+Nm7aIWh8MPN6bYWZemHCmtOt+Wiura+sbm4Wt4vbO7t5+6eCwqeJUEtogMY9lO8SKchbRhmaa03YiKRYhp61wfJP7rQcqFYujez1JaCDwMGIDRrA2Usu2uyK17V6p7DpXvudVKygn1XPfR57jzvBNyrBAvVf66PZjkgoaacKxUh3PTXSQYakZ4XRa7KaKJpiM8ZB2DI2woCrIZudO0alR+mgQS1ORRjP150SGhVITEZpOgfVI/fVy8T+vk+qBH2QsSlJNIzJfNEg50jHKf0d9JinRfGIIJpKZWxEZYYmJNgkVTQhLLy+TZsXxLhz3rlKuXS/iKMAxnMAZeHAJNbiFOjSAwBge4RlerMR6sl6tt3nrirWYOYJfsN6/ADgOjtc=</latexit>\u00b5\n\n<latexit sha1_base64=\"2xtfArd+6jiIrrxb8xG9C8PUOiw=\">AAAB83icbVDLSsNAFJ3UV62vqks3g43gKiRBa9wV3bisYB/QxDKZTtqhM0mYmQgl9DfcuFDErT/jzr9x0hZR64ELh3Pu5d57wpRRqWz70yitrK6tb5Q3K1vbO7t71f2DtkwygUkLJywR3RBJwmhMWooqRrqpIIiHjHTC8XXhdx6IkDSJ79QkJQFHw5hGFCOlJd80fUmHHN27ptmv1mzr0nOcugsLUj/zPOhY9gzfpAYWaParH/4gwRknscIMSdlz7FQFORKKYkamFT+TJEV4jIakp2mMOJFBPrt5Ck+0MoBRInTFCs7UnxM54lJOeKg7OVIj+dcrxP+8XqYiL8hpnGaKxHi+KMoYVAksAoADKghWbKIJwoLqWyEeIYGw0jFVdAhLLy+Ttms555Z969YaV4s4yuAIHINT4IAL0AA3oAlaAIMUPIJn8GJkxpPxarzNW0vGYuYQ/ILx/gWlgpDI</latexit>\n\n\u00002\n\n<latexit sha1_base64=\"IGuruciO9i8L1qsaALt5I7n1+Js=\">AAACAnicbVDLSgNBEJyNrxhfUU/iZUgQImLYFR+bW1QEDx4UjApJCLOT2WTI7M4y0yuGJXjxE/wFLx4U8epXePNvnCQiaixoKKq66e7yIsE12PaHlRobn5icSk9nZmbn5heyi0sXWsaKsgqVQqorj2gmeMgqwEGwq0gxEniCXXqdw75/ec2U5jI8h27E6gFphdznlICRGtmVGrAbSAreOj7ZP8KFSGrYbEu63mtk83axVNp2d1xsiLvrOlvYKdoDfJN8OVfbuP8od08b2fdaU9I4YCFQQbSuOnYE9YQo4FSwXqYWaxYR2iEtVjU0JAHT9WTwQg+vGaWJfalMhYAH6s+JhARadwPPdAYE2vqv1xf/86ox+G494WEUAwvpcJEfCwwS9/PATa4YBdE1hFDFza2YtokiFExqGRPCyMuj5GKr6OwU7TOTxgEaIo1WUQ4VkIP2UBkdo1NUQRTdogf0hJ6tO+vRerFeh60p62tmGf2C9fYJwOGZXg==</latexit>\n\n(b) LAE (post-hoc)\n\n<latexit sha1_base64=\"KnTu97m0JcXB3xIDlkbq5VJDXf4=\">AAAB7nicbVBNS8NAEJ34WetX1aOXxUbwFJKiNd6KXjxWsB/QhrLZbtulu0nY3Qgl9Ed48aCIV3+PN/+Nm7aIWh8MPN6bYWZemHCmtOt+Wiura+sbm4Wt4vbO7t5+6eCwqeJUEtogMY9lO8SKchbRhmaa03YiKRYhp61wfJP7rQcqFYujez1JaCDwMGIDRrA2Usu2uyK17V6p7DpXvudVKygn1XPfR57jzvBNyrBAvVf66PZjkgoaacKxUh3PTXSQYakZ4XRa7KaKJpiM8ZB2DI2woCrIZudO0alR+mgQS1ORRjP150SGhVITEZpOgfVI/fVy8T+vk+qBH2QsSlJNIzJfNEg50jHKf0d9JinRfGIIJpKZWxEZYYmJNgkVTQhLLy+TZsXxLhz3rlKuXS/iKMAxnMAZeHAJNbiFOjSAwBge4RlerMR6sl6tt3nrirWYOYJfsN6/ADgOjtc=</latexit>\u00b5\n\n<latexit sha1_base64=\"I3lyBSIdfE5Wl35PdctE+++DNmc=\">AAACAHicbVDLSgMxFM3Ud31VXbhwE1qEFqHMiI9xVxXBhQsF+4C2lEya1mAmMyR3xGHoxm/wD9y4UMStn+HOvzHTiqj1wIXDOfcm9x4vFFyDbX9YmYnJqemZ2bns/MLi0nJuZbWmg0hRVqWBCFTDI5oJLlkVOAjWCBUjvidY3bs+Tv36DVOaB/IS4pC1fdKXvMcpASN1custYLeQFGkJnx2e4GIg05dKg06uYJcPDnbcXRcb4u65zjZ2yvYQ36RQybe27j8q8Xkn997qBjTymQQqiNZNxw6hnRAFnAo2yLYizUJCr0mfNQ2VxGe6nQwPGOBNo3RxL1CmJOCh+nMiIb7Wse+ZTp/Alf7rpeJ/XjOCnttOuAwjYJKOPupFAkOA0zRwlytGQcSGEKq42RXTK6IIBZNZ1oQwdvI4qW2Xnd2yfWHSOEIjzKINlEdF5KB9VEGn6BxVEUUD9ICe0LN1Zz1aL9brqDVjfc2soV+w3j4BaeuYow==</latexit>\n\n(c) LAE (online)\n\n<latexit sha1_base64=\"KnTu97m0JcXB3xIDlkbq5VJDXf4=\">AAAB7nicbVBNS8NAEJ34WetX1aOXxUbwFJKiNd6KXjxWsB/QhrLZbtulu0nY3Qgl9Ed48aCIV3+PN/+Nm7aIWh8MPN6bYWZemHCmtOt+Wiura+sbm4Wt4vbO7t5+6eCwqeJUEtogMY9lO8SKchbRhmaa03YiKRYhp61wfJP7rQcqFYujez1JaCDwMGIDRrA2Usu2uyK17V6p7DpXvudVKygn1XPfR57jzvBNyrBAvVf66PZjkgoaacKxUh3PTXSQYakZ4XRa7KaKJpiM8ZB2DI2woCrIZudO0alR+mgQS1ORRjP150SGhVITEZpOgfVI/fVy8T+vk+qBH2QsSlJNIzJfNEg50jHKf0d9JinRfGIIJpKZWxEZYYmJNgkVTQhLLy+TZsXxLhz3rlKuXS/iKMAxnMAZeHAJNbiFOjSAwBge4RlerMR6sl6tt3nrirWYOYJfsN6/ADgOjtc=</latexit>\u00b5\n\n<latexit sha1_base64=\"2xtfArd+6jiIrrxb8xG9C8PUOiw=\">AAAB83icbVDLSsNAFJ3UV62vqks3g43gKiRBa9wV3bisYB/QxDKZTtqhM0mYmQgl9DfcuFDErT/jzr9x0hZR64ELh3Pu5d57wpRRqWz70yitrK6tb5Q3K1vbO7t71f2DtkwygUkLJywR3RBJwmhMWooqRrqpIIiHjHTC8XXhdx6IkDSJ79QkJQFHw5hGFCOlJd80fUmHHN27ptmv1mzr0nOcugsLUj/zPOhY9gzfpAYWaParH/4gwRknscIMSdlz7FQFORKKYkamFT+TJEV4jIakp2mMOJFBPrt5Ck+0MoBRInTFCs7UnxM54lJOeKg7OVIj+dcrxP+8XqYiL8hpnGaKxHi+KMoYVAksAoADKghWbKIJwoLqWyEeIYGw0jFVdAhLLy+Ttms555Z969YaV4s4yuAIHINT4IAL0AA3oAlaAIMUPIJn8GJkxpPxarzNW0vGYuYQ/ILx/gWlgpDI</latexit>\n\n\u00002\n\n<latexit sha1_base64=\"2xtfArd+6jiIrrxb8xG9C8PUOiw=\">AAAB83icbVDLSsNAFJ3UV62vqks3g43gKiRBa9wV3bisYB/QxDKZTtqhM0mYmQgl9DfcuFDErT/jzr9x0hZR64ELh3Pu5d57wpRRqWz70yitrK6tb5Q3K1vbO7t71f2DtkwygUkLJywR3RBJwmhMWooqRrqpIIiHjHTC8XXhdx6IkDSJ79QkJQFHw5hGFCOlJd80fUmHHN27ptmv1mzr0nOcugsLUj/zPOhY9gzfpAYWaParH/4gwRknscIMSdlz7FQFORKKYkamFT+TJEV4jIakp2mMOJFBPrt5Ck+0MoBRInTFCs7UnxM54lJOeKg7OVIj+dcrxP+8XqYiL8hpnGaKxHi+KMoYVAksAoADKghWbKIJwoLqWyEeIYGw0jFVdAhLLy+Ttms555Z969YaV4s4yuAIHINT4IAL0AA3oAlaAIMUPIJn8GJkxpPxarzNW0vGYuYQ/ILx/gWlgpDI</latexit>\n\n\u00002\n\nFigure 1: 2D latent representation of MNIST overlaid a heatmap that describes the decoder uncertainty\n(yellow/blue indicates a low/high variance of the reconstructions). To the right of the latent spaces,\nwe show the mean and variance of a reconstructed image (yellow indicates high values). (a) The VAE\nlearns to estimate high variance for low-level image features such as edges but fails at extrapolating\nuncertainties away from training data. (b) Applying post-hoc Laplace to the AE setup shows much\nbetter extrapolating capabilities, but fails in estimating calibrated uncertainties in output space. (c)\nOur online, sampling-based optimization of a Laplacian autoencoder (LAE) gives well-behaved\nuncertainties in both latent and output space.\n\nFrom a Bayesian perspective, the natural solution is to form an (approximate) posterior over the\nweights of the neural networks. To investigate, we adapt a state-of-the-art implementation of a\npost-hoc Laplace approximation (Daxberger et al., 2021) of the weight posterior to the autoencoder\ndomain. This amounts to training a regular autoencoder, and thereafter approximating the weight\nuncertainty with the Hessian of the loss (Sec. 1.1). Fig. 1b shows that uncertainty now grows, as\nintuitively expected, with the distance to the data distribution, but there seems to be little semantic\nstructure in the uncertainty in output space. This suggests that while the post-hoc procedure is\ncomputationally attractive it is too simplistic.\n\nIn this paper we introduce a new framework for Bayesian autoencoders in unsupervised represen-\ntation learning. Our method takes inspiration from the Laplace approximation to build a variational\ndistribution on the neural network weights. We \ufb01rst propose a post-hoc LA for autoencoders; show-\ncasing good out-of-distribution detection capabilities, but lack of properly calibrated uncertainties\nin-distribution. To address this, we develop a fast and memory-ef\ufb01cient Hessian approximation,\nwhich allows us to maximize a variational lower bound using Monte Carlo EM, such that model\nuncertainty is a key part of model training rather than estimated post-hoc. Fig. 1c gives an example\nof the corresponding uncertainty, which exhibits a natural and semantically meaningful behavior.\n\n1.1 Background\n\n|\n\nThe VAE is a latent variable model that parametrize the data density p(x) =\nz)p(z)dz\nusing a latent variable (representation) z. Here p(z) is a, usually standard normal, prior over the\nrepresentation, and p(x\n\np(x\n|\nz) is a likelihood with parameters predicted by a neural network.\n\nR\n\n|\n\n|\n\n|\n\nx)\n\np(z\n\nx) [p(x\n\nlog p(x)\n\nz)p(z)/q(z\n\nx) and write p(x) = Eq(z\n\nThe nonlinearity of the likelihood parameters renders the marginalization of z intractable, and\na variational lower bound of p(x) is considered instead. To arrive at this, one \ufb01rst introduces a\nvariational approximation q(z\nx)]. A lower\n\u21e1\nbound on p(x) then follows by a direct application of Jensen\u2019s inequality,\n\u0000L VAE(x) = Eq(z\n|\n(x\n\n\u00b5(z),\u0000 2(z)), we see that optimally \u00002(z) should be\nIf we momentarily assume that p(x\n|\nas large as possible away from training data in order to increase p(x) on the training data (Appendix F).\nYet this is not the observed empirical behavior in Fig. 1a. Since the \u00002 network is left untrained away\nfrom training data, its predictions depend on extrapolation. In practice, \u00002 takes fairly small values\nnear training data (assuming the mean \u00b5 provides a reasonable data \ufb01t), and \u00002 extrapolates arbitrary\neven if this is suboptimal in terms of data likelihood. Similar remarks hold for other likelihood\nmodels p(x\nx): relying on neural network extrapolation to predict\nuncertainty does not work.\n\nz) and encoder distributions q(z\n\np(z)).\nk\n\nx) [log p(x\n\nKL(q(z\n\nz) =\n\nz)]\n\n(1)\n\nx)\n\nN\n\n\u0000\n\n|\n\n|\n\n|\n\n|\n\n|\n\n|\n\n|\n\nThe Laplace approximation (Laplace, 1774; MacKay, 1992) is an integral part of our proposed\nsolution.\n(\nexp(\n\nIn the context of Bayesian neural networks, we seek the weight-posterior p(\u2713\n; \u2713)), where \u2713 are network weights,\n\n/\nis the applied loss\n\nis the training data, and\n\n|D\n\n)\n\n\u0000L\n\nD\n\nL\n\nD\n\n2\n\n\fFigure 2: Model overview. We learn a distribution over parameters such that we can sample encoders\ne  and decoders d\u0000. This allow us to compute the empirical mean and variance in both the latent\nspace z and the output space f\u2713(xn) = d\u0000(e (xn)).\n\nfunction interpreted as an unnormalized log-posterior. This is generally intractable and Laplace\u2019s\napproximation (LA) amounts to a second-order Taylor expansion around a chosen weight vector \u2713\u21e4\n\nlog p(\u2713\n\n) =\n\n|D\n\nL\n\n\u21e4 + (\u2713\n\n\u0000\n\n\u2713\u21e4)>\n\nrL\n\n\u21e4 +\n\n1\n2\n\n(\u2713\n\n\u0000\n\n\u2713\u21e4)>\n\n2\n\nr\n\nL\n\n\u21e4(\u2713\n\n\u0000\n\n\u2713\u21e4) +\n\n\u2713\n\n(\n\nk\n\nO\n\n\u0000\n\n\u2713\u21e4\n\n3)\nk\n\n(2)\n\n\u21e4 =\n\n) is\nwhere we use the short-hand\nGaussian. Note that when \u2713\u21e4 is a MAP estimate, the \ufb01rst order term vanishes and the second order\nterm is negative semi-de\ufb01nite. We provide more details on the Laplace approximation in Appendix B.\nIn practice, computing the full Hessian is too taxing both in terms of computation and memory, and\nvarious approximations are applied (Sec. 3).\n\n; \u2713\u21e4). The approximation, thus, assumes that p(\u2713\n\n|D\n\nD\n\nL\n\nL\n\n(\n\n", "methods": "\n\nN\nn=1 consisting of\nWe consider unsupervised representation learning from i.i.d. data\nRK.\nRD. We also de\ufb01ne a continuous latent space such that representations zn 2\nobservations xn 2\nRK\nSimilar to AEs (Hinton and Salakhutdinov, 2006), we consider two neural networks e  : RD\n!\nand d\u0000: RK\n , \u0000\n{\nWe refer to the composition of encoder and decoder as f\u2713 = d  \u0000\nModel overview. The autoencoder network structure implies that we model the data as being\ndistributed on a K-dimensional manifold parametrized by \u2713. We then seek the distribution of the re-\nconstruction xrec = f\u2713(x) given observation x, where the uncertainty comes from \u2713 being unknown,\n\nRD, widely known as the encoder and decoder. These have parameters \u2713 =\n\nxn}\n\ne\u0000.\n\n!\n\n=\n\nD\n\n}\n\n{\n\n.\n\n|\n\n\u21e0\n\np(\u2713\n\n\u2713, x, f )].\n\nx, f ) = E\u2713\n\nx,f )[p(xrec|\np(xrec|\nNotice that we explicitly condition on f , which is the operator \u2713\nf\u2713, even if this is not stochastic;\nthis conditioning will become important later on to distinguish between the distribution deduced\nby f and its linearization f (t). Mimicking the standard autoencoder reconstruction loss, we set\nx, f ) is unknown, the reconstruction likelihood Eq. (3)\np(xrec|\nis intractable, and approximations are in order. Similar to Blundell et al. (2015), we resort to a\nGaussian approximation, but rather than learning the variance variationally, we opt for LA. This will\nallow us to sample NNs and deduce uncertainties in both latent and output space as illustrated in Fig. 2.\n\nf\u2713(x), I). Since p(\u2713\n\n\u2713, x, f ) =\n\n(xrec|\n\n(3)\n\n7!\n\nN\n\n|\n\nIntractable joint distribution. Any meaningful approximate posterior over \u2713 should be similar to\nthe marginal of the joint distribution p(\u2713, xrec|\nx, f ) = Exrec\u21e0\n\nx, f ). This marginal is\n\nxrec, x, f )]\n\n|\nwhich can be bounded on a log-scale using Jensen\u2019s inequality,\n\nx,f )[p(\u2713\n\np(xrec|\n\np(\u2713\n\n(4)\n\n|\n\nx,f )[log p(\u2713\n\nxrec, x, f )].\n\n(5)\n\nlog p(\u2713\n\nx, f )\n\n|\n\n\u0000L \u2713 = Exrec\u21e0\nx, f )\n\np(xrec|\nqt(\u2713\n\n|\n(\u2713\n\n|\n\nOur \ufb01rst approximation is a LA of p(\u2713\nt ), where we postpone\nthe details on how to acquire \u2713t and Ht. These will eventually be iteratively computed from the\nlower bound Eq. (5); hence the t index. Fig. 3 (a) illustrates the situation thus far: p(\u2713\nx, f ) is\nx, f ) a non-trivial density.\napproximately Gaussian, but the non-linearity of f gives p(xrec|\nLinearization for gradient updates. Standard gradient-based learning can be viewed as a lineariza-\n= J\u2713f\u2713 rf l(f ), where J\u2713f\u2713 is the\ntion of f in \u2713, i.e. for a loss\n\n= l(f\u2713), the gradient is\n\nx, f ) =\n\n\u2713t, H\u0000\n\nr\u2713L\n\nN\n\n\u21e1\n\nL\n\n|\n\n|\n\n|\n\n1\n\n3\n\n\fxrec\n\nxrec\n\nx, f )\n\n\n\np(\u2713\n\n|\n\nx, f (t))\n\n\n\np(\u2713\n\n|\n\n\n\nx(t)\nrec\n\n\n\np(\u2713\n\n|\n\nx(t)\n\nrec, x, f )\n\nf\n\n\n\np(xrec|\n\nx, f )\n\n\n\nx(t)\nrec\n\n\n\np(\u2713\n\n|\n\nrec, x, f (t))\nx(t)\n\n\n\np(xrec|\n\nx, f (t))\n\n\n\nf (t)\n\nf\n\n\n\np(xrec|\n\n\u2713(t), x, f )\n\u2713t\n\n\n\n\n\n\u2713(t)\n\u2713t\n\u2713(t)\n(a) Non linear f\n\n\n\n\u2713\n\n\n\np(xrec|\n\n\u2713(t), x, f (t))\n\u2713t\n\n\n\n\n\n\u2713t\n\u2713(t)\n(b) Linear f (t)\n\n\u2713\n\nFigure 3: Illustrative example for \ufb01xed x. The likelihood for a \ufb01xed \u2713t, shown by the columns, are\nassumed Gaussian p(xrec|\nx, f )\n(\ufb01rst axis) over parameters \u2713 with Gaussians. With the additional assumption of linear f then\np(xrec|\n\n\u2713t, x, f ) = N (f\u2713t (x), I). We model the marginalised density p(\u2713\n\nx, f ) (second axis) is Gaussian. This makes the joint distribution tractable.\n\n|\n\nJacobian of f . In a similar spirit, we linearize f in \u2713 in order to arrive at a tractable approximation of\np(xrec|\n\nx, f ). Speci\ufb01cally, we perform a Taylor expansion around \u2713t\n\nf\u2713(x) = f\u2713t (x) + J\u2713f\u2713t (x)(\u2713\n\n\u2713t)\n\n+\n\n\u2713\n\n\u2713tk\n\n\u0000\n\nO\n\nk\n\n\u0000\n\n2\n\n(6)\n\n=:f (t)\n\n\u2713 (x)\n{z\n\n|\n\n}\n\n\u0000\n\n\u0000\n\nwhere f (t) denote the associated \ufb01rst-order approximation of f . Under this approximation, the joint\nx, f ) becomes Gaussian, p(\u2713, xrec|\ndistribution p(\u2713, xrec|\n\u2713t\nf (t)\n\u2713t\n\nJ\u2713f\u2713t (x)>HtJ\u2713f\u2713t (x)\n\n\u21e1N\nJ\u2713f\u2713t (x)>\n\nH\u0000\nt\nJ\u2713f\u2713t (x)\n\n(\u2713, xrec|\n\n\u00b5t, \u2303t), with\n\n, and \u2303t =\n\nx, f (t))\n\n\u00b5t =\n\n(x)\n\n(7)\n\n\u0000\n\n1\n\n1\n\n.\n\n\u2713\n\n\u25c6\n\n\u2713\n\n+ I\u25c6\n\nThis approximation is illustrated in Fig. 3(b). We provide the proof in Appendix D.4.\n\n\u0000\n\n\u0000\n\nIterative learning. With these approximations we can readily develop an iterative learning scheme\nwhich updates q(\u2713\nx, f ). The mean of this approximate posterior can be updated according to a\nstandard variational gradient step, \u2713t+1 = \u2713t + \u0000\n\n|\n\nr\u2713Lxrec , where\n\nLxrec = E\u2713\n\n\u21e0\n\nqt(\u2713\n\nx,f )[log p(xrec|\n\n|\n\n\u2713, x, f (t))],\n\n(8)\n\nis a lower bound on Eq. (3), which we evaluate with a single Monte Carlo sample. Following the LA,\nthe covariance of qt+1(\u2713\nx, f ) at \u2713t+1. Since\nx, f ), should be the inverse of the Hessian of log p(\u2713\nthis is intractable, we instead compute the Hessian of the lower bound Eq. (5)\n\n|\n\n|\n\n2\n\u2713L\u2713\n\nHt+1 =\n\n\u0000r\n\nx,f (t))\nh\n\u0000\n\u0000\nThe last term can be approximated since\n\n\u2713t+1\n\n= Ep(xrec|\n\n\u0000r\n\n2\n\n\u2713 log p(xrec|\n\u2713 log qt(\u2713\n2\nx, f )\n\n\u2713, x, f (t))\n\n\u2713 log qt(\u2713\n2\n\u0000r\n\n|\n\nx, f )\n\n\u2713t+1\n\n(9)\n\n|\u2713=\u2713t+1 =\n-term, we introduce a parameter \u21b5, such that the \ufb01nal approximation is\n\n).\n\u2713tk\n\nHt +\n\nr\n\nO\n\n\u0000\n\n(\n\n|\n\ni\u0000\n\u0000\n\u2713t+1 \u0000\n\u0000\nk\n\nTo ef\ufb01ciently cope with the\n\nHt+1 \u21e1\n\n(1\n\n= (1\n\nO\n\u21b5)Ht + Ep(xrec|\nJ\u2713f (t)\n\u2713\n\n\u21b5)Ht \u0000\n\n\u0000\n\n\u0000\n\nx,f )\n\n>\n\nr\n\n\u0000r\n\nh\n2\nxrec log p\n\n2\n\u2713 log p\n\nxrec\n\n\u2713, x, f (t)\n\n\u21e3\nxrec\n\n\u0000\n\u0000\n\n\u2713t+1, x, f (t)\n\n\u2713t+1\n\u2318i \u0000\n\u0000\nJ\u2713f (t)\n\u2713 .\n\u0000\n\n(10)\n\n(11)\n\n\u21e3\nis independent of which \u2713 we evaluate in and\n\nwhere J\u2713f (t)\nis trivial\n\u2713\nto compute for common losses, i.e. for MSE it is the identity. The parameter \u21b5 can be viewed\nas a geometric running average that is useful for smoothing out results computed on a minibatch\ninstead of on the full training set, similar to momentum-like training procedures. It further allows for\nnon-monotonically-increasing precision. Note that we revisit data during training, and the precision\n\n\u2318\n\u2713t+1, x, f (t)\n\n\u0000\n2\n\u0000\nxrec log p\n\nxrec\n\nr\n\n\u0000\n\u0000\n\n\u0000\n\n\u0000\n\n4\n\n\f\n\nf (t) = taylor(f, \u2713t)\n\nt+=1\n\n\n\n\u2713t+1 = \u2713t + \u0000E\u2713\n\nqt [\n\nr\u2713 log p (xrec |\n\n\u21e0\n\n\u2713, x, f )]\n\n\n\nqt\n\n\n\n\u2713t\n\n1\n\n\n\nH \u0000\nt\n\n\n\nHt+1 = (1\n\n\u21b5)Ht \u0000\n\n\u0000\n\nT\n\nJ\u2713f (t)\n\u2713\n\n2\n\nxrec log p(xrec|\n\nr\n\n\u2713, x, f (t)) J\u2713f (t)\n\u2713\n\nt+=1\n\n\u00b5\u0000=0\nFigure 4: Iterative training procedure. Given a distribution qt over parameters, and a linearized\nfunction f (t), compute \ufb01rst and second-order derivatives to update the distribution on parameters.\n\nf^{(t)}=\\textsc{taylor}(f,\\theta^{(t)})\n\nmatrix is updated for every revisit. Thus, the forgetting induced by \u21b5 is a wanted behavior to avoid\nin\ufb01nite precision in the limit of in\ufb01nite training time. In practice, we set \u21b5 equal to the learning rate,\nwhere the practical intuition is that when \u21b5 is small, the network uncertainty decreases faster.\nThe overall training procedure is summarized in Fig. 4. We initialize q0 as a Gaussian with \u27130 = 0\nand H0 = I. We provide more details on the model, the linearization, and iterative learning in\nAppendix D.\n\nWhy not just. . . ? The proposed training procedure may at \ufb01rst appear non-trivial, and it is reasonable\nto wonder if existing methods could be applied to similar results. Variational inference often achieves\nsimilar results to Laplace approximations, so could we use \u2018Bayes by Backprop\u2019 (Blundell et al., 2015)\nto get an alternative Gaussian approximate posterior over \u2713? Similar to the supervised experiences\nof Jospin et al. (2020), we, unfortunately, found this approach too brittle to allow for practical\nmodel \ufb01tting. But then perhaps a post-hoc LA as proposed by Daxberger et al. (2021) for supervised\nlearning? Empirically, we found it to be important to center the approximate posterior around a point\nwhere the Hessian provides useful uncertainty estimates. Our online training moves in this direction\nas the Hessian is part of the procedure, but this is not true for the post-hoc LA.\n\nConceptually, we argue that our approach, while novel, is not entirely separate from existing methods.\nOur reliance on lower bounds makes the method an instance of variational inference (Jordan et al.,\n1999; Opper and Archambeau, 2009b), and we maximize the bounds using Monte Carlo EM (Capp\u00e9,\n2009). We rely on a LA as our choice of variational distribution, which has also been explored by\nPark et al. (2019). Finally, we note that our linearization trick Eq. (6) has great similarities to classic\nextended Kalman \ufb01ltering (Gelb et al., 1974).\n\n", "experiments": "\n\nFirst, we demonstrate the computational advantages of the proposed Hessian approximation, and\nthen that our sampling-based training leads to well-calibrated uncertainties that can be used for OOD\ndetection, data imputation, and semi-supervised learning. For all the downstream tasks we consider\nthe following baselines: AE (Hinton and Salakhutdinov, 2006) with constant and learned variance, VAE\n(Rezende et al., 2014; Kingma and Welling, 2014), Monte-Carlo dropout AE (Gal and Ghahramani,\n2016) and Ensembles of AE (Lakshminarayanan et al., 2017). We extend StochManDetlefsen et al.\n(2021) with the Hessian backpropagation for the approximate and mixed diagonals. The training code\nis implemented in PyTorch and available2 . Appendix A provides more details on the experimental\nsetup.\n\nEf\ufb01cient Hessian Approximation. For practical applications, training time and memory usage of the\nHessian approximation must be kept low. We here show that the proposed approximate diagonal Hes-\nsian is suf\ufb01cient and even outperforms other approximations when combined with our online training.\n\nFig. 7 show the time and memory requirement for different approximation methods as a function\nof input size for a 5-layer convolutional network that preserves channel and input dimension. As\nbaselines we use ef\ufb01cient implementations of the exact and KFAC approximation (Daxberger et al.,\n3\n2021; Dangel et al., 2020). The exact diagonal approximation run out of memory for an\nimage on a 11 Gb NVIDIA GeForce GTX 1080 Ti. In contrast, our approximate diagonal Hessian\nscales linearly with the resolution, which is especially bene\ufb01cial for convolutional layers.\n\n36\n\n36\n\n\u21e5\n\n\u21e0\n\n\u21e5\n\n2 https://github.com/FrederikWarburg/LaplaceAE\n\n7\n\n\f\u00b5\n\n\u00b5\n\nFigure 9: Receiver Operator Curves (ROC) show that our online LAE is able to discriminate between\n\u00b5\nin and out of distribution in both latent and output space.\nlog p(x)\n\nHessian\n\nMSE\n\n\u00b5\n\n\u00b5\n\n\n\n\u00002\n\n\n\n\u00002\n\n\n\n\u00002\n\n\n\n\u00002\n\n\n\n\u00002\n\nKFAC\nExact\nApprox\nExact\u21e4\nApprox\u21e4\n\n\u0000\n9683.9\n283.3\n232.0\n25.8\n25.9\n\n\u00b1\n\u00b1\n\n#\n\n2455.0\n\u00b1\n88.6\n\u00b1\n65.5\n\u00b1\n0.2\n0.4\n\n#\n\n121.6\n27.1\n26.6\n25.7\n25.8\n\n\u00b1\n\u00b1\n\u00b1\n\u00b1\n\u00b1\n\n24.5\n0.9\n0.6\n0.2\n0.4\n\nApprox\n\nExact\n(a) Post-hoc\n\n\n\nFigure 8: Mean and variance of 100 sampled NN.\n\nKFAC\n\nExact\u21e4\n\nApprox\u21e4\n\nTable 2: Online training (indicated by \u21e4) outper-\nforms post-hoc LA. The approximate diagonal\nhas similar performance to the exact diagonal\nfor both post-hoc and online LA.\nFigure 10: Histograms of variance for in- and OOD reconstructions in output space. Note that\nMC-AE separates the distributions well, but the model assign higher variance to the in- than OOD\ndata.\n\n(b) Online\n\n\n\nTab. 2 shows that the exact or approximate Hessian diagonal has similar performance for both\npost-hoc and online training. Using post-hoc LA results in good mean reconstructions (low MSE),\nbut each sampled NN does not give good reconstructions (low log p(x)). Using our online training\nprocedure results in a much higher log-likelihood. This indicates that every sampled NN predicts\ngood reconstructions.\n\nthe uncertainties more well \ufb01tted, both in latent and data space. These well-\ufb01tted uncertainties have\nseveral practical downstream applications, which we demonstrate next.\n\n218\n\n219\n\n224\n\n225\n\n222\n\n223\n\n221\n\n220\n\nFig. 8 shows the latent representation, mean, and variance of the reconstructions with the KFAC, exact\nand approximate diagonal for both post-hoc and online setup. Note that the online training makes the\nuncertainties better \ufb01tted, both in latent and data space. These well-\ufb01tted uncertainties have several\npractical downstream applications, which we demonstrate next.\n\nOut-of-Distribution (OOD) Detection capabilities are critical for identifying distributional shifts,\noutliers and irregular user inputs, which can hinder the propagation of erroneous decision in an\nautomated system. We evaluate OOD performance on the commonly used benchmarks (Nalisnick\net al., 2019b), where we use FASHIONMNIST (Xiao et al., 2017) as in-distribution and MNIST\n(Lecun et al., 1998) as OOD. Tab. 3 shows that our online LAE outperforms existing models in\nboth log-likelihood and Typicality score (Nalisnick et al., 2019a). This stems from the calibrated\nmodel uncertainties, which are exempli\ufb01ed in the models ability to detect OOD examples from the\nuncertainty deduced in latent and output space; see Fig. 9 for ROC curves.\n\nOut-of-Distribution (OOD) Detection capabilities are critical for identifying distributional shifts,\noutliers, and irregular user inputs, which can hinder the propagation of erroneous decisions in an\nautomated system. We evaluate OOD performance on the commonly used benchmarks (Nalisnick\net al., 2019b), where we use FASHIONMNIST (Xiao et al., 2017) as in-distribution and MNIST\n(Lecun et al., 1998) as OOD. Fig. 9 (c) shows that our online LAE outperforms existing models in\nboth log-likelihood and Typicality score (Nalisnick et al., 2019a). This stems from the calibrated\nmodel uncertainties, which are exempli\ufb01ed in the models ability to detect OOD examples from the\nuncertainty deduced in latent and output space; see Fig. 9 (a,b) for ROC curves.\n\nFig. 10 shows distribution of the output variances for in- and OOD data. This illustrates that using LA\nimproves OoD detection. Furthermore, the online training improves the model calibration.\n\nMissing Data imputation. Another application of stochastic representation learning is to provide\ndistributions over unobserved values (Rezende et al., 2014). In many application domains, sensor\nreadings go missing, which we may mimic by letting parts of an image be unobserved. Rezende et al.\n(2014) show that we can then draw samples from the distribution of the entire image conditioned on\n\nFig. 10 shows distribution of the output variances for in- and OOD data. This illustrates that using LA\nimproves OoD detection. Furthermore, the online training improves the model calibration.\n\n226\n\n227\n\n232\n\n228\n\n229\n\n230\n\n231\n\n233\n\nMethod\n\nlogp(x) Typicality\n\n\u00002\n\noutput\n\n\u00002\n\nlatent\n\nAE MSE\nAE MSE\nAE likelihood\nAE likelihood\nVAE\nVAE\nMC-AE\nMC-AE\nLAE (post-hoc)\nLAE (post-hoc)\nLAE (online)\nLAE (online)\n\n\n\n\n\n0.98\n0.66\n0.54\n0.97\n0.98\n0.98\n\n0.98\n0.70\n0.62\n0.97\n0.98\n0.98\n\n0.65\n0.46\n0.08\n0.74\n0.91\n\n0.5\n0.31\n0.96\n0.96\n\n\n\n\n\nlatent\n\noutput\n\n(b) \u00002\n\n(a) \u00002\n\nTable 3: Out of Distribution detection. In-distribution data FashionMnist and OOD data Mnist.\nArea Under Receiver Operator Curve (AUROC\n) for log-likelihood, typicality score, latent space\n\"\n\u0000latent and output space \u0000output uncertainties. Our proposed LAE ELBO is able to discriminate between\nin and OOD using the deduced variances in latent and output space.\n\nFigure 9: Out of Distribution detection. In-distribution data FashionMnist and OOD data Mnist. (a)\n) for\nand (b) shows the ROC curves for latent and output space uncertainties. (c) shows (AUROC\n\"\nlog-likelihood, typicality score, latent space \u0000latent and output space \u0000output uncertainties. Online LAE\nis able to discriminate between in and OOD using the deduced variances in latent and output space.\n\n) for multiple statistics.\n\"\n\n(c) AUROC (\n\n(a) Latent variance, \u00002\n\n(b) Output variance, \u00002\n\noutput\n\nlatent\n\n\n\n\n\n\n\n8\n\nID\nOoD\n\nID\nOoD\n\n\n\n\n\n\n\n\n\n\n\n(a) AE likelihood\n\n(b) MC-AE\n\n(c) VAE\n\n(d) LAE (post-hoc)\n\n(e) LAE (online)\n\nFigure 10: Histograms of variance for in- and OOD reconstructions in output space. Note that MC-\nAE separates the distributions well, but the model assigns higher variance to the in- than OOD data.\n\n\n\n\n\n\n\n\n\n\n\n(a) AE likelihood\n\n(b) MC-AE\n\n(d) LAE (post-hoc)\n\n(e) LAE (online)\n\n(c) VAE\n8\n\n\f\n\n\n\n\n\n(a) VAE\n\n(b) LAE (post-hoc)\n\n(c) LAE (online)\n\nFigure 11: Missing data imputation & generative capabilities. Online training of LAE improves\nrepresentational robustness. This is exempli\ufb01ed by the multimodal behavior in the data imputation\n(top rows) that accurately model the ambiguity in the data. The bottom two rows show that the LAE\nis able to generate crisp digits from random noise.\n\nMethod\n\nClassi\ufb01er\nVAE\nMC-AE\nAE Ensemble\nLAE (post-hoc)\nLAE (online)\n\nMSE\n\n#\n\nlog p(x)\n\n\"\n\n104.73\n106.05\n96.23\n101.62\n99.59\n\n-104.75\n-106.05\n-100.94\n-107.25\n-106.29\n\n\"\n\nAcc.\n\n0.53\n0.22\n0.45\n0.53\n0.51\n0.53\n\n#\n\nECE\n\n0.16\n0.18\n0.28\n0.12\n0.16\n0.12\n\n#\n\nMCE\n\n0.25\n0.34\n0.38\n0.2\n0.27\n0.16\n\n#\n\nRMSCE\n\n0.18\n0.19\n0.29\n0.13\n0.18\n0.13\n\nTable 3: Reconstruction quality measured by the MSE and log-likelihood for the data imputation.\nOur well-calibrated uncertainties propagates to the MNIST classi\ufb01er and improves the calibration\nmetrics ECE, MCE and RMSCE.\n\nMissing Data imputation. Another application of stochastic representation learning is to provide\ndistributions over unobserved values (Rezende et al., 2014). In many application domains, sensor\nreadings go missing, which we may mimic by letting parts of an image be unobserved. Rezende et al.\n(2014) show that we can then draw samples from the distribution of the entire image conditioned on\nthe observed part, by imputing the missing pixels with noise and repeatedly encode and decode while\nkeeping observed pixels \ufb01xed. Fig. 11 show samples using this procedure from a VAE, a post-hoc\nLAE and our online LAE, where we only observe the lower half of an MNIST image. This implies\nambiguity about the original digit, e.g. the lower half of a \u201c5\u201c could be a \u201c3\u201c and similarly a \u201c7\u201c could\nbe a \u201c9\u201c. Our LAE captures this ambiguity, which is exempli\ufb01ed by the multi-modal reconstructions\nfrom the sampled networks in Fig. 11. The baselines only capture unimodal reconstructions.\n\nCapturing the ambiguity of partly missing data can improve downstream tasks such as the calibration\nof an image classi\ufb01er. In Fig. 11 (c) we demonstrate how averaging the predictions of a simple\nclassi\ufb01er across reconstructions improves standard calibration metrics. This is because the classi\ufb01er\ninherits the uncertainty and ambiguity in the learned representations. A deep ensemble of AEs\nperforms similarly to ours, but comes at the cost of training and storing multiple models.\n\nWhen the entire input image is missing, the imputation procedure can be seen as a sampling mecha-\nnism, such that our LAE can be viewed as a generative model. The bottom rows in Fig. 11 show that\nthe LAE indeed does generate sharp images from a multi-modal distribution.\n\nAttribute\n\nArched Eyebrows\nAttractive\nBald\nWearing Lipstick\nHeavy Makeup\n\nOverall\n\nAE\n\n0.50\n0.52\n0.98\n0.52\n0.45\n\n0.73\n\nVAE MC-AE LAE\u21e4\n\n0.52\n0.50\n0.98\n0.49\n0.52\n\n0.72\n\n0.55\n0.49\n0.98\n0.50\n0.49\n\n0.73\n\n0.60\n0.53\n0.98\n0.54\n0.56\n\n0.74\n\nTable 4: Semi-supervised classi\ufb01cation accuracy\non CELEBA using only 10 labeled datapoints. \u21e4\nrefers to online LAE.\n\n9\n\nFigure 12: Accuracy as an function of the\nnumber of labels per class on MNIST.\n\n\fFigure 13: Sample reconstructions on CELEBA. The top row shows the mean reconstruction and\nthe bottom row shows the variance of the reconstructed images.\n\nSemi-supervised learning combines a small amount of label data with a large amount of unlabeled\ndata. The hope is that the structure in the unlabeled data can be used to infer properties of the data that\ncannot be extracted from a few labeled points. Embedding the same labeled data point multiple times\nusing a stochastic representation scales up the amount of labeled data that is available during training.\n\nFig. 12 shows the accuracy of a K-nearest neighbor classi\ufb01er trained on different amounts of labeled\ndata from the MNIST dataset. For all models with a stochastic encoder, we encode each labeled\ndatapoint 100 times and repeat the experiment 5 times. When only a few labels per class are\navailable (1-20) we clearly observe that our LAE model outperforms all other models, stochastic and\ndeterministic. Increasing the number of labels beyond 100 per class makes the AE and LAE equal in\ntheir classi\ufb01cation performance with the AE model eventually outperforming the LAE model.\n\nIn Tab. 4 we conduct a similar experiment on the CELEBA (Liu et al., 2015) facial dataset, where the\nthe task is to predict 40 different binary labels per data point. When evaluating the overall accuracy of\npredicting all 40 facial attributes, we see no signi\ufb01cant difference in performance. However, when we\nzoom in on speci\ufb01c facial attributes we gain a clear performance advantage over other models. Fig. 13\nshows the mean and variance of \ufb01ve reconstructed images. The online LAE produces well-calibrated\nuncertainties in the output space and scales to large images.\n\nLimitations. Empirically, the LAE improvements are more signi\ufb01cant for overparameterized net-\nworks. The additional capacity seems to help the optimizer \ufb01nd a local mode where a Gaussian \ufb01t is ap-\npropriate. It seems the regularization induced by marginalizing \u2713 compensates for the added \ufb02exibility.\n\n", "conclusion": "\n\nIn this paper, we have introduced a Bayesian autoencoder that is realized using Laplace approxi-\nmations. Unlike current models, this Laplacian autoencoder produces well-behaved uncertainties\nin both latent and data space. We have proposed a novel variational lower-bound of the autoencoder\nevidence and an ef\ufb01cient way to compute its Hessian on high dimensional data that scales linearly\nwith data size. Empirically, we demonstrate that our proposed model predicts reliable stochastic\nrepresentations that are useful for a multitude of downstream tasks: out-of-distribution detection,\nmissing data imputation, and semi-supervised classi\ufb01cation. Our work opens the way for fully\nBayesian representation learning where we can marginalize the representation in downstream tasks.\nWe \ufb01nd this to consistently improve performance.\n\n", "appendix": "Laplacian Autoencoders for\nLearning Stochastic Representations\n\nMarco Miani1, Frederik Warburg1,\nPablo Moreno-Mu\u00f1oz, Nicke Skafte Detlefsen, S\u00f8ren Hauberg\n{mmia, frwa, pabmo, nsde, sohau}@dtu.dk\nTechnical University of Denmark\n\nhttps://github.com/FrederikWarburg/LaplaceAE\n\n\u2014 Supplementary Material \u2014\n\nThe supplementary material is organized as follows. First, we give more technical details on the\nexperiments. Second, we discuss the Laplace approximation and the \u201cmean shift\u201d issue encountered\nin non-local maxima. Third, we elaborate more on the difference between related works and our\nproposed method. Fourth, we present a much more thorough explanation of the proposed method\naccompanied by relevant proofs. Fifth, we present more details on the hessian derivations.\n\nA Experimental details\n\nAll experiments was conducted on one of the following datasets: MNIST (Lecun et al., 1998),\nFASHIONMNIST (Xiao et al., 2017) and CELEBA (Liu et al., 2015). For training and testing the default\nsplits were used. For validation, we sampled 5000 data points randomly from the training sets. All\nimages were normalized to be in the [0, 1] range and for the CELEBA dataset the images were resized\nto 64 \u00d7 64 additionally.\n\nIf nothing else is stated, the models were trained with the following configuration: we used Adam\noptimizer (Kingma and Ba, 2015b) with a learning rate of 0.001 and default Pytorch settings (Paszke\net al., 2019). The learning rate was adjusted using ReduceLROnPlateau learning rate scheduler with\nparameters factor=0.5 and patience=5, meaning that the learning rate was halved whenever 5 epochs\nhad passed where the validation loss had not decreased. The mean squared error loss was used as the\nreconstruction loss in all models. Models were trained until convergence, defined as whenever the\nvalidation loss had not decreased for 8 epochs. Models trained on MNIST and FASHIONMNIST used a\nbatch size of 64 and on CELEBA a batch size of 128 was used.\n\nModel-specific details:\n\n\u2022 VAE: Models were trained with KL-scaling of 0.001. We use two encoders and two decoders,\nsuch that the model has twice the number of parameters compared to the other models.\n\u2022 MC-AE: Models were trained with dropout between all trainable layers with probability\n\np = 0.2. We keep the same dropout rate during testing.\n\n\u2022 EMSEMBLE-AE: Each ensemble consists of 5 models, each initialized with a different\n\nrandom seed.\n\n\u2022 LAE (POSTHOC): For experiments with linear layers we used the Laplace Redux (Daxberger\net al., 2021) implementation. For convolutions, we found it necessary to use our proposed\nhessian approximation. We use a diagonal approximation of the hessian in all experi-\nments. After fitting the hessian, we optimize for the prior precision using the marginal\nlikelihood (Daxberger et al., 2021). We use 100 MC sampling in all experiments.\n\n1 Denotes equal contribution; author order determined by a simulated coin toss.\n\n36th Conference on Neural Information Processing Systems (NeurIPS 2022).\n\n\f\u2022 LAE (ONLINE): We use the exact diagonal in experiments with linear layers and the mixed\ndiagonal approximation in all experiments with convolutional layers. We use a hessian\nmemory factor of 0.0001 and sample only 1 network per iteration. We found that it was not\nnecessary to optimize for the prior precision when trained online.\n\nA.1 Hessian approximation\n\nWe use a linear encoder-decoder with three layers in the encoder and decoder, TANH activation\nfunctions, and latent size 2. We choose this architecture as Laplace Redux (Daxberger et al., 2021)\nsupports various hessian approximations for this simple network. We use Laplace Redux for all\npost-hoc experiments except for the approximate diagonal hessian.\n\nA.2 Out-of-distribution\n\nWe use a convolutional encoder-decoder architecture. The encoder consisted of a CONV2D, TANH,\nMAXPOOL2D, CONV2D, TANH, MAXPOOL2D, LINEAR, TANH, LINEAR, TANH, LINEAR, where the\ndecoder was mirrored but with nearest neighbour Upsampling rather than MAXPOOL2D. We used a\nlatent size of 2 in these experiments for all models.\n\nA.3 Missing data imputation\n\nTo elaborate on the procedure, we reconstruct 5 samples from the half/fully masked image. For each of\nthese reconstructions, we make 5 more reconstructions and take the average of these reconstructions.\nThe intuition is that the first stage explores the multi-modal behavior of the reconstructions. In the\nsecond stage, the uncertainty of the reconstructed digit is reduced, and each sample will reconstruct\nthe same modality. By averaging over these modalities, we achieve a more crisp reconstruction. We\nuse the same architecture as in the hessian approximation experiment.\n\nA.4 Semi-supervised learning\n\nFor the experiments on MNIST, we use a single convergence checkpoint for each model. We use\nthe same model architecture as in the hessian approximation. We did 5 repetitions for each model\nwhere we first sampled n labels from each of the 10 classes from the validation set, then embedded\nthe 10 \u00d7 n data points into latent space, trained a KNN classifier on the embedded points and finally\nevaluated the accuracy of the classifier on the remaining validation set. This procedure was repeated\nfor different values of n in the [1, 100] range. For the stochastic encoders (VAE, MC-AE, LAE), we\nrepeated the embedding step 100 times with the goal that the uncertainty could help the downstream\nclassification. For the KNN classifier we use cross-validation (K = 2) to find the optimal number of\nnearest neighbors.\n\nFor the experiments on CELEBA we repeated the exact same experiments but with a fixed value\nif n = 10. Additionally, the classifier was changed to a multi-label version KNN-classifier to\naccommodate the multiple binary features in the dataset. For CELEBA we use a convolutional\narchitecture. The encoder consists of 5 convolutional layers with TANH and MAXPOOL2D in between\neach parametric layer. We use a latent size of 64. The decoder mirrors the encoder, but we replace\nMAXPOOL2D with nearest neighbor upsampling.\n\nB Laplace Approximation\n\nLaplace approximation is an operator that maps local properties (derivatives) to global properties.\nThe idea is to infer a density on every point based on the curvature in a single point. This is done\nthrough a Taylor expansion.\nGiven a vectorial space \u0398 of size D, let P (\u03b8) be an arbitrary distribution on \u0398 and let \u03b8\u2217 \u2208 \u0398 be an\narbitrary point. Consider the second order Taylor expansion of the log density around \u03b8\u2217\n\nln P (\u03b8) = ln P (\u03b8\u2217) + \u2207\u03b8 ln P (\u03b8\u2217)(\u03b8 \u2212 \u03b8\u2217) +\n\n1\n2\n\n(\u03b8 \u2212 \u03b8\u2217)\u22a4\u22072\n\n\u03b8 ln P (\u03b8\u2217)(\u03b8 \u2212 \u03b8\u2217) + O(\u2225\u03b8 \u2212 \u03b8\u2217\u22253),\n\n2\n\n\fwhere\n\n[\u2207\u03b8 ln P (\u03b8\u2217)]i =\n\n\u2202\n\u2202\u03b8i\n\n\n\nln P (\u03b8)\n\n\u03b8=\u03b8\u2217\n\n[\u22072\n\n\u03b8 ln P (\u03b8\u2217)]ij =\n\n\u22022\n\u2202\u03b8i\u2202\u03b8j\n\nln P (\u03b8)\n\n\n\n\n\u03b8=\u03b8\u2217\n\n.\n\nare the first and second-order derivatives.\n\nNote that if P (\u03b8) is a Gaussian, then its log density is a second order polynomial and the second order\nTaylor expansion is exact. This implies that starting from a Gaussian, the Laplace approximation can\ninfer the full exact density just from the values of \u2207\u03b8 ln P and \u22072\nThe main takeaway from the Laplace approximation is the strong, intrinsic, tie between the covariance\nmatrix and the negative inverse of the hessian of the log probability: \u2212(\u22072\n\n\u03b8 ln P in a single point \u03b8\u2217.\n\n\u03b8 ln P (\u03b8\u2217))\u22121.\n\nB.1 If \u03b8\u2217 is a local maxima\n\nIf \u2207\u03b8 ln P (\u03b8\u2217) = 0 the Taylor expansion consists of only two terms and the Laplace derivation\nis easier. We also present this in order to develop intuition, although this case is a subcase of the\nnon-local maxima case. In the next section, we will consider the more general setting.\n\nDefine the Gaussian\n\nLAPLACEmax(\u03b8\u2217; P ) := N \u03b8|\u00b5 = \u03b8\u2217, \u03c32 = \u2212(\u22072\n\n\u03b8 ln P (\u03b8\u2217))\u22121 ,\n\n(1)\n\nwhich has density\n\nP (\u03b8\u2217)\nZQ\nwhere ZQ = P (\u03b8\u2217)(\u22122\u03c0)D det(\u22072\n\u03b8 ln P (\u03b8\u2217)) is the normalizing constant. Then, Q(\u03b8) is a good\napproximation of P (\u03b8) in the sense that\n\n\u03b8 ln P (\u03b8\u2217))\u22121(\u03b8\u2212\u03b8\u2217),\n\n2 (\u03b8\u2212\u03b8\u2217)\u22a4(\u22072\n\nQ(\u03b8) :=\n\ne\n\n1\n\nln Q(\u03b8) + ln ZQ = ln P (\u03b8\u2217) +\n\n\u223c= ln P (\u03b8\u2217) +\n\n= ln P (\u03b8).\n\n1\n2\n1\n2\n\n(\u03b8 \u2212 \u03b8\u2217)\u22a4(\u22072\n\n\u03b8 ln P (\u03b8\u2217))\u22121(\u03b8 \u2212 \u03b8\u2217) \u223c=\n\n(\u03b8 \u2212 \u03b8\u2217)\u22a4(\u22072\n\n\u03b8 ln P (\u03b8\u2217))\u22121(\u03b8 \u2212 \u03b8\u2217) + O(\u2225\u03b8 \u2212 \u03b8\u2217\u22253) =\n\nNotice that \u03b8\u2217 is in a local maximum, which ensures that the hessian \u22072\n\u03b8 ln P (\u03b8\u2217) is negative semi-\ndefinite. This in turn ensures that the normalizing constant ZQ exists and that LAPLACEmax(\u03b8\u2217; P ) is\nwell defined.\n\nB.2 If \u03b8\u2217 is not a local maxima\n\nIn order to proceed to a similar derivation when \u2207\u03b8 ln P (\u03b8\u2217) \u0338= 0, we first rearrange the terms in\nthe Taylor expansion. For a more compact notation, we write \u2207 ln P instead of \u2207\u03b8 ln P (\u03b8\u2217) and\n\u22072 ln P instead of \u22072\n\n\u03b8 ln P (\u03b8\u2217).\n\nln P (\u03b8) \u223c= ln P (\u03b8\u2217) + \u2207 ln P (\u03b8 \u2212 \u03b8\u2217) +\n\n(\u03b8 \u2212 \u03b8\u2217)\u22a4\u22072 ln P (\u03b8 \u2212 \u03b8\u2217) =\n\n1\n2\n\n= ln P (\u03b8\u2217) \u2212\n\n1\n2\n\n\u2207 ln P \u22a4\u22072 ln P \u22121\u2207 ln P\n\n+\n\n1\n2\n= ln P (\u03b8\u2217) \u2212\n\n(\u03b8 \u2212 \u03b8\u2217 + \u22072 ln P \u22121\u2207 ln P )\u22a4\u22072 ln P (\u03b8 \u2212 \u03b8\u2217 + \u22072 ln P \u22121\u2207 ln P ) =\n\n1\n2\n\n\u2207 ln P \u22a4\u22072 ln P \u22121\u2207 ln P +\n\n1\n2\n\n(\u03b8 \u2212 \u03b8\u2217\n\n1)\u22a4\u22072 ln P (\u03b8 \u2212 \u03b8\u2217\n\n1),\n\nwhere we define the new point \u03b8\u2217\n\n1 as\n\n\u03b8\u2217\n1 = \u03b8\u2217 \u2212 (\u22072\n\n\u03b8 ln P (\u03b8\u2217))\u22121\u2207\u03b8 ln P (\u03b8\u2217).\n\nDefine the Gaussian\n\nLAPLACE(\u03b8\u2217; P ) := N \u03b8|\u00b5 = \u03b8\u2217\n\n1, \u03c32 = \u2212(\u22072\n\n\u03b8 ln P (\u03b8\u2217)\u22121 ,\n\n(2)\n\n(3)\n\n3\n\n\fwhich has density\n\nQ(\u03b8) :=\n\ne\u2212 1\n\n2 \u2207 ln P \u22a4\u22072 ln P \u22121\u2207 ln P e\n\nP (\u03b8\u2217)\nZQ\n2 \u2207 ln P \u22a4\u22072 ln P \u22121\u2207 ln P (\u22122\u03c0)D det(\u22072\n\n2 (\u03b8\u2212\u03b8\u2217\n\n1\n\n1 )\u22a4(\u22072\n\n\u03b8 ln P (\u03b8\u2217))\u22121(\u03b8\u2212\u03b8\u2217\n\n1 ),\n\nwhere ZQ = P (\u03b8\u2217)e\u2212 1\nstant. Then, Q(\u03b8) is a good approximation of P (\u03b8) in the sense that\n\n\u03b8 ln P (\u03b8\u2217)) is the normalizing con-\n\nln Q(\u03b8) + ln ZQ = ln P (\u03b8\u2217) \u2212\n\n\u2207 ln P \u22a4\u22072 ln P \u22121\u2207 ln P +\n\n1\n2\n= ln P (\u03b8\u2217) + \u2207 ln P (\u03b8 \u2212 \u03b8\u2217) +\n\n\u223c= ln P (\u03b8\u2217) + \u2207 ln P (\u03b8 \u2212 \u03b8\u2217) +\n\n= ln P (\u03b8).\n\n1\n2\n\n(\u03b8 \u2212 \u03b8\u2217\n\n1)\u22a4\u22072 ln P (\u03b8 \u2212 \u03b8\u2217\n\n1) =\n\n(\u03b8 \u2212 \u03b8\u2217)\u22a4\u22072 ln P (\u03b8 \u2212 \u03b8\u2217) \u223c=\n\n(\u03b8 \u2212 \u03b8\u2217)\u22a4\u22072 ln P (\u03b8 \u2212 \u03b8\u2217) + O(\u2225\u03b8 \u2212 \u03b8\u2217\u22253) =\n\n1\n2\n1\n2\n\nWe highlight four points:\n(1) In order to ensure that the normalizing constant ZQ exists and consequently that LAPLACE(\u03b8\u2217; P )\n\u03b8 ln P (\u03b8\u2217) must be negative semi-definite. Differently from the locally\nis well defined, the hessian \u22072\nmaximal case, this is no longer guaranteed.\n(2) The method is numerically unstable if \u22072\nempirically observed to commonly be the case (Sagun et al., 2016).\n(3) We emphasize that the Taylor expansion is accurate around \u03b8\u2217, but the Laplace Gaussian is\ncentered in \u03b8\u2217\n1. This is often referred to as \u201cmean shift\u201d and implies that the sampled parameters from\nthe normal distribution over weights are sampled far away from the actual mean.\n\n\u03b8 ln P (\u03b8\u2217) has eigenvalues close to 0. This has been\n\n(4) Now, assume ln P (\u03b8) to be the composition of a function f (\u03b8) and a loss l(f ). If, as in our\nsetting, f is linear and l is concave, then ln P (\u03b8) is concave, its hessian is guaranteed to be negative\nsemi-definite, and the Laplace approximation is well-defined.\n\nC Extended related work\n\nOur proposed method\u2019s update rule resembles different methods in the literature. This is rather\ninteresting since, despite arriving at a similar algorithm, they follow different derivations. In this\nsection, we seek to explain the nuances between existing methods and ours.\n\nIt is useful to recall our precision update rule\n\nHt+1 = (1 \u2212 \u03b1)Ht + J\u03b8f t\n\n\u00b7 (x)\u22a4\u03a3\u22121\n\ndataJ\u03b8f t\n\n\u00b7 (x)\n\nH0 = \u03a3\u22121\n\nprior.\n\n(4)\n\nTo the best of our knowledge, a key conceptual difference with all related works in the literature\nis Exact vs. Approximate Hessian: The second term on the RHS, J \u22a4\u03a3\u22121J is identical in all the\nupdate formulations but comes from a different derivation. In the formulation of Daxberger et al.\n(2021) (and others) this is an approximation of the hessian, that happens to guarantee negative\ndefiniteness. In our formulation, this term comes from the linearization. This term is the exact hessian,\ni.e. no approximation, such that negative definiteness is implied by the linearization. This results in a\nmore intuitive understanding of the linearization error.\n\nC.1 Differences with Laplace Redux\n\nDaxberger et al. (2021) specifically develop a post-hoc method, which assumes access to a MAP\nparameter, nevertheless, they also present an online training scheme. This appears to be grounded in\nideas from second-order optimization, while ours is closer linked to the probabilistic model.\n\nNeglecting their prior optimization procedure, Daxberger et al. (2021) considers\n\nHt+1 = \u03a3\u22121\n\nprior + J\u03b8f t\n\n\u00b7 (x)\u22a4\u03a3\u22121\n\ndataJ\u03b8f t\n\n\u00b7 (x).\n\n(5)\n\nWe highlight two main differences.\n\n4\n\n\fIteratively Updating the Hessian vs. Using an Uninformed Prior: In our formulation the first term\nof the RHS is the previous precision, which we \u201cdiscount\u201d with a forgetting term \u03b1 that comes from\nthe error inherited by moving away from the previous linearization. In the formulation of Daxberger\net al. (2021) this is a prior that is assumed to be in the form \u03b32I, where the scalar \u03b3 is optimized at\nevery step through maximization of the evidence (Eq. 6 in their paper). This evidence maximization\ncomes with some drawbacks: (1) It tends to make the Laplace approximation overconfident to outliers.\nThey partially address this issue by adding to the evidence an auxiliary term that depends on an OOD\ndataset, penalizing it (Eq. 12 in their Appendix). (2) Besides inducing the avoidable need for an OOD\ndataset, this technique has the same pitfall as VAEs, namely uncertainty should be a derived quantity,\nnot a learned one.\n\nComputing the hessian at every iteration vs. every epoch: We update the hessian estimate every\niteration. In practice, Daxberger et al. (2021) updates the hessian every epoch. In principle, they\ncould update the hessian more regularly, but to the best of our knowledge, they do not explore this\npath.\n\nC.2 Differences with Variational Adaptive Newton Method\n\nKhan et al. (2017) propose the following precision update rule\n\nHt+1 = Ht + \u03b1J\u03b8f t\n\n\u00b7 (x)\u22a4\u03a3\u22121\n\ndataJ\u03b8f t\n\n\u00b7 (x).\n\n(6)\n\nThe updates are very similar to ours, where the only difference is that the scaling is made on the\nJacobian product instead of on the previous precision Ht. Both their and our updates are \u201cadditive\u201d\nin the sense that the magnitude is increasing, for them strictly monotonically, for us on average\ndepending on the magnitude of \u03b1. Thus, in both cases, the variance will approach 0 in the limit,\nmodeling epistemic uncertainty disappearing for infinitely long training.\n\nTheir update rule comes from the Variational Optimization setting, which can be viewed as an instance\nof Variational Inference neglecting the KL term. They highlight strong similarities with Newton\u2019s\nmethod.\n\nC.3 Differences with Noisy Natural Gradient\n\nZhang et al. (2017) propose the precision update rule\n\nHt+1 = (1 \u2212 \u03b1)Ht + \u03b1J\u03b8f t\n\n\u00b7 (x)\u22a4\u03a3\u22121\n\ndataJ\u03b8f t\n\n\u00b7 (x).\n\n(7)\n\nAgain, the update rule is very similar to ours, besides the scaling being applied to the Jacobian product.\nThey use a convex sum, which makes the update rule \u201cnorm preserving\u201d. Thus, these updates have\nvery different asymptotic behavior than ours.\n\nTheir update rule comes from applying, in the context of Variational Inference, natural gradient to the\nvariational posterior distribution, instead of directly on the parameters space. The natural gradient is\ndeeply connected with LAE and natural parameters highlight the importance of updating the precision\nmatrix instead of the covariance.\n\nThey also extend their derivation to the KFAC approximation of the hessian (in place of the exact\ndiagonal), this is made through the use of matrix variate Gaussian. Despite not being considered in\nthis work, a similar derivation is in principle feasible in our setting.\n\nC.4 Connection with Adam\n\nBoth Zhang et al. (2017) and Khan et al. (2017) highlight strong similarities with the Adam\nmethod (Kingma and Ba, 2015a). We share these similarities and we highlight them too. The\n\u201cconnection point\u201d is a noisy version of Adam. Zhang et al. (2017) describe this method and call it\n\u201cNoisy Adam\u201d (Algorithm 1 in their paper). The difference from the vanilla version is that at each\nstep, instead of using the current parameter, they use a noisy version of it. The noise magnitude is the\npseudo-second-order term (vt in the original Adam paper (Kingma and Ba, 2015b)).\n\nWe can then interpret Noisy Adam as an instance of our variational setting, specifically where the\nexpectation estimate Eq(\u03b8)[\u00b7] is made through Monte Carlo estimation with N = 1 samples. Having\nthe methods in the same setting, we can compare them and highlight the two main differences.\n\n5\n\n\fFirst, we emphasize the difference is in the second order term. Similarly to Zhang et al. (2017) and\nKhan et al. (2017), we use the diagonal of the hessian (technically the diagonal of the GGN), while\nAdam uses the pointwise square of the gradient, which they call the second raw moment and is\nintended as a cheaply computable approximation of the Hessian.\n\nAnother difference is that Adam applies the square root. This is a minor point since, as pointed out\nby Zhang et al. (2017), this change may affect optimization performance, but does not change the\nfixed points.\n\nC.5 Connection with Bayes by Backpropagation\n\nBayes by Backprop (Blundell et al., 2015) can be viewed as a sample-based approximation of Laplace.\nIn order to show this relation, we recall two very powerful equations (Opper and Archambeau, 2009).\nSpecifically, let \u00b5, \u03a3 be the parameter of a Gaussian distribution q(\u03b8) \u223c N (\u00b5, \u03a3), and let V (\u03b8) be\nan arbitrary L2 integrable function (the log-likelihood in our case). Then, we are interested in the\nderivatives of E\u03b8\u223cq[V (\u03b8)]. By standard Fourier analysis and integration by part, we have\n\n\u2207\u00b5E\u03b8\u223cq[V (\u03b8)] = E\u03b8\u223cq[\u2207\u03b8V (\u03b8)],\n\n\u2207\u03a3E\u03b8\u223cq[V (\u03b8)] =\n\n1\n2\n\nE\u03b8\u223cq[\u22072\n\n\u03b8V (\u03b8)].\n\n(8)\n\n(9)\n\nWhile the first equation is somehow trivial, the second highlight a very deep relationship. The LHS\ncan be rewritten as\n\n\u2207\u03a3E\u03b8\u223cq[V (\u03b8)] = \u2207\u03a3E\u03f5\u223cN (0,1)[V (\u00b5 + \u03f5\u03a3)] = E\u03f5\u223cN (0,1)[\u2207\u03a3V (\u00b5 + \u03f5\u03a3)].\n\n(10)\n\nWe can recognize that inside the expectation on the RHS is exactly the update rule for the variance in\nthe bayes by backprop method (Blundell et al., 2015). Thus, we can interpret bayes by backprop as\na one-sample Monte Carlo estimation of the expected value. The equations shows that in the limit\nof infinite samples, the bayes by backprop update step \u2207\u03a3V (\u00b5 + \u03f5\u03a3) converges to the (averaged)\nLaplace approximation step E\u03b8\u223cq[\u22072\n\u03b8V (\u03b8)] up to a factor 2. This motivates both the power and the\ninstability of bayes by backprop.\n\nD Model\n\nD.1 Overview\n\nLet X = RD be the data space, let Y = RD be the reconstruction space and let \u0398 = RP be\nthe parameter space. Let F = (\u0398 \u2192 (X \u2192 Y)) be the space of operators from \u0398 to the space of\noperators from X to Y. We will denote a function f \u2208 F applied to a parameter \u03b8 \u2208 \u0398 as f\u03b8 : X \u2192 Y.\nThis will represent, for example, a NN f with a specific set of parameter (weights) \u03b8 \u2208 \u0398, that maps\nsome data x \u2208 X to some reconstruction f\u03b8(x) = y \u2208 Y.\nLet X \u00d7 Y \u00d7 \u0398 \u00d7 F be a probability space. The only assumption we make on this space is\n\np(y|x, \u03b8, f ) \u223c N (y|\u00b5 = f\u03b8(x), \u03c32 = \u03a3)\n\n(11)\nwhere \u03a3 \u2208 M(RD \u00d7 RD) is a fixed variance matrix. This is a common assumption for regression\ntasks, and is sometimes referred to as the \u201cdata noise\u201d or \u201creconstruction error\u201d. In this paper, we fix\n\u03a3 = I, but the derivations hold for the general case. With only this assumption, the distribution is\nundefined and multiple solutions can exist. Thus, we require more assumptions.\nA dataset D = {xn} is a finite of infinite collection of data xn \u2208 X that is assumed to follow a\ncertain, fixed but unknown, distribution\n\n\u2200(x, \u03b8, f ) \u2208 X \u00d7 \u0398 \u00d7 F\n\nxn \u223c p(x).\n\n(12)\n\nSacrificing slim notation for the sake of clarity, we introduce an operator I : X \u2192 Y. This represents\nthe ideal reconstruction for a given x. In the standard supervised setting, this would be the operator\n(defined on the dataset only) that maps each data input to its label. In our unsupervised setting, where\nX and Y are the same space, the operator I is simply the identity (Indeed they are not the same space,\nthey are isomorphic spaces that we identify through the operator I). Since I is the identity, it is often\n\n6\n\n\fneglected in the literature, which can lead to unclear and potentially ambiguous Bayesian derivations.\nThus, we choose to adopt this heavier, but more precise notation.\nWe assume access to a specific f N N \u2208 F. Practically this will be our NN architecture, i.e. an operator\nthat, given a set of parameters \u03b8 \u2208 \u0398 gives rise to a function from X to Y. Having f N N fixed, one\nmay consider f not to be stochastic anymore, we choose to still explicitly condition on f in order to\nhave a clearer notation in later stages. Note that, despite not being covered in this work, a proper\nstochastic derivation also on the NN architecture should be feasible.\n\nD.2 Objective\n\nThe NN\u2019s parameter optimization process in this full Bayesian probabilistic framework can be viewed\nas: given a fixed f N N \u2208 F, namely the NN architecture, maximise the reconstruction probability of\nI(xn) over the dataset D\n\n\n\n\nExn\u223cp(x)\n\np(y|xn, f N N )\n\ny=I(xn)\n\n=\n\n\n\np(y|xn, f N N )\n\ny=I(xn),\n\n(13)\n\nxn\u2208D\n\nwhere the untractable p(y|xn, f N N ) can be expanded in \u03b8 and thus related to our hypothesis (11) as\n\np(y|xn, f N N ) = E\u03b8\u223cp(\u03b8|xn,f N N )\n\np(y|xn, \u03b8, f N N ) .\n\n(14)\n\nNotice that the only unfixed quantity is the distribution on the parameters, which we will optimize\nfor. We are not interested in finding a datapoint-dependant distribution, but rather one that maximise\nall reconstructions at the same time, i.e. p(\u03b8|f N N ) = p(\u03b8|xn, f N N ). We can then frame Bayesian\noptimization as: find a distribution on parameters such that\n\nq(\u03b8) \u2208 arg max\n\np(\u03b8|f N N )\n\n= arg max\n\np(\u03b8|f N N )\n\n\n\nxn\u2208D\n\n\nxn\u2208D\n\nE\u03b8\u223cp(\u03b8|f N N )\n\n\n\np(y|xn, \u03b8, f N N )\n\ny=I(xn)\n\n\n\nE\u03b8\u223cp(\u03b8|f N N )\n\np(I(xn)\n\nN (f N N\n\u03b8\n\n(xn), \u03a3)) .\n\n(15)\n\n(16)\n\nMoreover, finding this optimum in the space \u2206(\u0398) of all distributions on \u0398 is not tractable. So, as\ncommonly done, we restrict ourselves to the subset G(\u0398) \u2282 \u2206(\u0398) of Gaussians over \u0398. Then, a\nsolution in our context is\n\nq(\u03b8) \u2208 arg max\nq\u2208G(\u0398)\n\n\n\nxn\u2208D\n\nE\u03b8\u223cq(\u03b8)\n\np(I(xn)\n\nN (f N N\n\u03b8\n\n(xn), \u03a3)) .\n\n(17)\n\nWe emphasize that this solution has no guarantees of being unique, but we are interested in finding\none of them.\n\nD.3\n\nJoint distribution for a fixed datapoint\n\nLet us first get a better understanding of the joint distribution on Y\u00d7\u0398 conditional to a fixed datapoint\nx \u2208 X and a network architecture f \u2208 F\n\nThis distribution has two marginals\n\nand two conditionals\n\np(y, \u03b8|x, f ).\n\np(y|x, f )\n\np(\u03b8|x, f )\n\np(y|\u03b8, x, f )\n\np(\u03b8|y, x, f ).\n\nThese four quantities must satisfy the system of two \u201crecursive\u201d equations\n\np(y|x, f ) =\n\n\n\n\u0398\n\np(y|\u03b8, x, f )p(\u03b8|x, f )d\u03b8 =\n\n= E\u03b8\u223cp(\u03b8|x,f )[p(y|\u03b8, x, f )]\n\n7\n\n(18)\n\n(19)\n\n(20)\n\n(21)\n\n(22)\n\n(23)\n\n\fp(\u03b8|x, f ) =\n\n\n\nY\n\np(\u03b8|y, x, f )p(y|x, f )dy =\n\n= Ey\u223cp(y|x,f )[p(\u03b8|y, x, f )].\n\nIf these are satisfied then the joint is a well-defined distribution and we can apply Bayes rule\n\np(y|\u03b8, x, f )p(\u03b8|x, f ) = p(y, \u03b8|x, f ) = p(\u03b8|y, x, f )p(y|x, f )\n\nwhich in logarithmic form is\n\n(24)\n\n(25)\n\nlog p(y|\u03b8, x, f ) + log p(\u03b8|x, f ) = log p(\u03b8|y, x, f ) + log p(y|x, f ).\n(26)\nWe can factor in the assumptions. The \u201cdata noise\u201d assumption gives us one of the two conditionals:\np(y|\u03b8, x, f ) \u223c N (y|\u00b5 = f\u03b8(x), \u03c32 = \u03a3).\n(27)\n\nThe \u201cGaussian parameter\u201d assumption gives us one of the two marginals:\np(\u03b8|x, f ) = qt(\u03b8) \u223c N (\u03b8|\u00b5 = \u03b8t, \u03c32 = H\u22121\nt ).\nWith these in place, the joint distribution is uniquely defined. The other marginal, by Eq. 23, is\n\np(y|x, f ) = E\u03b8\u223cqt(\u03b8)[p(y|N (f\u03b8(x), \u03a3))]\n\nand the other conditional, by Bayes rule, is\n\np(\u03b8|y, x, f ) =\n\np(y|\u03b8, x, f )p(\u03b8|x, f )\np(y|x, f )\n\n.\n\n(28)\n\n(29)\n\n(30)\n\nDespite being uniquely defined, the integral in Eq. 29 is, with a general f , intractable, and so is the\njoint distribution.\n\nBut why do we even care? The intractability of p(y|x, f ) in Eq. 29 may at first glance appear\nirrelevant. This is the case, for instance, with bayes by backprop (Blundell et al., 2015) methods.\nThey simply need access to the gradient of this quantity. For this purpose, a simple Monte Carlo\nestimate of the expectation is enough.\n\nOn the other hand, we are interested in recovering a meaningful distribution on parameters. This\nimply that we aim at using Eq. 29 to enforce that Eq. 24 holds. For this purpose we need access to\nthe the density p(y|x, f ), so a Monte Carlo estimate of Eq. 29 is not enough.\n\nD.4 Linear f\n\nTheorem 1. Given the data noise assumption from Eq. 27\n\np(y|\u03b8, x, f ) \u223c N (y|\u00b5 = f\u03b8(x), \u03c32 = \u03a3),\ngiven the Gaussian parameter assumption from Eq. 28 for some \u03b8t, Ht\np(\u03b8|x, f ) = qt(\u03b8) \u223c N (\u03b8|\u00b5 = \u03b8t, \u03c32 = H\u22121\n\nt ),\n\nassume that f is linear in \u03b8, i.e.\n\nf\u03b8(x) = f0(x) + Jf\u00b7(x)\u03b8\n\n\u2200\u03b8 \u2208 \u0398, \u2200x \u2208 X.\n\nThen the joint distribution is Gaussian itself\n\np(\u03b8, y|x, f ) \u223c N ((\u03b8, y)|\u00b5t, \u03a3t)\n\nwhere\n\n\u00b5t =\n\n\n\n \u03b8t\nf\u03b8t(x)\n\n, and \u03a3t =\n\n H\u22121\nJf\u00b7(x)\u03a3 Jf\u00b7(x)\u22a4HtJf\u00b7(x)\u22121\n\n\u03a3Jf\u00b7(x)\u22a4\n\nt\n\n(31)\n\n(32)\n\n(33)\n\n(34)\n\n\n\n.\n\n+ \u03a3\n\nProof. With the further assumption of linearity of f , we can explicitly carry out the integral in the\nexpectation in Eq. 29\n\np(y|x, f ) = E\u03b8\u223cqt(\u03b8)[p(y|N (f\u03b8(x), \u03a3))] =\n\n=\n\n=\n\n=\n\n\n\n\u0398\n\n\n\n\u0398\n\n\n\n\u0398\n\np(y|N (f\u03b8(x), \u03a3))qt(\u03b8)d\u03b8 =\n\np(y|N (f\u03b8(x), \u03a3))p(\u03b8|N (\u03b8t, H\u22121\n\nt ))d\u03b8 =\n\np(y|N (f0(x) + Jf\u00b7(x)\u03b8, \u03a3))p(\u03b8|N (\u03b8t, H\u22121\n\nt ))d\u03b8 =\n\nWe emphasize that as a consequence \u22072\n\n\u03b8 log p(y|x, f ) is not dependent on \u03b8t.\n\n= p(y|N (f\u03b8t(x), (Jf\u00b7(x)\u22a4HtJf\u00b7(x))\u22121 + \u03a3)).\n\n8\n\n\fHaving Theorem 1 in place, we can go back to our original problem. We need to deal with a fixed\nnon-linear architecture f N N . We can exploit Theorem 1 by defining f t: a linearization of f N N with\na first-order Taylor expansion around \u03b8t\n\nand it holds that\n\n\u03b8(x) := TAYLOR(f N N , \u03b8t)(x) =\nf t\n\n= f N N\n\u03b8t\n\n(x) + J\u03b8f N N\n\n\u03b8t\n\n(x)(\u03b8 \u2212 \u03b8t)\n\nf N N\n\u03b8\n\n(x) = f t\n\n\u03b8(x) + O(\u2225\u03b8 \u2212 \u03b8t\u22252).\n\nRecalling Eq. 11 both for f N N and for f t\n\np(y|x, \u03b8, f N N ) \u223c N (y|\u00b5 = f N N\n\n\u03b8\n\n(x), \u03c32 = \u03a3)\n\np(y|x, \u03b8, f t) \u223c N (y|\u00b5 = f t\n\n\u03b8(x), \u03c32 = \u03a3)\n\nthat, together with Eq. 36, imply\n\nN y|\u00b5 = f N N\n\n\u03b8\n\n(x), \u03c32 = \u03a3 \u223c N y|\u00b5 = f t\n\n\u03b8(x) + O(\u2225\u03b8 \u2212 \u03b8t\u22252), \u03c32 = \u03a3,\n\n(35)\n\n(36)\n\n(37)\n\n(38)\n\n(39)\n\nwhere we can interpret the unknown O(\u2225\u03b8 \u2212 \u03b8t\u22252) as \u03b8-dependent noise. More specifically, calling\n\u03b3 > 0 the scalar constant of the O-term, we assume that\n\nO(\u2225\u03b8 \u2212 \u03b8t\u22252) \u2248 \u03f5(\u03b8 \u2212 \u03b8t)2\n\nwhere \u03f5 \u223c N (0, \u03b3I)\n\nand thus, from Eq. 39, we have\n\nN y|\u00b5 = f N N\n\n\u03b8\n\n(x), \u03c32 = \u03a3 \u223c N (y|\u00b5 = f t\n\n\u03b8(x), \u03c32 = \u03a3 + \u03b3\u2225\u03b8 \u2212 \u03b8t\u22252I).\n\n(40)\n\n(41)\n\nAt this point, integrals are not analytically tractable, and thus a proper proof is not feasible, the\nintuition is that this increased variance reflects in increased variance in p(\u03b8|x, f N N )\n\n\u22072\n\n\u03b8 log p(\u03b8|x, f N N )\n\n\u03b8=\u03b8t+1\n\n\u2248 \u22072\n\n\u03b8 log p(\u03b8|x, f t)\n\n\u03b8=\u03b8t+1\n\n+ \u03b3\u2225\u03b8t+1 \u2212 \u03b8t\u22252,\n\n(42)\n\nwhere we introduce a hyperparameter \u03b1 > 0 to cope with this added variance. If we then assume the\nJacobian J\u03b8f N N to be a Lipschitz function, then the Lipschitz constant is an upper bound on \u03b3, as\nfollows from the Taylor expansion of Eq. (35). If this Lipschitz constant is smaller than the inverse of\nthe gradient step 1/\u2225\u03b8t+1 \u2212 \u03b8t\u2225 (which is not an unreasonable assumption for the gradient ascent to\nbe stable) we have\n\n\u03b3\u2225\u03b8t+1 \u2212 \u03b8t\u22252 \u2248 \u2225\u03b8t+1 \u2212 \u03b8t\u2225\n\n(43)\n\nthat gives us a plausible order of magnitude for choosing the hyperparameter \u03b1.\nMotivation: During training, we produce a sequence of Gaussians qt(\u03b8) \u223c N (\u03b8t, H\u22121\nt ) that we\nassume to be the distribution p(\u03b8|x, f t), at every step t \u2265 0. This distribution qt is then used for (1)\nGaussian derivation in the linear case, for (2) Monte Carlo sampling in the update rule \u03b8t \u2192 \u03b8t+1\nand (3), as second order derivative, for update rule Ht \u2192 Ht+1.\nMoreover, given that this distribution qt is our \"best guess so far\", we assume it to be also the\ndistribution p(\u03b8|x, f N N ). This, being f N N not linear, (1) cannot be used for Gaussian derivation,\n(2) can reasonably be used for sampling (and thus we derive the improved update rule Eq. (52)), and\n(3) can somehow be used as second order derivative (and thus we derive the improved update rule\nEq. (53)), but the latter requires some more care. That is why we introduce the parameter \u03b1.\n\nD.5\n\nIterative learning\n\nOur learning method produces a sequence of Gaussians\n\nqt(\u03b8) \u223c N (\u03b8|\u00b5 = \u03b8t, \u03c32 = H\u22121\nt )\n\nand a sequence of linearized functions\n\nf t = TAYLOR(f N N , \u03b8t)\n\nfor every t \u2265 0.\n\n9\n\n(44)\n\n(45)\n\n\fInitialization is trivially done using a Gaussian prior on the parameters.\n\n\u03b80 = \u03b8prior\n\nH0 = (\u03a3prior)\u22121\n\n(46)\n\nIterative step is made in two steps. First, having access to \u03b8t, we \u201cgenerate\u201d the linearization f t.\nPractically this is equivalent to computing the two quantities f N N\n(x), that, together\nwith the value \u03b8t are actually equivalent to \u201cgenerating\u201d f t, as Eq. 35 shows.\nSecond, we compute the Gaussian parameters \u03b8t+1 and Ht+1.\n\n(x) and J\u03b8f N N\n\n\u03b8t\n\n\u03b8t\n\nRecalling our aim of maximizing the quantity Eq. (17), update on q(\u00b7) means, \u03b8t \u2192 \u03b8t+1, is ideally\nmade through gradient ascent steps on p(y|x, f )|y=I(x). As this is intractable, we instead do gradient\nsteps on the lower bound Ly of (the log of) Eq. 23\n\n\u03b8t+1 = \u03b8t + \u03bb\u2207\u03b8Ly\n\n\n\ny=I(x)\n\nwhere\n\nand so\n\nLy = E\u03b8\u223cp(\u03b8|x,f t)[log p(y|\u03b8, x, f t)] \u2264 log E\u03b8\u223cp(\u03b8|x,f t)[p(y|\u03b8, x, f t)]\n\n\u2207\u03b8Ly\n\n\ny=I(x) = E\u03b8\u223cp(\u03b8|x,f t)\n\n\n\u2207\u03b8 log p(y|\u03b8, x, f t)\n\n\n\n=\n\n= E\u03b8\u223cqt(\u03b8)\n\n\u2207\u03b8 log p(I(x)\n\nN (f t\n\ny=I(x)\n\u03b8(x), \u03a3)) .\n\n(47)\n\n(48)\n\n(49)\n\nRecalling the Laplace approximation Eq. (3), the negative precision, \u2212Ht+1, is ideally set to be the\nthe hessian of the log probability p(\u03b8|x, f ), evaluated in \u03b8t+1. As this is intractable we instead set it\nto the hessian of the lower bound L\u03b8 of (the log of) Eq. (24)\n\nHt+1 = \u2212\u22072\n\n\u03b8L\u03b8\n\n\n\n\u03b8=\u03b8t+1\n\nwhere\n\nand so\n\nL\u03b8 = Ey\u223cp(y|x,f t)[log p(\u03b8|y, x, f t)] \u2264 log Ey\u223cp(y|x,f t)[p(\u03b8|y, x, f t)]\n\n\u22072\n\n\u03b8L\u03b8\n\n\n\n\u03b8=\u03b8t+1\n\n= Ey\u223cp(y|x,f t)\n\n\n\u22072\n\n\u03b8 log p(\u03b8|y, x, f t)\n\n\n\n\n\n\u03b8=\u03b8t+1\n\n(50)\n\n(51)\n\nvia Eq. (26)\n\n= Ey\u223cp(y|x,f t)\n\n\n\n\u22072\n\n\u03b8 log p(y|\u03b8, x, f t)\n\n\u2212 \u22072\n\n\n\u03b8 log p(y|x, f t)\n\n\u03b8=\u03b8t+1\n\n\n\n\u03b8=\u03b8t+1\n\n\n+ \u22072\n\n\n\u03b8 log p(\u03b8|x, f t)\n\n\u03b8=\u03b8t+1\n\n+\n\nvia Theorem 1\n\n= Ey\u223cp(y|x,f t)\n\n\n\u22072\n\n\u03b8 log p(y|\u03b8, x, f t)\n\n\n\n\u03b8=\u03b8t+1\n\n+ \u22072\n\n\n\u03b8 log p(\u03b8|x, f t)\n\n\u03b8=\u03b8t+1\n\n\n\nvia the chain rule Eq. (55)\n\n= Ey\u223cp(y|x,f t)\n\n\nJ\u03b8f t\n\n\u00b7 (x)\u22a4\u22072\n\ny log p(y|\u03b8t+1, x, f t)J\u03b8f t\n\n\u00b7 (x) + \u22072\n\n\u03b8 log p(\u03b8|x, f t)\n\n\n\n\n\n\u03b8=\u03b8t+1\n\nvia HP Eq. (11)\n\n= Ey\u223cp(y|x,f t)\n\n\n\n\u2212J\u03b8f t\n\n\u00b7 (x)\u22a4\u03a3\u22121J\u03b8f t\n\n= \u2212J\u03b8f t\n\n\u00b7 (x)\u22a4\u03a3\u22121J\u03b8f t\n\n\u00b7 (x) + \u22072\n\n\u00b7 (x) + \u22072\n\n\n\u03b8 log p(\u03b8|x, f t)\n\n\u03b8=\u03b8t+1\n\n\u03b8 log p(\u03b8|x, f t)\n\n\u03b8=\u03b8t+1\n\n\n\n= \u2212J\u03b8f t\n\n\u00b7 (x)\u22a4\u03a3\u22121J\u03b8f t\n\n\u00b7 (x) + \u22072\n\n\u03b8 log qt(\u03b8)\n\n= \u2212J\u03b8f t\n\n\u00b7 (x)\u22a4\u03a3\u22121J\u03b8f t\n\n\u00b7 (x) \u2212 Ht.\n\n10\n\n\n\n\u03b8=\u03b8t+1\n\n\fD.5.1\n\nImproved update rule\n\nAs said, the update \u03b8t \u2192 \u03b8t+1 is ideally made through gradient ascent steps on p(y|x, f )|y=I(x),\nbut we instead use the tractable lower bound with f t. We can perform the same derivation using f N N\nin place of f t. Assuming p(\u03b8|x, f N N ) \u223c qt(\u03b8) for sampling, leads to the improved update rule\n\n\u03b8t+1 = \u03b8t + \u03bbE\u03b8\u223cqt(\u03b8)\n\n\u2207\u03b8 log p(I(x)\n\nN (f N N\n\u03b8\n\n(x), \u03a3)) .\n\n(52)\n\nSimilarly, the negative precision \u2212Ht+1 is ideally set to be the hessian of the log probability p(\u03b8|x, f ).\nbut instead, we use the tractable lower bound with f t. Here we cannot perform the same derivation,\nsince Theorem 1 does not hold anymore. Instead, we can rely on the estimate Eq. (42) to improve the\nterm\n\u22072\n\n\u2248 \u2212(1 \u2212 \u03b1)Ht,\n\n\u03b8 log p(\u03b8|x, f t)\n\n\n\u03b8 log p(\u03b8|x, f N N )\n\n\u03b8=\u03b8t+1\n\n\n\n\u03b8=\u03b8t+1\n\n= \u2212Ht\n\n\u2212\u2192\n\n\u22072\n\nand this leads to the improved update rule\n\nHt+1 = (1 \u2212 \u03b1)Ht + J\u03b8f t\n\n\u00b7 (x)\u22a4\u03a3\u22121J\u03b8f t\n\n\u00b7 (x).\n\n(53)\n\nE Fast Hessian\n\nWe are interested in computing the hessian of a loss function. For this purpose, the Jacobian of the\nNN w.r.t. parameters plays a crucial role. In this section we develop a better understanding of this\nobject, we derive the backpropagation (also used by the BackPack library (Dangel et al., 2020)) and\nfinally, we explain our approximated backpropagation that allows linear scaling.\n\nE.1 Jacobian of a Neural Network\n\nLet us first define some terminology that we will need for chain rule derivations. A NN is a composition\nof l functions f := fLl \u25e6 fLl\u22121 \u25e6 . . . \u25e6 fL2 \u25e6 fL1:\n\nx0 \u2212\u2212\u2212\u2212\u2212\u2192\n\nx1 \u2212\u2192 . . . \u2212\u2192 xi\u22121 \u2212\u2212\u2212\u2212\u2212\u2192\n\nxi \u2212\u2192 . . . \u2212\u2192 xl\u22121 \u2212\u2212\u2212\u2212\u2212\u2192\n\nxl\n\nfL1\n\nfLi\nwhere there are parametric and non-parametric function fLi . We here highlight two common\nparametric functions and a common non-parametric function.\n\nfLl\n\nParametric function such as a linear layer\n\nxi = fLi(xi\u22121) = \u03d5Lixi\u22121 where \u03d5Li \u2208 M(|xi|, |xi\u22121|)\n\nor convolution\nxi = fLi(xi\u22121) = convfeat=\u03d5Li\nNon-parametric such as activation functions Li = tanh, ReLU . . .\n\n(xi\u22121) where \u03d5Li \u2208 M(in channel, out channel, feat height, feat width)\n\nxi = fLi (xi\u22121) where |xi| = |xi\u22121|\n\nWhat is conventionally called layer is actually a composition of two functions: a linear function and\nan activation function. For sake of clarity in our derivation, we do not adopt this convention and we\nuse the word \u201clayer\u201d to indicate the singular \u201cfunction\u201d component of the NN.\n\nLet us now consider a NN with w parametric layers and l \u2212 w activation layers\n\nxl = f\u03d5(x0) = fLl \u25e6 \u00b7 \u00b7 \u00b7 \u25e6 fL1(x0)\n\nwhere \u03d5 = (\u03d51, . . . , \u03d5w),\n\nand define the bijection\n\nW :\n\n{1, . . . , w} \u2212\u2192 {i| s.t. Li is parametric layer} \u2286 {1, . . . , l}\n\np\n\n\u2212\u2192 i s.t. Li has parameters \u03d5p\n\nfrom the subset of parametric layers to the corresponding index in \u03d5 = (\u03d51, . . . , \u03d5w).\nThe Jacobian w.r.t. the parameters J\u03d5f\u03d5(x0) \u2208 M(|xl|, |\u03d51| + \u00b7 \u00b7 \u00b7 + |\u03d5w|) is a matrix with number-\nof-output |xl| rows and number-of-parameters |\u03d5| columns. For reference, just storing this matrix can\nexceed memory limits even with the smallest autoencoder working on MNIST.\n\nThere are two ways of looking at this matrix: (1) row by row, that is output by output or (2)\nblock-of-columns by block-of-columns, that is layer by layer.\n\n11\n\n\fE.2 Jacobian per output\n\nEach row of the Jacobian corresponds to the gradient w.r.t. the parameters of an element of the output\n\nJ\u03d5f\u03d5(x0) =\n\n\uf8eb\n\n\uf8ec\n\uf8ed\n\n\u2207\u03d5[f\u03d5(x0)]1\n...\n\u2207\u03d5[f\u03d5(x0)]|xl|\n\n\uf8f6\n\n\uf8f7\n\uf8f8\n\nThis can be computed by defining |xl| loss functions\n\nlossk(xl) := [xl]k\n\nfor k = 1, . . . , |xl|\n\nand backpropagating each of those to obtain one line at a time. The disadvantage of this formulation\nis that we cannot reuse computation for one loss function to improve the computation of other loss\nfunctions (they are independent). Moreover, we need to store all these rows at the same time in order\nto compute J \u22a4J, which is computationally impractical.\n\nE.3 Jacobian per layer\n\nEach column of the Jacobian is the derivative of the output vector w.r.t. a single parameter. We can\nthen group the parameters (i.e. columns) layer by layer\n\n\n\n\n\nJ\u03d5f\u03d5(x0) =\n\nJ\u03d51f\u03d5(x0)\n\n. . . J\u03d5w f\u03d5(x0)\n\nwhere J\u03d5p f\u03d5(x0) \u2208 M(|xl|, |\u03d5p|). Let us focus on the computation of J\u03d5p f\u03d5(x0) for a fixed layer\np = 1, . . . , w. First notice that the parameters \u03d5p in f\u03d5 = fLl \u25e6 . . . \u25e6 fL1 only appear in fLW(p) and\nso\n\n\u2202fLi\n\u2202\u03d5p\n\n(xi\u22121) = 0 if i \u0338= W(p).\n\nChain rule (informal)\n\n\u2202f\u03d5(x0)\n\u2202\u03d5p\n\n=\n\n=\n\n=\n\n=\n\n\u2202xl\n\u2202\u03d5p\n\u2202xl\n\u2202xl\u22121\n\u2202xl\n\u2202xl\u22121\n  \nlayer l\nw.r.t. input\n\n=\n\n\u2202xl\u22121\n\u2202\u03d5p\n\u2202xl\u22121\n\u2202xl\u22122\n  \nlayer l \u2212 1\nw.r.t. input\n\n. . .\n\n\u2202xW(p)+1\n\u2202xW(p)\n\n\n\nlayer W(p) + 1\nw.r.t. input\n\n\u2202xW(p)\n\u2202\u03d5p\n  \nlayer W(p)\nw.r.t. parameters\n\nChain rule (formal)\n\nJ\u03d5p f\u03d5(x0) =\n\n\uf8eb\n\n\uf8ed\n\nk=l\n\n\nW(p)+1\n\n\uf8f6\n\nJxk\u22121fLk (xk\u22121)\n\n\uf8f8 J\u03d5p fLW(p)(xW(p)\u22121)\n\n(54)\n\nThe intuition for the chain rule is that the Jacobian J\u03d5p f\u03d5(x0) is the composition of the Jacobians\nw.r.t. input of subsequent layers times the Jacobian w.r.t. parameters of the specific layer. Thus, we\ncan reuse computation for one layer to improve the computation of other layers, specifically the\nproduct of Jacobians w.r.t. input. Moreover, we can compute J \u22a4\np Jp layer by layer without ever storing\nthe full Jacobian.\n\nE.3.1\n\nJacobian of a layer w.r.t. to input\n\nThe Jacobian of a standard linear layer w.r.t. to the input is\n\nJxp\u22121f\u03d5p (xp\u22121) = \u03d5p\n\n12\n\n\fand this remains the same also in the case with a bias. The Jacobian of a convolutional layer w.r.t. to\nthe input is\n\nJxp\u22121 convfeat=\u03d5p (xp\u22121) = M(convfeat=\u03d5p )\nThe Jacobian of the activation function depends on the specific choice. Recall that, for each layer i,\nxi \u2208 R|xi| is a vector\n\nxi = [xi]k\n\n\n\nk=1,...,|xi|\n\nwhere [xi]k \u2208 R is the value in position k of the vector xi.\nIf Li is tanh\n\n[xi]k = [fLi(xi\u22121)]k = tanh([xi\u22121]k) =\n\ne[xi\u22121]k \u2212 e\u2212[xi\u22121]k\ne[xi\u22121]k + e\u2212[xi\u22121]k\n\nfor k = 1, . . . , |xi\u22121|\n\nthen\n\n[Jxi\u22121 fLi(xi\u22121)]kj = \u03b4kj\n\n1 \u2212 (tanh([xi\u22121]k))2 = \u03b4kj\n\n1 \u2212 [xi]2\n\nk\n\n\n\nfor k, j = 1, . . . , |xi\u22121|.\n\nIf Li is ReLU\n\n[xi]k = [fLi (xi\u22121)]k = ReLU([xi\u22121]k) = max(0, [xi\u22121]k)\n\nfor k = 1, . . . , |xi\u22121|\n\nthen\n\n[Jxi\u22121fLi(xi\u22121)]kj = \u03b4kj (1 if [xi\u22121]k > 0 else 0)\n\nfor k, j = 1, . . . , |xi\u22121|.\n\nE.3.2\n\nJacobian of a layer w.r.t. to parameters\n\nThe Jacobian of a standard linear layer w.r.t. to the parameters is\n\nJ\u03d5if\u03d5i(xi\u22121) = I|xi| \u2297 xi\u22121 \u2208 M(|xi|, |\u03d5i|)\n\nand in the case with bias bi \u2208 Rxi the Jacobian is\n\nJ\u03d5i,bif\u03d5i,bi (xi\u22121) = I|xi| \u2297 [xi\u22121, 1] \u2208 M(|xi|, |\u03d5i| + |bi|)\n\nThe Jacobian of a convolutional layer w.r.t. to the parameters is\n\nJ\u03d5iconvfeat=\u03d5i(xi\u22121) = J\u03d5iconvT\nand in the case with bias bi \u2208 Roi the Jacobian is\n\nfeat=rev(xi\u22121)(\u03d5i) = M(convfeat=rev(xi\u22121))T\n\n\u2208 M(|xi|, |\u03d5i|)\n\nJ\u03d5iconvfeat=\u03d5i,bias=bi(xi\u22121) = M(convfeat=rev(xi\u22121))T \n\nI \u2208 M(|xi|, |\u03d5i| + |bi|)\n\nE.4 Hessian of a Neural Network\n\nConsider a function L : R|xl| \u2192 R from the output of the NN to scalar value. This later will be\ninterpreted as loss or likelihood, but for now, let us stick to the general case.\n\nWe are interested in the hessian of this scalar value w.r.t. the parameters of the NN\n\n\n\n\u22072\n\u03d5\n\n\n\nL(f\u03d5(x0))\n\n\u2208 M\n\n w\n\n\np=1\n\n|\u03d5i|,\n\n\n\n|\u03d5i|\n\n.\n\nw\n\n\np=1\n\nSimilarly to the previous section, it is convenient to see this matrix as block matrices, separated\nlayer-wise\n\n\u22072\n\n\u03d5L(f\u03d5(x0)) =\n\n\uf8eb\n\n\uf8ec\n\uf8ec\n\uf8ec\n\uf8ec\n\uf8ec\n\uf8ec\n\uf8ec\n\uf8ec\n\uf8ed\n\n\u22072\n\u03d51\n\u22022\n\u2202\u03d52\u2202\u03d51\n\nL(f\u03d5(x0))\n\nL(f\u03d5(x0))\n\n...\n\nL(f\u03d5(x0))\n\n. . .\n\n\u22022\n\u2202\u03d51\u2202\u03d5w\n\nL(f\u03d5(x0))\n\n\u22022\n\u2202\u03d51\u2202\u03d52\n\u22072\n\u03d52\n\nL(f\u03d5(x0))\n\n. . .\n\n\u22022\n\u2202\u03d5w\u2202\u03d51\n\nL(f\u03d5(x0))\n\n\u22072\n\n\u03d5w\n\nL(f\u03d5(x0)\n\n13\n\n\uf8f6\n\n\uf8f7\n\uf8f7\n\uf8f7\n\uf8f7\n\uf8f7\n\uf8f7\n\uf8f7\n\uf8f7\n\uf8f8\n\n\fThe first common assumption is to consider layers to be independent of each other, i.e.\n\n\u22022\n\u2202\u03d5i\u2202\u03d5j\n\nL(f\u03d5(x0)) = 0\n\n\u2200i \u0338= j\n\nLet us now fix a layer p = 1, . . . , w and focus on a single diagonal block.\n\n\u22072\n\u03d5p\n\nL(f\u03d5(x0)) \u2208 M(|\u03d5p|, |\u03d5p|).\n\nAccording to the chain rule\n\n\u22072\n\u03d5p\n\nL(f\u03d5(x0)) = J\u03d5p f\u03d5(x0)T \u00b7 \u22072\nL(xl) \u00b7 J\u03d5p f\u03d5(x0)\nxl\n\n\n\n=:G(\u03d5)\n\n+\n\n|xl|\n\n\ni=1\n\n[\u2207xl L(xl)]i \u00b7 \u22072\n\u03d5p\n\n[f\u03d5(x0)]i\n\n(55)\n\nThe second term of the RHS is equal to 0 if the model perfectly fits the dataset, \u2207xl L(xl) = 0, OR if\nf is linear in the parameters, H\u03d5p [f\u03d5(x0)]i = 0.\n\nThe first term of the RHS of Eq. 55, G(\u03d5), is in literature referred to as Generalized Gauss-Newton\n(GGN) matrix. It can be computed efficiently thanks to the view of the Jacobian as layer by layer.\nUsing equation (54), the expression for the approximated hessian w.r.t. to \u03d5p is then\n\nG(\u03d5) = J\u03d5p f\u03d5(x0)T \u00b7 Hxl L(xl) \u00b7 J\u03d5p f\u03d5(x0) =\n\n= J\u03d5p f T\n\nLW(p)\n\n\uf8eb\n\n\uf8ed\n\nl\n\n\n\uf8f6\n\n\uf8eb\n\nJxk f T\nLk\n\n\uf8f8 Hxl L(xl)\n\n\uf8ed\n\nk=l\n\n\n\uf8f6\n\nJxk fLk\n\n\uf8f8 J\u03d5p fLW(p)\n\nk=W(p)+1\nand from this, we can build an efficient backpropagation algorithm.\n\nW(p)+1\n\nAlgorithm 1 Algorithm for J\u03d5f T \u00b7 \u22072L \u00b7 J\u03d5f\n\nM = \u22072\nL(xl)\nxl\nfor k = l, l \u2212 1, . . . , 1 do\n\nif Lk is parametric with \u03d5p (i.e. k = W(p)): then\n\u00b7 M \u00b7 J\u03d5p fLk\n\nHp = J\u03d5p f \u22a4\nLk\n\nend if\nM = Jxk f \u22a4\nLk\n\n\u00b7 M \u00b7 Jxk fLk\n\nend for\nreturn (H1, . . . , Hw)\n\nAs it is written, each Hp is a matrix |\u03d5p|\u00d7|\u03d5p| so we technically obtain the hessian in a block-diagonal\nform\n\n\uf8eb\n\nH1\n\n\uf8f6\n\n0\n\n\uf8ec\n\uf8ed\n\n. . .\n\n0\n\nHw\n\n\uf8f7\n\uf8f8\n\nif we are interested in the diagonal only, we can construct that by concatenation of the diagonals for\neach Hp.\n\nE.4.1 Fast approximated version of the Algorithm\n\nThe idea is to backpropagate only the diagonal of the matrix M , neglecting all the non-diagonal\nelements. In a single backpropagation step, we have\n\nM \u2032 = Jxp f \u22a4\nLp\n\n\u00b7 M \u00b7 Jxp fLp\n\nIn order to backpropagate the diagonal only, we need to use the operator\n\ndiag(M ) \u2192 diag(M \u2032)\nFor linear layers and activation functions, this operator is trivial. For the convolutional layer it turns\nout that this operator is itself a convolution\n\nwhere the kernel tensor \u03d5(2)\np\n\nis the pointwise square of the kernel tensor \u03d5p.\n\ndiag(M \u2032) = convfeat=\u03d5(2)\n\np\n\n(diag(M ))\n\n14\n\n\fFigure 1: Snapshots of optimization of a mixture of Gaussians with fixed means and component\nweights (i.e. only variances are learned). We observe that variances of components far away from\ndata increase in order to push more probability mass to the region where data resides. The Gaussian\nVAE should exhibit the same behavior in order to maximize data likelihood, but in practice, it does\nnot.\n\nE.5 Hessian of a Reconstruction Loss\n\nIn the previous Algorithm, the backpropagated quantity is initialized as the hessian of the loss w.r.t.\nthe output of the NN \u22072\nf L(f ). The value\nxl\nof this hessian clearly depends on the specific choice of loss function L.\n\nL(xl), or, in an equivalent but more compact notation \u22072\n\nThe most common choice of the likelihood for regression is the Gaussian distribution, while for\nclassification it is the Bernoulli distribution. The Gaussian log-likelihood is\n\nL(f ) := log p(x|\u00b5 = f\u03b8(x), \u03c32 = \u03c32\n\nd) = \u2212\n\n\u2225x \u2212 f\u03b8(x)\u22252 \u2212 log(\n\n\u221a\n\n2\u03c0\u03c3d)\n\n(56)\n\n1\n2\u03c32\nd\n\nand its hessian is identity scaled with \u03c3d\n\n\u22072\n\nf log p(x|\u00b5 = f\u03b8(x), \u03c32 = \u03c32\n\nd) = \u2212(\u03c3d)\u22122I\n\nThe Bernoulli log-likelihood is\n\nL(f ) := log p(c|f\u03b8(x)) = log[softmax(f\u03b8(x))]c = [f\u03b8(x)]c \u2212 log\n\n(57)\n\n\n\ne[f\u03b8(x)]i\n\n(58)\n\n\n\n\n\ni\n\nand its hessian can be written in terms of the vector \u03c0 = softmax(f\u03b8(x)) of predicted probabilities\n\n\n\n\n\u22072\n\nf log p(c|f\u03b8(x)) = \u2212\u22072\n\nf log\n\ne[f\u03b8(x)]i\n\n= diag(\u03c0) \u2212 \u03c0\u03c0T\n\n(59)\n\n\n\ni\n\nWe highlight that both Hessians are independent on the label, and thus the GGN is equal to the Fisher\nmatrix, this is true every time p(y|f\u03b8(x)) is an exponential family distribution with natural parameters\nf\u03b8(x). In this paper, we focus on the Gaussian log-likelihood, but we emphasize that the method is\nnot limited to this distribution.\n\nF Intuition on optimization of variance in VAEs\n\nOne can imagine the Gaussian VAE as an infinite mixture of Gaussians (see e.g. Mattei and Frellsen\n(2018) for an extensive discussion of this link) where the weights are fixed by the prior on the latent\nspace. To increase the probability of the training data p(x), optimizing the neural network will push\nthe probability mass from regions far away from training data to regions with training data. Thus, the\nnetwork should learn to have large variance far away from training data, and low variance close to\ntraining data.\n\nWe illustrate this idea with a toy example (see snapshots in Fig. 1 or animation at https:\n//frederikwarburg.github.io/gaussian_vae.html). In this example, we show a mixture\nof Gaussian with components, where we optimize the variance (and fix the mean and the weights of\nthe mixture components). We see that the variance of the components far away from the training data\nincreases, whereas the variance of the component close to the data decreases. The opening example\nof the paper demonstrates that the Gaussian VAE does not exhibit this behavior even if this is optimal\nin terms of data likelihood.\n\n15\n\n\f", "appendix A": "\n\nAll experiments was conducted on one of the following datasets: MNIST (Lecun et al., 1998),\nFASHIONMNIST (Xiao et al., 2017) and CELEBA (Liu et al., 2015). For training and testing the default\nsplits were used. For validation, we sampled 5000 data points randomly from the training sets. All\nimages were normalized to be in the [0, 1] range and for the CELEBA dataset the images were resized\nto 64 \u00d7 64 additionally.\n\nIf nothing else is stated, the models were trained with the following configuration: we used Adam\noptimizer (Kingma and Ba, 2015b) with a learning rate of 0.001 and default Pytorch settings (Paszke\net al., 2019). The learning rate was adjusted using ReduceLROnPlateau learning rate scheduler with\nparameters factor=0.5 and patience=5, meaning that the learning rate was halved whenever 5 epochs\nhad passed where the validation loss had not decreased. The mean squared error loss was used as the\nreconstruction loss in all models. Models were trained until convergence, defined as whenever the\nvalidation loss had not decreased for 8 epochs. Models trained on MNIST and FASHIONMNIST used a\nbatch size of 64 and on CELEBA a batch size of 128 was used.\n\nModel-specific details:\n\n\u2022 VAE: Models were trained with KL-scaling of 0.001. We use two encoders and two decoders,\nsuch that the model has twice the number of parameters compared to the other models.\n\u2022 MC-AE: Models were trained with dropout between all trainable layers with probability\n\np = 0.2. We keep the same dropout rate during testing.\n\n\u2022 EMSEMBLE-AE: Each ensemble consists of 5 models, each initialized with a different\n\nrandom seed.\n\n\u2022 LAE (POSTHOC): For experiments with linear layers we used the Laplace Redux (Daxberger\net al., 2021) implementation. For convolutions, we found it necessary to use our proposed\nhessian approximation. We use a diagonal approximation of the hessian in all experi-\nments. After fitting the hessian, we optimize for the prior precision using the marginal\nlikelihood (Daxberger et al., 2021). We use 100 MC sampling in all experiments.\n\n1 Denotes equal contribution; author order determined by a simulated coin toss.\n\n36th Conference on Neural Information Processing Systems (NeurIPS 2022).\n\n\f\u2022 LAE (ONLINE): We use the exact diagonal in experiments with linear layers and the mixed\ndiagonal approximation in all experiments with convolutional layers. We use a hessian\nmemory factor of 0.0001 and sample only 1 network per iteration. We found that it was not\nnecessary to optimize for the prior precision when trained online.\n\nA.1 Hessian approximation\n\nWe use a linear encoder-decoder with three layers in the encoder and decoder, TANH activation\nfunctions, and latent size 2. We choose this architecture as Laplace Redux (Daxberger et al., 2021)\nsupports various hessian approximations for this simple network. We use Laplace Redux for all\npost-hoc experiments except for the approximate diagonal hessian.\n\nA.2 Out-of-distribution\n\nWe use a convolutional encoder-decoder architecture. The encoder consisted of a CONV2D, TANH,\nMAXPOOL2D, CONV2D, TANH, MAXPOOL2D, LINEAR, TANH, LINEAR, TANH, LINEAR, where the\ndecoder was mirrored but with nearest neighbour Upsampling rather than MAXPOOL2D. We used a\nlatent size of 2 in these experiments for all models.\n\nA.3 Missing data imputation\n\nTo elaborate on the procedure, we reconstruct 5 samples from the half/fully masked image. For each of\nthese reconstructions, we make 5 more reconstructions and take the average of these reconstructions.\nThe intuition is that the first stage explores the multi-modal behavior of the reconstructions. In the\nsecond stage, the uncertainty of the reconstructed digit is reduced, and each sample will reconstruct\nthe same modality. By averaging over these modalities, we achieve a more crisp reconstruction. We\nuse the same architecture as in the hessian approximation experiment.\n\nA.4 Semi-supervised learning\n\nFor the experiments on MNIST, we use a single convergence checkpoint for each model. We use\nthe same model architecture as in the hessian approximation. We did 5 repetitions for each model\nwhere we first sampled n labels from each of the 10 classes from the validation set, then embedded\nthe 10 \u00d7 n data points into latent space, trained a KNN classifier on the embedded points and finally\nevaluated the accuracy of the classifier on the remaining validation set. This procedure was repeated\nfor different values of n in the [1, 100] range. For the stochastic encoders (VAE, MC-AE, LAE), we\nrepeated the embedding step 100 times with the goal that the uncertainty could help the downstream\nclassification. For the KNN classifier we use cross-validation (K = 2) to find the optimal number of\nnearest neighbors.\n\nFor the experiments on CELEBA we repeated the exact same experiments but with a fixed value\nif n = 10. Additionally, the classifier was changed to a multi-label version KNN-classifier to\naccommodate the multiple binary features in the dataset. For CELEBA we use a convolutional\narchitecture. The encoder consists of 5 convolutional layers with TANH and MAXPOOL2D in between\neach parametric layer. We use a latent size of 64. The decoder mirrors the encoder, but we replace\nMAXPOOL2D with nearest neighbor upsampling.\n\n", "appendix D": "\n\nD.1 Overview\n\nLet X = RD be the data space, let Y = RD be the reconstruction space and let \u0398 = RP be\nthe parameter space. Let F = (\u0398 \u2192 (X \u2192 Y)) be the space of operators from \u0398 to the space of\noperators from X to Y. We will denote a function f \u2208 F applied to a parameter \u03b8 \u2208 \u0398 as f\u03b8 : X (cid:55)\u2192 Y.\nThis will represent, for example, a NN f with a specific set of parameter (weights) \u03b8 \u2208 \u0398, that maps\nsome data x \u2208 X to some reconstruction f\u03b8(x) = y \u2208 Y.\nLet X \u00d7 Y \u00d7 \u0398 \u00d7 F be a probability space. The only assumption we make on this space is\n\np(y|x, \u03b8, f ) \u223c N (y|\u00b5 = f\u03b8(x), \u03c32 = \u03a3)\n\n(11)\nwhere \u03a3 \u2208 M(RD \u00d7 RD) is a fixed variance matrix. This is a common assumption for regression\ntasks, and is sometimes referred to as the \u201cdata noise\u201d or \u201creconstruction error\u201d. In this paper, we fix\n\u03a3 = I, but the derivations hold for the general case. With only this assumption, the distribution is\nundefined and multiple solutions can exist. Thus, we require more assumptions.\nA dataset D = {xn} is a finite of infinite collection of data xn \u2208 X that is assumed to follow a\ncertain, fixed but unknown, distribution\n\n\u2200(x, \u03b8, f ) \u2208 X \u00d7 \u0398 \u00d7 F\n\nxn \u223c p(x).\n\n(12)\n\nSacrificing slim notation for the sake of clarity, we introduce an operator I : X (cid:55)\u2192 Y. This represents\nthe ideal reconstruction for a given x. In the standard supervised setting, this would be the operator\n(defined on the dataset only) that maps each data input to its label. In our unsupervised setting, where\nX and Y are the same space, the operator I is simply the identity (Indeed they are not the same space,\nthey are isomorphic spaces that we identify through the operator I). Since I is the identity, it is often\n\n6\n\n\fneglected in the literature, which can lead to unclear and potentially ambiguous Bayesian derivations.\nThus, we choose to adopt this heavier, but more precise notation.\nWe assume access to a specific f N N \u2208 F. Practically this will be our NN architecture, i.e. an operator\nthat, given a set of parameters \u03b8 \u2208 \u0398 gives rise to a function from X to Y. Having f N N fixed, one\nmay consider f not to be stochastic anymore, we choose to still explicitly condition on f in order to\nhave a clearer notation in later stages. Note that, despite not being covered in this work, a proper\nstochastic derivation also on the NN architecture should be feasible.\n\nD.2 Objective\n\nThe NN\u2019s parameter optimization process in this full Bayesian probabilistic framework can be viewed\nas: given a fixed f N N \u2208 F, namely the NN architecture, maximise the reconstruction probability of\nI(xn) over the dataset D\n(cid:104)\n\n(cid:105)\n\nExn\u223cp(x)\n\np(y|xn, f N N )(cid:12)\n\n(cid:12)y=I(xn)\n\n=\n\n(cid:88)\n\np(y|xn, f N N )(cid:12)\n\n(cid:12)y=I(xn),\n\n(13)\n\nxn\u2208D\n\nwhere the untractable p(y|xn, f N N ) can be expanded in \u03b8 and thus related to our hypothesis (11) as\n\np(y|xn, f N N ) = E\u03b8\u223cp(\u03b8|xn,f N N )\n\n(cid:2)p(y|xn, \u03b8, f N N )(cid:3) .\n\n(14)\n\nNotice that the only unfixed quantity is the distribution on the parameters, which we will optimize\nfor. We are not interested in finding a datapoint-dependant distribution, but rather one that maximise\nall reconstructions at the same time, i.e. p(\u03b8|f N N ) = p(\u03b8|xn, f N N ). We can then frame Bayesian\noptimization as: find a distribution on parameters such that\n\nq(\u03b8) \u2208 arg max\n\np(\u03b8|f N N )\n\n= arg max\n\np(\u03b8|f N N )\n\n(cid:88)\n\nxn\u2208D\n(cid:88)\n\nxn\u2208D\n\nE\u03b8\u223cp(\u03b8|f N N )\n\n(cid:104)\n\np(y|xn, \u03b8, f N N )(cid:12)\n\n(cid:12)y=I(xn)\n\n(cid:105)\n\nE\u03b8\u223cp(\u03b8|f N N )\n\n(cid:2)p(I(xn)(cid:12)\n\n(cid:12)N (f N N\n\u03b8\n\n(xn), \u03a3))(cid:3) .\n\n(15)\n\n(16)\n\nMoreover, finding this optimum in the space \u2206(\u0398) of all distributions on \u0398 is not tractable. So, as\ncommonly done, we restrict ourselves to the subset G(\u0398) \u2282 \u2206(\u0398) of Gaussians over \u0398. Then, a\nsolution in our context is\n\nq(\u03b8) \u2208 arg max\nq\u2208G(\u0398)\n\n(cid:88)\n\nxn\u2208D\n\nE\u03b8\u223cq(\u03b8)\n\n(cid:2)p(I(xn)(cid:12)\n\n(cid:12)N (f N N\n\u03b8\n\n(xn), \u03a3))(cid:3) .\n\n(17)\n\nWe emphasize that this solution has no guarantees of being unique, but we are interested in finding\none of them.\n\nD.3\n\nJoint distribution for a fixed datapoint\n\nLet us first get a better understanding of the joint distribution on Y\u00d7\u0398 conditional to a fixed datapoint\nx \u2208 X and a network architecture f \u2208 F\n\nThis distribution has two marginals\n\nand two conditionals\n\np(y, \u03b8|x, f ).\n\np(y|x, f )\n\np(\u03b8|x, f )\n\np(y|\u03b8, x, f )\n\np(\u03b8|y, x, f ).\n\nThese four quantities must satisfy the system of two \u201crecursive\u201d equations\n\np(y|x, f ) =\n\n(cid:90)\n\n\u0398\n\np(y|\u03b8, x, f )p(\u03b8|x, f )d\u03b8 =\n\n= E\u03b8\u223cp(\u03b8|x,f )[p(y|\u03b8, x, f )]\n\n7\n\n(18)\n\n(19)\n\n(20)\n\n(21)\n\n(22)\n\n(23)\n\n\fp(\u03b8|x, f ) =\n\n(cid:90)\n\nY\n\np(\u03b8|y, x, f )p(y|x, f )dy =\n\n= Ey\u223cp(y|x,f )[p(\u03b8|y, x, f )].\n\nIf these are satisfied then the joint is a well-defined distribution and we can apply Bayes rule\n\np(y|\u03b8, x, f )p(\u03b8|x, f ) = p(y, \u03b8|x, f ) = p(\u03b8|y, x, f )p(y|x, f )\n\nwhich in logarithmic form is\n\n(24)\n\n(25)\n\nlog p(y|\u03b8, x, f ) + log p(\u03b8|x, f ) = log p(\u03b8|y, x, f ) + log p(y|x, f ).\n(26)\nWe can factor in the assumptions. The \u201cdata noise\u201d assumption gives us one of the two conditionals:\np(y|\u03b8, x, f ) \u223c N (y|\u00b5 = f\u03b8(x), \u03c32 = \u03a3).\n(27)\n\nThe \u201cGaussian parameter\u201d assumption gives us one of the two marginals:\np(\u03b8|x, f ) = qt(\u03b8) \u223c N (\u03b8|\u00b5 = \u03b8t, \u03c32 = H\u22121\nt ).\nWith these in place, the joint distribution is uniquely defined. The other marginal, by Eq. 23, is\n\np(y|x, f ) = E\u03b8\u223cqt(\u03b8)[p(y|N (f\u03b8(x), \u03a3))]\n\nand the other conditional, by Bayes rule, is\n\np(\u03b8|y, x, f ) =\n\np(y|\u03b8, x, f )p(\u03b8|x, f )\np(y|x, f )\n\n.\n\n(28)\n\n(29)\n\n(30)\n\nDespite being uniquely defined, the integral in Eq. 29 is, with a general f , intractable, and so is the\njoint distribution.\n\nBut why do we even care? The intractability of p(y|x, f ) in Eq. 29 may at first glance appear\nirrelevant. This is the case, for instance, with bayes by backprop (Blundell et al., 2015) methods.\nThey simply need access to the gradient of this quantity. For this purpose, a simple Monte Carlo\nestimate of the expectation is enough.\n\nOn the other hand, we are interested in recovering a meaningful distribution on parameters. This\nimply that we aim at using Eq. 29 to enforce that Eq. 24 holds. For this purpose we need access to\nthe the density p(y|x, f ), so a Monte Carlo estimate of Eq. 29 is not enough.\n\nD.4 Linear f\n\nTheorem 1. Given the data noise assumption from Eq. 27\n\np(y|\u03b8, x, f ) \u223c N (y|\u00b5 = f\u03b8(x), \u03c32 = \u03a3),\ngiven the Gaussian parameter assumption from Eq. 28 for some \u03b8t, Ht\np(\u03b8|x, f ) = qt(\u03b8) \u223c N (\u03b8|\u00b5 = \u03b8t, \u03c32 = H\u22121\n\nt ),\n\nassume that f is linear in \u03b8, i.e.\n\nf\u03b8(x) = f0(x) + Jf\u00b7(x)\u03b8\n\n\u2200\u03b8 \u2208 \u0398, \u2200x \u2208 X.\n\nThen the joint distribution is Gaussian itself\n\np(\u03b8, y|x, f ) \u223c N ((\u03b8, y)|\u00b5t, \u03a3t)\n\nwhere\n\n\u00b5t =\n\n(cid:19)\n\n(cid:18) \u03b8t\nf\u03b8t(x)\n\n, and \u03a3t =\n\n(cid:18) H\u22121\nJf\u00b7(x)\u03a3 (cid:0)Jf\u00b7(x)\u22a4HtJf\u00b7(x)(cid:1)\u22121\n\n\u03a3Jf\u00b7(x)\u22a4\n\nt\n\n(31)\n\n(32)\n\n(33)\n\n(34)\n\n(cid:19)\n\n.\n\n+ \u03a3\n\nProof. With the further assumption of linearity of f , we can explicitly carry out the integral in the\nexpectation in Eq. 29\n\np(y|x, f ) = E\u03b8\u223cqt(\u03b8)[p(y|N (f\u03b8(x), \u03a3))] =\n\n=\n\n=\n\n=\n\n(cid:90)\n\n\u0398\n\n(cid:90)\n\n\u0398\n\n(cid:90)\n\n\u0398\n\np(y|N (f\u03b8(x), \u03a3))qt(\u03b8)d\u03b8 =\n\np(y|N (f\u03b8(x), \u03a3))p(\u03b8|N (\u03b8t, H\u22121\n\nt ))d\u03b8 =\n\np(y|N (f0(x) + Jf\u00b7(x)\u03b8, \u03a3))p(\u03b8|N (\u03b8t, H\u22121\n\nt ))d\u03b8 =\n\nWe emphasize that as a consequence \u22072\n\n\u03b8 log p(y|x, f ) is not dependent on \u03b8t.\n\n= p(y|N (f\u03b8t(x), (Jf\u00b7(x)\u22a4HtJf\u00b7(x))\u22121 + \u03a3)).\n\n8\n\n\fHaving Theorem 1 in place, we can go back to our original problem. We need to deal with a fixed\nnon-linear architecture f N N . We can exploit Theorem 1 by defining f t: a linearization of f N N with\na first-order Taylor expansion around \u03b8t\n\nand it holds that\n\n\u03b8(x) := TAYLOR(f N N , \u03b8t)(x) =\nf t\n\n= f N N\n\u03b8t\n\n(x) + J\u03b8f N N\n\n\u03b8t\n\n(x)(\u03b8 \u2212 \u03b8t)\n\nf N N\n\u03b8\n\n(x) = f t\n\n\u03b8(x) + O(\u2225\u03b8 \u2212 \u03b8t\u22252).\n\nRecalling Eq. 11 both for f N N and for f t\n\np(y|x, \u03b8, f N N ) \u223c N (y|\u00b5 = f N N\n\n\u03b8\n\n(x), \u03c32 = \u03a3)\n\np(y|x, \u03b8, f t) \u223c N (y|\u00b5 = f t\n\n\u03b8(x), \u03c32 = \u03a3)\n\nthat, together with Eq. 36, imply\n\nN (cid:0)y|\u00b5 = f N N\n\n\u03b8\n\n(x), \u03c32 = \u03a3(cid:1) \u223c N (cid:0)y|\u00b5 = f t\n\n\u03b8(x) + O(\u2225\u03b8 \u2212 \u03b8t\u22252), \u03c32 = \u03a3(cid:1),\n\n(35)\n\n(36)\n\n(37)\n\n(38)\n\n(39)\n\nwhere we can interpret the unknown O(\u2225\u03b8 \u2212 \u03b8t\u22252) as \u03b8-dependent noise. More specifically, calling\n\u03b3 > 0 the scalar constant of the O-term, we assume that\n\nO(\u2225\u03b8 \u2212 \u03b8t\u22252) \u2248 \u03f5(\u03b8 \u2212 \u03b8t)2\n\nwhere \u03f5 \u223c N (0, \u03b3I)\n\nand thus, from Eq. 39, we have\n\nN (cid:0)y|\u00b5 = f N N\n\n\u03b8\n\n(x), \u03c32 = \u03a3(cid:1) \u223c N (y|\u00b5 = f t\n\n\u03b8(x), \u03c32 = \u03a3 + \u03b3\u2225\u03b8 \u2212 \u03b8t\u22252I).\n\n(40)\n\n(41)\n\nAt this point, integrals are not analytically tractable, and thus a proper proof is not feasible, the\nintuition is that this increased variance reflects in increased variance in p(\u03b8|x, f N N )\n\n\u22072\n\n\u03b8 log p(\u03b8|x, f N N )(cid:12)\n\n(cid:12)\u03b8=\u03b8t+1\n\n\u2248 \u22072\n\n\u03b8 log p(\u03b8|x, f t)(cid:12)\n\n(cid:12)\u03b8=\u03b8t+1\n\n+ \u03b3\u2225\u03b8t+1 \u2212 \u03b8t\u22252,\n\n(42)\n\nwhere we introduce a hyperparameter \u03b1 > 0 to cope with this added variance. If we then assume the\nJacobian J\u03b8f N N to be a Lipschitz function, then the Lipschitz constant is an upper bound on \u03b3, as\nfollows from the Taylor expansion of Eq. (35). If this Lipschitz constant is smaller than the inverse of\nthe gradient step 1/\u2225\u03b8t+1 \u2212 \u03b8t\u2225 (which is not an unreasonable assumption for the gradient ascent to\nbe stable) we have\n\n\u03b3\u2225\u03b8t+1 \u2212 \u03b8t\u22252 \u2248 \u2225\u03b8t+1 \u2212 \u03b8t\u2225\n\n(43)\n\nthat gives us a plausible order of magnitude for choosing the hyperparameter \u03b1.\nMotivation: During training, we produce a sequence of Gaussians qt(\u03b8) \u223c N (\u03b8t, H\u22121\nt ) that we\nassume to be the distribution p(\u03b8|x, f t), at every step t \u2265 0. This distribution qt is then used for (1)\nGaussian derivation in the linear case, for (2) Monte Carlo sampling in the update rule \u03b8t \u2192 \u03b8t+1\nand (3), as second order derivative, for update rule Ht \u2192 Ht+1.\nMoreover, given that this distribution qt is our \"best guess so far\", we assume it to be also the\ndistribution p(\u03b8|x, f N N ). This, being f N N not linear, (1) cannot be used for Gaussian derivation,\n(2) can reasonably be used for sampling (and thus we derive the improved update rule Eq. (52)), and\n(3) can somehow be used as second order derivative (and thus we derive the improved update rule\nEq. (53)), but the latter requires some more care. That is why we introduce the parameter \u03b1.\n\nD.5\n\nIterative learning\n\nOur learning method produces a sequence of Gaussians\n\nqt(\u03b8) \u223c N (\u03b8|\u00b5 = \u03b8t, \u03c32 = H\u22121\nt )\n\nand a sequence of linearized functions\n\nf t = TAYLOR(f N N , \u03b8t)\n\nfor every t \u2265 0.\n\n9\n\n(44)\n\n(45)\n\n\fInitialization is trivially done using a Gaussian prior on the parameters.\n\n\u03b80 = \u03b8prior\n\nH0 = (\u03a3prior)\u22121\n\n(46)\n\nIterative step is made in two steps. First, having access to \u03b8t, we \u201cgenerate\u201d the linearization f t.\nPractically this is equivalent to computing the two quantities f N N\n(x), that, together\nwith the value \u03b8t are actually equivalent to \u201cgenerating\u201d f t, as Eq. 35 shows.\nSecond, we compute the Gaussian parameters \u03b8t+1 and Ht+1.\n\n(x) and J\u03b8f N N\n\n\u03b8t\n\n\u03b8t\n\nRecalling our aim of maximizing the quantity Eq. (17), update on q(\u00b7) means, \u03b8t \u2192 \u03b8t+1, is ideally\nmade through gradient ascent steps on p(y|x, f )|y=I(x). As this is intractable, we instead do gradient\nsteps on the lower bound Ly of (the log of) Eq. 23\n\n\u03b8t+1 = \u03b8t + \u03bb\u2207\u03b8Ly\n\n(cid:12)\n(cid:12)\n(cid:12)y=I(x)\n\nwhere\n\nand so\n\nLy = E\u03b8\u223cp(\u03b8|x,f t)[log p(y|\u03b8, x, f t)] \u2264 log E\u03b8\u223cp(\u03b8|x,f t)[p(y|\u03b8, x, f t)]\n\n\u2207\u03b8Ly\n\n(cid:12)\n(cid:12)y=I(x) = E\u03b8\u223cp(\u03b8|x,f t)\n\n(cid:104)\n\u2207\u03b8 log p(y|\u03b8, x, f t)(cid:12)\n\n(cid:105)\n\n=\n\n= E\u03b8\u223cqt(\u03b8)\n\n(cid:2)\u2207\u03b8 log p(I(x)(cid:12)\n\n(cid:12)N (f t\n\n(cid:12)y=I(x)\n\u03b8(x), \u03a3))(cid:3) .\n\n(47)\n\n(48)\n\n(49)\n\nRecalling the Laplace approximation Eq. (3), the negative precision, \u2212Ht+1, is ideally set to be the\nthe hessian of the log probability p(\u03b8|x, f ), evaluated in \u03b8t+1. As this is intractable we instead set it\nto the hessian of the lower bound L\u03b8 of (the log of) Eq. (24)\n\nHt+1 = \u2212\u22072\n\n\u03b8L\u03b8\n\n(cid:12)\n(cid:12)\n(cid:12)\u03b8=\u03b8t+1\n\nwhere\n\nand so\n\nL\u03b8 = Ey\u223cp(y|x,f t)[log p(\u03b8|y, x, f t)] \u2264 log Ey\u223cp(y|x,f t)[p(\u03b8|y, x, f t)]\n\n\u22072\n\n\u03b8L\u03b8\n\n(cid:12)\n(cid:12)\n(cid:12)\u03b8=\u03b8t+1\n\n= Ey\u223cp(y|x,f t)\n\n(cid:20)\n\u22072\n\n\u03b8 log p(\u03b8|y, x, f t)\n\n(cid:21)\n\n(cid:12)\n(cid:12)\n(cid:12)\u03b8=\u03b8t+1\n\n(50)\n\n(51)\n\nvia Eq. (26)\n\n= Ey\u223cp(y|x,f t)\n\n(cid:104)\n\n\u22072\n\n\u03b8 log p(y|\u03b8, x, f t)\n\n\u2212 \u22072\n\n(cid:12)\n\u03b8 log p(y|x, f t)\n(cid:12)\n(cid:12)\u03b8=\u03b8t+1\n\n(cid:12)\n(cid:12)\n(cid:12)\u03b8=\u03b8t+1\n(cid:105)\n\n+ \u22072\n\n(cid:12)\n\u03b8 log p(\u03b8|x, f t)\n(cid:12)\n(cid:12)\u03b8=\u03b8t+1\n\n+\n\nvia Theorem 1\n\n= Ey\u223cp(y|x,f t)\n\n(cid:20)\n\u22072\n\n\u03b8 log p(y|\u03b8, x, f t)\n\n(cid:12)\n(cid:12)\n(cid:12)\u03b8=\u03b8t+1\n\n+ \u22072\n\n(cid:12)\n\u03b8 log p(\u03b8|x, f t)\n(cid:12)\n(cid:12)\u03b8=\u03b8t+1\n\n(cid:21)\n\nvia the chain rule Eq. (55)\n\n= Ey\u223cp(y|x,f t)\n\n(cid:20)\nJ\u03b8f t\n\n\u00b7 (x)\u22a4\u22072\n\ny log p(y|\u03b8t+1, x, f t)J\u03b8f t\n\n\u00b7 (x) + \u22072\n\n\u03b8 log p(\u03b8|x, f t)\n\n(cid:21)\n\n(cid:12)\n(cid:12)\n(cid:12)\u03b8=\u03b8t+1\n\nvia HP Eq. (11)\n\n= Ey\u223cp(y|x,f t)\n\n(cid:20)\n\n\u2212J\u03b8f t\n\n\u00b7 (x)\u22a4\u03a3\u22121J\u03b8f t\n\n= \u2212J\u03b8f t\n\n\u00b7 (x)\u22a4\u03a3\u22121J\u03b8f t\n\n\u00b7 (x) + \u22072\n\n\u00b7 (x) + \u22072\n\n(cid:12)\n\u03b8 log p(\u03b8|x, f t)\n(cid:12)\n(cid:12)\u03b8=\u03b8t+1\n(cid:12)\n\u03b8 log p(\u03b8|x, f t)\n(cid:12)\n(cid:12)\u03b8=\u03b8t+1\n\n(cid:21)\n\n= \u2212J\u03b8f t\n\n\u00b7 (x)\u22a4\u03a3\u22121J\u03b8f t\n\n\u00b7 (x) + \u22072\n\n\u03b8 log qt(\u03b8)\n\n= \u2212J\u03b8f t\n\n\u00b7 (x)\u22a4\u03a3\u22121J\u03b8f t\n\n\u00b7 (x) \u2212 Ht.\n\n10\n\n(cid:12)\n(cid:12)\n(cid:12)\u03b8=\u03b8t+1\n\n\fD.5.1\n\nImproved update rule\n\nAs said, the update \u03b8t \u2192 \u03b8t+1 is ideally made through gradient ascent steps on p(y|x, f )|y=I(x),\nbut we instead use the tractable lower bound with f t. We can perform the same derivation using f N N\nin place of f t. Assuming p(\u03b8|x, f N N ) \u223c qt(\u03b8) for sampling, leads to the improved update rule\n\n\u03b8t+1 = \u03b8t + \u03bbE\u03b8\u223cqt(\u03b8)\n\n(cid:2)\u2207\u03b8 log p(I(x)(cid:12)\n\n(cid:12)N (f N N\n\u03b8\n\n(x), \u03a3))(cid:3) .\n\n(52)\n\nSimilarly, the negative precision \u2212Ht+1 is ideally set to be the hessian of the log probability p(\u03b8|x, f ).\nbut instead, we use the tractable lower bound with f t. Here we cannot perform the same derivation,\nsince Theorem 1 does not hold anymore. Instead, we can rely on the estimate Eq. (42) to improve the\nterm\n\u22072\n\n\u2248 \u2212(1 \u2212 \u03b1)Ht,\n\n\u03b8 log p(\u03b8|x, f t)\n\n(cid:12)\n\u03b8 log p(\u03b8|x, f N N )\n(cid:12)\n(cid:12)\u03b8=\u03b8t+1\n\n(cid:12)\n(cid:12)\n(cid:12)\u03b8=\u03b8t+1\n\n= \u2212Ht\n\n\u2212\u2192\n\n\u22072\n\nand this leads to the improved update rule\n\nHt+1 = (1 \u2212 \u03b1)Ht + J\u03b8f t\n\n\u00b7 (x)\u22a4\u03a3\u22121J\u03b8f t\n\n\u00b7 (x).\n\n(53)\n\n", "system_prompts": [{"role": "system", "content": "You are a computer science researcher currently reviewing a paper titled \"Laplacian Autoencoders for Learning Stochastic Representations\" for the NeurIPS computer science conference. Your goal is to try to be as objective and truthful as possible in your answers about the paper provided. Your reviews will be used for causal reasoning in determining the quality of the paper."}], "prompts": {"1b": {"role": "user", "content": "The following is the experiments section of the paper you are reviewing:\n\n\nFirst, we demonstrate the computational advantages of the proposed Hessian approximation, and\nthen that our sampling-based training leads to well-calibrated uncertainties that can be used for OOD\ndetection, data imputation, and semi-supervised learning. For all the downstream tasks we consider\nthe following baselines: AE (Hinton and Salakhutdinov, 2006) with constant and learned variance, VAE\n(Rezende et al., 2014; Kingma and Welling, 2014), Monte-Carlo dropout AE (Gal and Ghahramani,\n2016) and Ensembles of AE (Lakshminarayanan et al., 2017). We extend StochManDetlefsen et al.\n(2021) with the Hessian backpropagation for the approximate and mixed diagonals. The training code\nis implemented in PyTorch and available2 . Appendix A provides more details on the experimental\nsetup.\n\nEf\ufb01cient Hessian Approximation. For practical applications, training time and memory usage of the\nHessian approximation must be kept low. We here show that the proposed approximate diagonal Hes-\nsian is suf\ufb01cient and even outperforms other approximations when combined with our online training.\n\nFig. 7 show the time and memory requirement for different approximation methods as a function\nof input size for a 5-layer convolutional network that preserves channel and input dimension. As\nbaselines we use ef\ufb01cient implementations of the exact and KFAC approximation (Daxberger et al.,\n3\n2021; Dangel et al., 2020). The exact diagonal approximation run out of memory for an\nimage on a 11 Gb NVIDIA GeForce GTX 1080 Ti. In contrast, our approximate diagonal Hessian\nscales linearly with the resolution, which is especially bene\ufb01cial for convolutional layers.\n\n36\n\n36\n\n\u21e5\n\n\u21e0\n\n\u21e5\n\n2 https://github.com/FrederikWarburg/LaplaceAE\n\n7\n\n\f\u00b5\n\n\u00b5\n\nFigure 9: Receiver Operator Curves (ROC) show that our online LAE is able to discriminate between\n\u00b5\nin and out of distribution in both latent and output space.\nlog p(x)\n\nHessian\n\nMSE\n\n\u00b5\n\n\u00b5\n\n\n\n\u00002\n\n\n\n\u00002\n\n\n\n\u00002\n\n\n\n\u00002\n\n\n\n\u00002\n\nKFAC\nExact\nApprox\nExact\u21e4\nApprox\u21e4\n\n\u0000\n9683.9\n283.3\n232.0\n25.8\n25.9\n\n\u00b1\n\u00b1\n\n#\n\n2455.0\n\u00b1\n88.6\n\u00b1\n65.5\n\u00b1\n0.2\n0.4\n\n#\n\n121.6\n27.1\n26.6\n25.7\n25.8\n\n\u00b1\n\u00b1\n\u00b1\n\u00b1\n\u00b1\n\n24.5\n0.9\n0.6\n0.2\n0.4\n\nApprox\n\nExact\n(a) Post-hoc\n\n\n\nFigure 8: Mean and variance of 100 sampled NN.\n\nKFAC\n\nExact\u21e4\n\nApprox\u21e4\n\nTable 2: Online training (indicated by \u21e4) outper-\nforms post-hoc LA. The approximate diagonal\nhas similar performance to the exact diagonal\nfor both post-hoc and online LA.\nFigure 10: Histograms of variance for in- and OOD reconstructions in output space. Note that\nMC-AE separates the distributions well, but the model assign higher variance to the in- than OOD\ndata.\n\n(b) Online\n\n\n\nTab. 2 shows that the exact or approximate Hessian diagonal has similar performance for both\npost-hoc and online training. Using post-hoc LA results in good mean reconstructions (low MSE),\nbut each sampled NN does not give good reconstructions (low log p(x)). Using our online training\nprocedure results in a much higher log-likelihood. This indicates that every sampled NN predicts\ngood reconstructions.\n\nthe uncertainties more well \ufb01tted, both in latent and data space. These well-\ufb01tted uncertainties have\nseveral practical downstream applications, which we demonstrate next.\n\n218\n\n219\n\n224\n\n225\n\n222\n\n223\n\n221\n\n220\n\nFig. 8 shows the latent representation, mean, and variance of the reconstructions with the KFAC, exact\nand approximate diagonal for both post-hoc and online setup. Note that the online training makes the\nuncertainties better \ufb01tted, both in latent and data space. These well-\ufb01tted uncertainties have several\npractical downstream applications, which we demonstrate next.\n\nOut-of-Distribution (OOD) Detection capabilities are critical for identifying distributional shifts,\noutliers and irregular user inputs, which can hinder the propagation of erroneous decision in an\nautomated system. We evaluate OOD performance on the commonly used benchmarks (Nalisnick\net al., 2019b), where we use FASHIONMNIST (Xiao et al., 2017) as in-distribution and MNIST\n(Lecun et al., 1998) as OOD. Tab. 3 shows that our online LAE outperforms existing models in\nboth log-likelihood and Typicality score (Nalisnick et al., 2019a). This stems from the calibrated\nmodel uncertainties, which are exempli\ufb01ed in the models ability to detect OOD examples from the\nuncertainty deduced in latent and output space; see Fig. 9 for ROC curves.\n\nOut-of-Distribution (OOD) Detection capabilities are critical for identifying distributional shifts,\noutliers, and irregular user inputs, which can hinder the propagation of erroneous decisions in an\nautomated system. We evaluate OOD performance on the commonly used benchmarks (Nalisnick\net al., 2019b), where we use FASHIONMNIST (Xiao et al., 2017) as in-distribution and MNIST\n(Lecun et al., 1998) as OOD. Fig. 9 (c) shows that our online LAE outperforms existing models in\nboth log-likelihood and Typicality score (Nalisnick et al., 2019a). This stems from the calibrated\nmodel uncertainties, which are exempli\ufb01ed in the models ability to detect OOD examples from the\nuncertainty deduced in latent and output space; see Fig. 9 (a,b) for ROC curves.\n\nFig. 10 shows distribution of the output variances for in- and OOD data. This illustrates that using LA\nimproves OoD detection. Furthermore, the online training improves the model calibration.\n\nMissing Data imputation. Another application of stochastic representation learning is to provide\ndistributions over unobserved values (Rezende et al., 2014). In many application domains, sensor\nreadings go missing, which we may mimic by letting parts of an image be unobserved. Rezende et al.\n(2014) show that we can then draw samples from the distribution of the entire image conditioned on\n\nFig. 10 shows distribution of the output variances for in- and OOD data. This illustrates that using LA\nimproves OoD detection. Furthermore, the online training improves the model calibration.\n\n226\n\n227\n\n232\n\n228\n\n229\n\n230\n\n231\n\n233\n\nMethod\n\nlogp(x) Typicality\n\n\u00002\n\noutput\n\n\u00002\n\nlatent\n\nAE MSE\nAE MSE\nAE likelihood\nAE likelihood\nVAE\nVAE\nMC-AE\nMC-AE\nLAE (post-hoc)\nLAE (post-hoc)\nLAE (online)\nLAE (online)\n\n\n\n\n\n0.98\n0.66\n0.54\n0.97\n0.98\n0.98\n\n0.98\n0.70\n0.62\n0.97\n0.98\n0.98\n\n0.65\n0.46\n0.08\n0.74\n0.91\n\n0.5\n0.31\n0.96\n0.96\n\n\n\n\n\nlatent\n\noutput\n\n(b) \u00002\n\n(a) \u00002\n\nTable 3: Out of Distribution detection. In-distribution data FashionMnist and OOD data Mnist.\nArea Under Receiver Operator Curve (AUROC\n) for log-likelihood, typicality score, latent space\n\"\n\u0000latent and output space \u0000output uncertainties. Our proposed LAE ELBO is able to discriminate between\nin and OOD using the deduced variances in latent and output space.\n\nFigure 9: Out of Distribution detection. In-distribution data FashionMnist and OOD data Mnist. (a)\n) for\nand (b) shows the ROC curves for latent and output space uncertainties. (c) shows (AUROC\n\"\nlog-likelihood, typicality score, latent space \u0000latent and output space \u0000output uncertainties. Online LAE\nis able to discriminate between in and OOD using the deduced variances in latent and output space.\n\n) for multiple statistics.\n\"\n\n(c) AUROC (\n\n(a) Latent variance, \u00002\n\n(b) Output variance, \u00002\n\noutput\n\nlatent\n\n\n\n\n\n\n\n8\n\nID\nOoD\n\nID\nOoD\n\n\n\n\n\n\n\n\n\n\n\n(a) AE likelihood\n\n(b) MC-AE\n\n(c) VAE\n\n(d) LAE (post-hoc)\n\n(e) LAE (online)\n\nFigure 10: Histograms of variance for in- and OOD reconstructions in output space. Note that MC-\nAE separates the distributions well, but the model assigns higher variance to the in- than OOD data.\n\n\n\n\n\n\n\n\n\n\n\n(a) AE likelihood\n\n(b) MC-AE\n\n(d) LAE (post-hoc)\n\n(e) LAE (online)\n\n(c) VAE\n8\n\n\f\n\n\n\n\n\n(a) VAE\n\n(b) LAE (post-hoc)\n\n(c) LAE (online)\n\nFigure 11: Missing data imputation & generative capabilities. Online training of LAE improves\nrepresentational robustness. This is exempli\ufb01ed by the multimodal behavior in the data imputation\n(top rows) that accurately model the ambiguity in the data. The bottom two rows show that the LAE\nis able to generate crisp digits from random noise.\n\nMethod\n\nClassi\ufb01er\nVAE\nMC-AE\nAE Ensemble\nLAE (post-hoc)\nLAE (online)\n\nMSE\n\n#\n\nlog p(x)\n\n\"\n\n104.73\n106.05\n96.23\n101.62\n99.59\n\n-104.75\n-106.05\n-100.94\n-107.25\n-106.29\n\n\"\n\nAcc.\n\n0.53\n0.22\n0.45\n0.53\n0.51\n0.53\n\n#\n\nECE\n\n0.16\n0.18\n0.28\n0.12\n0.16\n0.12\n\n#\n\nMCE\n\n0.25\n0.34\n0.38\n0.2\n0.27\n0.16\n\n#\n\nRMSCE\n\n0.18\n0.19\n0.29\n0.13\n0.18\n0.13\n\nTable 3: Reconstruction quality measured by the MSE and log-likelihood for the data imputation.\nOur well-calibrated uncertainties propagates to the MNIST classi\ufb01er and improves the calibration\nmetrics ECE, MCE and RMSCE.\n\nMissing Data imputation. Another application of stochastic representation learning is to provide\ndistributions over unobserved values (Rezende et al., 2014). In many application domains, sensor\nreadings go missing, which we may mimic by letting parts of an image be unobserved. Rezende et al.\n(2014) show that we can then draw samples from the distribution of the entire image conditioned on\nthe observed part, by imputing the missing pixels with noise and repeatedly encode and decode while\nkeeping observed pixels \ufb01xed. Fig. 11 show samples using this procedure from a VAE, a post-hoc\nLAE and our online LAE, where we only observe the lower half of an MNIST image. This implies\nambiguity about the original digit, e.g. the lower half of a \u201c5\u201c could be a \u201c3\u201c and similarly a \u201c7\u201c could\nbe a \u201c9\u201c. Our LAE captures this ambiguity, which is exempli\ufb01ed by the multi-modal reconstructions\nfrom the sampled networks in Fig. 11. The baselines only capture unimodal reconstructions.\n\nCapturing the ambiguity of partly missing data can improve downstream tasks such as the calibration\nof an image classi\ufb01er. In Fig. 11 (c) we demonstrate how averaging the predictions of a simple\nclassi\ufb01er across reconstructions improves standard calibration metrics. This is because the classi\ufb01er\ninherits the uncertainty and ambiguity in the learned representations. A deep ensemble of AEs\nperforms similarly to ours, but comes at the cost of training and storing multiple models.\n\nWhen the entire input image is missing, the imputation procedure can be seen as a sampling mecha-\nnism, such that our LAE can be viewed as a generative model. The bottom rows in Fig. 11 show that\nthe LAE indeed does generate sharp images from a multi-modal distribution.\n\nAttribute\n\nArched Eyebrows\nAttractive\nBald\nWearing Lipstick\nHeavy Makeup\n\nOverall\n\nAE\n\n0.50\n0.52\n0.98\n0.52\n0.45\n\n0.73\n\nVAE MC-AE LAE\u21e4\n\n0.52\n0.50\n0.98\n0.49\n0.52\n\n0.72\n\n0.55\n0.49\n0.98\n0.50\n0.49\n\n0.73\n\n0.60\n0.53\n0.98\n0.54\n0.56\n\n0.74\n\nTable 4: Semi-supervised classi\ufb01cation accuracy\non CELEBA using only 10 labeled datapoints. \u21e4\nrefers to online LAE.\n\n9\n\nFigure 12: Accuracy as an function of the\nnumber of labels per class on MNIST.\n\n\fFigure 13: Sample reconstructions on CELEBA. The top row shows the mean reconstruction and\nthe bottom row shows the variance of the reconstructed images.\n\nSemi-supervised learning combines a small amount of label data with a large amount of unlabeled\ndata. The hope is that the structure in the unlabeled data can be used to infer properties of the data that\ncannot be extracted from a few labeled points. Embedding the same labeled data point multiple times\nusing a stochastic representation scales up the amount of labeled data that is available during training.\n\nFig. 12 shows the accuracy of a K-nearest neighbor classi\ufb01er trained on different amounts of labeled\ndata from the MNIST dataset. For all models with a stochastic encoder, we encode each labeled\ndatapoint 100 times and repeat the experiment 5 times. When only a few labels per class are\navailable (1-20) we clearly observe that our LAE model outperforms all other models, stochastic and\ndeterministic. Increasing the number of labels beyond 100 per class makes the AE and LAE equal in\ntheir classi\ufb01cation performance with the AE model eventually outperforming the LAE model.\n\nIn Tab. 4 we conduct a similar experiment on the CELEBA (Liu et al., 2015) facial dataset, where the\nthe task is to predict 40 different binary labels per data point. When evaluating the overall accuracy of\npredicting all 40 facial attributes, we see no signi\ufb01cant difference in performance. However, when we\nzoom in on speci\ufb01c facial attributes we gain a clear performance advantage over other models. Fig. 13\nshows the mean and variance of \ufb01ve reconstructed images. The online LAE produces well-calibrated\nuncertainties in the output space and scales to large images.\n\nLimitations. Empirically, the LAE improvements are more signi\ufb01cant for overparameterized net-\nworks. The additional capacity seems to help the optimizer \ufb01nd a local mode where a Gaussian \ufb01t is ap-\npropriate. It seems the regularization induced by marginalizing \u2713 compensates for the added \ufb02exibility.\n\n\n\nBased on the section(s), please answer the following question with yes, no, or n/a and provide a brief justification for your answer.\nQuestion: Do the authors describe the limitations of their work?"}, "1c": {"role": "user", "content": "The following is the conclusion section of the paper you are reviewing:\n\n\nIn this paper, we have introduced a Bayesian autoencoder that is realized using Laplace approxi-\nmations. Unlike current models, this Laplacian autoencoder produces well-behaved uncertainties\nin both latent and data space. We have proposed a novel variational lower-bound of the autoencoder\nevidence and an ef\ufb01cient way to compute its Hessian on high dimensional data that scales linearly\nwith data size. Empirically, we demonstrate that our proposed model predicts reliable stochastic\nrepresentations that are useful for a multitude of downstream tasks: out-of-distribution detection,\nmissing data imputation, and semi-supervised classi\ufb01cation. Our work opens the way for fully\nBayesian representation learning where we can marginalize the representation in downstream tasks.\nWe \ufb01nd this to consistently improve performance.\n\n\n\nBased on the section(s), please answer the following question with yes, no, or n/a and provide a brief justification for your answer.\nQuestion: Do the authors discuss any potential negative societal impacts of their work?"}, "2a": {"role": "user", "content": "The following is the methods section of the paper you are reviewing:\n\n\nN\nn=1 consisting of\nWe consider unsupervised representation learning from i.i.d. data\nRK.\nRD. We also de\ufb01ne a continuous latent space such that representations zn 2\nobservations xn 2\nRK\nSimilar to AEs (Hinton and Salakhutdinov, 2006), we consider two neural networks e  : RD\n!\nand d\u0000: RK\n , \u0000\n{\nWe refer to the composition of encoder and decoder as f\u2713 = d  \u0000\nModel overview. The autoencoder network structure implies that we model the data as being\ndistributed on a K-dimensional manifold parametrized by \u2713. We then seek the distribution of the re-\nconstruction xrec = f\u2713(x) given observation x, where the uncertainty comes from \u2713 being unknown,\n\nRD, widely known as the encoder and decoder. These have parameters \u2713 =\n\nxn}\n\ne\u0000.\n\n!\n\n=\n\nD\n\n}\n\n{\n\n.\n\n|\n\n\u21e0\n\np(\u2713\n\n\u2713, x, f )].\n\nx, f ) = E\u2713\n\nx,f )[p(xrec|\np(xrec|\nNotice that we explicitly condition on f , which is the operator \u2713\nf\u2713, even if this is not stochastic;\nthis conditioning will become important later on to distinguish between the distribution deduced\nby f and its linearization f (t). Mimicking the standard autoencoder reconstruction loss, we set\nx, f ) is unknown, the reconstruction likelihood Eq. (3)\np(xrec|\nis intractable, and approximations are in order. Similar to Blundell et al. (2015), we resort to a\nGaussian approximation, but rather than learning the variance variationally, we opt for LA. This will\nallow us to sample NNs and deduce uncertainties in both latent and output space as illustrated in Fig. 2.\n\nf\u2713(x), I). Since p(\u2713\n\n\u2713, x, f ) =\n\n(xrec|\n\n(3)\n\n7!\n\nN\n\n|\n\nIntractable joint distribution. Any meaningful approximate posterior over \u2713 should be similar to\nthe marginal of the joint distribution p(\u2713, xrec|\nx, f ) = Exrec\u21e0\n\nx, f ). This marginal is\n\nxrec, x, f )]\n\n|\nwhich can be bounded on a log-scale using Jensen\u2019s inequality,\n\nx,f )[p(\u2713\n\np(xrec|\n\np(\u2713\n\n(4)\n\n|\n\nx,f )[log p(\u2713\n\nxrec, x, f )].\n\n(5)\n\nlog p(\u2713\n\nx, f )\n\n|\n\n\u0000L \u2713 = Exrec\u21e0\nx, f )\n\np(xrec|\nqt(\u2713\n\n|\n(\u2713\n\n|\n\nOur \ufb01rst approximation is a LA of p(\u2713\nt ), where we postpone\nthe details on how to acquire \u2713t and Ht. These will eventually be iteratively computed from the\nlower bound Eq. (5); hence the t index. Fig. 3 (a) illustrates the situation thus far: p(\u2713\nx, f ) is\nx, f ) a non-trivial density.\napproximately Gaussian, but the non-linearity of f gives p(xrec|\nLinearization for gradient updates. Standard gradient-based learning can be viewed as a lineariza-\n= J\u2713f\u2713 rf l(f ), where J\u2713f\u2713 is the\ntion of f in \u2713, i.e. for a loss\n\n= l(f\u2713), the gradient is\n\nx, f ) =\n\n\u2713t, H\u0000\n\nr\u2713L\n\nN\n\n\u21e1\n\nL\n\n|\n\n|\n\n|\n\n1\n\n3\n\n\fxrec\n\nxrec\n\nx, f )\n\n\n\np(\u2713\n\n|\n\nx, f (t))\n\n\n\np(\u2713\n\n|\n\n\n\nx(t)\nrec\n\n\n\np(\u2713\n\n|\n\nx(t)\n\nrec, x, f )\n\nf\n\n\n\np(xrec|\n\nx, f )\n\n\n\nx(t)\nrec\n\n\n\np(\u2713\n\n|\n\nrec, x, f (t))\nx(t)\n\n\n\np(xrec|\n\nx, f (t))\n\n\n\nf (t)\n\nf\n\n\n\np(xrec|\n\n\u2713(t), x, f )\n\u2713t\n\n\n\n\n\n\u2713(t)\n\u2713t\n\u2713(t)\n(a) Non linear f\n\n\n\n\u2713\n\n\n\np(xrec|\n\n\u2713(t), x, f (t))\n\u2713t\n\n\n\n\n\n\u2713t\n\u2713(t)\n(b) Linear f (t)\n\n\u2713\n\nFigure 3: Illustrative example for \ufb01xed x. The likelihood for a \ufb01xed \u2713t, shown by the columns, are\nassumed Gaussian p(xrec|\nx, f )\n(\ufb01rst axis) over parameters \u2713 with Gaussians. With the additional assumption of linear f then\np(xrec|\n\n\u2713t, x, f ) = N (f\u2713t (x), I). We model the marginalised density p(\u2713\n\nx, f ) (second axis) is Gaussian. This makes the joint distribution tractable.\n\n|\n\nJacobian of f . In a similar spirit, we linearize f in \u2713 in order to arrive at a tractable approximation of\np(xrec|\n\nx, f ). Speci\ufb01cally, we perform a Taylor expansion around \u2713t\n\nf\u2713(x) = f\u2713t (x) + J\u2713f\u2713t (x)(\u2713\n\n\u2713t)\n\n+\n\n\u2713\n\n\u2713tk\n\n\u0000\n\nO\n\nk\n\n\u0000\n\n2\n\n(6)\n\n=:f (t)\n\n\u2713 (x)\n{z\n\n|\n\n}\n\n\u0000\n\n\u0000\n\nwhere f (t) denote the associated \ufb01rst-order approximation of f . Under this approximation, the joint\nx, f ) becomes Gaussian, p(\u2713, xrec|\ndistribution p(\u2713, xrec|\n\u2713t\nf (t)\n\u2713t\n\nJ\u2713f\u2713t (x)>HtJ\u2713f\u2713t (x)\n\n\u21e1N\nJ\u2713f\u2713t (x)>\n\nH\u0000\nt\nJ\u2713f\u2713t (x)\n\n(\u2713, xrec|\n\n\u00b5t, \u2303t), with\n\n, and \u2303t =\n\nx, f (t))\n\n\u00b5t =\n\n(x)\n\n(7)\n\n\u0000\n\n1\n\n1\n\n.\n\n\u2713\n\n\u25c6\n\n\u2713\n\n+ I\u25c6\n\nThis approximation is illustrated in Fig. 3(b). We provide the proof in Appendix D.4.\n\n\u0000\n\n\u0000\n\nIterative learning. With these approximations we can readily develop an iterative learning scheme\nwhich updates q(\u2713\nx, f ). The mean of this approximate posterior can be updated according to a\nstandard variational gradient step, \u2713t+1 = \u2713t + \u0000\n\n|\n\nr\u2713Lxrec , where\n\nLxrec = E\u2713\n\n\u21e0\n\nqt(\u2713\n\nx,f )[log p(xrec|\n\n|\n\n\u2713, x, f (t))],\n\n(8)\n\nis a lower bound on Eq. (3), which we evaluate with a single Monte Carlo sample. Following the LA,\nthe covariance of qt+1(\u2713\nx, f ) at \u2713t+1. Since\nx, f ), should be the inverse of the Hessian of log p(\u2713\nthis is intractable, we instead compute the Hessian of the lower bound Eq. (5)\n\n|\n\n|\n\n2\n\u2713L\u2713\n\nHt+1 =\n\n\u0000r\n\nx,f (t))\nh\n\u0000\n\u0000\nThe last term can be approximated since\n\n\u2713t+1\n\n= Ep(xrec|\n\n\u0000r\n\n2\n\n\u2713 log p(xrec|\n\u2713 log qt(\u2713\n2\nx, f )\n\n\u2713, x, f (t))\n\n\u2713 log qt(\u2713\n2\n\u0000r\n\n|\n\nx, f )\n\n\u2713t+1\n\n(9)\n\n|\u2713=\u2713t+1 =\n-term, we introduce a parameter \u21b5, such that the \ufb01nal approximation is\n\n).\n\u2713tk\n\nHt +\n\nr\n\nO\n\n\u0000\n\n(\n\n|\n\ni\u0000\n\u0000\n\u2713t+1 \u0000\n\u0000\nk\n\nTo ef\ufb01ciently cope with the\n\nHt+1 \u21e1\n\n(1\n\n= (1\n\nO\n\u21b5)Ht + Ep(xrec|\nJ\u2713f (t)\n\u2713\n\n\u21b5)Ht \u0000\n\n\u0000\n\n\u0000\n\nx,f )\n\n>\n\nr\n\n\u0000r\n\nh\n2\nxrec log p\n\n2\n\u2713 log p\n\nxrec\n\n\u2713, x, f (t)\n\n\u21e3\nxrec\n\n\u0000\n\u0000\n\n\u2713t+1, x, f (t)\n\n\u2713t+1\n\u2318i \u0000\n\u0000\nJ\u2713f (t)\n\u2713 .\n\u0000\n\n(10)\n\n(11)\n\n\u21e3\nis independent of which \u2713 we evaluate in and\n\nwhere J\u2713f (t)\nis trivial\n\u2713\nto compute for common losses, i.e. for MSE it is the identity. The parameter \u21b5 can be viewed\nas a geometric running average that is useful for smoothing out results computed on a minibatch\ninstead of on the full training set, similar to momentum-like training procedures. It further allows for\nnon-monotonically-increasing precision. Note that we revisit data during training, and the precision\n\n\u2318\n\u2713t+1, x, f (t)\n\n\u0000\n2\n\u0000\nxrec log p\n\nxrec\n\nr\n\n\u0000\n\u0000\n\n\u0000\n\n\u0000\n\n4\n\n\f\n\nf (t) = taylor(f, \u2713t)\n\nt+=1\n\n\n\n\u2713t+1 = \u2713t + \u0000E\u2713\n\nqt [\n\nr\u2713 log p (xrec |\n\n\u21e0\n\n\u2713, x, f )]\n\n\n\nqt\n\n\n\n\u2713t\n\n1\n\n\n\nH \u0000\nt\n\n\n\nHt+1 = (1\n\n\u21b5)Ht \u0000\n\n\u0000\n\nT\n\nJ\u2713f (t)\n\u2713\n\n2\n\nxrec log p(xrec|\n\nr\n\n\u2713, x, f (t)) J\u2713f (t)\n\u2713\n\nt+=1\n\n\u00b5\u0000=0\nFigure 4: Iterative training procedure. Given a distribution qt over parameters, and a linearized\nfunction f (t), compute \ufb01rst and second-order derivatives to update the distribution on parameters.\n\nf^{(t)}=\\textsc{taylor}(f,\\theta^{(t)})\n\nmatrix is updated for every revisit. Thus, the forgetting induced by \u21b5 is a wanted behavior to avoid\nin\ufb01nite precision in the limit of in\ufb01nite training time. In practice, we set \u21b5 equal to the learning rate,\nwhere the practical intuition is that when \u21b5 is small, the network uncertainty decreases faster.\nThe overall training procedure is summarized in Fig. 4. We initialize q0 as a Gaussian with \u27130 = 0\nand H0 = I. We provide more details on the model, the linearization, and iterative learning in\nAppendix D.\n\nWhy not just. . . ? The proposed training procedure may at \ufb01rst appear non-trivial, and it is reasonable\nto wonder if existing methods could be applied to similar results. Variational inference often achieves\nsimilar results to Laplace approximations, so could we use \u2018Bayes by Backprop\u2019 (Blundell et al., 2015)\nto get an alternative Gaussian approximate posterior over \u2713? Similar to the supervised experiences\nof Jospin et al. (2020), we, unfortunately, found this approach too brittle to allow for practical\nmodel \ufb01tting. But then perhaps a post-hoc LA as proposed by Daxberger et al. (2021) for supervised\nlearning? Empirically, we found it to be important to center the approximate posterior around a point\nwhere the Hessian provides useful uncertainty estimates. Our online training moves in this direction\nas the Hessian is part of the procedure, but this is not true for the post-hoc LA.\n\nConceptually, we argue that our approach, while novel, is not entirely separate from existing methods.\nOur reliance on lower bounds makes the method an instance of variational inference (Jordan et al.,\n1999; Opper and Archambeau, 2009b), and we maximize the bounds using Monte Carlo EM (Capp\u00e9,\n2009). We rely on a LA as our choice of variational distribution, which has also been explored by\nPark et al. (2019). Finally, we note that our linearization trick Eq. (6) has great similarities to classic\nextended Kalman \ufb01ltering (Gelb et al., 1974).\n\n\n\nThe following is the appendix D section of the paper you are reviewing:\n\n\nD.1 Overview\n\nLet X = RD be the data space, let Y = RD be the reconstruction space and let \u0398 = RP be\nthe parameter space. Let F = (\u0398 \u2192 (X \u2192 Y)) be the space of operators from \u0398 to the space of\noperators from X to Y. We will denote a function f \u2208 F applied to a parameter \u03b8 \u2208 \u0398 as f\u03b8 : X (cid:55)\u2192 Y.\nThis will represent, for example, a NN f with a specific set of parameter (weights) \u03b8 \u2208 \u0398, that maps\nsome data x \u2208 X to some reconstruction f\u03b8(x) = y \u2208 Y.\nLet X \u00d7 Y \u00d7 \u0398 \u00d7 F be a probability space. The only assumption we make on this space is\n\np(y|x, \u03b8, f ) \u223c N (y|\u00b5 = f\u03b8(x), \u03c32 = \u03a3)\n\n(11)\nwhere \u03a3 \u2208 M(RD \u00d7 RD) is a fixed variance matrix. This is a common assumption for regression\ntasks, and is sometimes referred to as the \u201cdata noise\u201d or \u201creconstruction error\u201d. In this paper, we fix\n\u03a3 = I, but the derivations hold for the general case. With only this assumption, the distribution is\nundefined and multiple solutions can exist. Thus, we require more assumptions.\nA dataset D = {xn} is a finite of infinite collection of data xn \u2208 X that is assumed to follow a\ncertain, fixed but unknown, distribution\n\n\u2200(x, \u03b8, f ) \u2208 X \u00d7 \u0398 \u00d7 F\n\nxn \u223c p(x).\n\n(12)\n\nSacrificing slim notation for the sake of clarity, we introduce an operator I : X (cid:55)\u2192 Y. This represents\nthe ideal reconstruction for a given x. In the standard supervised setting, this would be the operator\n(defined on the dataset only) that maps each data input to its label. In our unsupervised setting, where\nX and Y are the same space, the operator I is simply the identity (Indeed they are not the same space,\nthey are isomorphic spaces that we identify through the operator I). Since I is the identity, it is often\n\n6\n\n\fneglected in the literature, which can lead to unclear and potentially ambiguous Bayesian derivations.\nThus, we choose to adopt this heavier, but more precise notation.\nWe assume access to a specific f N N \u2208 F. Practically this will be our NN architecture, i.e. an operator\nthat, given a set of parameters \u03b8 \u2208 \u0398 gives rise to a function from X to Y. Having f N N fixed, one\nmay consider f not to be stochastic anymore, we choose to still explicitly condition on f in order to\nhave a clearer notation in later stages. Note that, despite not being covered in this work, a proper\nstochastic derivation also on the NN architecture should be feasible.\n\nD.2 Objective\n\nThe NN\u2019s parameter optimization process in this full Bayesian probabilistic framework can be viewed\nas: given a fixed f N N \u2208 F, namely the NN architecture, maximise the reconstruction probability of\nI(xn) over the dataset D\n(cid:104)\n\n(cid:105)\n\nExn\u223cp(x)\n\np(y|xn, f N N )(cid:12)\n\n(cid:12)y=I(xn)\n\n=\n\n(cid:88)\n\np(y|xn, f N N )(cid:12)\n\n(cid:12)y=I(xn),\n\n(13)\n\nxn\u2208D\n\nwhere the untractable p(y|xn, f N N ) can be expanded in \u03b8 and thus related to our hypothesis (11) as\n\np(y|xn, f N N ) = E\u03b8\u223cp(\u03b8|xn,f N N )\n\n(cid:2)p(y|xn, \u03b8, f N N )(cid:3) .\n\n(14)\n\nNotice that the only unfixed quantity is the distribution on the parameters, which we will optimize\nfor. We are not interested in finding a datapoint-dependant distribution, but rather one that maximise\nall reconstructions at the same time, i.e. p(\u03b8|f N N ) = p(\u03b8|xn, f N N ). We can then frame Bayesian\noptimization as: find a distribution on parameters such that\n\nq(\u03b8) \u2208 arg max\n\np(\u03b8|f N N )\n\n= arg max\n\np(\u03b8|f N N )\n\n(cid:88)\n\nxn\u2208D\n(cid:88)\n\nxn\u2208D\n\nE\u03b8\u223cp(\u03b8|f N N )\n\n(cid:104)\n\np(y|xn, \u03b8, f N N )(cid:12)\n\n(cid:12)y=I(xn)\n\n(cid:105)\n\nE\u03b8\u223cp(\u03b8|f N N )\n\n(cid:2)p(I(xn)(cid:12)\n\n(cid:12)N (f N N\n\u03b8\n\n(xn), \u03a3))(cid:3) .\n\n(15)\n\n(16)\n\nMoreover, finding this optimum in the space \u2206(\u0398) of all distributions on \u0398 is not tractable. So, as\ncommonly done, we restrict ourselves to the subset G(\u0398) \u2282 \u2206(\u0398) of Gaussians over \u0398. Then, a\nsolution in our context is\n\nq(\u03b8) \u2208 arg max\nq\u2208G(\u0398)\n\n(cid:88)\n\nxn\u2208D\n\nE\u03b8\u223cq(\u03b8)\n\n(cid:2)p(I(xn)(cid:12)\n\n(cid:12)N (f N N\n\u03b8\n\n(xn), \u03a3))(cid:3) .\n\n(17)\n\nWe emphasize that this solution has no guarantees of being unique, but we are interested in finding\none of them.\n\nD.3\n\nJoint distribution for a fixed datapoint\n\nLet us first get a better understanding of the joint distribution on Y\u00d7\u0398 conditional to a fixed datapoint\nx \u2208 X and a network architecture f \u2208 F\n\nThis distribution has two marginals\n\nand two conditionals\n\np(y, \u03b8|x, f ).\n\np(y|x, f )\n\np(\u03b8|x, f )\n\np(y|\u03b8, x, f )\n\np(\u03b8|y, x, f ).\n\nThese four quantities must satisfy the system of two \u201crecursive\u201d equations\n\np(y|x, f ) =\n\n(cid:90)\n\n\u0398\n\np(y|\u03b8, x, f )p(\u03b8|x, f )d\u03b8 =\n\n= E\u03b8\u223cp(\u03b8|x,f )[p(y|\u03b8, x, f )]\n\n7\n\n(18)\n\n(19)\n\n(20)\n\n(21)\n\n(22)\n\n(23)\n\n\fp(\u03b8|x, f ) =\n\n(cid:90)\n\nY\n\np(\u03b8|y, x, f )p(y|x, f )dy =\n\n= Ey\u223cp(y|x,f )[p(\u03b8|y, x, f )].\n\nIf these are satisfied then the joint is a well-defined distribution and we can apply Bayes rule\n\np(y|\u03b8, x, f )p(\u03b8|x, f ) = p(y, \u03b8|x, f ) = p(\u03b8|y, x, f )p(y|x, f )\n\nwhich in logarithmic form is\n\n(24)\n\n(25)\n\nlog p(y|\u03b8, x, f ) + log p(\u03b8|x, f ) = log p(\u03b8|y, x, f ) + log p(y|x, f ).\n(26)\nWe can factor in the assumptions. The \u201cdata noise\u201d assumption gives us one of the two conditionals:\np(y|\u03b8, x, f ) \u223c N (y|\u00b5 = f\u03b8(x), \u03c32 = \u03a3).\n(27)\n\nThe \u201cGaussian parameter\u201d assumption gives us one of the two marginals:\np(\u03b8|x, f ) = qt(\u03b8) \u223c N (\u03b8|\u00b5 = \u03b8t, \u03c32 = H\u22121\nt ).\nWith these in place, the joint distribution is uniquely defined. The other marginal, by Eq. 23, is\n\np(y|x, f ) = E\u03b8\u223cqt(\u03b8)[p(y|N (f\u03b8(x), \u03a3))]\n\nand the other conditional, by Bayes rule, is\n\np(\u03b8|y, x, f ) =\n\np(y|\u03b8, x, f )p(\u03b8|x, f )\np(y|x, f )\n\n.\n\n(28)\n\n(29)\n\n(30)\n\nDespite being uniquely defined, the integral in Eq. 29 is, with a general f , intractable, and so is the\njoint distribution.\n\nBut why do we even care? The intractability of p(y|x, f ) in Eq. 29 may at first glance appear\nirrelevant. This is the case, for instance, with bayes by backprop (Blundell et al., 2015) methods.\nThey simply need access to the gradient of this quantity. For this purpose, a simple Monte Carlo\nestimate of the expectation is enough.\n\nOn the other hand, we are interested in recovering a meaningful distribution on parameters. This\nimply that we aim at using Eq. 29 to enforce that Eq. 24 holds. For this purpose we need access to\nthe the density p(y|x, f ), so a Monte Carlo estimate of Eq. 29 is not enough.\n\nD.4 Linear f\n\nTheorem 1. Given the data noise assumption from Eq. 27\n\np(y|\u03b8, x, f ) \u223c N (y|\u00b5 = f\u03b8(x), \u03c32 = \u03a3),\ngiven the Gaussian parameter assumption from Eq. 28 for some \u03b8t, Ht\np(\u03b8|x, f ) = qt(\u03b8) \u223c N (\u03b8|\u00b5 = \u03b8t, \u03c32 = H\u22121\n\nt ),\n\nassume that f is linear in \u03b8, i.e.\n\nf\u03b8(x) = f0(x) + Jf\u00b7(x)\u03b8\n\n\u2200\u03b8 \u2208 \u0398, \u2200x \u2208 X.\n\nThen the joint distribution is Gaussian itself\n\np(\u03b8, y|x, f ) \u223c N ((\u03b8, y)|\u00b5t, \u03a3t)\n\nwhere\n\n\u00b5t =\n\n(cid:19)\n\n(cid:18) \u03b8t\nf\u03b8t(x)\n\n, and \u03a3t =\n\n(cid:18) H\u22121\nJf\u00b7(x)\u03a3 (cid:0)Jf\u00b7(x)\u22a4HtJf\u00b7(x)(cid:1)\u22121\n\n\u03a3Jf\u00b7(x)\u22a4\n\nt\n\n(31)\n\n(32)\n\n(33)\n\n(34)\n\n(cid:19)\n\n.\n\n+ \u03a3\n\nProof. With the further assumption of linearity of f , we can explicitly carry out the integral in the\nexpectation in Eq. 29\n\np(y|x, f ) = E\u03b8\u223cqt(\u03b8)[p(y|N (f\u03b8(x), \u03a3))] =\n\n=\n\n=\n\n=\n\n(cid:90)\n\n\u0398\n\n(cid:90)\n\n\u0398\n\n(cid:90)\n\n\u0398\n\np(y|N (f\u03b8(x), \u03a3))qt(\u03b8)d\u03b8 =\n\np(y|N (f\u03b8(x), \u03a3))p(\u03b8|N (\u03b8t, H\u22121\n\nt ))d\u03b8 =\n\np(y|N (f0(x) + Jf\u00b7(x)\u03b8, \u03a3))p(\u03b8|N (\u03b8t, H\u22121\n\nt ))d\u03b8 =\n\nWe emphasize that as a consequence \u22072\n\n\u03b8 log p(y|x, f ) is not dependent on \u03b8t.\n\n= p(y|N (f\u03b8t(x), (Jf\u00b7(x)\u22a4HtJf\u00b7(x))\u22121 + \u03a3)).\n\n8\n\n\fHaving Theorem 1 in place, we can go back to our original problem. We need to deal with a fixed\nnon-linear architecture f N N . We can exploit Theorem 1 by defining f t: a linearization of f N N with\na first-order Taylor expansion around \u03b8t\n\nand it holds that\n\n\u03b8(x) := TAYLOR(f N N , \u03b8t)(x) =\nf t\n\n= f N N\n\u03b8t\n\n(x) + J\u03b8f N N\n\n\u03b8t\n\n(x)(\u03b8 \u2212 \u03b8t)\n\nf N N\n\u03b8\n\n(x) = f t\n\n\u03b8(x) + O(\u2225\u03b8 \u2212 \u03b8t\u22252).\n\nRecalling Eq. 11 both for f N N and for f t\n\np(y|x, \u03b8, f N N ) \u223c N (y|\u00b5 = f N N\n\n\u03b8\n\n(x), \u03c32 = \u03a3)\n\np(y|x, \u03b8, f t) \u223c N (y|\u00b5 = f t\n\n\u03b8(x), \u03c32 = \u03a3)\n\nthat, together with Eq. 36, imply\n\nN (cid:0)y|\u00b5 = f N N\n\n\u03b8\n\n(x), \u03c32 = \u03a3(cid:1) \u223c N (cid:0)y|\u00b5 = f t\n\n\u03b8(x) + O(\u2225\u03b8 \u2212 \u03b8t\u22252), \u03c32 = \u03a3(cid:1),\n\n(35)\n\n(36)\n\n(37)\n\n(38)\n\n(39)\n\nwhere we can interpret the unknown O(\u2225\u03b8 \u2212 \u03b8t\u22252) as \u03b8-dependent noise. More specifically, calling\n\u03b3 > 0 the scalar constant of the O-term, we assume that\n\nO(\u2225\u03b8 \u2212 \u03b8t\u22252) \u2248 \u03f5(\u03b8 \u2212 \u03b8t)2\n\nwhere \u03f5 \u223c N (0, \u03b3I)\n\nand thus, from Eq. 39, we have\n\nN (cid:0)y|\u00b5 = f N N\n\n\u03b8\n\n(x), \u03c32 = \u03a3(cid:1) \u223c N (y|\u00b5 = f t\n\n\u03b8(x), \u03c32 = \u03a3 + \u03b3\u2225\u03b8 \u2212 \u03b8t\u22252I).\n\n(40)\n\n(41)\n\nAt this point, integrals are not analytically tractable, and thus a proper proof is not feasible, the\nintuition is that this increased variance reflects in increased variance in p(\u03b8|x, f N N )\n\n\u22072\n\n\u03b8 log p(\u03b8|x, f N N )(cid:12)\n\n(cid:12)\u03b8=\u03b8t+1\n\n\u2248 \u22072\n\n\u03b8 log p(\u03b8|x, f t)(cid:12)\n\n(cid:12)\u03b8=\u03b8t+1\n\n+ \u03b3\u2225\u03b8t+1 \u2212 \u03b8t\u22252,\n\n(42)\n\nwhere we introduce a hyperparameter \u03b1 > 0 to cope with this added variance. If we then assume the\nJacobian J\u03b8f N N to be a Lipschitz function, then the Lipschitz constant is an upper bound on \u03b3, as\nfollows from the Taylor expansion of Eq. (35). If this Lipschitz constant is smaller than the inverse of\nthe gradient step 1/\u2225\u03b8t+1 \u2212 \u03b8t\u2225 (which is not an unreasonable assumption for the gradient ascent to\nbe stable) we have\n\n\u03b3\u2225\u03b8t+1 \u2212 \u03b8t\u22252 \u2248 \u2225\u03b8t+1 \u2212 \u03b8t\u2225\n\n(43)\n\nthat gives us a plausible order of magnitude for choosing the hyperparameter \u03b1.\nMotivation: During training, we produce a sequence of Gaussians qt(\u03b8) \u223c N (\u03b8t, H\u22121\nt ) that we\nassume to be the distribution p(\u03b8|x, f t), at every step t \u2265 0. This distribution qt is then used for (1)\nGaussian derivation in the linear case, for (2) Monte Carlo sampling in the update rule \u03b8t \u2192 \u03b8t+1\nand (3), as second order derivative, for update rule Ht \u2192 Ht+1.\nMoreover, given that this distribution qt is our \"best guess so far\", we assume it to be also the\ndistribution p(\u03b8|x, f N N ). This, being f N N not linear, (1) cannot be used for Gaussian derivation,\n(2) can reasonably be used for sampling (and thus we derive the improved update rule Eq. (52)), and\n(3) can somehow be used as second order derivative (and thus we derive the improved update rule\nEq. (53)), but the latter requires some more care. That is why we introduce the parameter \u03b1.\n\nD.5\n\nIterative learning\n\nOur learning method produces a sequence of Gaussians\n\nqt(\u03b8) \u223c N (\u03b8|\u00b5 = \u03b8t, \u03c32 = H\u22121\nt )\n\nand a sequence of linearized functions\n\nf t = TAYLOR(f N N , \u03b8t)\n\nfor every t \u2265 0.\n\n9\n\n(44)\n\n(45)\n\n\fInitialization is trivially done using a Gaussian prior on the parameters.\n\n\u03b80 = \u03b8prior\n\nH0 = (\u03a3prior)\u22121\n\n(46)\n\nIterative step is made in two steps. First, having access to \u03b8t, we \u201cgenerate\u201d the linearization f t.\nPractically this is equivalent to computing the two quantities f N N\n(x), that, together\nwith the value \u03b8t are actually equivalent to \u201cgenerating\u201d f t, as Eq. 35 shows.\nSecond, we compute the Gaussian parameters \u03b8t+1 and Ht+1.\n\n(x) and J\u03b8f N N\n\n\u03b8t\n\n\u03b8t\n\nRecalling our aim of maximizing the quantity Eq. (17), update on q(\u00b7) means, \u03b8t \u2192 \u03b8t+1, is ideally\nmade through gradient ascent steps on p(y|x, f )|y=I(x). As this is intractable, we instead do gradient\nsteps on the lower bound Ly of (the log of) Eq. 23\n\n\u03b8t+1 = \u03b8t + \u03bb\u2207\u03b8Ly\n\n(cid:12)\n(cid:12)\n(cid:12)y=I(x)\n\nwhere\n\nand so\n\nLy = E\u03b8\u223cp(\u03b8|x,f t)[log p(y|\u03b8, x, f t)] \u2264 log E\u03b8\u223cp(\u03b8|x,f t)[p(y|\u03b8, x, f t)]\n\n\u2207\u03b8Ly\n\n(cid:12)\n(cid:12)y=I(x) = E\u03b8\u223cp(\u03b8|x,f t)\n\n(cid:104)\n\u2207\u03b8 log p(y|\u03b8, x, f t)(cid:12)\n\n(cid:105)\n\n=\n\n= E\u03b8\u223cqt(\u03b8)\n\n(cid:2)\u2207\u03b8 log p(I(x)(cid:12)\n\n(cid:12)N (f t\n\n(cid:12)y=I(x)\n\u03b8(x), \u03a3))(cid:3) .\n\n(47)\n\n(48)\n\n(49)\n\nRecalling the Laplace approximation Eq. (3), the negative precision, \u2212Ht+1, is ideally set to be the\nthe hessian of the log probability p(\u03b8|x, f ), evaluated in \u03b8t+1. As this is intractable we instead set it\nto the hessian of the lower bound L\u03b8 of (the log of) Eq. (24)\n\nHt+1 = \u2212\u22072\n\n\u03b8L\u03b8\n\n(cid:12)\n(cid:12)\n(cid:12)\u03b8=\u03b8t+1\n\nwhere\n\nand so\n\nL\u03b8 = Ey\u223cp(y|x,f t)[log p(\u03b8|y, x, f t)] \u2264 log Ey\u223cp(y|x,f t)[p(\u03b8|y, x, f t)]\n\n\u22072\n\n\u03b8L\u03b8\n\n(cid:12)\n(cid:12)\n(cid:12)\u03b8=\u03b8t+1\n\n= Ey\u223cp(y|x,f t)\n\n(cid:20)\n\u22072\n\n\u03b8 log p(\u03b8|y, x, f t)\n\n(cid:21)\n\n(cid:12)\n(cid:12)\n(cid:12)\u03b8=\u03b8t+1\n\n(50)\n\n(51)\n\nvia Eq. (26)\n\n= Ey\u223cp(y|x,f t)\n\n(cid:104)\n\n\u22072\n\n\u03b8 log p(y|\u03b8, x, f t)\n\n\u2212 \u22072\n\n(cid:12)\n\u03b8 log p(y|x, f t)\n(cid:12)\n(cid:12)\u03b8=\u03b8t+1\n\n(cid:12)\n(cid:12)\n(cid:12)\u03b8=\u03b8t+1\n(cid:105)\n\n+ \u22072\n\n(cid:12)\n\u03b8 log p(\u03b8|x, f t)\n(cid:12)\n(cid:12)\u03b8=\u03b8t+1\n\n+\n\nvia Theorem 1\n\n= Ey\u223cp(y|x,f t)\n\n(cid:20)\n\u22072\n\n\u03b8 log p(y|\u03b8, x, f t)\n\n(cid:12)\n(cid:12)\n(cid:12)\u03b8=\u03b8t+1\n\n+ \u22072\n\n(cid:12)\n\u03b8 log p(\u03b8|x, f t)\n(cid:12)\n(cid:12)\u03b8=\u03b8t+1\n\n(cid:21)\n\nvia the chain rule Eq. (55)\n\n= Ey\u223cp(y|x,f t)\n\n(cid:20)\nJ\u03b8f t\n\n\u00b7 (x)\u22a4\u22072\n\ny log p(y|\u03b8t+1, x, f t)J\u03b8f t\n\n\u00b7 (x) + \u22072\n\n\u03b8 log p(\u03b8|x, f t)\n\n(cid:21)\n\n(cid:12)\n(cid:12)\n(cid:12)\u03b8=\u03b8t+1\n\nvia HP Eq. (11)\n\n= Ey\u223cp(y|x,f t)\n\n(cid:20)\n\n\u2212J\u03b8f t\n\n\u00b7 (x)\u22a4\u03a3\u22121J\u03b8f t\n\n= \u2212J\u03b8f t\n\n\u00b7 (x)\u22a4\u03a3\u22121J\u03b8f t\n\n\u00b7 (x) + \u22072\n\n\u00b7 (x) + \u22072\n\n(cid:12)\n\u03b8 log p(\u03b8|x, f t)\n(cid:12)\n(cid:12)\u03b8=\u03b8t+1\n(cid:12)\n\u03b8 log p(\u03b8|x, f t)\n(cid:12)\n(cid:12)\u03b8=\u03b8t+1\n\n(cid:21)\n\n= \u2212J\u03b8f t\n\n\u00b7 (x)\u22a4\u03a3\u22121J\u03b8f t\n\n\u00b7 (x) + \u22072\n\n\u03b8 log qt(\u03b8)\n\n= \u2212J\u03b8f t\n\n\u00b7 (x)\u22a4\u03a3\u22121J\u03b8f t\n\n\u00b7 (x) \u2212 Ht.\n\n10\n\n(cid:12)\n(cid:12)\n(cid:12)\u03b8=\u03b8t+1\n\n\fD.5.1\n\nImproved update rule\n\nAs said, the update \u03b8t \u2192 \u03b8t+1 is ideally made through gradient ascent steps on p(y|x, f )|y=I(x),\nbut we instead use the tractable lower bound with f t. We can perform the same derivation using f N N\nin place of f t. Assuming p(\u03b8|x, f N N ) \u223c qt(\u03b8) for sampling, leads to the improved update rule\n\n\u03b8t+1 = \u03b8t + \u03bbE\u03b8\u223cqt(\u03b8)\n\n(cid:2)\u2207\u03b8 log p(I(x)(cid:12)\n\n(cid:12)N (f N N\n\u03b8\n\n(x), \u03a3))(cid:3) .\n\n(52)\n\nSimilarly, the negative precision \u2212Ht+1 is ideally set to be the hessian of the log probability p(\u03b8|x, f ).\nbut instead, we use the tractable lower bound with f t. Here we cannot perform the same derivation,\nsince Theorem 1 does not hold anymore. Instead, we can rely on the estimate Eq. (42) to improve the\nterm\n\u22072\n\n\u2248 \u2212(1 \u2212 \u03b1)Ht,\n\n\u03b8 log p(\u03b8|x, f t)\n\n(cid:12)\n\u03b8 log p(\u03b8|x, f N N )\n(cid:12)\n(cid:12)\u03b8=\u03b8t+1\n\n(cid:12)\n(cid:12)\n(cid:12)\u03b8=\u03b8t+1\n\n= \u2212Ht\n\n\u2212\u2192\n\n\u22072\n\nand this leads to the improved update rule\n\nHt+1 = (1 \u2212 \u03b1)Ht + J\u03b8f t\n\n\u00b7 (x)\u22a4\u03a3\u22121J\u03b8f t\n\n\u00b7 (x).\n\n(53)\n\n\n\nBased on the section(s), please answer the following question with yes, no, or n/a and provide a brief justification for your answer.\nQuestion: If the authors include theoretical results, do the authors state the full set of assumptions of all theoretical results?"}, "2b": {"role": "user", "content": "The following is the methods section of the paper you are reviewing:\n\n\nN\nn=1 consisting of\nWe consider unsupervised representation learning from i.i.d. data\nRK.\nRD. We also de\ufb01ne a continuous latent space such that representations zn 2\nobservations xn 2\nRK\nSimilar to AEs (Hinton and Salakhutdinov, 2006), we consider two neural networks e  : RD\n!\nand d\u0000: RK\n , \u0000\n{\nWe refer to the composition of encoder and decoder as f\u2713 = d  \u0000\nModel overview. The autoencoder network structure implies that we model the data as being\ndistributed on a K-dimensional manifold parametrized by \u2713. We then seek the distribution of the re-\nconstruction xrec = f\u2713(x) given observation x, where the uncertainty comes from \u2713 being unknown,\n\nRD, widely known as the encoder and decoder. These have parameters \u2713 =\n\nxn}\n\ne\u0000.\n\n!\n\n=\n\nD\n\n}\n\n{\n\n.\n\n|\n\n\u21e0\n\np(\u2713\n\n\u2713, x, f )].\n\nx, f ) = E\u2713\n\nx,f )[p(xrec|\np(xrec|\nNotice that we explicitly condition on f , which is the operator \u2713\nf\u2713, even if this is not stochastic;\nthis conditioning will become important later on to distinguish between the distribution deduced\nby f and its linearization f (t). Mimicking the standard autoencoder reconstruction loss, we set\nx, f ) is unknown, the reconstruction likelihood Eq. (3)\np(xrec|\nis intractable, and approximations are in order. Similar to Blundell et al. (2015), we resort to a\nGaussian approximation, but rather than learning the variance variationally, we opt for LA. This will\nallow us to sample NNs and deduce uncertainties in both latent and output space as illustrated in Fig. 2.\n\nf\u2713(x), I). Since p(\u2713\n\n\u2713, x, f ) =\n\n(xrec|\n\n(3)\n\n7!\n\nN\n\n|\n\nIntractable joint distribution. Any meaningful approximate posterior over \u2713 should be similar to\nthe marginal of the joint distribution p(\u2713, xrec|\nx, f ) = Exrec\u21e0\n\nx, f ). This marginal is\n\nxrec, x, f )]\n\n|\nwhich can be bounded on a log-scale using Jensen\u2019s inequality,\n\nx,f )[p(\u2713\n\np(xrec|\n\np(\u2713\n\n(4)\n\n|\n\nx,f )[log p(\u2713\n\nxrec, x, f )].\n\n(5)\n\nlog p(\u2713\n\nx, f )\n\n|\n\n\u0000L \u2713 = Exrec\u21e0\nx, f )\n\np(xrec|\nqt(\u2713\n\n|\n(\u2713\n\n|\n\nOur \ufb01rst approximation is a LA of p(\u2713\nt ), where we postpone\nthe details on how to acquire \u2713t and Ht. These will eventually be iteratively computed from the\nlower bound Eq. (5); hence the t index. Fig. 3 (a) illustrates the situation thus far: p(\u2713\nx, f ) is\nx, f ) a non-trivial density.\napproximately Gaussian, but the non-linearity of f gives p(xrec|\nLinearization for gradient updates. Standard gradient-based learning can be viewed as a lineariza-\n= J\u2713f\u2713 rf l(f ), where J\u2713f\u2713 is the\ntion of f in \u2713, i.e. for a loss\n\n= l(f\u2713), the gradient is\n\nx, f ) =\n\n\u2713t, H\u0000\n\nr\u2713L\n\nN\n\n\u21e1\n\nL\n\n|\n\n|\n\n|\n\n1\n\n3\n\n\fxrec\n\nxrec\n\nx, f )\n\n\n\np(\u2713\n\n|\n\nx, f (t))\n\n\n\np(\u2713\n\n|\n\n\n\nx(t)\nrec\n\n\n\np(\u2713\n\n|\n\nx(t)\n\nrec, x, f )\n\nf\n\n\n\np(xrec|\n\nx, f )\n\n\n\nx(t)\nrec\n\n\n\np(\u2713\n\n|\n\nrec, x, f (t))\nx(t)\n\n\n\np(xrec|\n\nx, f (t))\n\n\n\nf (t)\n\nf\n\n\n\np(xrec|\n\n\u2713(t), x, f )\n\u2713t\n\n\n\n\n\n\u2713(t)\n\u2713t\n\u2713(t)\n(a) Non linear f\n\n\n\n\u2713\n\n\n\np(xrec|\n\n\u2713(t), x, f (t))\n\u2713t\n\n\n\n\n\n\u2713t\n\u2713(t)\n(b) Linear f (t)\n\n\u2713\n\nFigure 3: Illustrative example for \ufb01xed x. The likelihood for a \ufb01xed \u2713t, shown by the columns, are\nassumed Gaussian p(xrec|\nx, f )\n(\ufb01rst axis) over parameters \u2713 with Gaussians. With the additional assumption of linear f then\np(xrec|\n\n\u2713t, x, f ) = N (f\u2713t (x), I). We model the marginalised density p(\u2713\n\nx, f ) (second axis) is Gaussian. This makes the joint distribution tractable.\n\n|\n\nJacobian of f . In a similar spirit, we linearize f in \u2713 in order to arrive at a tractable approximation of\np(xrec|\n\nx, f ). Speci\ufb01cally, we perform a Taylor expansion around \u2713t\n\nf\u2713(x) = f\u2713t (x) + J\u2713f\u2713t (x)(\u2713\n\n\u2713t)\n\n+\n\n\u2713\n\n\u2713tk\n\n\u0000\n\nO\n\nk\n\n\u0000\n\n2\n\n(6)\n\n=:f (t)\n\n\u2713 (x)\n{z\n\n|\n\n}\n\n\u0000\n\n\u0000\n\nwhere f (t) denote the associated \ufb01rst-order approximation of f . Under this approximation, the joint\nx, f ) becomes Gaussian, p(\u2713, xrec|\ndistribution p(\u2713, xrec|\n\u2713t\nf (t)\n\u2713t\n\nJ\u2713f\u2713t (x)>HtJ\u2713f\u2713t (x)\n\n\u21e1N\nJ\u2713f\u2713t (x)>\n\nH\u0000\nt\nJ\u2713f\u2713t (x)\n\n(\u2713, xrec|\n\n\u00b5t, \u2303t), with\n\n, and \u2303t =\n\nx, f (t))\n\n\u00b5t =\n\n(x)\n\n(7)\n\n\u0000\n\n1\n\n1\n\n.\n\n\u2713\n\n\u25c6\n\n\u2713\n\n+ I\u25c6\n\nThis approximation is illustrated in Fig. 3(b). We provide the proof in Appendix D.4.\n\n\u0000\n\n\u0000\n\nIterative learning. With these approximations we can readily develop an iterative learning scheme\nwhich updates q(\u2713\nx, f ). The mean of this approximate posterior can be updated according to a\nstandard variational gradient step, \u2713t+1 = \u2713t + \u0000\n\n|\n\nr\u2713Lxrec , where\n\nLxrec = E\u2713\n\n\u21e0\n\nqt(\u2713\n\nx,f )[log p(xrec|\n\n|\n\n\u2713, x, f (t))],\n\n(8)\n\nis a lower bound on Eq. (3), which we evaluate with a single Monte Carlo sample. Following the LA,\nthe covariance of qt+1(\u2713\nx, f ) at \u2713t+1. Since\nx, f ), should be the inverse of the Hessian of log p(\u2713\nthis is intractable, we instead compute the Hessian of the lower bound Eq. (5)\n\n|\n\n|\n\n2\n\u2713L\u2713\n\nHt+1 =\n\n\u0000r\n\nx,f (t))\nh\n\u0000\n\u0000\nThe last term can be approximated since\n\n\u2713t+1\n\n= Ep(xrec|\n\n\u0000r\n\n2\n\n\u2713 log p(xrec|\n\u2713 log qt(\u2713\n2\nx, f )\n\n\u2713, x, f (t))\n\n\u2713 log qt(\u2713\n2\n\u0000r\n\n|\n\nx, f )\n\n\u2713t+1\n\n(9)\n\n|\u2713=\u2713t+1 =\n-term, we introduce a parameter \u21b5, such that the \ufb01nal approximation is\n\n).\n\u2713tk\n\nHt +\n\nr\n\nO\n\n\u0000\n\n(\n\n|\n\ni\u0000\n\u0000\n\u2713t+1 \u0000\n\u0000\nk\n\nTo ef\ufb01ciently cope with the\n\nHt+1 \u21e1\n\n(1\n\n= (1\n\nO\n\u21b5)Ht + Ep(xrec|\nJ\u2713f (t)\n\u2713\n\n\u21b5)Ht \u0000\n\n\u0000\n\n\u0000\n\nx,f )\n\n>\n\nr\n\n\u0000r\n\nh\n2\nxrec log p\n\n2\n\u2713 log p\n\nxrec\n\n\u2713, x, f (t)\n\n\u21e3\nxrec\n\n\u0000\n\u0000\n\n\u2713t+1, x, f (t)\n\n\u2713t+1\n\u2318i \u0000\n\u0000\nJ\u2713f (t)\n\u2713 .\n\u0000\n\n(10)\n\n(11)\n\n\u21e3\nis independent of which \u2713 we evaluate in and\n\nwhere J\u2713f (t)\nis trivial\n\u2713\nto compute for common losses, i.e. for MSE it is the identity. The parameter \u21b5 can be viewed\nas a geometric running average that is useful for smoothing out results computed on a minibatch\ninstead of on the full training set, similar to momentum-like training procedures. It further allows for\nnon-monotonically-increasing precision. Note that we revisit data during training, and the precision\n\n\u2318\n\u2713t+1, x, f (t)\n\n\u0000\n2\n\u0000\nxrec log p\n\nxrec\n\nr\n\n\u0000\n\u0000\n\n\u0000\n\n\u0000\n\n4\n\n\f\n\nf (t) = taylor(f, \u2713t)\n\nt+=1\n\n\n\n\u2713t+1 = \u2713t + \u0000E\u2713\n\nqt [\n\nr\u2713 log p (xrec |\n\n\u21e0\n\n\u2713, x, f )]\n\n\n\nqt\n\n\n\n\u2713t\n\n1\n\n\n\nH \u0000\nt\n\n\n\nHt+1 = (1\n\n\u21b5)Ht \u0000\n\n\u0000\n\nT\n\nJ\u2713f (t)\n\u2713\n\n2\n\nxrec log p(xrec|\n\nr\n\n\u2713, x, f (t)) J\u2713f (t)\n\u2713\n\nt+=1\n\n\u00b5\u0000=0\nFigure 4: Iterative training procedure. Given a distribution qt over parameters, and a linearized\nfunction f (t), compute \ufb01rst and second-order derivatives to update the distribution on parameters.\n\nf^{(t)}=\\textsc{taylor}(f,\\theta^{(t)})\n\nmatrix is updated for every revisit. Thus, the forgetting induced by \u21b5 is a wanted behavior to avoid\nin\ufb01nite precision in the limit of in\ufb01nite training time. In practice, we set \u21b5 equal to the learning rate,\nwhere the practical intuition is that when \u21b5 is small, the network uncertainty decreases faster.\nThe overall training procedure is summarized in Fig. 4. We initialize q0 as a Gaussian with \u27130 = 0\nand H0 = I. We provide more details on the model, the linearization, and iterative learning in\nAppendix D.\n\nWhy not just. . . ? The proposed training procedure may at \ufb01rst appear non-trivial, and it is reasonable\nto wonder if existing methods could be applied to similar results. Variational inference often achieves\nsimilar results to Laplace approximations, so could we use \u2018Bayes by Backprop\u2019 (Blundell et al., 2015)\nto get an alternative Gaussian approximate posterior over \u2713? Similar to the supervised experiences\nof Jospin et al. (2020), we, unfortunately, found this approach too brittle to allow for practical\nmodel \ufb01tting. But then perhaps a post-hoc LA as proposed by Daxberger et al. (2021) for supervised\nlearning? Empirically, we found it to be important to center the approximate posterior around a point\nwhere the Hessian provides useful uncertainty estimates. Our online training moves in this direction\nas the Hessian is part of the procedure, but this is not true for the post-hoc LA.\n\nConceptually, we argue that our approach, while novel, is not entirely separate from existing methods.\nOur reliance on lower bounds makes the method an instance of variational inference (Jordan et al.,\n1999; Opper and Archambeau, 2009b), and we maximize the bounds using Monte Carlo EM (Capp\u00e9,\n2009). We rely on a LA as our choice of variational distribution, which has also been explored by\nPark et al. (2019). Finally, we note that our linearization trick Eq. (6) has great similarities to classic\nextended Kalman \ufb01ltering (Gelb et al., 1974).\n\n\n\nThe following is the appendix D section of the paper you are reviewing:\n\n\nD.1 Overview\n\nLet X = RD be the data space, let Y = RD be the reconstruction space and let \u0398 = RP be\nthe parameter space. Let F = (\u0398 \u2192 (X \u2192 Y)) be the space of operators from \u0398 to the space of\noperators from X to Y. We will denote a function f \u2208 F applied to a parameter \u03b8 \u2208 \u0398 as f\u03b8 : X (cid:55)\u2192 Y.\nThis will represent, for example, a NN f with a specific set of parameter (weights) \u03b8 \u2208 \u0398, that maps\nsome data x \u2208 X to some reconstruction f\u03b8(x) = y \u2208 Y.\nLet X \u00d7 Y \u00d7 \u0398 \u00d7 F be a probability space. The only assumption we make on this space is\n\np(y|x, \u03b8, f ) \u223c N (y|\u00b5 = f\u03b8(x), \u03c32 = \u03a3)\n\n(11)\nwhere \u03a3 \u2208 M(RD \u00d7 RD) is a fixed variance matrix. This is a common assumption for regression\ntasks, and is sometimes referred to as the \u201cdata noise\u201d or \u201creconstruction error\u201d. In this paper, we fix\n\u03a3 = I, but the derivations hold for the general case. With only this assumption, the distribution is\nundefined and multiple solutions can exist. Thus, we require more assumptions.\nA dataset D = {xn} is a finite of infinite collection of data xn \u2208 X that is assumed to follow a\ncertain, fixed but unknown, distribution\n\n\u2200(x, \u03b8, f ) \u2208 X \u00d7 \u0398 \u00d7 F\n\nxn \u223c p(x).\n\n(12)\n\nSacrificing slim notation for the sake of clarity, we introduce an operator I : X (cid:55)\u2192 Y. This represents\nthe ideal reconstruction for a given x. In the standard supervised setting, this would be the operator\n(defined on the dataset only) that maps each data input to its label. In our unsupervised setting, where\nX and Y are the same space, the operator I is simply the identity (Indeed they are not the same space,\nthey are isomorphic spaces that we identify through the operator I). Since I is the identity, it is often\n\n6\n\n\fneglected in the literature, which can lead to unclear and potentially ambiguous Bayesian derivations.\nThus, we choose to adopt this heavier, but more precise notation.\nWe assume access to a specific f N N \u2208 F. Practically this will be our NN architecture, i.e. an operator\nthat, given a set of parameters \u03b8 \u2208 \u0398 gives rise to a function from X to Y. Having f N N fixed, one\nmay consider f not to be stochastic anymore, we choose to still explicitly condition on f in order to\nhave a clearer notation in later stages. Note that, despite not being covered in this work, a proper\nstochastic derivation also on the NN architecture should be feasible.\n\nD.2 Objective\n\nThe NN\u2019s parameter optimization process in this full Bayesian probabilistic framework can be viewed\nas: given a fixed f N N \u2208 F, namely the NN architecture, maximise the reconstruction probability of\nI(xn) over the dataset D\n(cid:104)\n\n(cid:105)\n\nExn\u223cp(x)\n\np(y|xn, f N N )(cid:12)\n\n(cid:12)y=I(xn)\n\n=\n\n(cid:88)\n\np(y|xn, f N N )(cid:12)\n\n(cid:12)y=I(xn),\n\n(13)\n\nxn\u2208D\n\nwhere the untractable p(y|xn, f N N ) can be expanded in \u03b8 and thus related to our hypothesis (11) as\n\np(y|xn, f N N ) = E\u03b8\u223cp(\u03b8|xn,f N N )\n\n(cid:2)p(y|xn, \u03b8, f N N )(cid:3) .\n\n(14)\n\nNotice that the only unfixed quantity is the distribution on the parameters, which we will optimize\nfor. We are not interested in finding a datapoint-dependant distribution, but rather one that maximise\nall reconstructions at the same time, i.e. p(\u03b8|f N N ) = p(\u03b8|xn, f N N ). We can then frame Bayesian\noptimization as: find a distribution on parameters such that\n\nq(\u03b8) \u2208 arg max\n\np(\u03b8|f N N )\n\n= arg max\n\np(\u03b8|f N N )\n\n(cid:88)\n\nxn\u2208D\n(cid:88)\n\nxn\u2208D\n\nE\u03b8\u223cp(\u03b8|f N N )\n\n(cid:104)\n\np(y|xn, \u03b8, f N N )(cid:12)\n\n(cid:12)y=I(xn)\n\n(cid:105)\n\nE\u03b8\u223cp(\u03b8|f N N )\n\n(cid:2)p(I(xn)(cid:12)\n\n(cid:12)N (f N N\n\u03b8\n\n(xn), \u03a3))(cid:3) .\n\n(15)\n\n(16)\n\nMoreover, finding this optimum in the space \u2206(\u0398) of all distributions on \u0398 is not tractable. So, as\ncommonly done, we restrict ourselves to the subset G(\u0398) \u2282 \u2206(\u0398) of Gaussians over \u0398. Then, a\nsolution in our context is\n\nq(\u03b8) \u2208 arg max\nq\u2208G(\u0398)\n\n(cid:88)\n\nxn\u2208D\n\nE\u03b8\u223cq(\u03b8)\n\n(cid:2)p(I(xn)(cid:12)\n\n(cid:12)N (f N N\n\u03b8\n\n(xn), \u03a3))(cid:3) .\n\n(17)\n\nWe emphasize that this solution has no guarantees of being unique, but we are interested in finding\none of them.\n\nD.3\n\nJoint distribution for a fixed datapoint\n\nLet us first get a better understanding of the joint distribution on Y\u00d7\u0398 conditional to a fixed datapoint\nx \u2208 X and a network architecture f \u2208 F\n\nThis distribution has two marginals\n\nand two conditionals\n\np(y, \u03b8|x, f ).\n\np(y|x, f )\n\np(\u03b8|x, f )\n\np(y|\u03b8, x, f )\n\np(\u03b8|y, x, f ).\n\nThese four quantities must satisfy the system of two \u201crecursive\u201d equations\n\np(y|x, f ) =\n\n(cid:90)\n\n\u0398\n\np(y|\u03b8, x, f )p(\u03b8|x, f )d\u03b8 =\n\n= E\u03b8\u223cp(\u03b8|x,f )[p(y|\u03b8, x, f )]\n\n7\n\n(18)\n\n(19)\n\n(20)\n\n(21)\n\n(22)\n\n(23)\n\n\fp(\u03b8|x, f ) =\n\n(cid:90)\n\nY\n\np(\u03b8|y, x, f )p(y|x, f )dy =\n\n= Ey\u223cp(y|x,f )[p(\u03b8|y, x, f )].\n\nIf these are satisfied then the joint is a well-defined distribution and we can apply Bayes rule\n\np(y|\u03b8, x, f )p(\u03b8|x, f ) = p(y, \u03b8|x, f ) = p(\u03b8|y, x, f )p(y|x, f )\n\nwhich in logarithmic form is\n\n(24)\n\n(25)\n\nlog p(y|\u03b8, x, f ) + log p(\u03b8|x, f ) = log p(\u03b8|y, x, f ) + log p(y|x, f ).\n(26)\nWe can factor in the assumptions. The \u201cdata noise\u201d assumption gives us one of the two conditionals:\np(y|\u03b8, x, f ) \u223c N (y|\u00b5 = f\u03b8(x), \u03c32 = \u03a3).\n(27)\n\nThe \u201cGaussian parameter\u201d assumption gives us one of the two marginals:\np(\u03b8|x, f ) = qt(\u03b8) \u223c N (\u03b8|\u00b5 = \u03b8t, \u03c32 = H\u22121\nt ).\nWith these in place, the joint distribution is uniquely defined. The other marginal, by Eq. 23, is\n\np(y|x, f ) = E\u03b8\u223cqt(\u03b8)[p(y|N (f\u03b8(x), \u03a3))]\n\nand the other conditional, by Bayes rule, is\n\np(\u03b8|y, x, f ) =\n\np(y|\u03b8, x, f )p(\u03b8|x, f )\np(y|x, f )\n\n.\n\n(28)\n\n(29)\n\n(30)\n\nDespite being uniquely defined, the integral in Eq. 29 is, with a general f , intractable, and so is the\njoint distribution.\n\nBut why do we even care? The intractability of p(y|x, f ) in Eq. 29 may at first glance appear\nirrelevant. This is the case, for instance, with bayes by backprop (Blundell et al., 2015) methods.\nThey simply need access to the gradient of this quantity. For this purpose, a simple Monte Carlo\nestimate of the expectation is enough.\n\nOn the other hand, we are interested in recovering a meaningful distribution on parameters. This\nimply that we aim at using Eq. 29 to enforce that Eq. 24 holds. For this purpose we need access to\nthe the density p(y|x, f ), so a Monte Carlo estimate of Eq. 29 is not enough.\n\nD.4 Linear f\n\nTheorem 1. Given the data noise assumption from Eq. 27\n\np(y|\u03b8, x, f ) \u223c N (y|\u00b5 = f\u03b8(x), \u03c32 = \u03a3),\ngiven the Gaussian parameter assumption from Eq. 28 for some \u03b8t, Ht\np(\u03b8|x, f ) = qt(\u03b8) \u223c N (\u03b8|\u00b5 = \u03b8t, \u03c32 = H\u22121\n\nt ),\n\nassume that f is linear in \u03b8, i.e.\n\nf\u03b8(x) = f0(x) + Jf\u00b7(x)\u03b8\n\n\u2200\u03b8 \u2208 \u0398, \u2200x \u2208 X.\n\nThen the joint distribution is Gaussian itself\n\np(\u03b8, y|x, f ) \u223c N ((\u03b8, y)|\u00b5t, \u03a3t)\n\nwhere\n\n\u00b5t =\n\n(cid:19)\n\n(cid:18) \u03b8t\nf\u03b8t(x)\n\n, and \u03a3t =\n\n(cid:18) H\u22121\nJf\u00b7(x)\u03a3 (cid:0)Jf\u00b7(x)\u22a4HtJf\u00b7(x)(cid:1)\u22121\n\n\u03a3Jf\u00b7(x)\u22a4\n\nt\n\n(31)\n\n(32)\n\n(33)\n\n(34)\n\n(cid:19)\n\n.\n\n+ \u03a3\n\nProof. With the further assumption of linearity of f , we can explicitly carry out the integral in the\nexpectation in Eq. 29\n\np(y|x, f ) = E\u03b8\u223cqt(\u03b8)[p(y|N (f\u03b8(x), \u03a3))] =\n\n=\n\n=\n\n=\n\n(cid:90)\n\n\u0398\n\n(cid:90)\n\n\u0398\n\n(cid:90)\n\n\u0398\n\np(y|N (f\u03b8(x), \u03a3))qt(\u03b8)d\u03b8 =\n\np(y|N (f\u03b8(x), \u03a3))p(\u03b8|N (\u03b8t, H\u22121\n\nt ))d\u03b8 =\n\np(y|N (f0(x) + Jf\u00b7(x)\u03b8, \u03a3))p(\u03b8|N (\u03b8t, H\u22121\n\nt ))d\u03b8 =\n\nWe emphasize that as a consequence \u22072\n\n\u03b8 log p(y|x, f ) is not dependent on \u03b8t.\n\n= p(y|N (f\u03b8t(x), (Jf\u00b7(x)\u22a4HtJf\u00b7(x))\u22121 + \u03a3)).\n\n8\n\n\fHaving Theorem 1 in place, we can go back to our original problem. We need to deal with a fixed\nnon-linear architecture f N N . We can exploit Theorem 1 by defining f t: a linearization of f N N with\na first-order Taylor expansion around \u03b8t\n\nand it holds that\n\n\u03b8(x) := TAYLOR(f N N , \u03b8t)(x) =\nf t\n\n= f N N\n\u03b8t\n\n(x) + J\u03b8f N N\n\n\u03b8t\n\n(x)(\u03b8 \u2212 \u03b8t)\n\nf N N\n\u03b8\n\n(x) = f t\n\n\u03b8(x) + O(\u2225\u03b8 \u2212 \u03b8t\u22252).\n\nRecalling Eq. 11 both for f N N and for f t\n\np(y|x, \u03b8, f N N ) \u223c N (y|\u00b5 = f N N\n\n\u03b8\n\n(x), \u03c32 = \u03a3)\n\np(y|x, \u03b8, f t) \u223c N (y|\u00b5 = f t\n\n\u03b8(x), \u03c32 = \u03a3)\n\nthat, together with Eq. 36, imply\n\nN (cid:0)y|\u00b5 = f N N\n\n\u03b8\n\n(x), \u03c32 = \u03a3(cid:1) \u223c N (cid:0)y|\u00b5 = f t\n\n\u03b8(x) + O(\u2225\u03b8 \u2212 \u03b8t\u22252), \u03c32 = \u03a3(cid:1),\n\n(35)\n\n(36)\n\n(37)\n\n(38)\n\n(39)\n\nwhere we can interpret the unknown O(\u2225\u03b8 \u2212 \u03b8t\u22252) as \u03b8-dependent noise. More specifically, calling\n\u03b3 > 0 the scalar constant of the O-term, we assume that\n\nO(\u2225\u03b8 \u2212 \u03b8t\u22252) \u2248 \u03f5(\u03b8 \u2212 \u03b8t)2\n\nwhere \u03f5 \u223c N (0, \u03b3I)\n\nand thus, from Eq. 39, we have\n\nN (cid:0)y|\u00b5 = f N N\n\n\u03b8\n\n(x), \u03c32 = \u03a3(cid:1) \u223c N (y|\u00b5 = f t\n\n\u03b8(x), \u03c32 = \u03a3 + \u03b3\u2225\u03b8 \u2212 \u03b8t\u22252I).\n\n(40)\n\n(41)\n\nAt this point, integrals are not analytically tractable, and thus a proper proof is not feasible, the\nintuition is that this increased variance reflects in increased variance in p(\u03b8|x, f N N )\n\n\u22072\n\n\u03b8 log p(\u03b8|x, f N N )(cid:12)\n\n(cid:12)\u03b8=\u03b8t+1\n\n\u2248 \u22072\n\n\u03b8 log p(\u03b8|x, f t)(cid:12)\n\n(cid:12)\u03b8=\u03b8t+1\n\n+ \u03b3\u2225\u03b8t+1 \u2212 \u03b8t\u22252,\n\n(42)\n\nwhere we introduce a hyperparameter \u03b1 > 0 to cope with this added variance. If we then assume the\nJacobian J\u03b8f N N to be a Lipschitz function, then the Lipschitz constant is an upper bound on \u03b3, as\nfollows from the Taylor expansion of Eq. (35). If this Lipschitz constant is smaller than the inverse of\nthe gradient step 1/\u2225\u03b8t+1 \u2212 \u03b8t\u2225 (which is not an unreasonable assumption for the gradient ascent to\nbe stable) we have\n\n\u03b3\u2225\u03b8t+1 \u2212 \u03b8t\u22252 \u2248 \u2225\u03b8t+1 \u2212 \u03b8t\u2225\n\n(43)\n\nthat gives us a plausible order of magnitude for choosing the hyperparameter \u03b1.\nMotivation: During training, we produce a sequence of Gaussians qt(\u03b8) \u223c N (\u03b8t, H\u22121\nt ) that we\nassume to be the distribution p(\u03b8|x, f t), at every step t \u2265 0. This distribution qt is then used for (1)\nGaussian derivation in the linear case, for (2) Monte Carlo sampling in the update rule \u03b8t \u2192 \u03b8t+1\nand (3), as second order derivative, for update rule Ht \u2192 Ht+1.\nMoreover, given that this distribution qt is our \"best guess so far\", we assume it to be also the\ndistribution p(\u03b8|x, f N N ). This, being f N N not linear, (1) cannot be used for Gaussian derivation,\n(2) can reasonably be used for sampling (and thus we derive the improved update rule Eq. (52)), and\n(3) can somehow be used as second order derivative (and thus we derive the improved update rule\nEq. (53)), but the latter requires some more care. That is why we introduce the parameter \u03b1.\n\nD.5\n\nIterative learning\n\nOur learning method produces a sequence of Gaussians\n\nqt(\u03b8) \u223c N (\u03b8|\u00b5 = \u03b8t, \u03c32 = H\u22121\nt )\n\nand a sequence of linearized functions\n\nf t = TAYLOR(f N N , \u03b8t)\n\nfor every t \u2265 0.\n\n9\n\n(44)\n\n(45)\n\n\fInitialization is trivially done using a Gaussian prior on the parameters.\n\n\u03b80 = \u03b8prior\n\nH0 = (\u03a3prior)\u22121\n\n(46)\n\nIterative step is made in two steps. First, having access to \u03b8t, we \u201cgenerate\u201d the linearization f t.\nPractically this is equivalent to computing the two quantities f N N\n(x), that, together\nwith the value \u03b8t are actually equivalent to \u201cgenerating\u201d f t, as Eq. 35 shows.\nSecond, we compute the Gaussian parameters \u03b8t+1 and Ht+1.\n\n(x) and J\u03b8f N N\n\n\u03b8t\n\n\u03b8t\n\nRecalling our aim of maximizing the quantity Eq. (17), update on q(\u00b7) means, \u03b8t \u2192 \u03b8t+1, is ideally\nmade through gradient ascent steps on p(y|x, f )|y=I(x). As this is intractable, we instead do gradient\nsteps on the lower bound Ly of (the log of) Eq. 23\n\n\u03b8t+1 = \u03b8t + \u03bb\u2207\u03b8Ly\n\n(cid:12)\n(cid:12)\n(cid:12)y=I(x)\n\nwhere\n\nand so\n\nLy = E\u03b8\u223cp(\u03b8|x,f t)[log p(y|\u03b8, x, f t)] \u2264 log E\u03b8\u223cp(\u03b8|x,f t)[p(y|\u03b8, x, f t)]\n\n\u2207\u03b8Ly\n\n(cid:12)\n(cid:12)y=I(x) = E\u03b8\u223cp(\u03b8|x,f t)\n\n(cid:104)\n\u2207\u03b8 log p(y|\u03b8, x, f t)(cid:12)\n\n(cid:105)\n\n=\n\n= E\u03b8\u223cqt(\u03b8)\n\n(cid:2)\u2207\u03b8 log p(I(x)(cid:12)\n\n(cid:12)N (f t\n\n(cid:12)y=I(x)\n\u03b8(x), \u03a3))(cid:3) .\n\n(47)\n\n(48)\n\n(49)\n\nRecalling the Laplace approximation Eq. (3), the negative precision, \u2212Ht+1, is ideally set to be the\nthe hessian of the log probability p(\u03b8|x, f ), evaluated in \u03b8t+1. As this is intractable we instead set it\nto the hessian of the lower bound L\u03b8 of (the log of) Eq. (24)\n\nHt+1 = \u2212\u22072\n\n\u03b8L\u03b8\n\n(cid:12)\n(cid:12)\n(cid:12)\u03b8=\u03b8t+1\n\nwhere\n\nand so\n\nL\u03b8 = Ey\u223cp(y|x,f t)[log p(\u03b8|y, x, f t)] \u2264 log Ey\u223cp(y|x,f t)[p(\u03b8|y, x, f t)]\n\n\u22072\n\n\u03b8L\u03b8\n\n(cid:12)\n(cid:12)\n(cid:12)\u03b8=\u03b8t+1\n\n= Ey\u223cp(y|x,f t)\n\n(cid:20)\n\u22072\n\n\u03b8 log p(\u03b8|y, x, f t)\n\n(cid:21)\n\n(cid:12)\n(cid:12)\n(cid:12)\u03b8=\u03b8t+1\n\n(50)\n\n(51)\n\nvia Eq. (26)\n\n= Ey\u223cp(y|x,f t)\n\n(cid:104)\n\n\u22072\n\n\u03b8 log p(y|\u03b8, x, f t)\n\n\u2212 \u22072\n\n(cid:12)\n\u03b8 log p(y|x, f t)\n(cid:12)\n(cid:12)\u03b8=\u03b8t+1\n\n(cid:12)\n(cid:12)\n(cid:12)\u03b8=\u03b8t+1\n(cid:105)\n\n+ \u22072\n\n(cid:12)\n\u03b8 log p(\u03b8|x, f t)\n(cid:12)\n(cid:12)\u03b8=\u03b8t+1\n\n+\n\nvia Theorem 1\n\n= Ey\u223cp(y|x,f t)\n\n(cid:20)\n\u22072\n\n\u03b8 log p(y|\u03b8, x, f t)\n\n(cid:12)\n(cid:12)\n(cid:12)\u03b8=\u03b8t+1\n\n+ \u22072\n\n(cid:12)\n\u03b8 log p(\u03b8|x, f t)\n(cid:12)\n(cid:12)\u03b8=\u03b8t+1\n\n(cid:21)\n\nvia the chain rule Eq. (55)\n\n= Ey\u223cp(y|x,f t)\n\n(cid:20)\nJ\u03b8f t\n\n\u00b7 (x)\u22a4\u22072\n\ny log p(y|\u03b8t+1, x, f t)J\u03b8f t\n\n\u00b7 (x) + \u22072\n\n\u03b8 log p(\u03b8|x, f t)\n\n(cid:21)\n\n(cid:12)\n(cid:12)\n(cid:12)\u03b8=\u03b8t+1\n\nvia HP Eq. (11)\n\n= Ey\u223cp(y|x,f t)\n\n(cid:20)\n\n\u2212J\u03b8f t\n\n\u00b7 (x)\u22a4\u03a3\u22121J\u03b8f t\n\n= \u2212J\u03b8f t\n\n\u00b7 (x)\u22a4\u03a3\u22121J\u03b8f t\n\n\u00b7 (x) + \u22072\n\n\u00b7 (x) + \u22072\n\n(cid:12)\n\u03b8 log p(\u03b8|x, f t)\n(cid:12)\n(cid:12)\u03b8=\u03b8t+1\n(cid:12)\n\u03b8 log p(\u03b8|x, f t)\n(cid:12)\n(cid:12)\u03b8=\u03b8t+1\n\n(cid:21)\n\n= \u2212J\u03b8f t\n\n\u00b7 (x)\u22a4\u03a3\u22121J\u03b8f t\n\n\u00b7 (x) + \u22072\n\n\u03b8 log qt(\u03b8)\n\n= \u2212J\u03b8f t\n\n\u00b7 (x)\u22a4\u03a3\u22121J\u03b8f t\n\n\u00b7 (x) \u2212 Ht.\n\n10\n\n(cid:12)\n(cid:12)\n(cid:12)\u03b8=\u03b8t+1\n\n\fD.5.1\n\nImproved update rule\n\nAs said, the update \u03b8t \u2192 \u03b8t+1 is ideally made through gradient ascent steps on p(y|x, f )|y=I(x),\nbut we instead use the tractable lower bound with f t. We can perform the same derivation using f N N\nin place of f t. Assuming p(\u03b8|x, f N N ) \u223c qt(\u03b8) for sampling, leads to the improved update rule\n\n\u03b8t+1 = \u03b8t + \u03bbE\u03b8\u223cqt(\u03b8)\n\n(cid:2)\u2207\u03b8 log p(I(x)(cid:12)\n\n(cid:12)N (f N N\n\u03b8\n\n(x), \u03a3))(cid:3) .\n\n(52)\n\nSimilarly, the negative precision \u2212Ht+1 is ideally set to be the hessian of the log probability p(\u03b8|x, f ).\nbut instead, we use the tractable lower bound with f t. Here we cannot perform the same derivation,\nsince Theorem 1 does not hold anymore. Instead, we can rely on the estimate Eq. (42) to improve the\nterm\n\u22072\n\n\u2248 \u2212(1 \u2212 \u03b1)Ht,\n\n\u03b8 log p(\u03b8|x, f t)\n\n(cid:12)\n\u03b8 log p(\u03b8|x, f N N )\n(cid:12)\n(cid:12)\u03b8=\u03b8t+1\n\n(cid:12)\n(cid:12)\n(cid:12)\u03b8=\u03b8t+1\n\n= \u2212Ht\n\n\u2212\u2192\n\n\u22072\n\nand this leads to the improved update rule\n\nHt+1 = (1 \u2212 \u03b1)Ht + J\u03b8f t\n\n\u00b7 (x)\u22a4\u03a3\u22121J\u03b8f t\n\n\u00b7 (x).\n\n(53)\n\n\n\nBased on the section(s), please answer the following question with yes, no, or n/a and provide a brief justification for your answer.\nQuestion: If the authors include theoretical results, do the authors include complete proofs of all theoretical results?"}, "3a": {"role": "user", "content": "The following is the experiments section of the paper you are reviewing:\n\n\nFirst, we demonstrate the computational advantages of the proposed Hessian approximation, and\nthen that our sampling-based training leads to well-calibrated uncertainties that can be used for OOD\ndetection, data imputation, and semi-supervised learning. For all the downstream tasks we consider\nthe following baselines: AE (Hinton and Salakhutdinov, 2006) with constant and learned variance, VAE\n(Rezende et al., 2014; Kingma and Welling, 2014), Monte-Carlo dropout AE (Gal and Ghahramani,\n2016) and Ensembles of AE (Lakshminarayanan et al., 2017). We extend StochManDetlefsen et al.\n(2021) with the Hessian backpropagation for the approximate and mixed diagonals. The training code\nis implemented in PyTorch and available2 . Appendix A provides more details on the experimental\nsetup.\n\nEf\ufb01cient Hessian Approximation. For practical applications, training time and memory usage of the\nHessian approximation must be kept low. We here show that the proposed approximate diagonal Hes-\nsian is suf\ufb01cient and even outperforms other approximations when combined with our online training.\n\nFig. 7 show the time and memory requirement for different approximation methods as a function\nof input size for a 5-layer convolutional network that preserves channel and input dimension. As\nbaselines we use ef\ufb01cient implementations of the exact and KFAC approximation (Daxberger et al.,\n3\n2021; Dangel et al., 2020). The exact diagonal approximation run out of memory for an\nimage on a 11 Gb NVIDIA GeForce GTX 1080 Ti. In contrast, our approximate diagonal Hessian\nscales linearly with the resolution, which is especially bene\ufb01cial for convolutional layers.\n\n36\n\n36\n\n\u21e5\n\n\u21e0\n\n\u21e5\n\n2 https://github.com/FrederikWarburg/LaplaceAE\n\n7\n\n\f\u00b5\n\n\u00b5\n\nFigure 9: Receiver Operator Curves (ROC) show that our online LAE is able to discriminate between\n\u00b5\nin and out of distribution in both latent and output space.\nlog p(x)\n\nHessian\n\nMSE\n\n\u00b5\n\n\u00b5\n\n\n\n\u00002\n\n\n\n\u00002\n\n\n\n\u00002\n\n\n\n\u00002\n\n\n\n\u00002\n\nKFAC\nExact\nApprox\nExact\u21e4\nApprox\u21e4\n\n\u0000\n9683.9\n283.3\n232.0\n25.8\n25.9\n\n\u00b1\n\u00b1\n\n#\n\n2455.0\n\u00b1\n88.6\n\u00b1\n65.5\n\u00b1\n0.2\n0.4\n\n#\n\n121.6\n27.1\n26.6\n25.7\n25.8\n\n\u00b1\n\u00b1\n\u00b1\n\u00b1\n\u00b1\n\n24.5\n0.9\n0.6\n0.2\n0.4\n\nApprox\n\nExact\n(a) Post-hoc\n\n\n\nFigure 8: Mean and variance of 100 sampled NN.\n\nKFAC\n\nExact\u21e4\n\nApprox\u21e4\n\nTable 2: Online training (indicated by \u21e4) outper-\nforms post-hoc LA. The approximate diagonal\nhas similar performance to the exact diagonal\nfor both post-hoc and online LA.\nFigure 10: Histograms of variance for in- and OOD reconstructions in output space. Note that\nMC-AE separates the distributions well, but the model assign higher variance to the in- than OOD\ndata.\n\n(b) Online\n\n\n\nTab. 2 shows that the exact or approximate Hessian diagonal has similar performance for both\npost-hoc and online training. Using post-hoc LA results in good mean reconstructions (low MSE),\nbut each sampled NN does not give good reconstructions (low log p(x)). Using our online training\nprocedure results in a much higher log-likelihood. This indicates that every sampled NN predicts\ngood reconstructions.\n\nthe uncertainties more well \ufb01tted, both in latent and data space. These well-\ufb01tted uncertainties have\nseveral practical downstream applications, which we demonstrate next.\n\n218\n\n219\n\n224\n\n225\n\n222\n\n223\n\n221\n\n220\n\nFig. 8 shows the latent representation, mean, and variance of the reconstructions with the KFAC, exact\nand approximate diagonal for both post-hoc and online setup. Note that the online training makes the\nuncertainties better \ufb01tted, both in latent and data space. These well-\ufb01tted uncertainties have several\npractical downstream applications, which we demonstrate next.\n\nOut-of-Distribution (OOD) Detection capabilities are critical for identifying distributional shifts,\noutliers and irregular user inputs, which can hinder the propagation of erroneous decision in an\nautomated system. We evaluate OOD performance on the commonly used benchmarks (Nalisnick\net al., 2019b), where we use FASHIONMNIST (Xiao et al., 2017) as in-distribution and MNIST\n(Lecun et al., 1998) as OOD. Tab. 3 shows that our online LAE outperforms existing models in\nboth log-likelihood and Typicality score (Nalisnick et al., 2019a). This stems from the calibrated\nmodel uncertainties, which are exempli\ufb01ed in the models ability to detect OOD examples from the\nuncertainty deduced in latent and output space; see Fig. 9 for ROC curves.\n\nOut-of-Distribution (OOD) Detection capabilities are critical for identifying distributional shifts,\noutliers, and irregular user inputs, which can hinder the propagation of erroneous decisions in an\nautomated system. We evaluate OOD performance on the commonly used benchmarks (Nalisnick\net al., 2019b), where we use FASHIONMNIST (Xiao et al., 2017) as in-distribution and MNIST\n(Lecun et al., 1998) as OOD. Fig. 9 (c) shows that our online LAE outperforms existing models in\nboth log-likelihood and Typicality score (Nalisnick et al., 2019a). This stems from the calibrated\nmodel uncertainties, which are exempli\ufb01ed in the models ability to detect OOD examples from the\nuncertainty deduced in latent and output space; see Fig. 9 (a,b) for ROC curves.\n\nFig. 10 shows distribution of the output variances for in- and OOD data. This illustrates that using LA\nimproves OoD detection. Furthermore, the online training improves the model calibration.\n\nMissing Data imputation. Another application of stochastic representation learning is to provide\ndistributions over unobserved values (Rezende et al., 2014). In many application domains, sensor\nreadings go missing, which we may mimic by letting parts of an image be unobserved. Rezende et al.\n(2014) show that we can then draw samples from the distribution of the entire image conditioned on\n\nFig. 10 shows distribution of the output variances for in- and OOD data. This illustrates that using LA\nimproves OoD detection. Furthermore, the online training improves the model calibration.\n\n226\n\n227\n\n232\n\n228\n\n229\n\n230\n\n231\n\n233\n\nMethod\n\nlogp(x) Typicality\n\n\u00002\n\noutput\n\n\u00002\n\nlatent\n\nAE MSE\nAE MSE\nAE likelihood\nAE likelihood\nVAE\nVAE\nMC-AE\nMC-AE\nLAE (post-hoc)\nLAE (post-hoc)\nLAE (online)\nLAE (online)\n\n\n\n\n\n0.98\n0.66\n0.54\n0.97\n0.98\n0.98\n\n0.98\n0.70\n0.62\n0.97\n0.98\n0.98\n\n0.65\n0.46\n0.08\n0.74\n0.91\n\n0.5\n0.31\n0.96\n0.96\n\n\n\n\n\nlatent\n\noutput\n\n(b) \u00002\n\n(a) \u00002\n\nTable 3: Out of Distribution detection. In-distribution data FashionMnist and OOD data Mnist.\nArea Under Receiver Operator Curve (AUROC\n) for log-likelihood, typicality score, latent space\n\"\n\u0000latent and output space \u0000output uncertainties. Our proposed LAE ELBO is able to discriminate between\nin and OOD using the deduced variances in latent and output space.\n\nFigure 9: Out of Distribution detection. In-distribution data FashionMnist and OOD data Mnist. (a)\n) for\nand (b) shows the ROC curves for latent and output space uncertainties. (c) shows (AUROC\n\"\nlog-likelihood, typicality score, latent space \u0000latent and output space \u0000output uncertainties. Online LAE\nis able to discriminate between in and OOD using the deduced variances in latent and output space.\n\n) for multiple statistics.\n\"\n\n(c) AUROC (\n\n(a) Latent variance, \u00002\n\n(b) Output variance, \u00002\n\noutput\n\nlatent\n\n\n\n\n\n\n\n8\n\nID\nOoD\n\nID\nOoD\n\n\n\n\n\n\n\n\n\n\n\n(a) AE likelihood\n\n(b) MC-AE\n\n(c) VAE\n\n(d) LAE (post-hoc)\n\n(e) LAE (online)\n\nFigure 10: Histograms of variance for in- and OOD reconstructions in output space. Note that MC-\nAE separates the distributions well, but the model assigns higher variance to the in- than OOD data.\n\n\n\n\n\n\n\n\n\n\n\n(a) AE likelihood\n\n(b) MC-AE\n\n(d) LAE (post-hoc)\n\n(e) LAE (online)\n\n(c) VAE\n8\n\n\f\n\n\n\n\n\n(a) VAE\n\n(b) LAE (post-hoc)\n\n(c) LAE (online)\n\nFigure 11: Missing data imputation & generative capabilities. Online training of LAE improves\nrepresentational robustness. This is exempli\ufb01ed by the multimodal behavior in the data imputation\n(top rows) that accurately model the ambiguity in the data. The bottom two rows show that the LAE\nis able to generate crisp digits from random noise.\n\nMethod\n\nClassi\ufb01er\nVAE\nMC-AE\nAE Ensemble\nLAE (post-hoc)\nLAE (online)\n\nMSE\n\n#\n\nlog p(x)\n\n\"\n\n104.73\n106.05\n96.23\n101.62\n99.59\n\n-104.75\n-106.05\n-100.94\n-107.25\n-106.29\n\n\"\n\nAcc.\n\n0.53\n0.22\n0.45\n0.53\n0.51\n0.53\n\n#\n\nECE\n\n0.16\n0.18\n0.28\n0.12\n0.16\n0.12\n\n#\n\nMCE\n\n0.25\n0.34\n0.38\n0.2\n0.27\n0.16\n\n#\n\nRMSCE\n\n0.18\n0.19\n0.29\n0.13\n0.18\n0.13\n\nTable 3: Reconstruction quality measured by the MSE and log-likelihood for the data imputation.\nOur well-calibrated uncertainties propagates to the MNIST classi\ufb01er and improves the calibration\nmetrics ECE, MCE and RMSCE.\n\nMissing Data imputation. Another application of stochastic representation learning is to provide\ndistributions over unobserved values (Rezende et al., 2014). In many application domains, sensor\nreadings go missing, which we may mimic by letting parts of an image be unobserved. Rezende et al.\n(2014) show that we can then draw samples from the distribution of the entire image conditioned on\nthe observed part, by imputing the missing pixels with noise and repeatedly encode and decode while\nkeeping observed pixels \ufb01xed. Fig. 11 show samples using this procedure from a VAE, a post-hoc\nLAE and our online LAE, where we only observe the lower half of an MNIST image. This implies\nambiguity about the original digit, e.g. the lower half of a \u201c5\u201c could be a \u201c3\u201c and similarly a \u201c7\u201c could\nbe a \u201c9\u201c. Our LAE captures this ambiguity, which is exempli\ufb01ed by the multi-modal reconstructions\nfrom the sampled networks in Fig. 11. The baselines only capture unimodal reconstructions.\n\nCapturing the ambiguity of partly missing data can improve downstream tasks such as the calibration\nof an image classi\ufb01er. In Fig. 11 (c) we demonstrate how averaging the predictions of a simple\nclassi\ufb01er across reconstructions improves standard calibration metrics. This is because the classi\ufb01er\ninherits the uncertainty and ambiguity in the learned representations. A deep ensemble of AEs\nperforms similarly to ours, but comes at the cost of training and storing multiple models.\n\nWhen the entire input image is missing, the imputation procedure can be seen as a sampling mecha-\nnism, such that our LAE can be viewed as a generative model. The bottom rows in Fig. 11 show that\nthe LAE indeed does generate sharp images from a multi-modal distribution.\n\nAttribute\n\nArched Eyebrows\nAttractive\nBald\nWearing Lipstick\nHeavy Makeup\n\nOverall\n\nAE\n\n0.50\n0.52\n0.98\n0.52\n0.45\n\n0.73\n\nVAE MC-AE LAE\u21e4\n\n0.52\n0.50\n0.98\n0.49\n0.52\n\n0.72\n\n0.55\n0.49\n0.98\n0.50\n0.49\n\n0.73\n\n0.60\n0.53\n0.98\n0.54\n0.56\n\n0.74\n\nTable 4: Semi-supervised classi\ufb01cation accuracy\non CELEBA using only 10 labeled datapoints. \u21e4\nrefers to online LAE.\n\n9\n\nFigure 12: Accuracy as an function of the\nnumber of labels per class on MNIST.\n\n\fFigure 13: Sample reconstructions on CELEBA. The top row shows the mean reconstruction and\nthe bottom row shows the variance of the reconstructed images.\n\nSemi-supervised learning combines a small amount of label data with a large amount of unlabeled\ndata. The hope is that the structure in the unlabeled data can be used to infer properties of the data that\ncannot be extracted from a few labeled points. Embedding the same labeled data point multiple times\nusing a stochastic representation scales up the amount of labeled data that is available during training.\n\nFig. 12 shows the accuracy of a K-nearest neighbor classi\ufb01er trained on different amounts of labeled\ndata from the MNIST dataset. For all models with a stochastic encoder, we encode each labeled\ndatapoint 100 times and repeat the experiment 5 times. When only a few labels per class are\navailable (1-20) we clearly observe that our LAE model outperforms all other models, stochastic and\ndeterministic. Increasing the number of labels beyond 100 per class makes the AE and LAE equal in\ntheir classi\ufb01cation performance with the AE model eventually outperforming the LAE model.\n\nIn Tab. 4 we conduct a similar experiment on the CELEBA (Liu et al., 2015) facial dataset, where the\nthe task is to predict 40 different binary labels per data point. When evaluating the overall accuracy of\npredicting all 40 facial attributes, we see no signi\ufb01cant difference in performance. However, when we\nzoom in on speci\ufb01c facial attributes we gain a clear performance advantage over other models. Fig. 13\nshows the mean and variance of \ufb01ve reconstructed images. The online LAE produces well-calibrated\nuncertainties in the output space and scales to large images.\n\nLimitations. Empirically, the LAE improvements are more signi\ufb01cant for overparameterized net-\nworks. The additional capacity seems to help the optimizer \ufb01nd a local mode where a Gaussian \ufb01t is ap-\npropriate. It seems the regularization induced by marginalizing \u2713 compensates for the added \ufb02exibility.\n\n\n\nBased on the section(s), please answer the following question with yes, no, or n/a and provide a brief justification for your answer.\nQuestion: If the authors ran experiments, do the authors include the code, data, and instructions needed to reproduce the main experimental results (either in the supplemental material or as a URL)?"}, "3b": {"role": "user", "content": "The following is the experiments section of the paper you are reviewing:\n\n\nFirst, we demonstrate the computational advantages of the proposed Hessian approximation, and\nthen that our sampling-based training leads to well-calibrated uncertainties that can be used for OOD\ndetection, data imputation, and semi-supervised learning. For all the downstream tasks we consider\nthe following baselines: AE (Hinton and Salakhutdinov, 2006) with constant and learned variance, VAE\n(Rezende et al., 2014; Kingma and Welling, 2014), Monte-Carlo dropout AE (Gal and Ghahramani,\n2016) and Ensembles of AE (Lakshminarayanan et al., 2017). We extend StochManDetlefsen et al.\n(2021) with the Hessian backpropagation for the approximate and mixed diagonals. The training code\nis implemented in PyTorch and available2 . Appendix A provides more details on the experimental\nsetup.\n\nEf\ufb01cient Hessian Approximation. For practical applications, training time and memory usage of the\nHessian approximation must be kept low. We here show that the proposed approximate diagonal Hes-\nsian is suf\ufb01cient and even outperforms other approximations when combined with our online training.\n\nFig. 7 show the time and memory requirement for different approximation methods as a function\nof input size for a 5-layer convolutional network that preserves channel and input dimension. As\nbaselines we use ef\ufb01cient implementations of the exact and KFAC approximation (Daxberger et al.,\n3\n2021; Dangel et al., 2020). The exact diagonal approximation run out of memory for an\nimage on a 11 Gb NVIDIA GeForce GTX 1080 Ti. In contrast, our approximate diagonal Hessian\nscales linearly with the resolution, which is especially bene\ufb01cial for convolutional layers.\n\n36\n\n36\n\n\u21e5\n\n\u21e0\n\n\u21e5\n\n2 https://github.com/FrederikWarburg/LaplaceAE\n\n7\n\n\f\u00b5\n\n\u00b5\n\nFigure 9: Receiver Operator Curves (ROC) show that our online LAE is able to discriminate between\n\u00b5\nin and out of distribution in both latent and output space.\nlog p(x)\n\nHessian\n\nMSE\n\n\u00b5\n\n\u00b5\n\n\n\n\u00002\n\n\n\n\u00002\n\n\n\n\u00002\n\n\n\n\u00002\n\n\n\n\u00002\n\nKFAC\nExact\nApprox\nExact\u21e4\nApprox\u21e4\n\n\u0000\n9683.9\n283.3\n232.0\n25.8\n25.9\n\n\u00b1\n\u00b1\n\n#\n\n2455.0\n\u00b1\n88.6\n\u00b1\n65.5\n\u00b1\n0.2\n0.4\n\n#\n\n121.6\n27.1\n26.6\n25.7\n25.8\n\n\u00b1\n\u00b1\n\u00b1\n\u00b1\n\u00b1\n\n24.5\n0.9\n0.6\n0.2\n0.4\n\nApprox\n\nExact\n(a) Post-hoc\n\n\n\nFigure 8: Mean and variance of 100 sampled NN.\n\nKFAC\n\nExact\u21e4\n\nApprox\u21e4\n\nTable 2: Online training (indicated by \u21e4) outper-\nforms post-hoc LA. The approximate diagonal\nhas similar performance to the exact diagonal\nfor both post-hoc and online LA.\nFigure 10: Histograms of variance for in- and OOD reconstructions in output space. Note that\nMC-AE separates the distributions well, but the model assign higher variance to the in- than OOD\ndata.\n\n(b) Online\n\n\n\nTab. 2 shows that the exact or approximate Hessian diagonal has similar performance for both\npost-hoc and online training. Using post-hoc LA results in good mean reconstructions (low MSE),\nbut each sampled NN does not give good reconstructions (low log p(x)). Using our online training\nprocedure results in a much higher log-likelihood. This indicates that every sampled NN predicts\ngood reconstructions.\n\nthe uncertainties more well \ufb01tted, both in latent and data space. These well-\ufb01tted uncertainties have\nseveral practical downstream applications, which we demonstrate next.\n\n218\n\n219\n\n224\n\n225\n\n222\n\n223\n\n221\n\n220\n\nFig. 8 shows the latent representation, mean, and variance of the reconstructions with the KFAC, exact\nand approximate diagonal for both post-hoc and online setup. Note that the online training makes the\nuncertainties better \ufb01tted, both in latent and data space. These well-\ufb01tted uncertainties have several\npractical downstream applications, which we demonstrate next.\n\nOut-of-Distribution (OOD) Detection capabilities are critical for identifying distributional shifts,\noutliers and irregular user inputs, which can hinder the propagation of erroneous decision in an\nautomated system. We evaluate OOD performance on the commonly used benchmarks (Nalisnick\net al., 2019b), where we use FASHIONMNIST (Xiao et al., 2017) as in-distribution and MNIST\n(Lecun et al., 1998) as OOD. Tab. 3 shows that our online LAE outperforms existing models in\nboth log-likelihood and Typicality score (Nalisnick et al., 2019a). This stems from the calibrated\nmodel uncertainties, which are exempli\ufb01ed in the models ability to detect OOD examples from the\nuncertainty deduced in latent and output space; see Fig. 9 for ROC curves.\n\nOut-of-Distribution (OOD) Detection capabilities are critical for identifying distributional shifts,\noutliers, and irregular user inputs, which can hinder the propagation of erroneous decisions in an\nautomated system. We evaluate OOD performance on the commonly used benchmarks (Nalisnick\net al., 2019b), where we use FASHIONMNIST (Xiao et al., 2017) as in-distribution and MNIST\n(Lecun et al., 1998) as OOD. Fig. 9 (c) shows that our online LAE outperforms existing models in\nboth log-likelihood and Typicality score (Nalisnick et al., 2019a). This stems from the calibrated\nmodel uncertainties, which are exempli\ufb01ed in the models ability to detect OOD examples from the\nuncertainty deduced in latent and output space; see Fig. 9 (a,b) for ROC curves.\n\nFig. 10 shows distribution of the output variances for in- and OOD data. This illustrates that using LA\nimproves OoD detection. Furthermore, the online training improves the model calibration.\n\nMissing Data imputation. Another application of stochastic representation learning is to provide\ndistributions over unobserved values (Rezende et al., 2014). In many application domains, sensor\nreadings go missing, which we may mimic by letting parts of an image be unobserved. Rezende et al.\n(2014) show that we can then draw samples from the distribution of the entire image conditioned on\n\nFig. 10 shows distribution of the output variances for in- and OOD data. This illustrates that using LA\nimproves OoD detection. Furthermore, the online training improves the model calibration.\n\n226\n\n227\n\n232\n\n228\n\n229\n\n230\n\n231\n\n233\n\nMethod\n\nlogp(x) Typicality\n\n\u00002\n\noutput\n\n\u00002\n\nlatent\n\nAE MSE\nAE MSE\nAE likelihood\nAE likelihood\nVAE\nVAE\nMC-AE\nMC-AE\nLAE (post-hoc)\nLAE (post-hoc)\nLAE (online)\nLAE (online)\n\n\n\n\n\n0.98\n0.66\n0.54\n0.97\n0.98\n0.98\n\n0.98\n0.70\n0.62\n0.97\n0.98\n0.98\n\n0.65\n0.46\n0.08\n0.74\n0.91\n\n0.5\n0.31\n0.96\n0.96\n\n\n\n\n\nlatent\n\noutput\n\n(b) \u00002\n\n(a) \u00002\n\nTable 3: Out of Distribution detection. In-distribution data FashionMnist and OOD data Mnist.\nArea Under Receiver Operator Curve (AUROC\n) for log-likelihood, typicality score, latent space\n\"\n\u0000latent and output space \u0000output uncertainties. Our proposed LAE ELBO is able to discriminate between\nin and OOD using the deduced variances in latent and output space.\n\nFigure 9: Out of Distribution detection. In-distribution data FashionMnist and OOD data Mnist. (a)\n) for\nand (b) shows the ROC curves for latent and output space uncertainties. (c) shows (AUROC\n\"\nlog-likelihood, typicality score, latent space \u0000latent and output space \u0000output uncertainties. Online LAE\nis able to discriminate between in and OOD using the deduced variances in latent and output space.\n\n) for multiple statistics.\n\"\n\n(c) AUROC (\n\n(a) Latent variance, \u00002\n\n(b) Output variance, \u00002\n\noutput\n\nlatent\n\n\n\n\n\n\n\n8\n\nID\nOoD\n\nID\nOoD\n\n\n\n\n\n\n\n\n\n\n\n(a) AE likelihood\n\n(b) MC-AE\n\n(c) VAE\n\n(d) LAE (post-hoc)\n\n(e) LAE (online)\n\nFigure 10: Histograms of variance for in- and OOD reconstructions in output space. Note that MC-\nAE separates the distributions well, but the model assigns higher variance to the in- than OOD data.\n\n\n\n\n\n\n\n\n\n\n\n(a) AE likelihood\n\n(b) MC-AE\n\n(d) LAE (post-hoc)\n\n(e) LAE (online)\n\n(c) VAE\n8\n\n\f\n\n\n\n\n\n(a) VAE\n\n(b) LAE (post-hoc)\n\n(c) LAE (online)\n\nFigure 11: Missing data imputation & generative capabilities. Online training of LAE improves\nrepresentational robustness. This is exempli\ufb01ed by the multimodal behavior in the data imputation\n(top rows) that accurately model the ambiguity in the data. The bottom two rows show that the LAE\nis able to generate crisp digits from random noise.\n\nMethod\n\nClassi\ufb01er\nVAE\nMC-AE\nAE Ensemble\nLAE (post-hoc)\nLAE (online)\n\nMSE\n\n#\n\nlog p(x)\n\n\"\n\n104.73\n106.05\n96.23\n101.62\n99.59\n\n-104.75\n-106.05\n-100.94\n-107.25\n-106.29\n\n\"\n\nAcc.\n\n0.53\n0.22\n0.45\n0.53\n0.51\n0.53\n\n#\n\nECE\n\n0.16\n0.18\n0.28\n0.12\n0.16\n0.12\n\n#\n\nMCE\n\n0.25\n0.34\n0.38\n0.2\n0.27\n0.16\n\n#\n\nRMSCE\n\n0.18\n0.19\n0.29\n0.13\n0.18\n0.13\n\nTable 3: Reconstruction quality measured by the MSE and log-likelihood for the data imputation.\nOur well-calibrated uncertainties propagates to the MNIST classi\ufb01er and improves the calibration\nmetrics ECE, MCE and RMSCE.\n\nMissing Data imputation. Another application of stochastic representation learning is to provide\ndistributions over unobserved values (Rezende et al., 2014). In many application domains, sensor\nreadings go missing, which we may mimic by letting parts of an image be unobserved. Rezende et al.\n(2014) show that we can then draw samples from the distribution of the entire image conditioned on\nthe observed part, by imputing the missing pixels with noise and repeatedly encode and decode while\nkeeping observed pixels \ufb01xed. Fig. 11 show samples using this procedure from a VAE, a post-hoc\nLAE and our online LAE, where we only observe the lower half of an MNIST image. This implies\nambiguity about the original digit, e.g. the lower half of a \u201c5\u201c could be a \u201c3\u201c and similarly a \u201c7\u201c could\nbe a \u201c9\u201c. Our LAE captures this ambiguity, which is exempli\ufb01ed by the multi-modal reconstructions\nfrom the sampled networks in Fig. 11. The baselines only capture unimodal reconstructions.\n\nCapturing the ambiguity of partly missing data can improve downstream tasks such as the calibration\nof an image classi\ufb01er. In Fig. 11 (c) we demonstrate how averaging the predictions of a simple\nclassi\ufb01er across reconstructions improves standard calibration metrics. This is because the classi\ufb01er\ninherits the uncertainty and ambiguity in the learned representations. A deep ensemble of AEs\nperforms similarly to ours, but comes at the cost of training and storing multiple models.\n\nWhen the entire input image is missing, the imputation procedure can be seen as a sampling mecha-\nnism, such that our LAE can be viewed as a generative model. The bottom rows in Fig. 11 show that\nthe LAE indeed does generate sharp images from a multi-modal distribution.\n\nAttribute\n\nArched Eyebrows\nAttractive\nBald\nWearing Lipstick\nHeavy Makeup\n\nOverall\n\nAE\n\n0.50\n0.52\n0.98\n0.52\n0.45\n\n0.73\n\nVAE MC-AE LAE\u21e4\n\n0.52\n0.50\n0.98\n0.49\n0.52\n\n0.72\n\n0.55\n0.49\n0.98\n0.50\n0.49\n\n0.73\n\n0.60\n0.53\n0.98\n0.54\n0.56\n\n0.74\n\nTable 4: Semi-supervised classi\ufb01cation accuracy\non CELEBA using only 10 labeled datapoints. \u21e4\nrefers to online LAE.\n\n9\n\nFigure 12: Accuracy as an function of the\nnumber of labels per class on MNIST.\n\n\fFigure 13: Sample reconstructions on CELEBA. The top row shows the mean reconstruction and\nthe bottom row shows the variance of the reconstructed images.\n\nSemi-supervised learning combines a small amount of label data with a large amount of unlabeled\ndata. The hope is that the structure in the unlabeled data can be used to infer properties of the data that\ncannot be extracted from a few labeled points. Embedding the same labeled data point multiple times\nusing a stochastic representation scales up the amount of labeled data that is available during training.\n\nFig. 12 shows the accuracy of a K-nearest neighbor classi\ufb01er trained on different amounts of labeled\ndata from the MNIST dataset. For all models with a stochastic encoder, we encode each labeled\ndatapoint 100 times and repeat the experiment 5 times. When only a few labels per class are\navailable (1-20) we clearly observe that our LAE model outperforms all other models, stochastic and\ndeterministic. Increasing the number of labels beyond 100 per class makes the AE and LAE equal in\ntheir classi\ufb01cation performance with the AE model eventually outperforming the LAE model.\n\nIn Tab. 4 we conduct a similar experiment on the CELEBA (Liu et al., 2015) facial dataset, where the\nthe task is to predict 40 different binary labels per data point. When evaluating the overall accuracy of\npredicting all 40 facial attributes, we see no signi\ufb01cant difference in performance. However, when we\nzoom in on speci\ufb01c facial attributes we gain a clear performance advantage over other models. Fig. 13\nshows the mean and variance of \ufb01ve reconstructed images. The online LAE produces well-calibrated\nuncertainties in the output space and scales to large images.\n\nLimitations. Empirically, the LAE improvements are more signi\ufb01cant for overparameterized net-\nworks. The additional capacity seems to help the optimizer \ufb01nd a local mode where a Gaussian \ufb01t is ap-\npropriate. It seems the regularization induced by marginalizing \u2713 compensates for the added \ufb02exibility.\n\n\n\nThe following is the appendix A section of the paper you are reviewing:\n\n\nAll experiments was conducted on one of the following datasets: MNIST (Lecun et al., 1998),\nFASHIONMNIST (Xiao et al., 2017) and CELEBA (Liu et al., 2015). For training and testing the default\nsplits were used. For validation, we sampled 5000 data points randomly from the training sets. All\nimages were normalized to be in the [0, 1] range and for the CELEBA dataset the images were resized\nto 64 \u00d7 64 additionally.\n\nIf nothing else is stated, the models were trained with the following configuration: we used Adam\noptimizer (Kingma and Ba, 2015b) with a learning rate of 0.001 and default Pytorch settings (Paszke\net al., 2019). The learning rate was adjusted using ReduceLROnPlateau learning rate scheduler with\nparameters factor=0.5 and patience=5, meaning that the learning rate was halved whenever 5 epochs\nhad passed where the validation loss had not decreased. The mean squared error loss was used as the\nreconstruction loss in all models. Models were trained until convergence, defined as whenever the\nvalidation loss had not decreased for 8 epochs. Models trained on MNIST and FASHIONMNIST used a\nbatch size of 64 and on CELEBA a batch size of 128 was used.\n\nModel-specific details:\n\n\u2022 VAE: Models were trained with KL-scaling of 0.001. We use two encoders and two decoders,\nsuch that the model has twice the number of parameters compared to the other models.\n\u2022 MC-AE: Models were trained with dropout between all trainable layers with probability\n\np = 0.2. We keep the same dropout rate during testing.\n\n\u2022 EMSEMBLE-AE: Each ensemble consists of 5 models, each initialized with a different\n\nrandom seed.\n\n\u2022 LAE (POSTHOC): For experiments with linear layers we used the Laplace Redux (Daxberger\net al., 2021) implementation. For convolutions, we found it necessary to use our proposed\nhessian approximation. We use a diagonal approximation of the hessian in all experi-\nments. After fitting the hessian, we optimize for the prior precision using the marginal\nlikelihood (Daxberger et al., 2021). We use 100 MC sampling in all experiments.\n\n1 Denotes equal contribution; author order determined by a simulated coin toss.\n\n36th Conference on Neural Information Processing Systems (NeurIPS 2022).\n\n\f\u2022 LAE (ONLINE): We use the exact diagonal in experiments with linear layers and the mixed\ndiagonal approximation in all experiments with convolutional layers. We use a hessian\nmemory factor of 0.0001 and sample only 1 network per iteration. We found that it was not\nnecessary to optimize for the prior precision when trained online.\n\nA.1 Hessian approximation\n\nWe use a linear encoder-decoder with three layers in the encoder and decoder, TANH activation\nfunctions, and latent size 2. We choose this architecture as Laplace Redux (Daxberger et al., 2021)\nsupports various hessian approximations for this simple network. We use Laplace Redux for all\npost-hoc experiments except for the approximate diagonal hessian.\n\nA.2 Out-of-distribution\n\nWe use a convolutional encoder-decoder architecture. The encoder consisted of a CONV2D, TANH,\nMAXPOOL2D, CONV2D, TANH, MAXPOOL2D, LINEAR, TANH, LINEAR, TANH, LINEAR, where the\ndecoder was mirrored but with nearest neighbour Upsampling rather than MAXPOOL2D. We used a\nlatent size of 2 in these experiments for all models.\n\nA.3 Missing data imputation\n\nTo elaborate on the procedure, we reconstruct 5 samples from the half/fully masked image. For each of\nthese reconstructions, we make 5 more reconstructions and take the average of these reconstructions.\nThe intuition is that the first stage explores the multi-modal behavior of the reconstructions. In the\nsecond stage, the uncertainty of the reconstructed digit is reduced, and each sample will reconstruct\nthe same modality. By averaging over these modalities, we achieve a more crisp reconstruction. We\nuse the same architecture as in the hessian approximation experiment.\n\nA.4 Semi-supervised learning\n\nFor the experiments on MNIST, we use a single convergence checkpoint for each model. We use\nthe same model architecture as in the hessian approximation. We did 5 repetitions for each model\nwhere we first sampled n labels from each of the 10 classes from the validation set, then embedded\nthe 10 \u00d7 n data points into latent space, trained a KNN classifier on the embedded points and finally\nevaluated the accuracy of the classifier on the remaining validation set. This procedure was repeated\nfor different values of n in the [1, 100] range. For the stochastic encoders (VAE, MC-AE, LAE), we\nrepeated the embedding step 100 times with the goal that the uncertainty could help the downstream\nclassification. For the KNN classifier we use cross-validation (K = 2) to find the optimal number of\nnearest neighbors.\n\nFor the experiments on CELEBA we repeated the exact same experiments but with a fixed value\nif n = 10. Additionally, the classifier was changed to a multi-label version KNN-classifier to\naccommodate the multiple binary features in the dataset. For CELEBA we use a convolutional\narchitecture. The encoder consists of 5 convolutional layers with TANH and MAXPOOL2D in between\neach parametric layer. We use a latent size of 64. The decoder mirrors the encoder, but we replace\nMAXPOOL2D with nearest neighbor upsampling.\n\n\n\nBased on the section(s), please answer the following question with yes, no, or n/a and provide a brief justification for your answer.\nQuestion: If the authors ran experiments, do the authors specify all the training details (e.g., data splits, hyperparameters, how they were chosen)?"}, "3c": {"role": "user", "content": "The following is the experiments section of the paper you are reviewing:\n\n\nFirst, we demonstrate the computational advantages of the proposed Hessian approximation, and\nthen that our sampling-based training leads to well-calibrated uncertainties that can be used for OOD\ndetection, data imputation, and semi-supervised learning. For all the downstream tasks we consider\nthe following baselines: AE (Hinton and Salakhutdinov, 2006) with constant and learned variance, VAE\n(Rezende et al., 2014; Kingma and Welling, 2014), Monte-Carlo dropout AE (Gal and Ghahramani,\n2016) and Ensembles of AE (Lakshminarayanan et al., 2017). We extend StochManDetlefsen et al.\n(2021) with the Hessian backpropagation for the approximate and mixed diagonals. The training code\nis implemented in PyTorch and available2 . Appendix A provides more details on the experimental\nsetup.\n\nEf\ufb01cient Hessian Approximation. For practical applications, training time and memory usage of the\nHessian approximation must be kept low. We here show that the proposed approximate diagonal Hes-\nsian is suf\ufb01cient and even outperforms other approximations when combined with our online training.\n\nFig. 7 show the time and memory requirement for different approximation methods as a function\nof input size for a 5-layer convolutional network that preserves channel and input dimension. As\nbaselines we use ef\ufb01cient implementations of the exact and KFAC approximation (Daxberger et al.,\n3\n2021; Dangel et al., 2020). The exact diagonal approximation run out of memory for an\nimage on a 11 Gb NVIDIA GeForce GTX 1080 Ti. In contrast, our approximate diagonal Hessian\nscales linearly with the resolution, which is especially bene\ufb01cial for convolutional layers.\n\n36\n\n36\n\n\u21e5\n\n\u21e0\n\n\u21e5\n\n2 https://github.com/FrederikWarburg/LaplaceAE\n\n7\n\n\f\u00b5\n\n\u00b5\n\nFigure 9: Receiver Operator Curves (ROC) show that our online LAE is able to discriminate between\n\u00b5\nin and out of distribution in both latent and output space.\nlog p(x)\n\nHessian\n\nMSE\n\n\u00b5\n\n\u00b5\n\n\n\n\u00002\n\n\n\n\u00002\n\n\n\n\u00002\n\n\n\n\u00002\n\n\n\n\u00002\n\nKFAC\nExact\nApprox\nExact\u21e4\nApprox\u21e4\n\n\u0000\n9683.9\n283.3\n232.0\n25.8\n25.9\n\n\u00b1\n\u00b1\n\n#\n\n2455.0\n\u00b1\n88.6\n\u00b1\n65.5\n\u00b1\n0.2\n0.4\n\n#\n\n121.6\n27.1\n26.6\n25.7\n25.8\n\n\u00b1\n\u00b1\n\u00b1\n\u00b1\n\u00b1\n\n24.5\n0.9\n0.6\n0.2\n0.4\n\nApprox\n\nExact\n(a) Post-hoc\n\n\n\nFigure 8: Mean and variance of 100 sampled NN.\n\nKFAC\n\nExact\u21e4\n\nApprox\u21e4\n\nTable 2: Online training (indicated by \u21e4) outper-\nforms post-hoc LA. The approximate diagonal\nhas similar performance to the exact diagonal\nfor both post-hoc and online LA.\nFigure 10: Histograms of variance for in- and OOD reconstructions in output space. Note that\nMC-AE separates the distributions well, but the model assign higher variance to the in- than OOD\ndata.\n\n(b) Online\n\n\n\nTab. 2 shows that the exact or approximate Hessian diagonal has similar performance for both\npost-hoc and online training. Using post-hoc LA results in good mean reconstructions (low MSE),\nbut each sampled NN does not give good reconstructions (low log p(x)). Using our online training\nprocedure results in a much higher log-likelihood. This indicates that every sampled NN predicts\ngood reconstructions.\n\nthe uncertainties more well \ufb01tted, both in latent and data space. These well-\ufb01tted uncertainties have\nseveral practical downstream applications, which we demonstrate next.\n\n218\n\n219\n\n224\n\n225\n\n222\n\n223\n\n221\n\n220\n\nFig. 8 shows the latent representation, mean, and variance of the reconstructions with the KFAC, exact\nand approximate diagonal for both post-hoc and online setup. Note that the online training makes the\nuncertainties better \ufb01tted, both in latent and data space. These well-\ufb01tted uncertainties have several\npractical downstream applications, which we demonstrate next.\n\nOut-of-Distribution (OOD) Detection capabilities are critical for identifying distributional shifts,\noutliers and irregular user inputs, which can hinder the propagation of erroneous decision in an\nautomated system. We evaluate OOD performance on the commonly used benchmarks (Nalisnick\net al., 2019b), where we use FASHIONMNIST (Xiao et al., 2017) as in-distribution and MNIST\n(Lecun et al., 1998) as OOD. Tab. 3 shows that our online LAE outperforms existing models in\nboth log-likelihood and Typicality score (Nalisnick et al., 2019a). This stems from the calibrated\nmodel uncertainties, which are exempli\ufb01ed in the models ability to detect OOD examples from the\nuncertainty deduced in latent and output space; see Fig. 9 for ROC curves.\n\nOut-of-Distribution (OOD) Detection capabilities are critical for identifying distributional shifts,\noutliers, and irregular user inputs, which can hinder the propagation of erroneous decisions in an\nautomated system. We evaluate OOD performance on the commonly used benchmarks (Nalisnick\net al., 2019b), where we use FASHIONMNIST (Xiao et al., 2017) as in-distribution and MNIST\n(Lecun et al., 1998) as OOD. Fig. 9 (c) shows that our online LAE outperforms existing models in\nboth log-likelihood and Typicality score (Nalisnick et al., 2019a). This stems from the calibrated\nmodel uncertainties, which are exempli\ufb01ed in the models ability to detect OOD examples from the\nuncertainty deduced in latent and output space; see Fig. 9 (a,b) for ROC curves.\n\nFig. 10 shows distribution of the output variances for in- and OOD data. This illustrates that using LA\nimproves OoD detection. Furthermore, the online training improves the model calibration.\n\nMissing Data imputation. Another application of stochastic representation learning is to provide\ndistributions over unobserved values (Rezende et al., 2014). In many application domains, sensor\nreadings go missing, which we may mimic by letting parts of an image be unobserved. Rezende et al.\n(2014) show that we can then draw samples from the distribution of the entire image conditioned on\n\nFig. 10 shows distribution of the output variances for in- and OOD data. This illustrates that using LA\nimproves OoD detection. Furthermore, the online training improves the model calibration.\n\n226\n\n227\n\n232\n\n228\n\n229\n\n230\n\n231\n\n233\n\nMethod\n\nlogp(x) Typicality\n\n\u00002\n\noutput\n\n\u00002\n\nlatent\n\nAE MSE\nAE MSE\nAE likelihood\nAE likelihood\nVAE\nVAE\nMC-AE\nMC-AE\nLAE (post-hoc)\nLAE (post-hoc)\nLAE (online)\nLAE (online)\n\n\n\n\n\n0.98\n0.66\n0.54\n0.97\n0.98\n0.98\n\n0.98\n0.70\n0.62\n0.97\n0.98\n0.98\n\n0.65\n0.46\n0.08\n0.74\n0.91\n\n0.5\n0.31\n0.96\n0.96\n\n\n\n\n\nlatent\n\noutput\n\n(b) \u00002\n\n(a) \u00002\n\nTable 3: Out of Distribution detection. In-distribution data FashionMnist and OOD data Mnist.\nArea Under Receiver Operator Curve (AUROC\n) for log-likelihood, typicality score, latent space\n\"\n\u0000latent and output space \u0000output uncertainties. Our proposed LAE ELBO is able to discriminate between\nin and OOD using the deduced variances in latent and output space.\n\nFigure 9: Out of Distribution detection. In-distribution data FashionMnist and OOD data Mnist. (a)\n) for\nand (b) shows the ROC curves for latent and output space uncertainties. (c) shows (AUROC\n\"\nlog-likelihood, typicality score, latent space \u0000latent and output space \u0000output uncertainties. Online LAE\nis able to discriminate between in and OOD using the deduced variances in latent and output space.\n\n) for multiple statistics.\n\"\n\n(c) AUROC (\n\n(a) Latent variance, \u00002\n\n(b) Output variance, \u00002\n\noutput\n\nlatent\n\n\n\n\n\n\n\n8\n\nID\nOoD\n\nID\nOoD\n\n\n\n\n\n\n\n\n\n\n\n(a) AE likelihood\n\n(b) MC-AE\n\n(c) VAE\n\n(d) LAE (post-hoc)\n\n(e) LAE (online)\n\nFigure 10: Histograms of variance for in- and OOD reconstructions in output space. Note that MC-\nAE separates the distributions well, but the model assigns higher variance to the in- than OOD data.\n\n\n\n\n\n\n\n\n\n\n\n(a) AE likelihood\n\n(b) MC-AE\n\n(d) LAE (post-hoc)\n\n(e) LAE (online)\n\n(c) VAE\n8\n\n\f\n\n\n\n\n\n(a) VAE\n\n(b) LAE (post-hoc)\n\n(c) LAE (online)\n\nFigure 11: Missing data imputation & generative capabilities. Online training of LAE improves\nrepresentational robustness. This is exempli\ufb01ed by the multimodal behavior in the data imputation\n(top rows) that accurately model the ambiguity in the data. The bottom two rows show that the LAE\nis able to generate crisp digits from random noise.\n\nMethod\n\nClassi\ufb01er\nVAE\nMC-AE\nAE Ensemble\nLAE (post-hoc)\nLAE (online)\n\nMSE\n\n#\n\nlog p(x)\n\n\"\n\n104.73\n106.05\n96.23\n101.62\n99.59\n\n-104.75\n-106.05\n-100.94\n-107.25\n-106.29\n\n\"\n\nAcc.\n\n0.53\n0.22\n0.45\n0.53\n0.51\n0.53\n\n#\n\nECE\n\n0.16\n0.18\n0.28\n0.12\n0.16\n0.12\n\n#\n\nMCE\n\n0.25\n0.34\n0.38\n0.2\n0.27\n0.16\n\n#\n\nRMSCE\n\n0.18\n0.19\n0.29\n0.13\n0.18\n0.13\n\nTable 3: Reconstruction quality measured by the MSE and log-likelihood for the data imputation.\nOur well-calibrated uncertainties propagates to the MNIST classi\ufb01er and improves the calibration\nmetrics ECE, MCE and RMSCE.\n\nMissing Data imputation. Another application of stochastic representation learning is to provide\ndistributions over unobserved values (Rezende et al., 2014). In many application domains, sensor\nreadings go missing, which we may mimic by letting parts of an image be unobserved. Rezende et al.\n(2014) show that we can then draw samples from the distribution of the entire image conditioned on\nthe observed part, by imputing the missing pixels with noise and repeatedly encode and decode while\nkeeping observed pixels \ufb01xed. Fig. 11 show samples using this procedure from a VAE, a post-hoc\nLAE and our online LAE, where we only observe the lower half of an MNIST image. This implies\nambiguity about the original digit, e.g. the lower half of a \u201c5\u201c could be a \u201c3\u201c and similarly a \u201c7\u201c could\nbe a \u201c9\u201c. Our LAE captures this ambiguity, which is exempli\ufb01ed by the multi-modal reconstructions\nfrom the sampled networks in Fig. 11. The baselines only capture unimodal reconstructions.\n\nCapturing the ambiguity of partly missing data can improve downstream tasks such as the calibration\nof an image classi\ufb01er. In Fig. 11 (c) we demonstrate how averaging the predictions of a simple\nclassi\ufb01er across reconstructions improves standard calibration metrics. This is because the classi\ufb01er\ninherits the uncertainty and ambiguity in the learned representations. A deep ensemble of AEs\nperforms similarly to ours, but comes at the cost of training and storing multiple models.\n\nWhen the entire input image is missing, the imputation procedure can be seen as a sampling mecha-\nnism, such that our LAE can be viewed as a generative model. The bottom rows in Fig. 11 show that\nthe LAE indeed does generate sharp images from a multi-modal distribution.\n\nAttribute\n\nArched Eyebrows\nAttractive\nBald\nWearing Lipstick\nHeavy Makeup\n\nOverall\n\nAE\n\n0.50\n0.52\n0.98\n0.52\n0.45\n\n0.73\n\nVAE MC-AE LAE\u21e4\n\n0.52\n0.50\n0.98\n0.49\n0.52\n\n0.72\n\n0.55\n0.49\n0.98\n0.50\n0.49\n\n0.73\n\n0.60\n0.53\n0.98\n0.54\n0.56\n\n0.74\n\nTable 4: Semi-supervised classi\ufb01cation accuracy\non CELEBA using only 10 labeled datapoints. \u21e4\nrefers to online LAE.\n\n9\n\nFigure 12: Accuracy as an function of the\nnumber of labels per class on MNIST.\n\n\fFigure 13: Sample reconstructions on CELEBA. The top row shows the mean reconstruction and\nthe bottom row shows the variance of the reconstructed images.\n\nSemi-supervised learning combines a small amount of label data with a large amount of unlabeled\ndata. The hope is that the structure in the unlabeled data can be used to infer properties of the data that\ncannot be extracted from a few labeled points. Embedding the same labeled data point multiple times\nusing a stochastic representation scales up the amount of labeled data that is available during training.\n\nFig. 12 shows the accuracy of a K-nearest neighbor classi\ufb01er trained on different amounts of labeled\ndata from the MNIST dataset. For all models with a stochastic encoder, we encode each labeled\ndatapoint 100 times and repeat the experiment 5 times. When only a few labels per class are\navailable (1-20) we clearly observe that our LAE model outperforms all other models, stochastic and\ndeterministic. Increasing the number of labels beyond 100 per class makes the AE and LAE equal in\ntheir classi\ufb01cation performance with the AE model eventually outperforming the LAE model.\n\nIn Tab. 4 we conduct a similar experiment on the CELEBA (Liu et al., 2015) facial dataset, where the\nthe task is to predict 40 different binary labels per data point. When evaluating the overall accuracy of\npredicting all 40 facial attributes, we see no signi\ufb01cant difference in performance. However, when we\nzoom in on speci\ufb01c facial attributes we gain a clear performance advantage over other models. Fig. 13\nshows the mean and variance of \ufb01ve reconstructed images. The online LAE produces well-calibrated\nuncertainties in the output space and scales to large images.\n\nLimitations. Empirically, the LAE improvements are more signi\ufb01cant for overparameterized net-\nworks. The additional capacity seems to help the optimizer \ufb01nd a local mode where a Gaussian \ufb01t is ap-\npropriate. It seems the regularization induced by marginalizing \u2713 compensates for the added \ufb02exibility.\n\n\n\nBased on the section(s), please answer the following question with yes, no, or n/a and provide a brief justification for your answer.\nQuestion: If the authors ran experiments, do the authors report error bars (e.g., with respect to the random seed after running experiments multiple times)?"}, "3d": {"role": "user", "content": "The following is the experiments section of the paper you are reviewing:\n\n\nFirst, we demonstrate the computational advantages of the proposed Hessian approximation, and\nthen that our sampling-based training leads to well-calibrated uncertainties that can be used for OOD\ndetection, data imputation, and semi-supervised learning. For all the downstream tasks we consider\nthe following baselines: AE (Hinton and Salakhutdinov, 2006) with constant and learned variance, VAE\n(Rezende et al., 2014; Kingma and Welling, 2014), Monte-Carlo dropout AE (Gal and Ghahramani,\n2016) and Ensembles of AE (Lakshminarayanan et al., 2017). We extend StochManDetlefsen et al.\n(2021) with the Hessian backpropagation for the approximate and mixed diagonals. The training code\nis implemented in PyTorch and available2 . Appendix A provides more details on the experimental\nsetup.\n\nEf\ufb01cient Hessian Approximation. For practical applications, training time and memory usage of the\nHessian approximation must be kept low. We here show that the proposed approximate diagonal Hes-\nsian is suf\ufb01cient and even outperforms other approximations when combined with our online training.\n\nFig. 7 show the time and memory requirement for different approximation methods as a function\nof input size for a 5-layer convolutional network that preserves channel and input dimension. As\nbaselines we use ef\ufb01cient implementations of the exact and KFAC approximation (Daxberger et al.,\n3\n2021; Dangel et al., 2020). The exact diagonal approximation run out of memory for an\nimage on a 11 Gb NVIDIA GeForce GTX 1080 Ti. In contrast, our approximate diagonal Hessian\nscales linearly with the resolution, which is especially bene\ufb01cial for convolutional layers.\n\n36\n\n36\n\n\u21e5\n\n\u21e0\n\n\u21e5\n\n2 https://github.com/FrederikWarburg/LaplaceAE\n\n7\n\n\f\u00b5\n\n\u00b5\n\nFigure 9: Receiver Operator Curves (ROC) show that our online LAE is able to discriminate between\n\u00b5\nin and out of distribution in both latent and output space.\nlog p(x)\n\nHessian\n\nMSE\n\n\u00b5\n\n\u00b5\n\n\n\n\u00002\n\n\n\n\u00002\n\n\n\n\u00002\n\n\n\n\u00002\n\n\n\n\u00002\n\nKFAC\nExact\nApprox\nExact\u21e4\nApprox\u21e4\n\n\u0000\n9683.9\n283.3\n232.0\n25.8\n25.9\n\n\u00b1\n\u00b1\n\n#\n\n2455.0\n\u00b1\n88.6\n\u00b1\n65.5\n\u00b1\n0.2\n0.4\n\n#\n\n121.6\n27.1\n26.6\n25.7\n25.8\n\n\u00b1\n\u00b1\n\u00b1\n\u00b1\n\u00b1\n\n24.5\n0.9\n0.6\n0.2\n0.4\n\nApprox\n\nExact\n(a) Post-hoc\n\n\n\nFigure 8: Mean and variance of 100 sampled NN.\n\nKFAC\n\nExact\u21e4\n\nApprox\u21e4\n\nTable 2: Online training (indicated by \u21e4) outper-\nforms post-hoc LA. The approximate diagonal\nhas similar performance to the exact diagonal\nfor both post-hoc and online LA.\nFigure 10: Histograms of variance for in- and OOD reconstructions in output space. Note that\nMC-AE separates the distributions well, but the model assign higher variance to the in- than OOD\ndata.\n\n(b) Online\n\n\n\nTab. 2 shows that the exact or approximate Hessian diagonal has similar performance for both\npost-hoc and online training. Using post-hoc LA results in good mean reconstructions (low MSE),\nbut each sampled NN does not give good reconstructions (low log p(x)). Using our online training\nprocedure results in a much higher log-likelihood. This indicates that every sampled NN predicts\ngood reconstructions.\n\nthe uncertainties more well \ufb01tted, both in latent and data space. These well-\ufb01tted uncertainties have\nseveral practical downstream applications, which we demonstrate next.\n\n218\n\n219\n\n224\n\n225\n\n222\n\n223\n\n221\n\n220\n\nFig. 8 shows the latent representation, mean, and variance of the reconstructions with the KFAC, exact\nand approximate diagonal for both post-hoc and online setup. Note that the online training makes the\nuncertainties better \ufb01tted, both in latent and data space. These well-\ufb01tted uncertainties have several\npractical downstream applications, which we demonstrate next.\n\nOut-of-Distribution (OOD) Detection capabilities are critical for identifying distributional shifts,\noutliers and irregular user inputs, which can hinder the propagation of erroneous decision in an\nautomated system. We evaluate OOD performance on the commonly used benchmarks (Nalisnick\net al., 2019b), where we use FASHIONMNIST (Xiao et al., 2017) as in-distribution and MNIST\n(Lecun et al., 1998) as OOD. Tab. 3 shows that our online LAE outperforms existing models in\nboth log-likelihood and Typicality score (Nalisnick et al., 2019a). This stems from the calibrated\nmodel uncertainties, which are exempli\ufb01ed in the models ability to detect OOD examples from the\nuncertainty deduced in latent and output space; see Fig. 9 for ROC curves.\n\nOut-of-Distribution (OOD) Detection capabilities are critical for identifying distributional shifts,\noutliers, and irregular user inputs, which can hinder the propagation of erroneous decisions in an\nautomated system. We evaluate OOD performance on the commonly used benchmarks (Nalisnick\net al., 2019b), where we use FASHIONMNIST (Xiao et al., 2017) as in-distribution and MNIST\n(Lecun et al., 1998) as OOD. Fig. 9 (c) shows that our online LAE outperforms existing models in\nboth log-likelihood and Typicality score (Nalisnick et al., 2019a). This stems from the calibrated\nmodel uncertainties, which are exempli\ufb01ed in the models ability to detect OOD examples from the\nuncertainty deduced in latent and output space; see Fig. 9 (a,b) for ROC curves.\n\nFig. 10 shows distribution of the output variances for in- and OOD data. This illustrates that using LA\nimproves OoD detection. Furthermore, the online training improves the model calibration.\n\nMissing Data imputation. Another application of stochastic representation learning is to provide\ndistributions over unobserved values (Rezende et al., 2014). In many application domains, sensor\nreadings go missing, which we may mimic by letting parts of an image be unobserved. Rezende et al.\n(2014) show that we can then draw samples from the distribution of the entire image conditioned on\n\nFig. 10 shows distribution of the output variances for in- and OOD data. This illustrates that using LA\nimproves OoD detection. Furthermore, the online training improves the model calibration.\n\n226\n\n227\n\n232\n\n228\n\n229\n\n230\n\n231\n\n233\n\nMethod\n\nlogp(x) Typicality\n\n\u00002\n\noutput\n\n\u00002\n\nlatent\n\nAE MSE\nAE MSE\nAE likelihood\nAE likelihood\nVAE\nVAE\nMC-AE\nMC-AE\nLAE (post-hoc)\nLAE (post-hoc)\nLAE (online)\nLAE (online)\n\n\n\n\n\n0.98\n0.66\n0.54\n0.97\n0.98\n0.98\n\n0.98\n0.70\n0.62\n0.97\n0.98\n0.98\n\n0.65\n0.46\n0.08\n0.74\n0.91\n\n0.5\n0.31\n0.96\n0.96\n\n\n\n\n\nlatent\n\noutput\n\n(b) \u00002\n\n(a) \u00002\n\nTable 3: Out of Distribution detection. In-distribution data FashionMnist and OOD data Mnist.\nArea Under Receiver Operator Curve (AUROC\n) for log-likelihood, typicality score, latent space\n\"\n\u0000latent and output space \u0000output uncertainties. Our proposed LAE ELBO is able to discriminate between\nin and OOD using the deduced variances in latent and output space.\n\nFigure 9: Out of Distribution detection. In-distribution data FashionMnist and OOD data Mnist. (a)\n) for\nand (b) shows the ROC curves for latent and output space uncertainties. (c) shows (AUROC\n\"\nlog-likelihood, typicality score, latent space \u0000latent and output space \u0000output uncertainties. Online LAE\nis able to discriminate between in and OOD using the deduced variances in latent and output space.\n\n) for multiple statistics.\n\"\n\n(c) AUROC (\n\n(a) Latent variance, \u00002\n\n(b) Output variance, \u00002\n\noutput\n\nlatent\n\n\n\n\n\n\n\n8\n\nID\nOoD\n\nID\nOoD\n\n\n\n\n\n\n\n\n\n\n\n(a) AE likelihood\n\n(b) MC-AE\n\n(c) VAE\n\n(d) LAE (post-hoc)\n\n(e) LAE (online)\n\nFigure 10: Histograms of variance for in- and OOD reconstructions in output space. Note that MC-\nAE separates the distributions well, but the model assigns higher variance to the in- than OOD data.\n\n\n\n\n\n\n\n\n\n\n\n(a) AE likelihood\n\n(b) MC-AE\n\n(d) LAE (post-hoc)\n\n(e) LAE (online)\n\n(c) VAE\n8\n\n\f\n\n\n\n\n\n(a) VAE\n\n(b) LAE (post-hoc)\n\n(c) LAE (online)\n\nFigure 11: Missing data imputation & generative capabilities. Online training of LAE improves\nrepresentational robustness. This is exempli\ufb01ed by the multimodal behavior in the data imputation\n(top rows) that accurately model the ambiguity in the data. The bottom two rows show that the LAE\nis able to generate crisp digits from random noise.\n\nMethod\n\nClassi\ufb01er\nVAE\nMC-AE\nAE Ensemble\nLAE (post-hoc)\nLAE (online)\n\nMSE\n\n#\n\nlog p(x)\n\n\"\n\n104.73\n106.05\n96.23\n101.62\n99.59\n\n-104.75\n-106.05\n-100.94\n-107.25\n-106.29\n\n\"\n\nAcc.\n\n0.53\n0.22\n0.45\n0.53\n0.51\n0.53\n\n#\n\nECE\n\n0.16\n0.18\n0.28\n0.12\n0.16\n0.12\n\n#\n\nMCE\n\n0.25\n0.34\n0.38\n0.2\n0.27\n0.16\n\n#\n\nRMSCE\n\n0.18\n0.19\n0.29\n0.13\n0.18\n0.13\n\nTable 3: Reconstruction quality measured by the MSE and log-likelihood for the data imputation.\nOur well-calibrated uncertainties propagates to the MNIST classi\ufb01er and improves the calibration\nmetrics ECE, MCE and RMSCE.\n\nMissing Data imputation. Another application of stochastic representation learning is to provide\ndistributions over unobserved values (Rezende et al., 2014). In many application domains, sensor\nreadings go missing, which we may mimic by letting parts of an image be unobserved. Rezende et al.\n(2014) show that we can then draw samples from the distribution of the entire image conditioned on\nthe observed part, by imputing the missing pixels with noise and repeatedly encode and decode while\nkeeping observed pixels \ufb01xed. Fig. 11 show samples using this procedure from a VAE, a post-hoc\nLAE and our online LAE, where we only observe the lower half of an MNIST image. This implies\nambiguity about the original digit, e.g. the lower half of a \u201c5\u201c could be a \u201c3\u201c and similarly a \u201c7\u201c could\nbe a \u201c9\u201c. Our LAE captures this ambiguity, which is exempli\ufb01ed by the multi-modal reconstructions\nfrom the sampled networks in Fig. 11. The baselines only capture unimodal reconstructions.\n\nCapturing the ambiguity of partly missing data can improve downstream tasks such as the calibration\nof an image classi\ufb01er. In Fig. 11 (c) we demonstrate how averaging the predictions of a simple\nclassi\ufb01er across reconstructions improves standard calibration metrics. This is because the classi\ufb01er\ninherits the uncertainty and ambiguity in the learned representations. A deep ensemble of AEs\nperforms similarly to ours, but comes at the cost of training and storing multiple models.\n\nWhen the entire input image is missing, the imputation procedure can be seen as a sampling mecha-\nnism, such that our LAE can be viewed as a generative model. The bottom rows in Fig. 11 show that\nthe LAE indeed does generate sharp images from a multi-modal distribution.\n\nAttribute\n\nArched Eyebrows\nAttractive\nBald\nWearing Lipstick\nHeavy Makeup\n\nOverall\n\nAE\n\n0.50\n0.52\n0.98\n0.52\n0.45\n\n0.73\n\nVAE MC-AE LAE\u21e4\n\n0.52\n0.50\n0.98\n0.49\n0.52\n\n0.72\n\n0.55\n0.49\n0.98\n0.50\n0.49\n\n0.73\n\n0.60\n0.53\n0.98\n0.54\n0.56\n\n0.74\n\nTable 4: Semi-supervised classi\ufb01cation accuracy\non CELEBA using only 10 labeled datapoints. \u21e4\nrefers to online LAE.\n\n9\n\nFigure 12: Accuracy as an function of the\nnumber of labels per class on MNIST.\n\n\fFigure 13: Sample reconstructions on CELEBA. The top row shows the mean reconstruction and\nthe bottom row shows the variance of the reconstructed images.\n\nSemi-supervised learning combines a small amount of label data with a large amount of unlabeled\ndata. The hope is that the structure in the unlabeled data can be used to infer properties of the data that\ncannot be extracted from a few labeled points. Embedding the same labeled data point multiple times\nusing a stochastic representation scales up the amount of labeled data that is available during training.\n\nFig. 12 shows the accuracy of a K-nearest neighbor classi\ufb01er trained on different amounts of labeled\ndata from the MNIST dataset. For all models with a stochastic encoder, we encode each labeled\ndatapoint 100 times and repeat the experiment 5 times. When only a few labels per class are\navailable (1-20) we clearly observe that our LAE model outperforms all other models, stochastic and\ndeterministic. Increasing the number of labels beyond 100 per class makes the AE and LAE equal in\ntheir classi\ufb01cation performance with the AE model eventually outperforming the LAE model.\n\nIn Tab. 4 we conduct a similar experiment on the CELEBA (Liu et al., 2015) facial dataset, where the\nthe task is to predict 40 different binary labels per data point. When evaluating the overall accuracy of\npredicting all 40 facial attributes, we see no signi\ufb01cant difference in performance. However, when we\nzoom in on speci\ufb01c facial attributes we gain a clear performance advantage over other models. Fig. 13\nshows the mean and variance of \ufb01ve reconstructed images. The online LAE produces well-calibrated\nuncertainties in the output space and scales to large images.\n\nLimitations. Empirically, the LAE improvements are more signi\ufb01cant for overparameterized net-\nworks. The additional capacity seems to help the optimizer \ufb01nd a local mode where a Gaussian \ufb01t is ap-\npropriate. It seems the regularization induced by marginalizing \u2713 compensates for the added \ufb02exibility.\n\n\n\nBased on the section(s), please answer the following question with yes, no, or n/a and provide a brief justification for your answer.\nQuestion: If the authors ran experiments, do the authors include the total amount of compute and the type of resources used (e.g., type of GPUs, internal cluster, or cloud provider)?"}, "4a": {"role": "user", "content": "The following is the experiments section of the paper you are reviewing:\n\n\nFirst, we demonstrate the computational advantages of the proposed Hessian approximation, and\nthen that our sampling-based training leads to well-calibrated uncertainties that can be used for OOD\ndetection, data imputation, and semi-supervised learning. For all the downstream tasks we consider\nthe following baselines: AE (Hinton and Salakhutdinov, 2006) with constant and learned variance, VAE\n(Rezende et al., 2014; Kingma and Welling, 2014), Monte-Carlo dropout AE (Gal and Ghahramani,\n2016) and Ensembles of AE (Lakshminarayanan et al., 2017). We extend StochManDetlefsen et al.\n(2021) with the Hessian backpropagation for the approximate and mixed diagonals. The training code\nis implemented in PyTorch and available2 . Appendix A provides more details on the experimental\nsetup.\n\nEf\ufb01cient Hessian Approximation. For practical applications, training time and memory usage of the\nHessian approximation must be kept low. We here show that the proposed approximate diagonal Hes-\nsian is suf\ufb01cient and even outperforms other approximations when combined with our online training.\n\nFig. 7 show the time and memory requirement for different approximation methods as a function\nof input size for a 5-layer convolutional network that preserves channel and input dimension. As\nbaselines we use ef\ufb01cient implementations of the exact and KFAC approximation (Daxberger et al.,\n3\n2021; Dangel et al., 2020). The exact diagonal approximation run out of memory for an\nimage on a 11 Gb NVIDIA GeForce GTX 1080 Ti. In contrast, our approximate diagonal Hessian\nscales linearly with the resolution, which is especially bene\ufb01cial for convolutional layers.\n\n36\n\n36\n\n\u21e5\n\n\u21e0\n\n\u21e5\n\n2 https://github.com/FrederikWarburg/LaplaceAE\n\n7\n\n\f\u00b5\n\n\u00b5\n\nFigure 9: Receiver Operator Curves (ROC) show that our online LAE is able to discriminate between\n\u00b5\nin and out of distribution in both latent and output space.\nlog p(x)\n\nHessian\n\nMSE\n\n\u00b5\n\n\u00b5\n\n\n\n\u00002\n\n\n\n\u00002\n\n\n\n\u00002\n\n\n\n\u00002\n\n\n\n\u00002\n\nKFAC\nExact\nApprox\nExact\u21e4\nApprox\u21e4\n\n\u0000\n9683.9\n283.3\n232.0\n25.8\n25.9\n\n\u00b1\n\u00b1\n\n#\n\n2455.0\n\u00b1\n88.6\n\u00b1\n65.5\n\u00b1\n0.2\n0.4\n\n#\n\n121.6\n27.1\n26.6\n25.7\n25.8\n\n\u00b1\n\u00b1\n\u00b1\n\u00b1\n\u00b1\n\n24.5\n0.9\n0.6\n0.2\n0.4\n\nApprox\n\nExact\n(a) Post-hoc\n\n\n\nFigure 8: Mean and variance of 100 sampled NN.\n\nKFAC\n\nExact\u21e4\n\nApprox\u21e4\n\nTable 2: Online training (indicated by \u21e4) outper-\nforms post-hoc LA. The approximate diagonal\nhas similar performance to the exact diagonal\nfor both post-hoc and online LA.\nFigure 10: Histograms of variance for in- and OOD reconstructions in output space. Note that\nMC-AE separates the distributions well, but the model assign higher variance to the in- than OOD\ndata.\n\n(b) Online\n\n\n\nTab. 2 shows that the exact or approximate Hessian diagonal has similar performance for both\npost-hoc and online training. Using post-hoc LA results in good mean reconstructions (low MSE),\nbut each sampled NN does not give good reconstructions (low log p(x)). Using our online training\nprocedure results in a much higher log-likelihood. This indicates that every sampled NN predicts\ngood reconstructions.\n\nthe uncertainties more well \ufb01tted, both in latent and data space. These well-\ufb01tted uncertainties have\nseveral practical downstream applications, which we demonstrate next.\n\n218\n\n219\n\n224\n\n225\n\n222\n\n223\n\n221\n\n220\n\nFig. 8 shows the latent representation, mean, and variance of the reconstructions with the KFAC, exact\nand approximate diagonal for both post-hoc and online setup. Note that the online training makes the\nuncertainties better \ufb01tted, both in latent and data space. These well-\ufb01tted uncertainties have several\npractical downstream applications, which we demonstrate next.\n\nOut-of-Distribution (OOD) Detection capabilities are critical for identifying distributional shifts,\noutliers and irregular user inputs, which can hinder the propagation of erroneous decision in an\nautomated system. We evaluate OOD performance on the commonly used benchmarks (Nalisnick\net al., 2019b), where we use FASHIONMNIST (Xiao et al., 2017) as in-distribution and MNIST\n(Lecun et al., 1998) as OOD. Tab. 3 shows that our online LAE outperforms existing models in\nboth log-likelihood and Typicality score (Nalisnick et al., 2019a). This stems from the calibrated\nmodel uncertainties, which are exempli\ufb01ed in the models ability to detect OOD examples from the\nuncertainty deduced in latent and output space; see Fig. 9 for ROC curves.\n\nOut-of-Distribution (OOD) Detection capabilities are critical for identifying distributional shifts,\noutliers, and irregular user inputs, which can hinder the propagation of erroneous decisions in an\nautomated system. We evaluate OOD performance on the commonly used benchmarks (Nalisnick\net al., 2019b), where we use FASHIONMNIST (Xiao et al., 2017) as in-distribution and MNIST\n(Lecun et al., 1998) as OOD. Fig. 9 (c) shows that our online LAE outperforms existing models in\nboth log-likelihood and Typicality score (Nalisnick et al., 2019a). This stems from the calibrated\nmodel uncertainties, which are exempli\ufb01ed in the models ability to detect OOD examples from the\nuncertainty deduced in latent and output space; see Fig. 9 (a,b) for ROC curves.\n\nFig. 10 shows distribution of the output variances for in- and OOD data. This illustrates that using LA\nimproves OoD detection. Furthermore, the online training improves the model calibration.\n\nMissing Data imputation. Another application of stochastic representation learning is to provide\ndistributions over unobserved values (Rezende et al., 2014). In many application domains, sensor\nreadings go missing, which we may mimic by letting parts of an image be unobserved. Rezende et al.\n(2014) show that we can then draw samples from the distribution of the entire image conditioned on\n\nFig. 10 shows distribution of the output variances for in- and OOD data. This illustrates that using LA\nimproves OoD detection. Furthermore, the online training improves the model calibration.\n\n226\n\n227\n\n232\n\n228\n\n229\n\n230\n\n231\n\n233\n\nMethod\n\nlogp(x) Typicality\n\n\u00002\n\noutput\n\n\u00002\n\nlatent\n\nAE MSE\nAE MSE\nAE likelihood\nAE likelihood\nVAE\nVAE\nMC-AE\nMC-AE\nLAE (post-hoc)\nLAE (post-hoc)\nLAE (online)\nLAE (online)\n\n\n\n\n\n0.98\n0.66\n0.54\n0.97\n0.98\n0.98\n\n0.98\n0.70\n0.62\n0.97\n0.98\n0.98\n\n0.65\n0.46\n0.08\n0.74\n0.91\n\n0.5\n0.31\n0.96\n0.96\n\n\n\n\n\nlatent\n\noutput\n\n(b) \u00002\n\n(a) \u00002\n\nTable 3: Out of Distribution detection. In-distribution data FashionMnist and OOD data Mnist.\nArea Under Receiver Operator Curve (AUROC\n) for log-likelihood, typicality score, latent space\n\"\n\u0000latent and output space \u0000output uncertainties. Our proposed LAE ELBO is able to discriminate between\nin and OOD using the deduced variances in latent and output space.\n\nFigure 9: Out of Distribution detection. In-distribution data FashionMnist and OOD data Mnist. (a)\n) for\nand (b) shows the ROC curves for latent and output space uncertainties. (c) shows (AUROC\n\"\nlog-likelihood, typicality score, latent space \u0000latent and output space \u0000output uncertainties. Online LAE\nis able to discriminate between in and OOD using the deduced variances in latent and output space.\n\n) for multiple statistics.\n\"\n\n(c) AUROC (\n\n(a) Latent variance, \u00002\n\n(b) Output variance, \u00002\n\noutput\n\nlatent\n\n\n\n\n\n\n\n8\n\nID\nOoD\n\nID\nOoD\n\n\n\n\n\n\n\n\n\n\n\n(a) AE likelihood\n\n(b) MC-AE\n\n(c) VAE\n\n(d) LAE (post-hoc)\n\n(e) LAE (online)\n\nFigure 10: Histograms of variance for in- and OOD reconstructions in output space. Note that MC-\nAE separates the distributions well, but the model assigns higher variance to the in- than OOD data.\n\n\n\n\n\n\n\n\n\n\n\n(a) AE likelihood\n\n(b) MC-AE\n\n(d) LAE (post-hoc)\n\n(e) LAE (online)\n\n(c) VAE\n8\n\n\f\n\n\n\n\n\n(a) VAE\n\n(b) LAE (post-hoc)\n\n(c) LAE (online)\n\nFigure 11: Missing data imputation & generative capabilities. Online training of LAE improves\nrepresentational robustness. This is exempli\ufb01ed by the multimodal behavior in the data imputation\n(top rows) that accurately model the ambiguity in the data. The bottom two rows show that the LAE\nis able to generate crisp digits from random noise.\n\nMethod\n\nClassi\ufb01er\nVAE\nMC-AE\nAE Ensemble\nLAE (post-hoc)\nLAE (online)\n\nMSE\n\n#\n\nlog p(x)\n\n\"\n\n104.73\n106.05\n96.23\n101.62\n99.59\n\n-104.75\n-106.05\n-100.94\n-107.25\n-106.29\n\n\"\n\nAcc.\n\n0.53\n0.22\n0.45\n0.53\n0.51\n0.53\n\n#\n\nECE\n\n0.16\n0.18\n0.28\n0.12\n0.16\n0.12\n\n#\n\nMCE\n\n0.25\n0.34\n0.38\n0.2\n0.27\n0.16\n\n#\n\nRMSCE\n\n0.18\n0.19\n0.29\n0.13\n0.18\n0.13\n\nTable 3: Reconstruction quality measured by the MSE and log-likelihood for the data imputation.\nOur well-calibrated uncertainties propagates to the MNIST classi\ufb01er and improves the calibration\nmetrics ECE, MCE and RMSCE.\n\nMissing Data imputation. Another application of stochastic representation learning is to provide\ndistributions over unobserved values (Rezende et al., 2014). In many application domains, sensor\nreadings go missing, which we may mimic by letting parts of an image be unobserved. Rezende et al.\n(2014) show that we can then draw samples from the distribution of the entire image conditioned on\nthe observed part, by imputing the missing pixels with noise and repeatedly encode and decode while\nkeeping observed pixels \ufb01xed. Fig. 11 show samples using this procedure from a VAE, a post-hoc\nLAE and our online LAE, where we only observe the lower half of an MNIST image. This implies\nambiguity about the original digit, e.g. the lower half of a \u201c5\u201c could be a \u201c3\u201c and similarly a \u201c7\u201c could\nbe a \u201c9\u201c. Our LAE captures this ambiguity, which is exempli\ufb01ed by the multi-modal reconstructions\nfrom the sampled networks in Fig. 11. The baselines only capture unimodal reconstructions.\n\nCapturing the ambiguity of partly missing data can improve downstream tasks such as the calibration\nof an image classi\ufb01er. In Fig. 11 (c) we demonstrate how averaging the predictions of a simple\nclassi\ufb01er across reconstructions improves standard calibration metrics. This is because the classi\ufb01er\ninherits the uncertainty and ambiguity in the learned representations. A deep ensemble of AEs\nperforms similarly to ours, but comes at the cost of training and storing multiple models.\n\nWhen the entire input image is missing, the imputation procedure can be seen as a sampling mecha-\nnism, such that our LAE can be viewed as a generative model. The bottom rows in Fig. 11 show that\nthe LAE indeed does generate sharp images from a multi-modal distribution.\n\nAttribute\n\nArched Eyebrows\nAttractive\nBald\nWearing Lipstick\nHeavy Makeup\n\nOverall\n\nAE\n\n0.50\n0.52\n0.98\n0.52\n0.45\n\n0.73\n\nVAE MC-AE LAE\u21e4\n\n0.52\n0.50\n0.98\n0.49\n0.52\n\n0.72\n\n0.55\n0.49\n0.98\n0.50\n0.49\n\n0.73\n\n0.60\n0.53\n0.98\n0.54\n0.56\n\n0.74\n\nTable 4: Semi-supervised classi\ufb01cation accuracy\non CELEBA using only 10 labeled datapoints. \u21e4\nrefers to online LAE.\n\n9\n\nFigure 12: Accuracy as an function of the\nnumber of labels per class on MNIST.\n\n\fFigure 13: Sample reconstructions on CELEBA. The top row shows the mean reconstruction and\nthe bottom row shows the variance of the reconstructed images.\n\nSemi-supervised learning combines a small amount of label data with a large amount of unlabeled\ndata. The hope is that the structure in the unlabeled data can be used to infer properties of the data that\ncannot be extracted from a few labeled points. Embedding the same labeled data point multiple times\nusing a stochastic representation scales up the amount of labeled data that is available during training.\n\nFig. 12 shows the accuracy of a K-nearest neighbor classi\ufb01er trained on different amounts of labeled\ndata from the MNIST dataset. For all models with a stochastic encoder, we encode each labeled\ndatapoint 100 times and repeat the experiment 5 times. When only a few labels per class are\navailable (1-20) we clearly observe that our LAE model outperforms all other models, stochastic and\ndeterministic. Increasing the number of labels beyond 100 per class makes the AE and LAE equal in\ntheir classi\ufb01cation performance with the AE model eventually outperforming the LAE model.\n\nIn Tab. 4 we conduct a similar experiment on the CELEBA (Liu et al., 2015) facial dataset, where the\nthe task is to predict 40 different binary labels per data point. When evaluating the overall accuracy of\npredicting all 40 facial attributes, we see no signi\ufb01cant difference in performance. However, when we\nzoom in on speci\ufb01c facial attributes we gain a clear performance advantage over other models. Fig. 13\nshows the mean and variance of \ufb01ve reconstructed images. The online LAE produces well-calibrated\nuncertainties in the output space and scales to large images.\n\nLimitations. Empirically, the LAE improvements are more signi\ufb01cant for overparameterized net-\nworks. The additional capacity seems to help the optimizer \ufb01nd a local mode where a Gaussian \ufb01t is ap-\npropriate. It seems the regularization induced by marginalizing \u2713 compensates for the added \ufb02exibility.\n\n\n\nThe following is the appendix A section of the paper you are reviewing:\n\n\nAll experiments was conducted on one of the following datasets: MNIST (Lecun et al., 1998),\nFASHIONMNIST (Xiao et al., 2017) and CELEBA (Liu et al., 2015). For training and testing the default\nsplits were used. For validation, we sampled 5000 data points randomly from the training sets. All\nimages were normalized to be in the [0, 1] range and for the CELEBA dataset the images were resized\nto 64 \u00d7 64 additionally.\n\nIf nothing else is stated, the models were trained with the following configuration: we used Adam\noptimizer (Kingma and Ba, 2015b) with a learning rate of 0.001 and default Pytorch settings (Paszke\net al., 2019). The learning rate was adjusted using ReduceLROnPlateau learning rate scheduler with\nparameters factor=0.5 and patience=5, meaning that the learning rate was halved whenever 5 epochs\nhad passed where the validation loss had not decreased. The mean squared error loss was used as the\nreconstruction loss in all models. Models were trained until convergence, defined as whenever the\nvalidation loss had not decreased for 8 epochs. Models trained on MNIST and FASHIONMNIST used a\nbatch size of 64 and on CELEBA a batch size of 128 was used.\n\nModel-specific details:\n\n\u2022 VAE: Models were trained with KL-scaling of 0.001. We use two encoders and two decoders,\nsuch that the model has twice the number of parameters compared to the other models.\n\u2022 MC-AE: Models were trained with dropout between all trainable layers with probability\n\np = 0.2. We keep the same dropout rate during testing.\n\n\u2022 EMSEMBLE-AE: Each ensemble consists of 5 models, each initialized with a different\n\nrandom seed.\n\n\u2022 LAE (POSTHOC): For experiments with linear layers we used the Laplace Redux (Daxberger\net al., 2021) implementation. For convolutions, we found it necessary to use our proposed\nhessian approximation. We use a diagonal approximation of the hessian in all experi-\nments. After fitting the hessian, we optimize for the prior precision using the marginal\nlikelihood (Daxberger et al., 2021). We use 100 MC sampling in all experiments.\n\n1 Denotes equal contribution; author order determined by a simulated coin toss.\n\n36th Conference on Neural Information Processing Systems (NeurIPS 2022).\n\n\f\u2022 LAE (ONLINE): We use the exact diagonal in experiments with linear layers and the mixed\ndiagonal approximation in all experiments with convolutional layers. We use a hessian\nmemory factor of 0.0001 and sample only 1 network per iteration. We found that it was not\nnecessary to optimize for the prior precision when trained online.\n\nA.1 Hessian approximation\n\nWe use a linear encoder-decoder with three layers in the encoder and decoder, TANH activation\nfunctions, and latent size 2. We choose this architecture as Laplace Redux (Daxberger et al., 2021)\nsupports various hessian approximations for this simple network. We use Laplace Redux for all\npost-hoc experiments except for the approximate diagonal hessian.\n\nA.2 Out-of-distribution\n\nWe use a convolutional encoder-decoder architecture. The encoder consisted of a CONV2D, TANH,\nMAXPOOL2D, CONV2D, TANH, MAXPOOL2D, LINEAR, TANH, LINEAR, TANH, LINEAR, where the\ndecoder was mirrored but with nearest neighbour Upsampling rather than MAXPOOL2D. We used a\nlatent size of 2 in these experiments for all models.\n\nA.3 Missing data imputation\n\nTo elaborate on the procedure, we reconstruct 5 samples from the half/fully masked image. For each of\nthese reconstructions, we make 5 more reconstructions and take the average of these reconstructions.\nThe intuition is that the first stage explores the multi-modal behavior of the reconstructions. In the\nsecond stage, the uncertainty of the reconstructed digit is reduced, and each sample will reconstruct\nthe same modality. By averaging over these modalities, we achieve a more crisp reconstruction. We\nuse the same architecture as in the hessian approximation experiment.\n\nA.4 Semi-supervised learning\n\nFor the experiments on MNIST, we use a single convergence checkpoint for each model. We use\nthe same model architecture as in the hessian approximation. We did 5 repetitions for each model\nwhere we first sampled n labels from each of the 10 classes from the validation set, then embedded\nthe 10 \u00d7 n data points into latent space, trained a KNN classifier on the embedded points and finally\nevaluated the accuracy of the classifier on the remaining validation set. This procedure was repeated\nfor different values of n in the [1, 100] range. For the stochastic encoders (VAE, MC-AE, LAE), we\nrepeated the embedding step 100 times with the goal that the uncertainty could help the downstream\nclassification. For the KNN classifier we use cross-validation (K = 2) to find the optimal number of\nnearest neighbors.\n\nFor the experiments on CELEBA we repeated the exact same experiments but with a fixed value\nif n = 10. Additionally, the classifier was changed to a multi-label version KNN-classifier to\naccommodate the multiple binary features in the dataset. For CELEBA we use a convolutional\narchitecture. The encoder consists of 5 convolutional layers with TANH and MAXPOOL2D in between\neach parametric layer. We use a latent size of 64. The decoder mirrors the encoder, but we replace\nMAXPOOL2D with nearest neighbor upsampling.\n\n\n\nBased on the section(s), please answer the following question with yes, no, or n/a and provide a brief justification for your answer.\nQuestion: If the authors use existing assets (e.g., code, data, models), do the authors cite the creators?"}, "4b": {"role": "user", "content": "The following is the experiments section of the paper you are reviewing:\n\n\nFirst, we demonstrate the computational advantages of the proposed Hessian approximation, and\nthen that our sampling-based training leads to well-calibrated uncertainties that can be used for OOD\ndetection, data imputation, and semi-supervised learning. For all the downstream tasks we consider\nthe following baselines: AE (Hinton and Salakhutdinov, 2006) with constant and learned variance, VAE\n(Rezende et al., 2014; Kingma and Welling, 2014), Monte-Carlo dropout AE (Gal and Ghahramani,\n2016) and Ensembles of AE (Lakshminarayanan et al., 2017). We extend StochManDetlefsen et al.\n(2021) with the Hessian backpropagation for the approximate and mixed diagonals. The training code\nis implemented in PyTorch and available2 . Appendix A provides more details on the experimental\nsetup.\n\nEf\ufb01cient Hessian Approximation. For practical applications, training time and memory usage of the\nHessian approximation must be kept low. We here show that the proposed approximate diagonal Hes-\nsian is suf\ufb01cient and even outperforms other approximations when combined with our online training.\n\nFig. 7 show the time and memory requirement for different approximation methods as a function\nof input size for a 5-layer convolutional network that preserves channel and input dimension. As\nbaselines we use ef\ufb01cient implementations of the exact and KFAC approximation (Daxberger et al.,\n3\n2021; Dangel et al., 2020). The exact diagonal approximation run out of memory for an\nimage on a 11 Gb NVIDIA GeForce GTX 1080 Ti. In contrast, our approximate diagonal Hessian\nscales linearly with the resolution, which is especially bene\ufb01cial for convolutional layers.\n\n36\n\n36\n\n\u21e5\n\n\u21e0\n\n\u21e5\n\n2 https://github.com/FrederikWarburg/LaplaceAE\n\n7\n\n\f\u00b5\n\n\u00b5\n\nFigure 9: Receiver Operator Curves (ROC) show that our online LAE is able to discriminate between\n\u00b5\nin and out of distribution in both latent and output space.\nlog p(x)\n\nHessian\n\nMSE\n\n\u00b5\n\n\u00b5\n\n\n\n\u00002\n\n\n\n\u00002\n\n\n\n\u00002\n\n\n\n\u00002\n\n\n\n\u00002\n\nKFAC\nExact\nApprox\nExact\u21e4\nApprox\u21e4\n\n\u0000\n9683.9\n283.3\n232.0\n25.8\n25.9\n\n\u00b1\n\u00b1\n\n#\n\n2455.0\n\u00b1\n88.6\n\u00b1\n65.5\n\u00b1\n0.2\n0.4\n\n#\n\n121.6\n27.1\n26.6\n25.7\n25.8\n\n\u00b1\n\u00b1\n\u00b1\n\u00b1\n\u00b1\n\n24.5\n0.9\n0.6\n0.2\n0.4\n\nApprox\n\nExact\n(a) Post-hoc\n\n\n\nFigure 8: Mean and variance of 100 sampled NN.\n\nKFAC\n\nExact\u21e4\n\nApprox\u21e4\n\nTable 2: Online training (indicated by \u21e4) outper-\nforms post-hoc LA. The approximate diagonal\nhas similar performance to the exact diagonal\nfor both post-hoc and online LA.\nFigure 10: Histograms of variance for in- and OOD reconstructions in output space. Note that\nMC-AE separates the distributions well, but the model assign higher variance to the in- than OOD\ndata.\n\n(b) Online\n\n\n\nTab. 2 shows that the exact or approximate Hessian diagonal has similar performance for both\npost-hoc and online training. Using post-hoc LA results in good mean reconstructions (low MSE),\nbut each sampled NN does not give good reconstructions (low log p(x)). Using our online training\nprocedure results in a much higher log-likelihood. This indicates that every sampled NN predicts\ngood reconstructions.\n\nthe uncertainties more well \ufb01tted, both in latent and data space. These well-\ufb01tted uncertainties have\nseveral practical downstream applications, which we demonstrate next.\n\n218\n\n219\n\n224\n\n225\n\n222\n\n223\n\n221\n\n220\n\nFig. 8 shows the latent representation, mean, and variance of the reconstructions with the KFAC, exact\nand approximate diagonal for both post-hoc and online setup. Note that the online training makes the\nuncertainties better \ufb01tted, both in latent and data space. These well-\ufb01tted uncertainties have several\npractical downstream applications, which we demonstrate next.\n\nOut-of-Distribution (OOD) Detection capabilities are critical for identifying distributional shifts,\noutliers and irregular user inputs, which can hinder the propagation of erroneous decision in an\nautomated system. We evaluate OOD performance on the commonly used benchmarks (Nalisnick\net al., 2019b), where we use FASHIONMNIST (Xiao et al., 2017) as in-distribution and MNIST\n(Lecun et al., 1998) as OOD. Tab. 3 shows that our online LAE outperforms existing models in\nboth log-likelihood and Typicality score (Nalisnick et al., 2019a). This stems from the calibrated\nmodel uncertainties, which are exempli\ufb01ed in the models ability to detect OOD examples from the\nuncertainty deduced in latent and output space; see Fig. 9 for ROC curves.\n\nOut-of-Distribution (OOD) Detection capabilities are critical for identifying distributional shifts,\noutliers, and irregular user inputs, which can hinder the propagation of erroneous decisions in an\nautomated system. We evaluate OOD performance on the commonly used benchmarks (Nalisnick\net al., 2019b), where we use FASHIONMNIST (Xiao et al., 2017) as in-distribution and MNIST\n(Lecun et al., 1998) as OOD. Fig. 9 (c) shows that our online LAE outperforms existing models in\nboth log-likelihood and Typicality score (Nalisnick et al., 2019a). This stems from the calibrated\nmodel uncertainties, which are exempli\ufb01ed in the models ability to detect OOD examples from the\nuncertainty deduced in latent and output space; see Fig. 9 (a,b) for ROC curves.\n\nFig. 10 shows distribution of the output variances for in- and OOD data. This illustrates that using LA\nimproves OoD detection. Furthermore, the online training improves the model calibration.\n\nMissing Data imputation. Another application of stochastic representation learning is to provide\ndistributions over unobserved values (Rezende et al., 2014). In many application domains, sensor\nreadings go missing, which we may mimic by letting parts of an image be unobserved. Rezende et al.\n(2014) show that we can then draw samples from the distribution of the entire image conditioned on\n\nFig. 10 shows distribution of the output variances for in- and OOD data. This illustrates that using LA\nimproves OoD detection. Furthermore, the online training improves the model calibration.\n\n226\n\n227\n\n232\n\n228\n\n229\n\n230\n\n231\n\n233\n\nMethod\n\nlogp(x) Typicality\n\n\u00002\n\noutput\n\n\u00002\n\nlatent\n\nAE MSE\nAE MSE\nAE likelihood\nAE likelihood\nVAE\nVAE\nMC-AE\nMC-AE\nLAE (post-hoc)\nLAE (post-hoc)\nLAE (online)\nLAE (online)\n\n\n\n\n\n0.98\n0.66\n0.54\n0.97\n0.98\n0.98\n\n0.98\n0.70\n0.62\n0.97\n0.98\n0.98\n\n0.65\n0.46\n0.08\n0.74\n0.91\n\n0.5\n0.31\n0.96\n0.96\n\n\n\n\n\nlatent\n\noutput\n\n(b) \u00002\n\n(a) \u00002\n\nTable 3: Out of Distribution detection. In-distribution data FashionMnist and OOD data Mnist.\nArea Under Receiver Operator Curve (AUROC\n) for log-likelihood, typicality score, latent space\n\"\n\u0000latent and output space \u0000output uncertainties. Our proposed LAE ELBO is able to discriminate between\nin and OOD using the deduced variances in latent and output space.\n\nFigure 9: Out of Distribution detection. In-distribution data FashionMnist and OOD data Mnist. (a)\n) for\nand (b) shows the ROC curves for latent and output space uncertainties. (c) shows (AUROC\n\"\nlog-likelihood, typicality score, latent space \u0000latent and output space \u0000output uncertainties. Online LAE\nis able to discriminate between in and OOD using the deduced variances in latent and output space.\n\n) for multiple statistics.\n\"\n\n(c) AUROC (\n\n(a) Latent variance, \u00002\n\n(b) Output variance, \u00002\n\noutput\n\nlatent\n\n\n\n\n\n\n\n8\n\nID\nOoD\n\nID\nOoD\n\n\n\n\n\n\n\n\n\n\n\n(a) AE likelihood\n\n(b) MC-AE\n\n(c) VAE\n\n(d) LAE (post-hoc)\n\n(e) LAE (online)\n\nFigure 10: Histograms of variance for in- and OOD reconstructions in output space. Note that MC-\nAE separates the distributions well, but the model assigns higher variance to the in- than OOD data.\n\n\n\n\n\n\n\n\n\n\n\n(a) AE likelihood\n\n(b) MC-AE\n\n(d) LAE (post-hoc)\n\n(e) LAE (online)\n\n(c) VAE\n8\n\n\f\n\n\n\n\n\n(a) VAE\n\n(b) LAE (post-hoc)\n\n(c) LAE (online)\n\nFigure 11: Missing data imputation & generative capabilities. Online training of LAE improves\nrepresentational robustness. This is exempli\ufb01ed by the multimodal behavior in the data imputation\n(top rows) that accurately model the ambiguity in the data. The bottom two rows show that the LAE\nis able to generate crisp digits from random noise.\n\nMethod\n\nClassi\ufb01er\nVAE\nMC-AE\nAE Ensemble\nLAE (post-hoc)\nLAE (online)\n\nMSE\n\n#\n\nlog p(x)\n\n\"\n\n104.73\n106.05\n96.23\n101.62\n99.59\n\n-104.75\n-106.05\n-100.94\n-107.25\n-106.29\n\n\"\n\nAcc.\n\n0.53\n0.22\n0.45\n0.53\n0.51\n0.53\n\n#\n\nECE\n\n0.16\n0.18\n0.28\n0.12\n0.16\n0.12\n\n#\n\nMCE\n\n0.25\n0.34\n0.38\n0.2\n0.27\n0.16\n\n#\n\nRMSCE\n\n0.18\n0.19\n0.29\n0.13\n0.18\n0.13\n\nTable 3: Reconstruction quality measured by the MSE and log-likelihood for the data imputation.\nOur well-calibrated uncertainties propagates to the MNIST classi\ufb01er and improves the calibration\nmetrics ECE, MCE and RMSCE.\n\nMissing Data imputation. Another application of stochastic representation learning is to provide\ndistributions over unobserved values (Rezende et al., 2014). In many application domains, sensor\nreadings go missing, which we may mimic by letting parts of an image be unobserved. Rezende et al.\n(2014) show that we can then draw samples from the distribution of the entire image conditioned on\nthe observed part, by imputing the missing pixels with noise and repeatedly encode and decode while\nkeeping observed pixels \ufb01xed. Fig. 11 show samples using this procedure from a VAE, a post-hoc\nLAE and our online LAE, where we only observe the lower half of an MNIST image. This implies\nambiguity about the original digit, e.g. the lower half of a \u201c5\u201c could be a \u201c3\u201c and similarly a \u201c7\u201c could\nbe a \u201c9\u201c. Our LAE captures this ambiguity, which is exempli\ufb01ed by the multi-modal reconstructions\nfrom the sampled networks in Fig. 11. The baselines only capture unimodal reconstructions.\n\nCapturing the ambiguity of partly missing data can improve downstream tasks such as the calibration\nof an image classi\ufb01er. In Fig. 11 (c) we demonstrate how averaging the predictions of a simple\nclassi\ufb01er across reconstructions improves standard calibration metrics. This is because the classi\ufb01er\ninherits the uncertainty and ambiguity in the learned representations. A deep ensemble of AEs\nperforms similarly to ours, but comes at the cost of training and storing multiple models.\n\nWhen the entire input image is missing, the imputation procedure can be seen as a sampling mecha-\nnism, such that our LAE can be viewed as a generative model. The bottom rows in Fig. 11 show that\nthe LAE indeed does generate sharp images from a multi-modal distribution.\n\nAttribute\n\nArched Eyebrows\nAttractive\nBald\nWearing Lipstick\nHeavy Makeup\n\nOverall\n\nAE\n\n0.50\n0.52\n0.98\n0.52\n0.45\n\n0.73\n\nVAE MC-AE LAE\u21e4\n\n0.52\n0.50\n0.98\n0.49\n0.52\n\n0.72\n\n0.55\n0.49\n0.98\n0.50\n0.49\n\n0.73\n\n0.60\n0.53\n0.98\n0.54\n0.56\n\n0.74\n\nTable 4: Semi-supervised classi\ufb01cation accuracy\non CELEBA using only 10 labeled datapoints. \u21e4\nrefers to online LAE.\n\n9\n\nFigure 12: Accuracy as an function of the\nnumber of labels per class on MNIST.\n\n\fFigure 13: Sample reconstructions on CELEBA. The top row shows the mean reconstruction and\nthe bottom row shows the variance of the reconstructed images.\n\nSemi-supervised learning combines a small amount of label data with a large amount of unlabeled\ndata. The hope is that the structure in the unlabeled data can be used to infer properties of the data that\ncannot be extracted from a few labeled points. Embedding the same labeled data point multiple times\nusing a stochastic representation scales up the amount of labeled data that is available during training.\n\nFig. 12 shows the accuracy of a K-nearest neighbor classi\ufb01er trained on different amounts of labeled\ndata from the MNIST dataset. For all models with a stochastic encoder, we encode each labeled\ndatapoint 100 times and repeat the experiment 5 times. When only a few labels per class are\navailable (1-20) we clearly observe that our LAE model outperforms all other models, stochastic and\ndeterministic. Increasing the number of labels beyond 100 per class makes the AE and LAE equal in\ntheir classi\ufb01cation performance with the AE model eventually outperforming the LAE model.\n\nIn Tab. 4 we conduct a similar experiment on the CELEBA (Liu et al., 2015) facial dataset, where the\nthe task is to predict 40 different binary labels per data point. When evaluating the overall accuracy of\npredicting all 40 facial attributes, we see no signi\ufb01cant difference in performance. However, when we\nzoom in on speci\ufb01c facial attributes we gain a clear performance advantage over other models. Fig. 13\nshows the mean and variance of \ufb01ve reconstructed images. The online LAE produces well-calibrated\nuncertainties in the output space and scales to large images.\n\nLimitations. Empirically, the LAE improvements are more signi\ufb01cant for overparameterized net-\nworks. The additional capacity seems to help the optimizer \ufb01nd a local mode where a Gaussian \ufb01t is ap-\npropriate. It seems the regularization induced by marginalizing \u2713 compensates for the added \ufb02exibility.\n\n\n\nThe following is the appendix A section of the paper you are reviewing:\n\n\nAll experiments was conducted on one of the following datasets: MNIST (Lecun et al., 1998),\nFASHIONMNIST (Xiao et al., 2017) and CELEBA (Liu et al., 2015). For training and testing the default\nsplits were used. For validation, we sampled 5000 data points randomly from the training sets. All\nimages were normalized to be in the [0, 1] range and for the CELEBA dataset the images were resized\nto 64 \u00d7 64 additionally.\n\nIf nothing else is stated, the models were trained with the following configuration: we used Adam\noptimizer (Kingma and Ba, 2015b) with a learning rate of 0.001 and default Pytorch settings (Paszke\net al., 2019). The learning rate was adjusted using ReduceLROnPlateau learning rate scheduler with\nparameters factor=0.5 and patience=5, meaning that the learning rate was halved whenever 5 epochs\nhad passed where the validation loss had not decreased. The mean squared error loss was used as the\nreconstruction loss in all models. Models were trained until convergence, defined as whenever the\nvalidation loss had not decreased for 8 epochs. Models trained on MNIST and FASHIONMNIST used a\nbatch size of 64 and on CELEBA a batch size of 128 was used.\n\nModel-specific details:\n\n\u2022 VAE: Models were trained with KL-scaling of 0.001. We use two encoders and two decoders,\nsuch that the model has twice the number of parameters compared to the other models.\n\u2022 MC-AE: Models were trained with dropout between all trainable layers with probability\n\np = 0.2. We keep the same dropout rate during testing.\n\n\u2022 EMSEMBLE-AE: Each ensemble consists of 5 models, each initialized with a different\n\nrandom seed.\n\n\u2022 LAE (POSTHOC): For experiments with linear layers we used the Laplace Redux (Daxberger\net al., 2021) implementation. For convolutions, we found it necessary to use our proposed\nhessian approximation. We use a diagonal approximation of the hessian in all experi-\nments. After fitting the hessian, we optimize for the prior precision using the marginal\nlikelihood (Daxberger et al., 2021). We use 100 MC sampling in all experiments.\n\n1 Denotes equal contribution; author order determined by a simulated coin toss.\n\n36th Conference on Neural Information Processing Systems (NeurIPS 2022).\n\n\f\u2022 LAE (ONLINE): We use the exact diagonal in experiments with linear layers and the mixed\ndiagonal approximation in all experiments with convolutional layers. We use a hessian\nmemory factor of 0.0001 and sample only 1 network per iteration. We found that it was not\nnecessary to optimize for the prior precision when trained online.\n\nA.1 Hessian approximation\n\nWe use a linear encoder-decoder with three layers in the encoder and decoder, TANH activation\nfunctions, and latent size 2. We choose this architecture as Laplace Redux (Daxberger et al., 2021)\nsupports various hessian approximations for this simple network. We use Laplace Redux for all\npost-hoc experiments except for the approximate diagonal hessian.\n\nA.2 Out-of-distribution\n\nWe use a convolutional encoder-decoder architecture. The encoder consisted of a CONV2D, TANH,\nMAXPOOL2D, CONV2D, TANH, MAXPOOL2D, LINEAR, TANH, LINEAR, TANH, LINEAR, where the\ndecoder was mirrored but with nearest neighbour Upsampling rather than MAXPOOL2D. We used a\nlatent size of 2 in these experiments for all models.\n\nA.3 Missing data imputation\n\nTo elaborate on the procedure, we reconstruct 5 samples from the half/fully masked image. For each of\nthese reconstructions, we make 5 more reconstructions and take the average of these reconstructions.\nThe intuition is that the first stage explores the multi-modal behavior of the reconstructions. In the\nsecond stage, the uncertainty of the reconstructed digit is reduced, and each sample will reconstruct\nthe same modality. By averaging over these modalities, we achieve a more crisp reconstruction. We\nuse the same architecture as in the hessian approximation experiment.\n\nA.4 Semi-supervised learning\n\nFor the experiments on MNIST, we use a single convergence checkpoint for each model. We use\nthe same model architecture as in the hessian approximation. We did 5 repetitions for each model\nwhere we first sampled n labels from each of the 10 classes from the validation set, then embedded\nthe 10 \u00d7 n data points into latent space, trained a KNN classifier on the embedded points and finally\nevaluated the accuracy of the classifier on the remaining validation set. This procedure was repeated\nfor different values of n in the [1, 100] range. For the stochastic encoders (VAE, MC-AE, LAE), we\nrepeated the embedding step 100 times with the goal that the uncertainty could help the downstream\nclassification. For the KNN classifier we use cross-validation (K = 2) to find the optimal number of\nnearest neighbors.\n\nFor the experiments on CELEBA we repeated the exact same experiments but with a fixed value\nif n = 10. Additionally, the classifier was changed to a multi-label version KNN-classifier to\naccommodate the multiple binary features in the dataset. For CELEBA we use a convolutional\narchitecture. The encoder consists of 5 convolutional layers with TANH and MAXPOOL2D in between\neach parametric layer. We use a latent size of 64. The decoder mirrors the encoder, but we replace\nMAXPOOL2D with nearest neighbor upsampling.\n\n\n\nBased on the section(s), please answer the following question with yes, no, or n/a and provide a brief justification for your answer.\nQuestion: If the authors use existing assets (e.g., code, data, models) or curate/release new assets, do the authors mention the license of the assets?"}, "4c": {"role": "user", "content": "The following is the experiments section of the paper you are reviewing:\n\n\nFirst, we demonstrate the computational advantages of the proposed Hessian approximation, and\nthen that our sampling-based training leads to well-calibrated uncertainties that can be used for OOD\ndetection, data imputation, and semi-supervised learning. For all the downstream tasks we consider\nthe following baselines: AE (Hinton and Salakhutdinov, 2006) with constant and learned variance, VAE\n(Rezende et al., 2014; Kingma and Welling, 2014), Monte-Carlo dropout AE (Gal and Ghahramani,\n2016) and Ensembles of AE (Lakshminarayanan et al., 2017). We extend StochManDetlefsen et al.\n(2021) with the Hessian backpropagation for the approximate and mixed diagonals. The training code\nis implemented in PyTorch and available2 . Appendix A provides more details on the experimental\nsetup.\n\nEf\ufb01cient Hessian Approximation. For practical applications, training time and memory usage of the\nHessian approximation must be kept low. We here show that the proposed approximate diagonal Hes-\nsian is suf\ufb01cient and even outperforms other approximations when combined with our online training.\n\nFig. 7 show the time and memory requirement for different approximation methods as a function\nof input size for a 5-layer convolutional network that preserves channel and input dimension. As\nbaselines we use ef\ufb01cient implementations of the exact and KFAC approximation (Daxberger et al.,\n3\n2021; Dangel et al., 2020). The exact diagonal approximation run out of memory for an\nimage on a 11 Gb NVIDIA GeForce GTX 1080 Ti. In contrast, our approximate diagonal Hessian\nscales linearly with the resolution, which is especially bene\ufb01cial for convolutional layers.\n\n36\n\n36\n\n\u21e5\n\n\u21e0\n\n\u21e5\n\n2 https://github.com/FrederikWarburg/LaplaceAE\n\n7\n\n\f\u00b5\n\n\u00b5\n\nFigure 9: Receiver Operator Curves (ROC) show that our online LAE is able to discriminate between\n\u00b5\nin and out of distribution in both latent and output space.\nlog p(x)\n\nHessian\n\nMSE\n\n\u00b5\n\n\u00b5\n\n\n\n\u00002\n\n\n\n\u00002\n\n\n\n\u00002\n\n\n\n\u00002\n\n\n\n\u00002\n\nKFAC\nExact\nApprox\nExact\u21e4\nApprox\u21e4\n\n\u0000\n9683.9\n283.3\n232.0\n25.8\n25.9\n\n\u00b1\n\u00b1\n\n#\n\n2455.0\n\u00b1\n88.6\n\u00b1\n65.5\n\u00b1\n0.2\n0.4\n\n#\n\n121.6\n27.1\n26.6\n25.7\n25.8\n\n\u00b1\n\u00b1\n\u00b1\n\u00b1\n\u00b1\n\n24.5\n0.9\n0.6\n0.2\n0.4\n\nApprox\n\nExact\n(a) Post-hoc\n\n\n\nFigure 8: Mean and variance of 100 sampled NN.\n\nKFAC\n\nExact\u21e4\n\nApprox\u21e4\n\nTable 2: Online training (indicated by \u21e4) outper-\nforms post-hoc LA. The approximate diagonal\nhas similar performance to the exact diagonal\nfor both post-hoc and online LA.\nFigure 10: Histograms of variance for in- and OOD reconstructions in output space. Note that\nMC-AE separates the distributions well, but the model assign higher variance to the in- than OOD\ndata.\n\n(b) Online\n\n\n\nTab. 2 shows that the exact or approximate Hessian diagonal has similar performance for both\npost-hoc and online training. Using post-hoc LA results in good mean reconstructions (low MSE),\nbut each sampled NN does not give good reconstructions (low log p(x)). Using our online training\nprocedure results in a much higher log-likelihood. This indicates that every sampled NN predicts\ngood reconstructions.\n\nthe uncertainties more well \ufb01tted, both in latent and data space. These well-\ufb01tted uncertainties have\nseveral practical downstream applications, which we demonstrate next.\n\n218\n\n219\n\n224\n\n225\n\n222\n\n223\n\n221\n\n220\n\nFig. 8 shows the latent representation, mean, and variance of the reconstructions with the KFAC, exact\nand approximate diagonal for both post-hoc and online setup. Note that the online training makes the\nuncertainties better \ufb01tted, both in latent and data space. These well-\ufb01tted uncertainties have several\npractical downstream applications, which we demonstrate next.\n\nOut-of-Distribution (OOD) Detection capabilities are critical for identifying distributional shifts,\noutliers and irregular user inputs, which can hinder the propagation of erroneous decision in an\nautomated system. We evaluate OOD performance on the commonly used benchmarks (Nalisnick\net al., 2019b), where we use FASHIONMNIST (Xiao et al., 2017) as in-distribution and MNIST\n(Lecun et al., 1998) as OOD. Tab. 3 shows that our online LAE outperforms existing models in\nboth log-likelihood and Typicality score (Nalisnick et al., 2019a). This stems from the calibrated\nmodel uncertainties, which are exempli\ufb01ed in the models ability to detect OOD examples from the\nuncertainty deduced in latent and output space; see Fig. 9 for ROC curves.\n\nOut-of-Distribution (OOD) Detection capabilities are critical for identifying distributional shifts,\noutliers, and irregular user inputs, which can hinder the propagation of erroneous decisions in an\nautomated system. We evaluate OOD performance on the commonly used benchmarks (Nalisnick\net al., 2019b), where we use FASHIONMNIST (Xiao et al., 2017) as in-distribution and MNIST\n(Lecun et al., 1998) as OOD. Fig. 9 (c) shows that our online LAE outperforms existing models in\nboth log-likelihood and Typicality score (Nalisnick et al., 2019a). This stems from the calibrated\nmodel uncertainties, which are exempli\ufb01ed in the models ability to detect OOD examples from the\nuncertainty deduced in latent and output space; see Fig. 9 (a,b) for ROC curves.\n\nFig. 10 shows distribution of the output variances for in- and OOD data. This illustrates that using LA\nimproves OoD detection. Furthermore, the online training improves the model calibration.\n\nMissing Data imputation. Another application of stochastic representation learning is to provide\ndistributions over unobserved values (Rezende et al., 2014). In many application domains, sensor\nreadings go missing, which we may mimic by letting parts of an image be unobserved. Rezende et al.\n(2014) show that we can then draw samples from the distribution of the entire image conditioned on\n\nFig. 10 shows distribution of the output variances for in- and OOD data. This illustrates that using LA\nimproves OoD detection. Furthermore, the online training improves the model calibration.\n\n226\n\n227\n\n232\n\n228\n\n229\n\n230\n\n231\n\n233\n\nMethod\n\nlogp(x) Typicality\n\n\u00002\n\noutput\n\n\u00002\n\nlatent\n\nAE MSE\nAE MSE\nAE likelihood\nAE likelihood\nVAE\nVAE\nMC-AE\nMC-AE\nLAE (post-hoc)\nLAE (post-hoc)\nLAE (online)\nLAE (online)\n\n\n\n\n\n0.98\n0.66\n0.54\n0.97\n0.98\n0.98\n\n0.98\n0.70\n0.62\n0.97\n0.98\n0.98\n\n0.65\n0.46\n0.08\n0.74\n0.91\n\n0.5\n0.31\n0.96\n0.96\n\n\n\n\n\nlatent\n\noutput\n\n(b) \u00002\n\n(a) \u00002\n\nTable 3: Out of Distribution detection. In-distribution data FashionMnist and OOD data Mnist.\nArea Under Receiver Operator Curve (AUROC\n) for log-likelihood, typicality score, latent space\n\"\n\u0000latent and output space \u0000output uncertainties. Our proposed LAE ELBO is able to discriminate between\nin and OOD using the deduced variances in latent and output space.\n\nFigure 9: Out of Distribution detection. In-distribution data FashionMnist and OOD data Mnist. (a)\n) for\nand (b) shows the ROC curves for latent and output space uncertainties. (c) shows (AUROC\n\"\nlog-likelihood, typicality score, latent space \u0000latent and output space \u0000output uncertainties. Online LAE\nis able to discriminate between in and OOD using the deduced variances in latent and output space.\n\n) for multiple statistics.\n\"\n\n(c) AUROC (\n\n(a) Latent variance, \u00002\n\n(b) Output variance, \u00002\n\noutput\n\nlatent\n\n\n\n\n\n\n\n8\n\nID\nOoD\n\nID\nOoD\n\n\n\n\n\n\n\n\n\n\n\n(a) AE likelihood\n\n(b) MC-AE\n\n(c) VAE\n\n(d) LAE (post-hoc)\n\n(e) LAE (online)\n\nFigure 10: Histograms of variance for in- and OOD reconstructions in output space. Note that MC-\nAE separates the distributions well, but the model assigns higher variance to the in- than OOD data.\n\n\n\n\n\n\n\n\n\n\n\n(a) AE likelihood\n\n(b) MC-AE\n\n(d) LAE (post-hoc)\n\n(e) LAE (online)\n\n(c) VAE\n8\n\n\f\n\n\n\n\n\n(a) VAE\n\n(b) LAE (post-hoc)\n\n(c) LAE (online)\n\nFigure 11: Missing data imputation & generative capabilities. Online training of LAE improves\nrepresentational robustness. This is exempli\ufb01ed by the multimodal behavior in the data imputation\n(top rows) that accurately model the ambiguity in the data. The bottom two rows show that the LAE\nis able to generate crisp digits from random noise.\n\nMethod\n\nClassi\ufb01er\nVAE\nMC-AE\nAE Ensemble\nLAE (post-hoc)\nLAE (online)\n\nMSE\n\n#\n\nlog p(x)\n\n\"\n\n104.73\n106.05\n96.23\n101.62\n99.59\n\n-104.75\n-106.05\n-100.94\n-107.25\n-106.29\n\n\"\n\nAcc.\n\n0.53\n0.22\n0.45\n0.53\n0.51\n0.53\n\n#\n\nECE\n\n0.16\n0.18\n0.28\n0.12\n0.16\n0.12\n\n#\n\nMCE\n\n0.25\n0.34\n0.38\n0.2\n0.27\n0.16\n\n#\n\nRMSCE\n\n0.18\n0.19\n0.29\n0.13\n0.18\n0.13\n\nTable 3: Reconstruction quality measured by the MSE and log-likelihood for the data imputation.\nOur well-calibrated uncertainties propagates to the MNIST classi\ufb01er and improves the calibration\nmetrics ECE, MCE and RMSCE.\n\nMissing Data imputation. Another application of stochastic representation learning is to provide\ndistributions over unobserved values (Rezende et al., 2014). In many application domains, sensor\nreadings go missing, which we may mimic by letting parts of an image be unobserved. Rezende et al.\n(2014) show that we can then draw samples from the distribution of the entire image conditioned on\nthe observed part, by imputing the missing pixels with noise and repeatedly encode and decode while\nkeeping observed pixels \ufb01xed. Fig. 11 show samples using this procedure from a VAE, a post-hoc\nLAE and our online LAE, where we only observe the lower half of an MNIST image. This implies\nambiguity about the original digit, e.g. the lower half of a \u201c5\u201c could be a \u201c3\u201c and similarly a \u201c7\u201c could\nbe a \u201c9\u201c. Our LAE captures this ambiguity, which is exempli\ufb01ed by the multi-modal reconstructions\nfrom the sampled networks in Fig. 11. The baselines only capture unimodal reconstructions.\n\nCapturing the ambiguity of partly missing data can improve downstream tasks such as the calibration\nof an image classi\ufb01er. In Fig. 11 (c) we demonstrate how averaging the predictions of a simple\nclassi\ufb01er across reconstructions improves standard calibration metrics. This is because the classi\ufb01er\ninherits the uncertainty and ambiguity in the learned representations. A deep ensemble of AEs\nperforms similarly to ours, but comes at the cost of training and storing multiple models.\n\nWhen the entire input image is missing, the imputation procedure can be seen as a sampling mecha-\nnism, such that our LAE can be viewed as a generative model. The bottom rows in Fig. 11 show that\nthe LAE indeed does generate sharp images from a multi-modal distribution.\n\nAttribute\n\nArched Eyebrows\nAttractive\nBald\nWearing Lipstick\nHeavy Makeup\n\nOverall\n\nAE\n\n0.50\n0.52\n0.98\n0.52\n0.45\n\n0.73\n\nVAE MC-AE LAE\u21e4\n\n0.52\n0.50\n0.98\n0.49\n0.52\n\n0.72\n\n0.55\n0.49\n0.98\n0.50\n0.49\n\n0.73\n\n0.60\n0.53\n0.98\n0.54\n0.56\n\n0.74\n\nTable 4: Semi-supervised classi\ufb01cation accuracy\non CELEBA using only 10 labeled datapoints. \u21e4\nrefers to online LAE.\n\n9\n\nFigure 12: Accuracy as an function of the\nnumber of labels per class on MNIST.\n\n\fFigure 13: Sample reconstructions on CELEBA. The top row shows the mean reconstruction and\nthe bottom row shows the variance of the reconstructed images.\n\nSemi-supervised learning combines a small amount of label data with a large amount of unlabeled\ndata. The hope is that the structure in the unlabeled data can be used to infer properties of the data that\ncannot be extracted from a few labeled points. Embedding the same labeled data point multiple times\nusing a stochastic representation scales up the amount of labeled data that is available during training.\n\nFig. 12 shows the accuracy of a K-nearest neighbor classi\ufb01er trained on different amounts of labeled\ndata from the MNIST dataset. For all models with a stochastic encoder, we encode each labeled\ndatapoint 100 times and repeat the experiment 5 times. When only a few labels per class are\navailable (1-20) we clearly observe that our LAE model outperforms all other models, stochastic and\ndeterministic. Increasing the number of labels beyond 100 per class makes the AE and LAE equal in\ntheir classi\ufb01cation performance with the AE model eventually outperforming the LAE model.\n\nIn Tab. 4 we conduct a similar experiment on the CELEBA (Liu et al., 2015) facial dataset, where the\nthe task is to predict 40 different binary labels per data point. When evaluating the overall accuracy of\npredicting all 40 facial attributes, we see no signi\ufb01cant difference in performance. However, when we\nzoom in on speci\ufb01c facial attributes we gain a clear performance advantage over other models. Fig. 13\nshows the mean and variance of \ufb01ve reconstructed images. The online LAE produces well-calibrated\nuncertainties in the output space and scales to large images.\n\nLimitations. Empirically, the LAE improvements are more signi\ufb01cant for overparameterized net-\nworks. The additional capacity seems to help the optimizer \ufb01nd a local mode where a Gaussian \ufb01t is ap-\npropriate. It seems the regularization induced by marginalizing \u2713 compensates for the added \ufb02exibility.\n\n\n\nBased on the section(s), please answer the following question with yes, no, or n/a and provide a brief justification for your answer.\nQuestion: If the authors curate/release new assets (e.g., code, data, models), do the authors include any new assets either in the supplemental material or as a URL?"}, "4d": {"role": "user", "content": "The following is the experiments section of the paper you are reviewing:\n\n\nFirst, we demonstrate the computational advantages of the proposed Hessian approximation, and\nthen that our sampling-based training leads to well-calibrated uncertainties that can be used for OOD\ndetection, data imputation, and semi-supervised learning. For all the downstream tasks we consider\nthe following baselines: AE (Hinton and Salakhutdinov, 2006) with constant and learned variance, VAE\n(Rezende et al., 2014; Kingma and Welling, 2014), Monte-Carlo dropout AE (Gal and Ghahramani,\n2016) and Ensembles of AE (Lakshminarayanan et al., 2017). We extend StochManDetlefsen et al.\n(2021) with the Hessian backpropagation for the approximate and mixed diagonals. The training code\nis implemented in PyTorch and available2 . Appendix A provides more details on the experimental\nsetup.\n\nEf\ufb01cient Hessian Approximation. For practical applications, training time and memory usage of the\nHessian approximation must be kept low. We here show that the proposed approximate diagonal Hes-\nsian is suf\ufb01cient and even outperforms other approximations when combined with our online training.\n\nFig. 7 show the time and memory requirement for different approximation methods as a function\nof input size for a 5-layer convolutional network that preserves channel and input dimension. As\nbaselines we use ef\ufb01cient implementations of the exact and KFAC approximation (Daxberger et al.,\n3\n2021; Dangel et al., 2020). The exact diagonal approximation run out of memory for an\nimage on a 11 Gb NVIDIA GeForce GTX 1080 Ti. In contrast, our approximate diagonal Hessian\nscales linearly with the resolution, which is especially bene\ufb01cial for convolutional layers.\n\n36\n\n36\n\n\u21e5\n\n\u21e0\n\n\u21e5\n\n2 https://github.com/FrederikWarburg/LaplaceAE\n\n7\n\n\f\u00b5\n\n\u00b5\n\nFigure 9: Receiver Operator Curves (ROC) show that our online LAE is able to discriminate between\n\u00b5\nin and out of distribution in both latent and output space.\nlog p(x)\n\nHessian\n\nMSE\n\n\u00b5\n\n\u00b5\n\n\n\n\u00002\n\n\n\n\u00002\n\n\n\n\u00002\n\n\n\n\u00002\n\n\n\n\u00002\n\nKFAC\nExact\nApprox\nExact\u21e4\nApprox\u21e4\n\n\u0000\n9683.9\n283.3\n232.0\n25.8\n25.9\n\n\u00b1\n\u00b1\n\n#\n\n2455.0\n\u00b1\n88.6\n\u00b1\n65.5\n\u00b1\n0.2\n0.4\n\n#\n\n121.6\n27.1\n26.6\n25.7\n25.8\n\n\u00b1\n\u00b1\n\u00b1\n\u00b1\n\u00b1\n\n24.5\n0.9\n0.6\n0.2\n0.4\n\nApprox\n\nExact\n(a) Post-hoc\n\n\n\nFigure 8: Mean and variance of 100 sampled NN.\n\nKFAC\n\nExact\u21e4\n\nApprox\u21e4\n\nTable 2: Online training (indicated by \u21e4) outper-\nforms post-hoc LA. The approximate diagonal\nhas similar performance to the exact diagonal\nfor both post-hoc and online LA.\nFigure 10: Histograms of variance for in- and OOD reconstructions in output space. Note that\nMC-AE separates the distributions well, but the model assign higher variance to the in- than OOD\ndata.\n\n(b) Online\n\n\n\nTab. 2 shows that the exact or approximate Hessian diagonal has similar performance for both\npost-hoc and online training. Using post-hoc LA results in good mean reconstructions (low MSE),\nbut each sampled NN does not give good reconstructions (low log p(x)). Using our online training\nprocedure results in a much higher log-likelihood. This indicates that every sampled NN predicts\ngood reconstructions.\n\nthe uncertainties more well \ufb01tted, both in latent and data space. These well-\ufb01tted uncertainties have\nseveral practical downstream applications, which we demonstrate next.\n\n218\n\n219\n\n224\n\n225\n\n222\n\n223\n\n221\n\n220\n\nFig. 8 shows the latent representation, mean, and variance of the reconstructions with the KFAC, exact\nand approximate diagonal for both post-hoc and online setup. Note that the online training makes the\nuncertainties better \ufb01tted, both in latent and data space. These well-\ufb01tted uncertainties have several\npractical downstream applications, which we demonstrate next.\n\nOut-of-Distribution (OOD) Detection capabilities are critical for identifying distributional shifts,\noutliers and irregular user inputs, which can hinder the propagation of erroneous decision in an\nautomated system. We evaluate OOD performance on the commonly used benchmarks (Nalisnick\net al., 2019b), where we use FASHIONMNIST (Xiao et al., 2017) as in-distribution and MNIST\n(Lecun et al., 1998) as OOD. Tab. 3 shows that our online LAE outperforms existing models in\nboth log-likelihood and Typicality score (Nalisnick et al., 2019a). This stems from the calibrated\nmodel uncertainties, which are exempli\ufb01ed in the models ability to detect OOD examples from the\nuncertainty deduced in latent and output space; see Fig. 9 for ROC curves.\n\nOut-of-Distribution (OOD) Detection capabilities are critical for identifying distributional shifts,\noutliers, and irregular user inputs, which can hinder the propagation of erroneous decisions in an\nautomated system. We evaluate OOD performance on the commonly used benchmarks (Nalisnick\net al., 2019b), where we use FASHIONMNIST (Xiao et al., 2017) as in-distribution and MNIST\n(Lecun et al., 1998) as OOD. Fig. 9 (c) shows that our online LAE outperforms existing models in\nboth log-likelihood and Typicality score (Nalisnick et al., 2019a). This stems from the calibrated\nmodel uncertainties, which are exempli\ufb01ed in the models ability to detect OOD examples from the\nuncertainty deduced in latent and output space; see Fig. 9 (a,b) for ROC curves.\n\nFig. 10 shows distribution of the output variances for in- and OOD data. This illustrates that using LA\nimproves OoD detection. Furthermore, the online training improves the model calibration.\n\nMissing Data imputation. Another application of stochastic representation learning is to provide\ndistributions over unobserved values (Rezende et al., 2014). In many application domains, sensor\nreadings go missing, which we may mimic by letting parts of an image be unobserved. Rezende et al.\n(2014) show that we can then draw samples from the distribution of the entire image conditioned on\n\nFig. 10 shows distribution of the output variances for in- and OOD data. This illustrates that using LA\nimproves OoD detection. Furthermore, the online training improves the model calibration.\n\n226\n\n227\n\n232\n\n228\n\n229\n\n230\n\n231\n\n233\n\nMethod\n\nlogp(x) Typicality\n\n\u00002\n\noutput\n\n\u00002\n\nlatent\n\nAE MSE\nAE MSE\nAE likelihood\nAE likelihood\nVAE\nVAE\nMC-AE\nMC-AE\nLAE (post-hoc)\nLAE (post-hoc)\nLAE (online)\nLAE (online)\n\n\n\n\n\n0.98\n0.66\n0.54\n0.97\n0.98\n0.98\n\n0.98\n0.70\n0.62\n0.97\n0.98\n0.98\n\n0.65\n0.46\n0.08\n0.74\n0.91\n\n0.5\n0.31\n0.96\n0.96\n\n\n\n\n\nlatent\n\noutput\n\n(b) \u00002\n\n(a) \u00002\n\nTable 3: Out of Distribution detection. In-distribution data FashionMnist and OOD data Mnist.\nArea Under Receiver Operator Curve (AUROC\n) for log-likelihood, typicality score, latent space\n\"\n\u0000latent and output space \u0000output uncertainties. Our proposed LAE ELBO is able to discriminate between\nin and OOD using the deduced variances in latent and output space.\n\nFigure 9: Out of Distribution detection. In-distribution data FashionMnist and OOD data Mnist. (a)\n) for\nand (b) shows the ROC curves for latent and output space uncertainties. (c) shows (AUROC\n\"\nlog-likelihood, typicality score, latent space \u0000latent and output space \u0000output uncertainties. Online LAE\nis able to discriminate between in and OOD using the deduced variances in latent and output space.\n\n) for multiple statistics.\n\"\n\n(c) AUROC (\n\n(a) Latent variance, \u00002\n\n(b) Output variance, \u00002\n\noutput\n\nlatent\n\n\n\n\n\n\n\n8\n\nID\nOoD\n\nID\nOoD\n\n\n\n\n\n\n\n\n\n\n\n(a) AE likelihood\n\n(b) MC-AE\n\n(c) VAE\n\n(d) LAE (post-hoc)\n\n(e) LAE (online)\n\nFigure 10: Histograms of variance for in- and OOD reconstructions in output space. Note that MC-\nAE separates the distributions well, but the model assigns higher variance to the in- than OOD data.\n\n\n\n\n\n\n\n\n\n\n\n(a) AE likelihood\n\n(b) MC-AE\n\n(d) LAE (post-hoc)\n\n(e) LAE (online)\n\n(c) VAE\n8\n\n\f\n\n\n\n\n\n(a) VAE\n\n(b) LAE (post-hoc)\n\n(c) LAE (online)\n\nFigure 11: Missing data imputation & generative capabilities. Online training of LAE improves\nrepresentational robustness. This is exempli\ufb01ed by the multimodal behavior in the data imputation\n(top rows) that accurately model the ambiguity in the data. The bottom two rows show that the LAE\nis able to generate crisp digits from random noise.\n\nMethod\n\nClassi\ufb01er\nVAE\nMC-AE\nAE Ensemble\nLAE (post-hoc)\nLAE (online)\n\nMSE\n\n#\n\nlog p(x)\n\n\"\n\n104.73\n106.05\n96.23\n101.62\n99.59\n\n-104.75\n-106.05\n-100.94\n-107.25\n-106.29\n\n\"\n\nAcc.\n\n0.53\n0.22\n0.45\n0.53\n0.51\n0.53\n\n#\n\nECE\n\n0.16\n0.18\n0.28\n0.12\n0.16\n0.12\n\n#\n\nMCE\n\n0.25\n0.34\n0.38\n0.2\n0.27\n0.16\n\n#\n\nRMSCE\n\n0.18\n0.19\n0.29\n0.13\n0.18\n0.13\n\nTable 3: Reconstruction quality measured by the MSE and log-likelihood for the data imputation.\nOur well-calibrated uncertainties propagates to the MNIST classi\ufb01er and improves the calibration\nmetrics ECE, MCE and RMSCE.\n\nMissing Data imputation. Another application of stochastic representation learning is to provide\ndistributions over unobserved values (Rezende et al., 2014). In many application domains, sensor\nreadings go missing, which we may mimic by letting parts of an image be unobserved. Rezende et al.\n(2014) show that we can then draw samples from the distribution of the entire image conditioned on\nthe observed part, by imputing the missing pixels with noise and repeatedly encode and decode while\nkeeping observed pixels \ufb01xed. Fig. 11 show samples using this procedure from a VAE, a post-hoc\nLAE and our online LAE, where we only observe the lower half of an MNIST image. This implies\nambiguity about the original digit, e.g. the lower half of a \u201c5\u201c could be a \u201c3\u201c and similarly a \u201c7\u201c could\nbe a \u201c9\u201c. Our LAE captures this ambiguity, which is exempli\ufb01ed by the multi-modal reconstructions\nfrom the sampled networks in Fig. 11. The baselines only capture unimodal reconstructions.\n\nCapturing the ambiguity of partly missing data can improve downstream tasks such as the calibration\nof an image classi\ufb01er. In Fig. 11 (c) we demonstrate how averaging the predictions of a simple\nclassi\ufb01er across reconstructions improves standard calibration metrics. This is because the classi\ufb01er\ninherits the uncertainty and ambiguity in the learned representations. A deep ensemble of AEs\nperforms similarly to ours, but comes at the cost of training and storing multiple models.\n\nWhen the entire input image is missing, the imputation procedure can be seen as a sampling mecha-\nnism, such that our LAE can be viewed as a generative model. The bottom rows in Fig. 11 show that\nthe LAE indeed does generate sharp images from a multi-modal distribution.\n\nAttribute\n\nArched Eyebrows\nAttractive\nBald\nWearing Lipstick\nHeavy Makeup\n\nOverall\n\nAE\n\n0.50\n0.52\n0.98\n0.52\n0.45\n\n0.73\n\nVAE MC-AE LAE\u21e4\n\n0.52\n0.50\n0.98\n0.49\n0.52\n\n0.72\n\n0.55\n0.49\n0.98\n0.50\n0.49\n\n0.73\n\n0.60\n0.53\n0.98\n0.54\n0.56\n\n0.74\n\nTable 4: Semi-supervised classi\ufb01cation accuracy\non CELEBA using only 10 labeled datapoints. \u21e4\nrefers to online LAE.\n\n9\n\nFigure 12: Accuracy as an function of the\nnumber of labels per class on MNIST.\n\n\fFigure 13: Sample reconstructions on CELEBA. The top row shows the mean reconstruction and\nthe bottom row shows the variance of the reconstructed images.\n\nSemi-supervised learning combines a small amount of label data with a large amount of unlabeled\ndata. The hope is that the structure in the unlabeled data can be used to infer properties of the data that\ncannot be extracted from a few labeled points. Embedding the same labeled data point multiple times\nusing a stochastic representation scales up the amount of labeled data that is available during training.\n\nFig. 12 shows the accuracy of a K-nearest neighbor classi\ufb01er trained on different amounts of labeled\ndata from the MNIST dataset. For all models with a stochastic encoder, we encode each labeled\ndatapoint 100 times and repeat the experiment 5 times. When only a few labels per class are\navailable (1-20) we clearly observe that our LAE model outperforms all other models, stochastic and\ndeterministic. Increasing the number of labels beyond 100 per class makes the AE and LAE equal in\ntheir classi\ufb01cation performance with the AE model eventually outperforming the LAE model.\n\nIn Tab. 4 we conduct a similar experiment on the CELEBA (Liu et al., 2015) facial dataset, where the\nthe task is to predict 40 different binary labels per data point. When evaluating the overall accuracy of\npredicting all 40 facial attributes, we see no signi\ufb01cant difference in performance. However, when we\nzoom in on speci\ufb01c facial attributes we gain a clear performance advantage over other models. Fig. 13\nshows the mean and variance of \ufb01ve reconstructed images. The online LAE produces well-calibrated\nuncertainties in the output space and scales to large images.\n\nLimitations. Empirically, the LAE improvements are more signi\ufb01cant for overparameterized net-\nworks. The additional capacity seems to help the optimizer \ufb01nd a local mode where a Gaussian \ufb01t is ap-\npropriate. It seems the regularization induced by marginalizing \u2713 compensates for the added \ufb02exibility.\n\n\n\nBased on the section(s), please answer the following question with yes, no, or n/a and provide a brief justification for your answer.\nQuestion: If the authors curate/release new assets (e.g., code, data, models), do the authors discuss whether and how consent was obtained from people whose data they are using/curating?"}, "4e": {"role": "user", "content": "The following is the experiments section of the paper you are reviewing:\n\n\nFirst, we demonstrate the computational advantages of the proposed Hessian approximation, and\nthen that our sampling-based training leads to well-calibrated uncertainties that can be used for OOD\ndetection, data imputation, and semi-supervised learning. For all the downstream tasks we consider\nthe following baselines: AE (Hinton and Salakhutdinov, 2006) with constant and learned variance, VAE\n(Rezende et al., 2014; Kingma and Welling, 2014), Monte-Carlo dropout AE (Gal and Ghahramani,\n2016) and Ensembles of AE (Lakshminarayanan et al., 2017). We extend StochManDetlefsen et al.\n(2021) with the Hessian backpropagation for the approximate and mixed diagonals. The training code\nis implemented in PyTorch and available2 . Appendix A provides more details on the experimental\nsetup.\n\nEf\ufb01cient Hessian Approximation. For practical applications, training time and memory usage of the\nHessian approximation must be kept low. We here show that the proposed approximate diagonal Hes-\nsian is suf\ufb01cient and even outperforms other approximations when combined with our online training.\n\nFig. 7 show the time and memory requirement for different approximation methods as a function\nof input size for a 5-layer convolutional network that preserves channel and input dimension. As\nbaselines we use ef\ufb01cient implementations of the exact and KFAC approximation (Daxberger et al.,\n3\n2021; Dangel et al., 2020). The exact diagonal approximation run out of memory for an\nimage on a 11 Gb NVIDIA GeForce GTX 1080 Ti. In contrast, our approximate diagonal Hessian\nscales linearly with the resolution, which is especially bene\ufb01cial for convolutional layers.\n\n36\n\n36\n\n\u21e5\n\n\u21e0\n\n\u21e5\n\n2 https://github.com/FrederikWarburg/LaplaceAE\n\n7\n\n\f\u00b5\n\n\u00b5\n\nFigure 9: Receiver Operator Curves (ROC) show that our online LAE is able to discriminate between\n\u00b5\nin and out of distribution in both latent and output space.\nlog p(x)\n\nHessian\n\nMSE\n\n\u00b5\n\n\u00b5\n\n\n\n\u00002\n\n\n\n\u00002\n\n\n\n\u00002\n\n\n\n\u00002\n\n\n\n\u00002\n\nKFAC\nExact\nApprox\nExact\u21e4\nApprox\u21e4\n\n\u0000\n9683.9\n283.3\n232.0\n25.8\n25.9\n\n\u00b1\n\u00b1\n\n#\n\n2455.0\n\u00b1\n88.6\n\u00b1\n65.5\n\u00b1\n0.2\n0.4\n\n#\n\n121.6\n27.1\n26.6\n25.7\n25.8\n\n\u00b1\n\u00b1\n\u00b1\n\u00b1\n\u00b1\n\n24.5\n0.9\n0.6\n0.2\n0.4\n\nApprox\n\nExact\n(a) Post-hoc\n\n\n\nFigure 8: Mean and variance of 100 sampled NN.\n\nKFAC\n\nExact\u21e4\n\nApprox\u21e4\n\nTable 2: Online training (indicated by \u21e4) outper-\nforms post-hoc LA. The approximate diagonal\nhas similar performance to the exact diagonal\nfor both post-hoc and online LA.\nFigure 10: Histograms of variance for in- and OOD reconstructions in output space. Note that\nMC-AE separates the distributions well, but the model assign higher variance to the in- than OOD\ndata.\n\n(b) Online\n\n\n\nTab. 2 shows that the exact or approximate Hessian diagonal has similar performance for both\npost-hoc and online training. Using post-hoc LA results in good mean reconstructions (low MSE),\nbut each sampled NN does not give good reconstructions (low log p(x)). Using our online training\nprocedure results in a much higher log-likelihood. This indicates that every sampled NN predicts\ngood reconstructions.\n\nthe uncertainties more well \ufb01tted, both in latent and data space. These well-\ufb01tted uncertainties have\nseveral practical downstream applications, which we demonstrate next.\n\n218\n\n219\n\n224\n\n225\n\n222\n\n223\n\n221\n\n220\n\nFig. 8 shows the latent representation, mean, and variance of the reconstructions with the KFAC, exact\nand approximate diagonal for both post-hoc and online setup. Note that the online training makes the\nuncertainties better \ufb01tted, both in latent and data space. These well-\ufb01tted uncertainties have several\npractical downstream applications, which we demonstrate next.\n\nOut-of-Distribution (OOD) Detection capabilities are critical for identifying distributional shifts,\noutliers and irregular user inputs, which can hinder the propagation of erroneous decision in an\nautomated system. We evaluate OOD performance on the commonly used benchmarks (Nalisnick\net al., 2019b), where we use FASHIONMNIST (Xiao et al., 2017) as in-distribution and MNIST\n(Lecun et al., 1998) as OOD. Tab. 3 shows that our online LAE outperforms existing models in\nboth log-likelihood and Typicality score (Nalisnick et al., 2019a). This stems from the calibrated\nmodel uncertainties, which are exempli\ufb01ed in the models ability to detect OOD examples from the\nuncertainty deduced in latent and output space; see Fig. 9 for ROC curves.\n\nOut-of-Distribution (OOD) Detection capabilities are critical for identifying distributional shifts,\noutliers, and irregular user inputs, which can hinder the propagation of erroneous decisions in an\nautomated system. We evaluate OOD performance on the commonly used benchmarks (Nalisnick\net al., 2019b), where we use FASHIONMNIST (Xiao et al., 2017) as in-distribution and MNIST\n(Lecun et al., 1998) as OOD. Fig. 9 (c) shows that our online LAE outperforms existing models in\nboth log-likelihood and Typicality score (Nalisnick et al., 2019a). This stems from the calibrated\nmodel uncertainties, which are exempli\ufb01ed in the models ability to detect OOD examples from the\nuncertainty deduced in latent and output space; see Fig. 9 (a,b) for ROC curves.\n\nFig. 10 shows distribution of the output variances for in- and OOD data. This illustrates that using LA\nimproves OoD detection. Furthermore, the online training improves the model calibration.\n\nMissing Data imputation. Another application of stochastic representation learning is to provide\ndistributions over unobserved values (Rezende et al., 2014). In many application domains, sensor\nreadings go missing, which we may mimic by letting parts of an image be unobserved. Rezende et al.\n(2014) show that we can then draw samples from the distribution of the entire image conditioned on\n\nFig. 10 shows distribution of the output variances for in- and OOD data. This illustrates that using LA\nimproves OoD detection. Furthermore, the online training improves the model calibration.\n\n226\n\n227\n\n232\n\n228\n\n229\n\n230\n\n231\n\n233\n\nMethod\n\nlogp(x) Typicality\n\n\u00002\n\noutput\n\n\u00002\n\nlatent\n\nAE MSE\nAE MSE\nAE likelihood\nAE likelihood\nVAE\nVAE\nMC-AE\nMC-AE\nLAE (post-hoc)\nLAE (post-hoc)\nLAE (online)\nLAE (online)\n\n\n\n\n\n0.98\n0.66\n0.54\n0.97\n0.98\n0.98\n\n0.98\n0.70\n0.62\n0.97\n0.98\n0.98\n\n0.65\n0.46\n0.08\n0.74\n0.91\n\n0.5\n0.31\n0.96\n0.96\n\n\n\n\n\nlatent\n\noutput\n\n(b) \u00002\n\n(a) \u00002\n\nTable 3: Out of Distribution detection. In-distribution data FashionMnist and OOD data Mnist.\nArea Under Receiver Operator Curve (AUROC\n) for log-likelihood, typicality score, latent space\n\"\n\u0000latent and output space \u0000output uncertainties. Our proposed LAE ELBO is able to discriminate between\nin and OOD using the deduced variances in latent and output space.\n\nFigure 9: Out of Distribution detection. In-distribution data FashionMnist and OOD data Mnist. (a)\n) for\nand (b) shows the ROC curves for latent and output space uncertainties. (c) shows (AUROC\n\"\nlog-likelihood, typicality score, latent space \u0000latent and output space \u0000output uncertainties. Online LAE\nis able to discriminate between in and OOD using the deduced variances in latent and output space.\n\n) for multiple statistics.\n\"\n\n(c) AUROC (\n\n(a) Latent variance, \u00002\n\n(b) Output variance, \u00002\n\noutput\n\nlatent\n\n\n\n\n\n\n\n8\n\nID\nOoD\n\nID\nOoD\n\n\n\n\n\n\n\n\n\n\n\n(a) AE likelihood\n\n(b) MC-AE\n\n(c) VAE\n\n(d) LAE (post-hoc)\n\n(e) LAE (online)\n\nFigure 10: Histograms of variance for in- and OOD reconstructions in output space. Note that MC-\nAE separates the distributions well, but the model assigns higher variance to the in- than OOD data.\n\n\n\n\n\n\n\n\n\n\n\n(a) AE likelihood\n\n(b) MC-AE\n\n(d) LAE (post-hoc)\n\n(e) LAE (online)\n\n(c) VAE\n8\n\n\f\n\n\n\n\n\n(a) VAE\n\n(b) LAE (post-hoc)\n\n(c) LAE (online)\n\nFigure 11: Missing data imputation & generative capabilities. Online training of LAE improves\nrepresentational robustness. This is exempli\ufb01ed by the multimodal behavior in the data imputation\n(top rows) that accurately model the ambiguity in the data. The bottom two rows show that the LAE\nis able to generate crisp digits from random noise.\n\nMethod\n\nClassi\ufb01er\nVAE\nMC-AE\nAE Ensemble\nLAE (post-hoc)\nLAE (online)\n\nMSE\n\n#\n\nlog p(x)\n\n\"\n\n104.73\n106.05\n96.23\n101.62\n99.59\n\n-104.75\n-106.05\n-100.94\n-107.25\n-106.29\n\n\"\n\nAcc.\n\n0.53\n0.22\n0.45\n0.53\n0.51\n0.53\n\n#\n\nECE\n\n0.16\n0.18\n0.28\n0.12\n0.16\n0.12\n\n#\n\nMCE\n\n0.25\n0.34\n0.38\n0.2\n0.27\n0.16\n\n#\n\nRMSCE\n\n0.18\n0.19\n0.29\n0.13\n0.18\n0.13\n\nTable 3: Reconstruction quality measured by the MSE and log-likelihood for the data imputation.\nOur well-calibrated uncertainties propagates to the MNIST classi\ufb01er and improves the calibration\nmetrics ECE, MCE and RMSCE.\n\nMissing Data imputation. Another application of stochastic representation learning is to provide\ndistributions over unobserved values (Rezende et al., 2014). In many application domains, sensor\nreadings go missing, which we may mimic by letting parts of an image be unobserved. Rezende et al.\n(2014) show that we can then draw samples from the distribution of the entire image conditioned on\nthe observed part, by imputing the missing pixels with noise and repeatedly encode and decode while\nkeeping observed pixels \ufb01xed. Fig. 11 show samples using this procedure from a VAE, a post-hoc\nLAE and our online LAE, where we only observe the lower half of an MNIST image. This implies\nambiguity about the original digit, e.g. the lower half of a \u201c5\u201c could be a \u201c3\u201c and similarly a \u201c7\u201c could\nbe a \u201c9\u201c. Our LAE captures this ambiguity, which is exempli\ufb01ed by the multi-modal reconstructions\nfrom the sampled networks in Fig. 11. The baselines only capture unimodal reconstructions.\n\nCapturing the ambiguity of partly missing data can improve downstream tasks such as the calibration\nof an image classi\ufb01er. In Fig. 11 (c) we demonstrate how averaging the predictions of a simple\nclassi\ufb01er across reconstructions improves standard calibration metrics. This is because the classi\ufb01er\ninherits the uncertainty and ambiguity in the learned representations. A deep ensemble of AEs\nperforms similarly to ours, but comes at the cost of training and storing multiple models.\n\nWhen the entire input image is missing, the imputation procedure can be seen as a sampling mecha-\nnism, such that our LAE can be viewed as a generative model. The bottom rows in Fig. 11 show that\nthe LAE indeed does generate sharp images from a multi-modal distribution.\n\nAttribute\n\nArched Eyebrows\nAttractive\nBald\nWearing Lipstick\nHeavy Makeup\n\nOverall\n\nAE\n\n0.50\n0.52\n0.98\n0.52\n0.45\n\n0.73\n\nVAE MC-AE LAE\u21e4\n\n0.52\n0.50\n0.98\n0.49\n0.52\n\n0.72\n\n0.55\n0.49\n0.98\n0.50\n0.49\n\n0.73\n\n0.60\n0.53\n0.98\n0.54\n0.56\n\n0.74\n\nTable 4: Semi-supervised classi\ufb01cation accuracy\non CELEBA using only 10 labeled datapoints. \u21e4\nrefers to online LAE.\n\n9\n\nFigure 12: Accuracy as an function of the\nnumber of labels per class on MNIST.\n\n\fFigure 13: Sample reconstructions on CELEBA. The top row shows the mean reconstruction and\nthe bottom row shows the variance of the reconstructed images.\n\nSemi-supervised learning combines a small amount of label data with a large amount of unlabeled\ndata. The hope is that the structure in the unlabeled data can be used to infer properties of the data that\ncannot be extracted from a few labeled points. Embedding the same labeled data point multiple times\nusing a stochastic representation scales up the amount of labeled data that is available during training.\n\nFig. 12 shows the accuracy of a K-nearest neighbor classi\ufb01er trained on different amounts of labeled\ndata from the MNIST dataset. For all models with a stochastic encoder, we encode each labeled\ndatapoint 100 times and repeat the experiment 5 times. When only a few labels per class are\navailable (1-20) we clearly observe that our LAE model outperforms all other models, stochastic and\ndeterministic. Increasing the number of labels beyond 100 per class makes the AE and LAE equal in\ntheir classi\ufb01cation performance with the AE model eventually outperforming the LAE model.\n\nIn Tab. 4 we conduct a similar experiment on the CELEBA (Liu et al., 2015) facial dataset, where the\nthe task is to predict 40 different binary labels per data point. When evaluating the overall accuracy of\npredicting all 40 facial attributes, we see no signi\ufb01cant difference in performance. However, when we\nzoom in on speci\ufb01c facial attributes we gain a clear performance advantage over other models. Fig. 13\nshows the mean and variance of \ufb01ve reconstructed images. The online LAE produces well-calibrated\nuncertainties in the output space and scales to large images.\n\nLimitations. Empirically, the LAE improvements are more signi\ufb01cant for overparameterized net-\nworks. The additional capacity seems to help the optimizer \ufb01nd a local mode where a Gaussian \ufb01t is ap-\npropriate. It seems the regularization induced by marginalizing \u2713 compensates for the added \ufb02exibility.\n\n\n\nBased on the section(s), please answer the following question with yes, no, or n/a and provide a brief justification for your answer.\nQuestion: If the authors curate/release new assets (e.g., code, data, models), do the authors discuss whether the data they are using/curating contains personally identifiable information or offensive content?"}, "5a": {"role": "user", "content": "The following is the experiments section of the paper you are reviewing:\n\n\nFirst, we demonstrate the computational advantages of the proposed Hessian approximation, and\nthen that our sampling-based training leads to well-calibrated uncertainties that can be used for OOD\ndetection, data imputation, and semi-supervised learning. For all the downstream tasks we consider\nthe following baselines: AE (Hinton and Salakhutdinov, 2006) with constant and learned variance, VAE\n(Rezende et al., 2014; Kingma and Welling, 2014), Monte-Carlo dropout AE (Gal and Ghahramani,\n2016) and Ensembles of AE (Lakshminarayanan et al., 2017). We extend StochManDetlefsen et al.\n(2021) with the Hessian backpropagation for the approximate and mixed diagonals. The training code\nis implemented in PyTorch and available2 . Appendix A provides more details on the experimental\nsetup.\n\nEf\ufb01cient Hessian Approximation. For practical applications, training time and memory usage of the\nHessian approximation must be kept low. We here show that the proposed approximate diagonal Hes-\nsian is suf\ufb01cient and even outperforms other approximations when combined with our online training.\n\nFig. 7 show the time and memory requirement for different approximation methods as a function\nof input size for a 5-layer convolutional network that preserves channel and input dimension. As\nbaselines we use ef\ufb01cient implementations of the exact and KFAC approximation (Daxberger et al.,\n3\n2021; Dangel et al., 2020). The exact diagonal approximation run out of memory for an\nimage on a 11 Gb NVIDIA GeForce GTX 1080 Ti. In contrast, our approximate diagonal Hessian\nscales linearly with the resolution, which is especially bene\ufb01cial for convolutional layers.\n\n36\n\n36\n\n\u21e5\n\n\u21e0\n\n\u21e5\n\n2 https://github.com/FrederikWarburg/LaplaceAE\n\n7\n\n\f\u00b5\n\n\u00b5\n\nFigure 9: Receiver Operator Curves (ROC) show that our online LAE is able to discriminate between\n\u00b5\nin and out of distribution in both latent and output space.\nlog p(x)\n\nHessian\n\nMSE\n\n\u00b5\n\n\u00b5\n\n\n\n\u00002\n\n\n\n\u00002\n\n\n\n\u00002\n\n\n\n\u00002\n\n\n\n\u00002\n\nKFAC\nExact\nApprox\nExact\u21e4\nApprox\u21e4\n\n\u0000\n9683.9\n283.3\n232.0\n25.8\n25.9\n\n\u00b1\n\u00b1\n\n#\n\n2455.0\n\u00b1\n88.6\n\u00b1\n65.5\n\u00b1\n0.2\n0.4\n\n#\n\n121.6\n27.1\n26.6\n25.7\n25.8\n\n\u00b1\n\u00b1\n\u00b1\n\u00b1\n\u00b1\n\n24.5\n0.9\n0.6\n0.2\n0.4\n\nApprox\n\nExact\n(a) Post-hoc\n\n\n\nFigure 8: Mean and variance of 100 sampled NN.\n\nKFAC\n\nExact\u21e4\n\nApprox\u21e4\n\nTable 2: Online training (indicated by \u21e4) outper-\nforms post-hoc LA. The approximate diagonal\nhas similar performance to the exact diagonal\nfor both post-hoc and online LA.\nFigure 10: Histograms of variance for in- and OOD reconstructions in output space. Note that\nMC-AE separates the distributions well, but the model assign higher variance to the in- than OOD\ndata.\n\n(b) Online\n\n\n\nTab. 2 shows that the exact or approximate Hessian diagonal has similar performance for both\npost-hoc and online training. Using post-hoc LA results in good mean reconstructions (low MSE),\nbut each sampled NN does not give good reconstructions (low log p(x)). Using our online training\nprocedure results in a much higher log-likelihood. This indicates that every sampled NN predicts\ngood reconstructions.\n\nthe uncertainties more well \ufb01tted, both in latent and data space. These well-\ufb01tted uncertainties have\nseveral practical downstream applications, which we demonstrate next.\n\n218\n\n219\n\n224\n\n225\n\n222\n\n223\n\n221\n\n220\n\nFig. 8 shows the latent representation, mean, and variance of the reconstructions with the KFAC, exact\nand approximate diagonal for both post-hoc and online setup. Note that the online training makes the\nuncertainties better \ufb01tted, both in latent and data space. These well-\ufb01tted uncertainties have several\npractical downstream applications, which we demonstrate next.\n\nOut-of-Distribution (OOD) Detection capabilities are critical for identifying distributional shifts,\noutliers and irregular user inputs, which can hinder the propagation of erroneous decision in an\nautomated system. We evaluate OOD performance on the commonly used benchmarks (Nalisnick\net al., 2019b), where we use FASHIONMNIST (Xiao et al., 2017) as in-distribution and MNIST\n(Lecun et al., 1998) as OOD. Tab. 3 shows that our online LAE outperforms existing models in\nboth log-likelihood and Typicality score (Nalisnick et al., 2019a). This stems from the calibrated\nmodel uncertainties, which are exempli\ufb01ed in the models ability to detect OOD examples from the\nuncertainty deduced in latent and output space; see Fig. 9 for ROC curves.\n\nOut-of-Distribution (OOD) Detection capabilities are critical for identifying distributional shifts,\noutliers, and irregular user inputs, which can hinder the propagation of erroneous decisions in an\nautomated system. We evaluate OOD performance on the commonly used benchmarks (Nalisnick\net al., 2019b), where we use FASHIONMNIST (Xiao et al., 2017) as in-distribution and MNIST\n(Lecun et al., 1998) as OOD. Fig. 9 (c) shows that our online LAE outperforms existing models in\nboth log-likelihood and Typicality score (Nalisnick et al., 2019a). This stems from the calibrated\nmodel uncertainties, which are exempli\ufb01ed in the models ability to detect OOD examples from the\nuncertainty deduced in latent and output space; see Fig. 9 (a,b) for ROC curves.\n\nFig. 10 shows distribution of the output variances for in- and OOD data. This illustrates that using LA\nimproves OoD detection. Furthermore, the online training improves the model calibration.\n\nMissing Data imputation. Another application of stochastic representation learning is to provide\ndistributions over unobserved values (Rezende et al., 2014). In many application domains, sensor\nreadings go missing, which we may mimic by letting parts of an image be unobserved. Rezende et al.\n(2014) show that we can then draw samples from the distribution of the entire image conditioned on\n\nFig. 10 shows distribution of the output variances for in- and OOD data. This illustrates that using LA\nimproves OoD detection. Furthermore, the online training improves the model calibration.\n\n226\n\n227\n\n232\n\n228\n\n229\n\n230\n\n231\n\n233\n\nMethod\n\nlogp(x) Typicality\n\n\u00002\n\noutput\n\n\u00002\n\nlatent\n\nAE MSE\nAE MSE\nAE likelihood\nAE likelihood\nVAE\nVAE\nMC-AE\nMC-AE\nLAE (post-hoc)\nLAE (post-hoc)\nLAE (online)\nLAE (online)\n\n\n\n\n\n0.98\n0.66\n0.54\n0.97\n0.98\n0.98\n\n0.98\n0.70\n0.62\n0.97\n0.98\n0.98\n\n0.65\n0.46\n0.08\n0.74\n0.91\n\n0.5\n0.31\n0.96\n0.96\n\n\n\n\n\nlatent\n\noutput\n\n(b) \u00002\n\n(a) \u00002\n\nTable 3: Out of Distribution detection. In-distribution data FashionMnist and OOD data Mnist.\nArea Under Receiver Operator Curve (AUROC\n) for log-likelihood, typicality score, latent space\n\"\n\u0000latent and output space \u0000output uncertainties. Our proposed LAE ELBO is able to discriminate between\nin and OOD using the deduced variances in latent and output space.\n\nFigure 9: Out of Distribution detection. In-distribution data FashionMnist and OOD data Mnist. (a)\n) for\nand (b) shows the ROC curves for latent and output space uncertainties. (c) shows (AUROC\n\"\nlog-likelihood, typicality score, latent space \u0000latent and output space \u0000output uncertainties. Online LAE\nis able to discriminate between in and OOD using the deduced variances in latent and output space.\n\n) for multiple statistics.\n\"\n\n(c) AUROC (\n\n(a) Latent variance, \u00002\n\n(b) Output variance, \u00002\n\noutput\n\nlatent\n\n\n\n\n\n\n\n8\n\nID\nOoD\n\nID\nOoD\n\n\n\n\n\n\n\n\n\n\n\n(a) AE likelihood\n\n(b) MC-AE\n\n(c) VAE\n\n(d) LAE (post-hoc)\n\n(e) LAE (online)\n\nFigure 10: Histograms of variance for in- and OOD reconstructions in output space. Note that MC-\nAE separates the distributions well, but the model assigns higher variance to the in- than OOD data.\n\n\n\n\n\n\n\n\n\n\n\n(a) AE likelihood\n\n(b) MC-AE\n\n(d) LAE (post-hoc)\n\n(e) LAE (online)\n\n(c) VAE\n8\n\n\f\n\n\n\n\n\n(a) VAE\n\n(b) LAE (post-hoc)\n\n(c) LAE (online)\n\nFigure 11: Missing data imputation & generative capabilities. Online training of LAE improves\nrepresentational robustness. This is exempli\ufb01ed by the multimodal behavior in the data imputation\n(top rows) that accurately model the ambiguity in the data. The bottom two rows show that the LAE\nis able to generate crisp digits from random noise.\n\nMethod\n\nClassi\ufb01er\nVAE\nMC-AE\nAE Ensemble\nLAE (post-hoc)\nLAE (online)\n\nMSE\n\n#\n\nlog p(x)\n\n\"\n\n104.73\n106.05\n96.23\n101.62\n99.59\n\n-104.75\n-106.05\n-100.94\n-107.25\n-106.29\n\n\"\n\nAcc.\n\n0.53\n0.22\n0.45\n0.53\n0.51\n0.53\n\n#\n\nECE\n\n0.16\n0.18\n0.28\n0.12\n0.16\n0.12\n\n#\n\nMCE\n\n0.25\n0.34\n0.38\n0.2\n0.27\n0.16\n\n#\n\nRMSCE\n\n0.18\n0.19\n0.29\n0.13\n0.18\n0.13\n\nTable 3: Reconstruction quality measured by the MSE and log-likelihood for the data imputation.\nOur well-calibrated uncertainties propagates to the MNIST classi\ufb01er and improves the calibration\nmetrics ECE, MCE and RMSCE.\n\nMissing Data imputation. Another application of stochastic representation learning is to provide\ndistributions over unobserved values (Rezende et al., 2014). In many application domains, sensor\nreadings go missing, which we may mimic by letting parts of an image be unobserved. Rezende et al.\n(2014) show that we can then draw samples from the distribution of the entire image conditioned on\nthe observed part, by imputing the missing pixels with noise and repeatedly encode and decode while\nkeeping observed pixels \ufb01xed. Fig. 11 show samples using this procedure from a VAE, a post-hoc\nLAE and our online LAE, where we only observe the lower half of an MNIST image. This implies\nambiguity about the original digit, e.g. the lower half of a \u201c5\u201c could be a \u201c3\u201c and similarly a \u201c7\u201c could\nbe a \u201c9\u201c. Our LAE captures this ambiguity, which is exempli\ufb01ed by the multi-modal reconstructions\nfrom the sampled networks in Fig. 11. The baselines only capture unimodal reconstructions.\n\nCapturing the ambiguity of partly missing data can improve downstream tasks such as the calibration\nof an image classi\ufb01er. In Fig. 11 (c) we demonstrate how averaging the predictions of a simple\nclassi\ufb01er across reconstructions improves standard calibration metrics. This is because the classi\ufb01er\ninherits the uncertainty and ambiguity in the learned representations. A deep ensemble of AEs\nperforms similarly to ours, but comes at the cost of training and storing multiple models.\n\nWhen the entire input image is missing, the imputation procedure can be seen as a sampling mecha-\nnism, such that our LAE can be viewed as a generative model. The bottom rows in Fig. 11 show that\nthe LAE indeed does generate sharp images from a multi-modal distribution.\n\nAttribute\n\nArched Eyebrows\nAttractive\nBald\nWearing Lipstick\nHeavy Makeup\n\nOverall\n\nAE\n\n0.50\n0.52\n0.98\n0.52\n0.45\n\n0.73\n\nVAE MC-AE LAE\u21e4\n\n0.52\n0.50\n0.98\n0.49\n0.52\n\n0.72\n\n0.55\n0.49\n0.98\n0.50\n0.49\n\n0.73\n\n0.60\n0.53\n0.98\n0.54\n0.56\n\n0.74\n\nTable 4: Semi-supervised classi\ufb01cation accuracy\non CELEBA using only 10 labeled datapoints. \u21e4\nrefers to online LAE.\n\n9\n\nFigure 12: Accuracy as an function of the\nnumber of labels per class on MNIST.\n\n\fFigure 13: Sample reconstructions on CELEBA. The top row shows the mean reconstruction and\nthe bottom row shows the variance of the reconstructed images.\n\nSemi-supervised learning combines a small amount of label data with a large amount of unlabeled\ndata. The hope is that the structure in the unlabeled data can be used to infer properties of the data that\ncannot be extracted from a few labeled points. Embedding the same labeled data point multiple times\nusing a stochastic representation scales up the amount of labeled data that is available during training.\n\nFig. 12 shows the accuracy of a K-nearest neighbor classi\ufb01er trained on different amounts of labeled\ndata from the MNIST dataset. For all models with a stochastic encoder, we encode each labeled\ndatapoint 100 times and repeat the experiment 5 times. When only a few labels per class are\navailable (1-20) we clearly observe that our LAE model outperforms all other models, stochastic and\ndeterministic. Increasing the number of labels beyond 100 per class makes the AE and LAE equal in\ntheir classi\ufb01cation performance with the AE model eventually outperforming the LAE model.\n\nIn Tab. 4 we conduct a similar experiment on the CELEBA (Liu et al., 2015) facial dataset, where the\nthe task is to predict 40 different binary labels per data point. When evaluating the overall accuracy of\npredicting all 40 facial attributes, we see no signi\ufb01cant difference in performance. However, when we\nzoom in on speci\ufb01c facial attributes we gain a clear performance advantage over other models. Fig. 13\nshows the mean and variance of \ufb01ve reconstructed images. The online LAE produces well-calibrated\nuncertainties in the output space and scales to large images.\n\nLimitations. Empirically, the LAE improvements are more signi\ufb01cant for overparameterized net-\nworks. The additional capacity seems to help the optimizer \ufb01nd a local mode where a Gaussian \ufb01t is ap-\npropriate. It seems the regularization induced by marginalizing \u2713 compensates for the added \ufb02exibility.\n\n\n\nBased on the section(s), please answer the following question with yes, no, or n/a and provide a brief justification for your answer.\nQuestion: If the authors used crowdsourcing or conducted research with human subjects, do the authors include the full text of instructions given to participants and screenshots, if applicable?"}, "5b": {"role": "user", "content": "The following is the experiments section of the paper you are reviewing:\n\n\nFirst, we demonstrate the computational advantages of the proposed Hessian approximation, and\nthen that our sampling-based training leads to well-calibrated uncertainties that can be used for OOD\ndetection, data imputation, and semi-supervised learning. For all the downstream tasks we consider\nthe following baselines: AE (Hinton and Salakhutdinov, 2006) with constant and learned variance, VAE\n(Rezende et al., 2014; Kingma and Welling, 2014), Monte-Carlo dropout AE (Gal and Ghahramani,\n2016) and Ensembles of AE (Lakshminarayanan et al., 2017). We extend StochManDetlefsen et al.\n(2021) with the Hessian backpropagation for the approximate and mixed diagonals. The training code\nis implemented in PyTorch and available2 . Appendix A provides more details on the experimental\nsetup.\n\nEf\ufb01cient Hessian Approximation. For practical applications, training time and memory usage of the\nHessian approximation must be kept low. We here show that the proposed approximate diagonal Hes-\nsian is suf\ufb01cient and even outperforms other approximations when combined with our online training.\n\nFig. 7 show the time and memory requirement for different approximation methods as a function\nof input size for a 5-layer convolutional network that preserves channel and input dimension. As\nbaselines we use ef\ufb01cient implementations of the exact and KFAC approximation (Daxberger et al.,\n3\n2021; Dangel et al., 2020). The exact diagonal approximation run out of memory for an\nimage on a 11 Gb NVIDIA GeForce GTX 1080 Ti. In contrast, our approximate diagonal Hessian\nscales linearly with the resolution, which is especially bene\ufb01cial for convolutional layers.\n\n36\n\n36\n\n\u21e5\n\n\u21e0\n\n\u21e5\n\n2 https://github.com/FrederikWarburg/LaplaceAE\n\n7\n\n\f\u00b5\n\n\u00b5\n\nFigure 9: Receiver Operator Curves (ROC) show that our online LAE is able to discriminate between\n\u00b5\nin and out of distribution in both latent and output space.\nlog p(x)\n\nHessian\n\nMSE\n\n\u00b5\n\n\u00b5\n\n\n\n\u00002\n\n\n\n\u00002\n\n\n\n\u00002\n\n\n\n\u00002\n\n\n\n\u00002\n\nKFAC\nExact\nApprox\nExact\u21e4\nApprox\u21e4\n\n\u0000\n9683.9\n283.3\n232.0\n25.8\n25.9\n\n\u00b1\n\u00b1\n\n#\n\n2455.0\n\u00b1\n88.6\n\u00b1\n65.5\n\u00b1\n0.2\n0.4\n\n#\n\n121.6\n27.1\n26.6\n25.7\n25.8\n\n\u00b1\n\u00b1\n\u00b1\n\u00b1\n\u00b1\n\n24.5\n0.9\n0.6\n0.2\n0.4\n\nApprox\n\nExact\n(a) Post-hoc\n\n\n\nFigure 8: Mean and variance of 100 sampled NN.\n\nKFAC\n\nExact\u21e4\n\nApprox\u21e4\n\nTable 2: Online training (indicated by \u21e4) outper-\nforms post-hoc LA. The approximate diagonal\nhas similar performance to the exact diagonal\nfor both post-hoc and online LA.\nFigure 10: Histograms of variance for in- and OOD reconstructions in output space. Note that\nMC-AE separates the distributions well, but the model assign higher variance to the in- than OOD\ndata.\n\n(b) Online\n\n\n\nTab. 2 shows that the exact or approximate Hessian diagonal has similar performance for both\npost-hoc and online training. Using post-hoc LA results in good mean reconstructions (low MSE),\nbut each sampled NN does not give good reconstructions (low log p(x)). Using our online training\nprocedure results in a much higher log-likelihood. This indicates that every sampled NN predicts\ngood reconstructions.\n\nthe uncertainties more well \ufb01tted, both in latent and data space. These well-\ufb01tted uncertainties have\nseveral practical downstream applications, which we demonstrate next.\n\n218\n\n219\n\n224\n\n225\n\n222\n\n223\n\n221\n\n220\n\nFig. 8 shows the latent representation, mean, and variance of the reconstructions with the KFAC, exact\nand approximate diagonal for both post-hoc and online setup. Note that the online training makes the\nuncertainties better \ufb01tted, both in latent and data space. These well-\ufb01tted uncertainties have several\npractical downstream applications, which we demonstrate next.\n\nOut-of-Distribution (OOD) Detection capabilities are critical for identifying distributional shifts,\noutliers and irregular user inputs, which can hinder the propagation of erroneous decision in an\nautomated system. We evaluate OOD performance on the commonly used benchmarks (Nalisnick\net al., 2019b), where we use FASHIONMNIST (Xiao et al., 2017) as in-distribution and MNIST\n(Lecun et al., 1998) as OOD. Tab. 3 shows that our online LAE outperforms existing models in\nboth log-likelihood and Typicality score (Nalisnick et al., 2019a). This stems from the calibrated\nmodel uncertainties, which are exempli\ufb01ed in the models ability to detect OOD examples from the\nuncertainty deduced in latent and output space; see Fig. 9 for ROC curves.\n\nOut-of-Distribution (OOD) Detection capabilities are critical for identifying distributional shifts,\noutliers, and irregular user inputs, which can hinder the propagation of erroneous decisions in an\nautomated system. We evaluate OOD performance on the commonly used benchmarks (Nalisnick\net al., 2019b), where we use FASHIONMNIST (Xiao et al., 2017) as in-distribution and MNIST\n(Lecun et al., 1998) as OOD. Fig. 9 (c) shows that our online LAE outperforms existing models in\nboth log-likelihood and Typicality score (Nalisnick et al., 2019a). This stems from the calibrated\nmodel uncertainties, which are exempli\ufb01ed in the models ability to detect OOD examples from the\nuncertainty deduced in latent and output space; see Fig. 9 (a,b) for ROC curves.\n\nFig. 10 shows distribution of the output variances for in- and OOD data. This illustrates that using LA\nimproves OoD detection. Furthermore, the online training improves the model calibration.\n\nMissing Data imputation. Another application of stochastic representation learning is to provide\ndistributions over unobserved values (Rezende et al., 2014). In many application domains, sensor\nreadings go missing, which we may mimic by letting parts of an image be unobserved. Rezende et al.\n(2014) show that we can then draw samples from the distribution of the entire image conditioned on\n\nFig. 10 shows distribution of the output variances for in- and OOD data. This illustrates that using LA\nimproves OoD detection. Furthermore, the online training improves the model calibration.\n\n226\n\n227\n\n232\n\n228\n\n229\n\n230\n\n231\n\n233\n\nMethod\n\nlogp(x) Typicality\n\n\u00002\n\noutput\n\n\u00002\n\nlatent\n\nAE MSE\nAE MSE\nAE likelihood\nAE likelihood\nVAE\nVAE\nMC-AE\nMC-AE\nLAE (post-hoc)\nLAE (post-hoc)\nLAE (online)\nLAE (online)\n\n\n\n\n\n0.98\n0.66\n0.54\n0.97\n0.98\n0.98\n\n0.98\n0.70\n0.62\n0.97\n0.98\n0.98\n\n0.65\n0.46\n0.08\n0.74\n0.91\n\n0.5\n0.31\n0.96\n0.96\n\n\n\n\n\nlatent\n\noutput\n\n(b) \u00002\n\n(a) \u00002\n\nTable 3: Out of Distribution detection. In-distribution data FashionMnist and OOD data Mnist.\nArea Under Receiver Operator Curve (AUROC\n) for log-likelihood, typicality score, latent space\n\"\n\u0000latent and output space \u0000output uncertainties. Our proposed LAE ELBO is able to discriminate between\nin and OOD using the deduced variances in latent and output space.\n\nFigure 9: Out of Distribution detection. In-distribution data FashionMnist and OOD data Mnist. (a)\n) for\nand (b) shows the ROC curves for latent and output space uncertainties. (c) shows (AUROC\n\"\nlog-likelihood, typicality score, latent space \u0000latent and output space \u0000output uncertainties. Online LAE\nis able to discriminate between in and OOD using the deduced variances in latent and output space.\n\n) for multiple statistics.\n\"\n\n(c) AUROC (\n\n(a) Latent variance, \u00002\n\n(b) Output variance, \u00002\n\noutput\n\nlatent\n\n\n\n\n\n\n\n8\n\nID\nOoD\n\nID\nOoD\n\n\n\n\n\n\n\n\n\n\n\n(a) AE likelihood\n\n(b) MC-AE\n\n(c) VAE\n\n(d) LAE (post-hoc)\n\n(e) LAE (online)\n\nFigure 10: Histograms of variance for in- and OOD reconstructions in output space. Note that MC-\nAE separates the distributions well, but the model assigns higher variance to the in- than OOD data.\n\n\n\n\n\n\n\n\n\n\n\n(a) AE likelihood\n\n(b) MC-AE\n\n(d) LAE (post-hoc)\n\n(e) LAE (online)\n\n(c) VAE\n8\n\n\f\n\n\n\n\n\n(a) VAE\n\n(b) LAE (post-hoc)\n\n(c) LAE (online)\n\nFigure 11: Missing data imputation & generative capabilities. Online training of LAE improves\nrepresentational robustness. This is exempli\ufb01ed by the multimodal behavior in the data imputation\n(top rows) that accurately model the ambiguity in the data. The bottom two rows show that the LAE\nis able to generate crisp digits from random noise.\n\nMethod\n\nClassi\ufb01er\nVAE\nMC-AE\nAE Ensemble\nLAE (post-hoc)\nLAE (online)\n\nMSE\n\n#\n\nlog p(x)\n\n\"\n\n104.73\n106.05\n96.23\n101.62\n99.59\n\n-104.75\n-106.05\n-100.94\n-107.25\n-106.29\n\n\"\n\nAcc.\n\n0.53\n0.22\n0.45\n0.53\n0.51\n0.53\n\n#\n\nECE\n\n0.16\n0.18\n0.28\n0.12\n0.16\n0.12\n\n#\n\nMCE\n\n0.25\n0.34\n0.38\n0.2\n0.27\n0.16\n\n#\n\nRMSCE\n\n0.18\n0.19\n0.29\n0.13\n0.18\n0.13\n\nTable 3: Reconstruction quality measured by the MSE and log-likelihood for the data imputation.\nOur well-calibrated uncertainties propagates to the MNIST classi\ufb01er and improves the calibration\nmetrics ECE, MCE and RMSCE.\n\nMissing Data imputation. Another application of stochastic representation learning is to provide\ndistributions over unobserved values (Rezende et al., 2014). In many application domains, sensor\nreadings go missing, which we may mimic by letting parts of an image be unobserved. Rezende et al.\n(2014) show that we can then draw samples from the distribution of the entire image conditioned on\nthe observed part, by imputing the missing pixels with noise and repeatedly encode and decode while\nkeeping observed pixels \ufb01xed. Fig. 11 show samples using this procedure from a VAE, a post-hoc\nLAE and our online LAE, where we only observe the lower half of an MNIST image. This implies\nambiguity about the original digit, e.g. the lower half of a \u201c5\u201c could be a \u201c3\u201c and similarly a \u201c7\u201c could\nbe a \u201c9\u201c. Our LAE captures this ambiguity, which is exempli\ufb01ed by the multi-modal reconstructions\nfrom the sampled networks in Fig. 11. The baselines only capture unimodal reconstructions.\n\nCapturing the ambiguity of partly missing data can improve downstream tasks such as the calibration\nof an image classi\ufb01er. In Fig. 11 (c) we demonstrate how averaging the predictions of a simple\nclassi\ufb01er across reconstructions improves standard calibration metrics. This is because the classi\ufb01er\ninherits the uncertainty and ambiguity in the learned representations. A deep ensemble of AEs\nperforms similarly to ours, but comes at the cost of training and storing multiple models.\n\nWhen the entire input image is missing, the imputation procedure can be seen as a sampling mecha-\nnism, such that our LAE can be viewed as a generative model. The bottom rows in Fig. 11 show that\nthe LAE indeed does generate sharp images from a multi-modal distribution.\n\nAttribute\n\nArched Eyebrows\nAttractive\nBald\nWearing Lipstick\nHeavy Makeup\n\nOverall\n\nAE\n\n0.50\n0.52\n0.98\n0.52\n0.45\n\n0.73\n\nVAE MC-AE LAE\u21e4\n\n0.52\n0.50\n0.98\n0.49\n0.52\n\n0.72\n\n0.55\n0.49\n0.98\n0.50\n0.49\n\n0.73\n\n0.60\n0.53\n0.98\n0.54\n0.56\n\n0.74\n\nTable 4: Semi-supervised classi\ufb01cation accuracy\non CELEBA using only 10 labeled datapoints. \u21e4\nrefers to online LAE.\n\n9\n\nFigure 12: Accuracy as an function of the\nnumber of labels per class on MNIST.\n\n\fFigure 13: Sample reconstructions on CELEBA. The top row shows the mean reconstruction and\nthe bottom row shows the variance of the reconstructed images.\n\nSemi-supervised learning combines a small amount of label data with a large amount of unlabeled\ndata. The hope is that the structure in the unlabeled data can be used to infer properties of the data that\ncannot be extracted from a few labeled points. Embedding the same labeled data point multiple times\nusing a stochastic representation scales up the amount of labeled data that is available during training.\n\nFig. 12 shows the accuracy of a K-nearest neighbor classi\ufb01er trained on different amounts of labeled\ndata from the MNIST dataset. For all models with a stochastic encoder, we encode each labeled\ndatapoint 100 times and repeat the experiment 5 times. When only a few labels per class are\navailable (1-20) we clearly observe that our LAE model outperforms all other models, stochastic and\ndeterministic. Increasing the number of labels beyond 100 per class makes the AE and LAE equal in\ntheir classi\ufb01cation performance with the AE model eventually outperforming the LAE model.\n\nIn Tab. 4 we conduct a similar experiment on the CELEBA (Liu et al., 2015) facial dataset, where the\nthe task is to predict 40 different binary labels per data point. When evaluating the overall accuracy of\npredicting all 40 facial attributes, we see no signi\ufb01cant difference in performance. However, when we\nzoom in on speci\ufb01c facial attributes we gain a clear performance advantage over other models. Fig. 13\nshows the mean and variance of \ufb01ve reconstructed images. The online LAE produces well-calibrated\nuncertainties in the output space and scales to large images.\n\nLimitations. Empirically, the LAE improvements are more signi\ufb01cant for overparameterized net-\nworks. The additional capacity seems to help the optimizer \ufb01nd a local mode where a Gaussian \ufb01t is ap-\npropriate. It seems the regularization induced by marginalizing \u2713 compensates for the added \ufb02exibility.\n\n\n\nBased on the section(s), please answer the following question with yes, no, or n/a and provide a brief justification for your answer.\nQuestion: If the authors used crowdsourcing or conducted research with human subjects, do the authors describe any potential participant risks, with links to Institutional Review Board (IRB) approvals, if applicable?"}, "5c": {"role": "user", "content": "The following is the experiments section of the paper you are reviewing:\n\n\nFirst, we demonstrate the computational advantages of the proposed Hessian approximation, and\nthen that our sampling-based training leads to well-calibrated uncertainties that can be used for OOD\ndetection, data imputation, and semi-supervised learning. For all the downstream tasks we consider\nthe following baselines: AE (Hinton and Salakhutdinov, 2006) with constant and learned variance, VAE\n(Rezende et al., 2014; Kingma and Welling, 2014), Monte-Carlo dropout AE (Gal and Ghahramani,\n2016) and Ensembles of AE (Lakshminarayanan et al., 2017). We extend StochManDetlefsen et al.\n(2021) with the Hessian backpropagation for the approximate and mixed diagonals. The training code\nis implemented in PyTorch and available2 . Appendix A provides more details on the experimental\nsetup.\n\nEf\ufb01cient Hessian Approximation. For practical applications, training time and memory usage of the\nHessian approximation must be kept low. We here show that the proposed approximate diagonal Hes-\nsian is suf\ufb01cient and even outperforms other approximations when combined with our online training.\n\nFig. 7 show the time and memory requirement for different approximation methods as a function\nof input size for a 5-layer convolutional network that preserves channel and input dimension. As\nbaselines we use ef\ufb01cient implementations of the exact and KFAC approximation (Daxberger et al.,\n3\n2021; Dangel et al., 2020). The exact diagonal approximation run out of memory for an\nimage on a 11 Gb NVIDIA GeForce GTX 1080 Ti. In contrast, our approximate diagonal Hessian\nscales linearly with the resolution, which is especially bene\ufb01cial for convolutional layers.\n\n36\n\n36\n\n\u21e5\n\n\u21e0\n\n\u21e5\n\n2 https://github.com/FrederikWarburg/LaplaceAE\n\n7\n\n\f\u00b5\n\n\u00b5\n\nFigure 9: Receiver Operator Curves (ROC) show that our online LAE is able to discriminate between\n\u00b5\nin and out of distribution in both latent and output space.\nlog p(x)\n\nHessian\n\nMSE\n\n\u00b5\n\n\u00b5\n\n\n\n\u00002\n\n\n\n\u00002\n\n\n\n\u00002\n\n\n\n\u00002\n\n\n\n\u00002\n\nKFAC\nExact\nApprox\nExact\u21e4\nApprox\u21e4\n\n\u0000\n9683.9\n283.3\n232.0\n25.8\n25.9\n\n\u00b1\n\u00b1\n\n#\n\n2455.0\n\u00b1\n88.6\n\u00b1\n65.5\n\u00b1\n0.2\n0.4\n\n#\n\n121.6\n27.1\n26.6\n25.7\n25.8\n\n\u00b1\n\u00b1\n\u00b1\n\u00b1\n\u00b1\n\n24.5\n0.9\n0.6\n0.2\n0.4\n\nApprox\n\nExact\n(a) Post-hoc\n\n\n\nFigure 8: Mean and variance of 100 sampled NN.\n\nKFAC\n\nExact\u21e4\n\nApprox\u21e4\n\nTable 2: Online training (indicated by \u21e4) outper-\nforms post-hoc LA. The approximate diagonal\nhas similar performance to the exact diagonal\nfor both post-hoc and online LA.\nFigure 10: Histograms of variance for in- and OOD reconstructions in output space. Note that\nMC-AE separates the distributions well, but the model assign higher variance to the in- than OOD\ndata.\n\n(b) Online\n\n\n\nTab. 2 shows that the exact or approximate Hessian diagonal has similar performance for both\npost-hoc and online training. Using post-hoc LA results in good mean reconstructions (low MSE),\nbut each sampled NN does not give good reconstructions (low log p(x)). Using our online training\nprocedure results in a much higher log-likelihood. This indicates that every sampled NN predicts\ngood reconstructions.\n\nthe uncertainties more well \ufb01tted, both in latent and data space. These well-\ufb01tted uncertainties have\nseveral practical downstream applications, which we demonstrate next.\n\n218\n\n219\n\n224\n\n225\n\n222\n\n223\n\n221\n\n220\n\nFig. 8 shows the latent representation, mean, and variance of the reconstructions with the KFAC, exact\nand approximate diagonal for both post-hoc and online setup. Note that the online training makes the\nuncertainties better \ufb01tted, both in latent and data space. These well-\ufb01tted uncertainties have several\npractical downstream applications, which we demonstrate next.\n\nOut-of-Distribution (OOD) Detection capabilities are critical for identifying distributional shifts,\noutliers and irregular user inputs, which can hinder the propagation of erroneous decision in an\nautomated system. We evaluate OOD performance on the commonly used benchmarks (Nalisnick\net al., 2019b), where we use FASHIONMNIST (Xiao et al., 2017) as in-distribution and MNIST\n(Lecun et al., 1998) as OOD. Tab. 3 shows that our online LAE outperforms existing models in\nboth log-likelihood and Typicality score (Nalisnick et al., 2019a). This stems from the calibrated\nmodel uncertainties, which are exempli\ufb01ed in the models ability to detect OOD examples from the\nuncertainty deduced in latent and output space; see Fig. 9 for ROC curves.\n\nOut-of-Distribution (OOD) Detection capabilities are critical for identifying distributional shifts,\noutliers, and irregular user inputs, which can hinder the propagation of erroneous decisions in an\nautomated system. We evaluate OOD performance on the commonly used benchmarks (Nalisnick\net al., 2019b), where we use FASHIONMNIST (Xiao et al., 2017) as in-distribution and MNIST\n(Lecun et al., 1998) as OOD. Fig. 9 (c) shows that our online LAE outperforms existing models in\nboth log-likelihood and Typicality score (Nalisnick et al., 2019a). This stems from the calibrated\nmodel uncertainties, which are exempli\ufb01ed in the models ability to detect OOD examples from the\nuncertainty deduced in latent and output space; see Fig. 9 (a,b) for ROC curves.\n\nFig. 10 shows distribution of the output variances for in- and OOD data. This illustrates that using LA\nimproves OoD detection. Furthermore, the online training improves the model calibration.\n\nMissing Data imputation. Another application of stochastic representation learning is to provide\ndistributions over unobserved values (Rezende et al., 2014). In many application domains, sensor\nreadings go missing, which we may mimic by letting parts of an image be unobserved. Rezende et al.\n(2014) show that we can then draw samples from the distribution of the entire image conditioned on\n\nFig. 10 shows distribution of the output variances for in- and OOD data. This illustrates that using LA\nimproves OoD detection. Furthermore, the online training improves the model calibration.\n\n226\n\n227\n\n232\n\n228\n\n229\n\n230\n\n231\n\n233\n\nMethod\n\nlogp(x) Typicality\n\n\u00002\n\noutput\n\n\u00002\n\nlatent\n\nAE MSE\nAE MSE\nAE likelihood\nAE likelihood\nVAE\nVAE\nMC-AE\nMC-AE\nLAE (post-hoc)\nLAE (post-hoc)\nLAE (online)\nLAE (online)\n\n\n\n\n\n0.98\n0.66\n0.54\n0.97\n0.98\n0.98\n\n0.98\n0.70\n0.62\n0.97\n0.98\n0.98\n\n0.65\n0.46\n0.08\n0.74\n0.91\n\n0.5\n0.31\n0.96\n0.96\n\n\n\n\n\nlatent\n\noutput\n\n(b) \u00002\n\n(a) \u00002\n\nTable 3: Out of Distribution detection. In-distribution data FashionMnist and OOD data Mnist.\nArea Under Receiver Operator Curve (AUROC\n) for log-likelihood, typicality score, latent space\n\"\n\u0000latent and output space \u0000output uncertainties. Our proposed LAE ELBO is able to discriminate between\nin and OOD using the deduced variances in latent and output space.\n\nFigure 9: Out of Distribution detection. In-distribution data FashionMnist and OOD data Mnist. (a)\n) for\nand (b) shows the ROC curves for latent and output space uncertainties. (c) shows (AUROC\n\"\nlog-likelihood, typicality score, latent space \u0000latent and output space \u0000output uncertainties. Online LAE\nis able to discriminate between in and OOD using the deduced variances in latent and output space.\n\n) for multiple statistics.\n\"\n\n(c) AUROC (\n\n(a) Latent variance, \u00002\n\n(b) Output variance, \u00002\n\noutput\n\nlatent\n\n\n\n\n\n\n\n8\n\nID\nOoD\n\nID\nOoD\n\n\n\n\n\n\n\n\n\n\n\n(a) AE likelihood\n\n(b) MC-AE\n\n(c) VAE\n\n(d) LAE (post-hoc)\n\n(e) LAE (online)\n\nFigure 10: Histograms of variance for in- and OOD reconstructions in output space. Note that MC-\nAE separates the distributions well, but the model assigns higher variance to the in- than OOD data.\n\n\n\n\n\n\n\n\n\n\n\n(a) AE likelihood\n\n(b) MC-AE\n\n(d) LAE (post-hoc)\n\n(e) LAE (online)\n\n(c) VAE\n8\n\n\f\n\n\n\n\n\n(a) VAE\n\n(b) LAE (post-hoc)\n\n(c) LAE (online)\n\nFigure 11: Missing data imputation & generative capabilities. Online training of LAE improves\nrepresentational robustness. This is exempli\ufb01ed by the multimodal behavior in the data imputation\n(top rows) that accurately model the ambiguity in the data. The bottom two rows show that the LAE\nis able to generate crisp digits from random noise.\n\nMethod\n\nClassi\ufb01er\nVAE\nMC-AE\nAE Ensemble\nLAE (post-hoc)\nLAE (online)\n\nMSE\n\n#\n\nlog p(x)\n\n\"\n\n104.73\n106.05\n96.23\n101.62\n99.59\n\n-104.75\n-106.05\n-100.94\n-107.25\n-106.29\n\n\"\n\nAcc.\n\n0.53\n0.22\n0.45\n0.53\n0.51\n0.53\n\n#\n\nECE\n\n0.16\n0.18\n0.28\n0.12\n0.16\n0.12\n\n#\n\nMCE\n\n0.25\n0.34\n0.38\n0.2\n0.27\n0.16\n\n#\n\nRMSCE\n\n0.18\n0.19\n0.29\n0.13\n0.18\n0.13\n\nTable 3: Reconstruction quality measured by the MSE and log-likelihood for the data imputation.\nOur well-calibrated uncertainties propagates to the MNIST classi\ufb01er and improves the calibration\nmetrics ECE, MCE and RMSCE.\n\nMissing Data imputation. Another application of stochastic representation learning is to provide\ndistributions over unobserved values (Rezende et al., 2014). In many application domains, sensor\nreadings go missing, which we may mimic by letting parts of an image be unobserved. Rezende et al.\n(2014) show that we can then draw samples from the distribution of the entire image conditioned on\nthe observed part, by imputing the missing pixels with noise and repeatedly encode and decode while\nkeeping observed pixels \ufb01xed. Fig. 11 show samples using this procedure from a VAE, a post-hoc\nLAE and our online LAE, where we only observe the lower half of an MNIST image. This implies\nambiguity about the original digit, e.g. the lower half of a \u201c5\u201c could be a \u201c3\u201c and similarly a \u201c7\u201c could\nbe a \u201c9\u201c. Our LAE captures this ambiguity, which is exempli\ufb01ed by the multi-modal reconstructions\nfrom the sampled networks in Fig. 11. The baselines only capture unimodal reconstructions.\n\nCapturing the ambiguity of partly missing data can improve downstream tasks such as the calibration\nof an image classi\ufb01er. In Fig. 11 (c) we demonstrate how averaging the predictions of a simple\nclassi\ufb01er across reconstructions improves standard calibration metrics. This is because the classi\ufb01er\ninherits the uncertainty and ambiguity in the learned representations. A deep ensemble of AEs\nperforms similarly to ours, but comes at the cost of training and storing multiple models.\n\nWhen the entire input image is missing, the imputation procedure can be seen as a sampling mecha-\nnism, such that our LAE can be viewed as a generative model. The bottom rows in Fig. 11 show that\nthe LAE indeed does generate sharp images from a multi-modal distribution.\n\nAttribute\n\nArched Eyebrows\nAttractive\nBald\nWearing Lipstick\nHeavy Makeup\n\nOverall\n\nAE\n\n0.50\n0.52\n0.98\n0.52\n0.45\n\n0.73\n\nVAE MC-AE LAE\u21e4\n\n0.52\n0.50\n0.98\n0.49\n0.52\n\n0.72\n\n0.55\n0.49\n0.98\n0.50\n0.49\n\n0.73\n\n0.60\n0.53\n0.98\n0.54\n0.56\n\n0.74\n\nTable 4: Semi-supervised classi\ufb01cation accuracy\non CELEBA using only 10 labeled datapoints. \u21e4\nrefers to online LAE.\n\n9\n\nFigure 12: Accuracy as an function of the\nnumber of labels per class on MNIST.\n\n\fFigure 13: Sample reconstructions on CELEBA. The top row shows the mean reconstruction and\nthe bottom row shows the variance of the reconstructed images.\n\nSemi-supervised learning combines a small amount of label data with a large amount of unlabeled\ndata. The hope is that the structure in the unlabeled data can be used to infer properties of the data that\ncannot be extracted from a few labeled points. Embedding the same labeled data point multiple times\nusing a stochastic representation scales up the amount of labeled data that is available during training.\n\nFig. 12 shows the accuracy of a K-nearest neighbor classi\ufb01er trained on different amounts of labeled\ndata from the MNIST dataset. For all models with a stochastic encoder, we encode each labeled\ndatapoint 100 times and repeat the experiment 5 times. When only a few labels per class are\navailable (1-20) we clearly observe that our LAE model outperforms all other models, stochastic and\ndeterministic. Increasing the number of labels beyond 100 per class makes the AE and LAE equal in\ntheir classi\ufb01cation performance with the AE model eventually outperforming the LAE model.\n\nIn Tab. 4 we conduct a similar experiment on the CELEBA (Liu et al., 2015) facial dataset, where the\nthe task is to predict 40 different binary labels per data point. When evaluating the overall accuracy of\npredicting all 40 facial attributes, we see no signi\ufb01cant difference in performance. However, when we\nzoom in on speci\ufb01c facial attributes we gain a clear performance advantage over other models. Fig. 13\nshows the mean and variance of \ufb01ve reconstructed images. The online LAE produces well-calibrated\nuncertainties in the output space and scales to large images.\n\nLimitations. Empirically, the LAE improvements are more signi\ufb01cant for overparameterized net-\nworks. The additional capacity seems to help the optimizer \ufb01nd a local mode where a Gaussian \ufb01t is ap-\npropriate. It seems the regularization induced by marginalizing \u2713 compensates for the added \ufb02exibility.\n\n\n\nBased on the section(s), please answer the following question with yes, no, or n/a and provide a brief justification for your answer.\nQuestion: If the authors used crowdsourcing or conducted research with human subjects, do the authors include the estimated hourly wage paid to participants and the total amount spent on participant compensation?"}}}