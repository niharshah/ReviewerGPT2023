{"paper_index": 3, "title": "ReCo: Retrieve and Co-segment for Zero-shot Transfer", "abstract": "", "introduction": "", "methods": "", "experiments": "\nIn this section, we start by describing the datasets used for our experiments (Sec. 4.1) and implemen-\ntation details (Sec. 4.2). Then, we conduct an ablation study (Sec. 4.3) and compare our model to\nstate-of-the-art methods for unsupervised semantic segmentation with and without language-image\npre-training (Sec. 4.4). Finally, we showcase our model\u2019s ability to segment rare-category objects\n(Sec. 4.5).\n\n4.1 Datasets\nFor our ablation study, we use the ImageNet1K [16] validation set to curate archive for concepts\nof interest. The dataset covers 1K classes with 50 images for each class. To measure segmentation\nperformance in the zero-shot transfer setting, we use the PASCAL-Context [58] validation set for\nevaluation, which has 5,104 images of 59 categories excluding the background class.\n\nTo compare with previous unsupervised segmentation methods, we use the ImageNet1K training set\nto construct the reference embedding for the concept we wish to segment. The dataset has 1K classes\nwith 1.2M images. We evaluate on standard benchmarks including the Cityscapes [14] validation\n\n6\n\n\fsplit, which has 500 urban scene images of 27 categories, KITTI-STEP [87] validation set, which\nis composed of 2,981 urban scene images of 19 categories, and COCO-Stuff [6] validation split,\nwhich has 4,172 images of 171 low-level thing and stuff categories excluding background class.\nFollowing [12, 24], we use the 27 mid-level categories for evaluation. For unsupervised adaptation\nwith ReCo+ (Sec. 3.2), we train on ReCo pseudo-labels on the Cityscapes training set with 2,975\nimages, KITTI-STEP training set which contains 5,027 images, and the COCO-Stuff10K subset\nwhich has 9,000 images for each respective benchmark. We emphasise that no ground-truth labels\nare used for training.\n\nFinally, to demonstrate our model\u2019s ability to segment rare concepts, we use the LAION-5B\ndataset [75] with 5 billion images as a large collection of images that we expect to satisfy our\nfirst hypothesis, namely that it will have coverage of rare concepts. To assess performance, we use\nthe FireNet dataset [62] which has 1,452 images spanning rare fire safety-related classes. For our\nexperiment, we select the fire extinguisher class as an example of a concept that is important but\nrare in vision datasets (it is not contained in ImageNet1K [74], for example) and evaluate ReCo on\n263 images containing at least one instance of the category. As a further proof of concept, we also\ndemonstrate co-segmentations of the Antikythera mechanism (a historical item that does not appear\nin WordNet [57], or any labelled vision datasets that we are aware of).\n\nImplementation details\n\n4.2\nHere, we describe the hyperparameters used to train ReCo+, inference details and evaluation metrics.\nOur implementation is based on the PyTorch library [66] and made publicly available.2\n\nReCo+ Training. While ReCo does not require training, we train ReCo+ based on the\nDeepLabv3+ [10] segmentation architecture with a ResNet101 [26] backone on the predictions\nfrom ReCo as described in 3.2. All training images are resized and center-cropped to 320\u00d7320 pixels\nand data augmentations such as random scaling, cropping, and horizontal flipping are applied with\nrandom color jittering and Gaussian blurring. We use the Adam optimiser [43] with an initial learning\nrate of 5 \u00d7 10\u22124 and a weight decay of 2 \u00d7 10\u22124 with the Poly learning rate schedule as in [50, 10].\nTraining consists of 20K gradient iterations with a batch size of 8 and takes about 5 hours on a single\n24GB NVIDIA P40 GPU.\n\nInference. For each benchmark, we pre-compute reference image embeddings for a list of categories\nfor the benchmark and store the embeddings to form a classifier. Whenever DenseCLIP is employed,\nwe use the ResNet50x16 model (following [96]) to construct a saliency map for each image. Unless\notherwise stated, for the COCO-Stuff and Cityscapes benchmarks, we resize and center crop the\ninput images to 320\u00d7320 pixels as in [24]. For the KITTI-STEP validation set, we use the original\nresolution of each image as in [41]. For the FireNet benchmark, we resize the shorter side of images\nto 512 pixels and predict a single class of fire extinguisher by thresholding the predicted heatmap\nwith probability of 0.5.\n\nEvaluation metrics. Following the common practice [34, 12, 24], we report pixel accuracy (Acc.)\nand mean intersection-over-union (mIoU).\n\n4.3 Ablation studies\n\nAbility of CLIP to curate archives. We begin by assessing the validity of our second hypothesis\u2014\nnamely that CLIP is capable of achieving high purity archives from unlabelled images. To this end,\nwe evaluate the retrieval performance of different CLIP models on the ImageNet1K validation set\nwhen constructing different archive sizes. In detail, for each archive size, k, we compute the precision\nof the top-k retrieved images based on whether the the ground-truth image-labels match the query\ntext. As can be seen in Fig. 4 (left), all CLIP models achieve solid retrieval performance, suggesting\ntheir potential for curating high purity archives as part of ReCo. Since ViT-L/14@336px performs\nbest, we employ this as our retrieval model in the remaining experiments.\n\nInfluence of archive size and visual encoder used for co-segmentation. In Fig. 4 (right) we illustrate\nthe effect of using different pre-trained architectures, e.g. MoCov2 [25], DINO [7], CLIP [71], DeiT-\nSIN [60], as the archive size (and thus the number of images used for co-segmentation) changes. The\ny-axis depicts segmentation performance for ReCo with these configurations on the PASCAL-Context\nbenchmark. We observe that using larger archives tends to improve performance (likely due to their\nreasonably high purity) albeit non-monotonically, and that features from DeiT-SIN perform best. We\n\n2Code available at https://github.com/NoelShin/reco\n\n7\n\n\fFigure 4: Ablation studies. Left: Image retrieval performance of different CLIP models on the\nImageNet1K validation set with k ranging from 5 to 50. ViT-L/14@336px performs particularly\nstrongly, suggesting the ability to curate archives of high purity. Right: Co-segmentation performance\non PASCAL-Context validation set as we vary the archive size and choice of visual encoders. We\nobserve a general trend towards improved performance with increasing archive size for all encoders.\n\nDenseCLIP LGC CE CRF Acc. mIoU\n\u2717\n\u2717\n\u2717\n\u2713\n\u2713\n\u2713\n\n16.8\n41.1\n43.1\n49.7\n50.9\n51.6\n\n5.7\n21.8\n23.1\n26.0\n26.6\n27.2\n\n\u2717\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\n\u2717\n\u2717\n\u2713\n\u2717\n\u2713\n\u2713\n\n\u2717\n\u2717\n\u2717\n\u2717\n\u2717\n\u2713\n\nTable 1: Influence of ReCo components for zero-shot transfer on PASCAL-Context [58]. We\nobserve that integrating DenseCLIP during inference, Language-guided co-segmentation (LGC),\nContext elimination (CE), and CRF [44] post-processing each contribute to improved performance.\nAll comparisons use a DeiT-SIN visual backbone for co-segmentation and ViT-L/14@336px for\narchive curation.\n\ntherefore use DeiT-SIN with an archive size of k=50 for the remaining experiments unless otherwise\nstated. See Fig. 3 for qualitative examples of our co-segmentation results.\n\nInfluence of ReCo framework components. We next assess the effect of employing DenseCLIP\nduring inference, the language-guided co-segmentation and context elimination components of ReCo\nwhich seek to improve the quality of co-segmentation achieved across each archive to boost down-\nstream segmentation performance. When applying context elimination, we select tree, sky, building,\nroad, and person as common background concepts appearing in natural images to be suppressed.\nIn Tab. 1, we show the effect of three of these strategies, together with the effect of applying a\nCRF [44] as post-processing. We observe that integrating DenseCLIP into the inference procedure\nbrings a significant gain in performance which we believe is driven by the notable robustness of CLIP\nfeatures under zero-shot transfer [71]. In addition, language-guided co-segmentation and context\nelimination further boost co-segmentation performance, while the CRF brings a small gain. We\ntherefore use each of these strategies (including CRF post-processing) in the remaining experiments.\n4.4 Comparison to state-of-the-art unsupervised methods\nWe compare ReCo and ReCo+ to state-of-the-art unsupervised semantic segmentation models\nwith and without vision-language pre-training on standard benchmarks, including COCO-Stuff [6],\nCityscapes [14] and KITTI-STEP [87] under both zero-shot transfer and unsupervised adaptation\n(training without labels on the target distribution). For COCO-Stuff, we observe that the mid-level\ncategories used for evaluation are somewhat abstract for retrieval (for instance, one mid-level category\nis \u201coutdoor objects\u201d, which may include many low-level categories beyond the target hierarchy). To\navoid introducing ambiguity to the co-segmentation procedure, we instead directly use the low-level\ncategories and then merge the predictions into the mid-level categories. Additionally, we rephrase\ntwo category names to reduce ambiguity (parking to parking lot and vegetation to tree) in Cityscapes\nand KITTI-STEP based on the descriptions found in [14]. A detailed discussion can be found in the\nsupplementary material.\n\nAs shown in Tab. 2, ReCo strongly outperforms prior models on all benchmarks for zero-shot transfer.\nUnder an unsupervised adaptation protocol, ReCo+ outperforms the state-of-the-art by a large margin\non the Cityscapes and KITTI-STEP. On COCO-Stuff, ReCo+ achieves slightly lower pixel accuracy\n\n8\n\n1020304050Top k0.20.30.40.50.60.70.8PrecisionRN50RN50x16RN50x64ViT-B/32ViT-B/16ViT-L/14ViT-L/14@336px1020304050archive size (k)0.200.220.240.260.28mIoURN50-CLIPRN50-MoCov2DeiT-S/16-SINViT-S/16-DINOViT-B/16-DINODenseCLIP\fModel\n\nAcc. mIoU\n\nModel\n\nAcc. mIoU\n\nModel\n\nAcc. mIoU\n\nZero-shot transfer\nDenseCLIP\u2021 [96] 32.3\nReCo\u2021 (Ours)\n46.6\n\nUnsupervised adaptation\nIIC [34]\nMDC [12]\nPiCIE [12]\nPiCIE + H [12]\nSTEGO [24]\nReCo+\u2021 (Ours)\n\n21.8\n32.2\n48.1\n50.0\n56.9\n54.5\n\n19.8\n27.2\n\n6.7\n9.8\n13.8\n14.4\n28.2\n33.0\n\nZero-shot transfer\nDenseCLIP\u22c6\u2021 [96] 35.9\nMDC\u22c6\u2020 [12]\nPiCIE\u22c6\u2020 [12]\nD&S\u22c6\u2020 [85]\nReCo\u22c6\u2021 (Ours)\n\n-\n-\n-\n65.4\n\nUnsupervised adaptation\n47.9\nIIC [34]\n40.7\nMDC [12]\n65.5\nPiCIE [12]\n73.2\nSTEGO [24]\nReCo+\u2021 (Ours)\n83.7\n\n10.0\n7.0\n9.7\n16.2\n22.0\n\n6.4\n7.1\n12.3\n21.0\n24.2\n\nZero-shot transfer\nDenseCLIP\u2021 [96] 34.1\nReCo\u2021 (Ours)\n70.6\n\nUnsupervised adaptation\nSegSort [33]\nHSG [41]\nReCo+\u2021 (Ours)\n\n69.8\n73.8\n75.3\n\n15.3\n29.8\n\n19.2\n21.7\n31.9\n\nTable 2: Comparison to state-of-the art approaches on COCO-Stuff (left), Cityscapes (middle),\nand KITTI-STEP (right) validation sets. \u22c6Evaluated at the original resolution. \u2020Models trained\non Waymo Open [81] (reported from [85]). \u2021Models that leverage a language-image pre-training\nmodel to assign a concept name to a prediction. The best score for each metric under each protocol is\nhighlighted in bold. We observe that ReCo and ReCo+ perform strongly relative to prior work under\nzero-shot transfer and unsupervised adaptation protocols, respectively.\n\nFigure 5: Co-segmentations of rare concepts. Left: fire extinguisher. Right: Antikythera mech-\nanism. We show selected samples from ReCo archives for each concept, together with their co-\nsegmentations. In each case, ReCo successfully identifies the regions associated with the concept.\n\ncompared to [24] but a considerably higher mIoU. In Fig. 1, we visualise the sample predictions of\nour models on COCO-Stuff.\n\n4.5 Segmenting rare concepts\nBy virtue of inheriting CLIP\u2019s diverse knowledge of nameable visual concepts, ReCo exhibits the\nability to segment rare categories. We first demonstrate this ability for fire extinguisher objects,\nwhich have important fire-safety implications but seldom appear in popular semantic segmentation\nbenchmarks. To assess performance, we evaluate segmentation quality on the FireNet dataset (as\ndescribed in Sec. 4.1) and achieve reasonable performance on pixel accuracy (93.3) and IoU (44.9)\nmetrics. In Fig. 5 (left) we visualise the co-segmentation produced by ReCo across sample images\ncontainer fire extinguishers. As an additional demonstration (Fig. 5, right), we also show co-\nsegmentations for images containing the views of the (unique) Antikythera mechanism. In both cases,\nwe observe that ReCo is capable of co-segmenting the concept of interest without labelled examples.\n\n", "conclusion": "\n\nOur work has several limitations: (1) There may be cases for which our first hypothesis does not\nhold\u2014concepts so rare that they do not appear in billion-image scale datasets. In such cases, e.g. \u201ca\npurple elephant with square orange feet wearing an inverted cowboy hat in front of the Doge\u2019s\nPalace\u201d, ReCo will struggle. (2) During inference, we make use of a visual encoder and a pre-trained\nlanguage-image model, i.e., CLIP (a computationally heavy model). Future work could address this\nwith training a light student network which distills knowledge from both models. (3) We employ\nImageNet as the unlabelled image collection for our primary investigations, which is known to exhibit\nan object-centric bias. While qualitative results suggest that ReCo can learn from extremely diverse\ndata [75], a more comprehensive empirical evaluation would strengthen this result. (4) Models like\nCLIP are expensive to train and are consequently updated infrequently. If a new concept emerges\n(e.g. a new product) after CLIP was trained, ReCo will be unable to build an archive to enable\nco-segmentation for this concept. (5) Although our work is unsupervised in the sense that it is\n\n9\n\n\fnot trained on pixel-level annotation, the ablation studies on PASCAL-Context have guided the\ndevelopment of our method, and thus it benefits from a form of indirect supervision. As a result, our\ndesign likely contains choices that subtly favour performance on the evaluation tasks.\n\n6 Broader impact\nSemantic segmentation is a dual use technology. It enables many applications with the potential for\nsignificant positive societal impact across domains in medical imaging, wildlife monitoring, improved\nfault detection in manufacturing processes etc. However, it is also vulnerable to abuse: it may enable\nunlawful surveillance or invasions of privacy, for example. By removing the requirement to collect\npixel masks for concepts of interests, ReCo lowers the barrier to entry for any individual that wish to\nmake practical use of segmentation, but makes no distinction on the ethical implications of the use\ncase, positive or negative.\n\nReCo makes use of large-scale, unlabelled image collections. By their nature, such images are subject\nto minimal curation and sanitisation, and thus may contain not only biases across demographics, but\nalso content that does not align with the ethical values of the user. Consequently, we emphasise that\nReCo represents a research proof of concept that is not appropriate for real-world usage without\nextensive additional analysis of the specific deployment context in which it will be used and, in\nparticular, safeguards to moderate the archive curation process.\n\n7 Conclusion\nIn this work, we introduced the ReCo framework for semantic segmentation zero-shot transfer. By\ndrawing on the strengths of large-scale language-image pre-training and modern visual backbones,\nReCo attains the ability to segment rare concepts and to directly assign names to concepts without\nlabelled examples from the target distribution. In addition, experimental comparisons demonstrate\nthat ReCo strongly outperforms existing zero-shot transfer approaches that forgo pixel supervision.\n\n", "appendix": "we first discuss the role of supervisory signals for ReCo and alternative\napproaches, and the datasets used in our work (Appendix A). Next we provide further details\nof experiments conducted in the main paper together with hyperparameters for training ReCo+\nin Appendix B. We then report additional ablations investigating the influence of common category\nselection for context elimination, reducing ambiguity in category name, architectural chioces for\nCLIP and DenseCLIP, and the number of seed pixels considered for computing a reference image\nembedding in Appendix C. Finally, we provide additional qualitative results in Sec. D to illustrate\nboth successful and failure cases for our method.\n\nA Discussion of supervision and data\n\nA.1 Supervisory signals for ReCo and prior work\n\nIn the main paper, we compare to previous methods that are typically described as unsupervised. In\npractice, however, many methods (including ours) either implicitly or explicitly engage humans in\nthe data curation process at some stage.\n\nSupervision used by ReCo. (1) Similarly to prior work, our experiments make use of datasets\nconstructed from photographs taken by humans for both training and evaluation. These photographs\nare naturally biased towards content that humans find interesting and are typically well-framed (with\na concept of interest featuring prominently) or taken from a vantage point that offers a convenient\nscene overview (e.g. a roof-mounted camera on a vehicle driving on public roads). By training\nand evaluating on such data, our experimental results likely provide an optimistic assessment of\nperformance when contrasted with other distributions (e.g. the video feed received by an autonomous\nmobile robot). (2) For our comparisons to prior work, we use the ImageNet training subset without\nlabels to curate archives. However, in practice, this dataset is not free from human involvement: it\nwas curated by human workers who were asked to verify that each image contains a particular synset\ncategory (see [5] for a discussion of the collection process). While annotators were encouraged to\nselect images regardless of occlusions, the number of objects and clutter in the scene, this process\nnevertheless produced a relatively clean dataset with fairly object-centric images. (3) Several of\nour experiments make use of DeiT-SIN [13], which is trained on stylised ImageNet [6] with labels.\nWe don\u2019t believe that this supervision is critical, since in Fig. 4 of the main paper, we showed that\nResNet50-MoCov2 [8] which does not use labels achieves similar performance (less than 1 mIoU\ndifference on PASCAL-Context [12]). Moreover, we note that previous unsupervised methods to\nwhich we compare (e.g. [10, 3]) initialise their approach from supervised ImageNet training with the\nconvention that unsupervised in this context denotes the fact that no pixel-level supervision is used.\n(4) By using CLIP [17], we also make use of a different kind of supervision, namely images paired\nwith alt-text scraped from the web. Empirically, this data source has been shown to be extremely\nscalable and to enable generalisation to very large numbers of concepts [17, 11, 16]. Nevertheless,\nthe creation of the original alt-text image descriptions (mostly) derives from a human source, and\ntherefore provides a form of human supervision. In contrast to using ImageNet classification labels,\nthis source of supervision is indispensable to ReCo.\n\nWe believe that the key factors to be considered when discussing the question of supervision are\nscalability and generalisation. We are typically not interested in unsupervised methods for their own\nsake, but rather because they offer the ability to cheaply scale up machine learning to larger training\ndata sets that improve performance, and to build methods that go beyond the functionality afforded by\nlabelled datasets (e.g. new classes, new tasks etc.). Subject to the caveats (e.g. human photographer\nbias) outlined above, we believe ReCo has the flexibility to scale up far beyond the experimental\ncomparisons conducted in this work without requiring any changes to the underlying framework.\n\nA.2 Discussion of consent in used datasets\n\nIn this work, we work primarily with widely used Computer Vision benchmarks: ImageNet [5],\nPASCAL-Context [12], Cityscapes [4], COCO-Stuff [1], KITTI-STEP [21]. For these datasets, we\ndo not conduct an independent investigation of consent beyond the considerations of the authors that\nreleased these datasets. For our final exploratory studies which make use of LAION-5B [18], we\nmanually verified that no humans were present in the archives that were curated by ReCo.\n\n2\n\n\fA.3 Discussion on whether data contains personally identifiable information or offensive\n\ncontent\n\nWe do not release any data as part of this work. By working with widely used Computer Vision\nbenchmarks, we also restrict ourselves to imagery that is available in the public domain. We therefore\nbelieve that the risk that our work builds on harmful content or contributes to the leakage of personal\ninformation is low.\n\nOne exception to this is our use of the LAION-5B dataset for qualitative studies. We manually\nverified that no personally identifiable information or harmful content (as judged by the authors) was\npresent in the archives curated by ReCo.\n\nA.4 Dataset licenses\n\nHere we describe the terms/licenses of datasets used in our paper. For images in PASCAL-Context\nand COCO dataset, we comply with the Flickr Terms of Use and the Creative Commons Attribution\n4.0 License for the COCO-Stuff annotations. For Cityscapes and ImageNet1K, we follow the terms\nstated on their official website1 and the Attribution-NonCommercial-ShareAlike 3.0 Unported (CC\nBY-NC-SA 3.0) licence for KITTI-STEP.\n\nB Experiment details\n\nHere we provide pseudocode for ReCo and details of experiments conducted in the main paper.\n\nB.1 Pseudocode for ReCo\n\nIn Alg. 1, we describe the pseudocode for the core of ReCo (to maintain readability, language-guided\nco-segmentation and context elimination are omitted since these follow a similar structure).\n\nB.2 Prompt engineering\n\nTo obtain the text embedding for a concept, we ensemble the textual features from 85 templates,\ne.g., \u201ca photo of the {concept}\u201d and \u201cthere is a {concept} in the scene\u201d\nfollowing [22].\n\nB.3 Details of ablation study to assess CLIP retrieval performance\n\nWe observe that two of the ImageNet1K class labels are not unique\u2014they occur twice with different\nmeanings (e.g., \u201ccrane\u201d is used to represent both bird and machine), which makes retrieval inference\nand evaluation ambiguous. Therefore, we exclude those classes and use the remaining 996 categories\nfor the experimental results reported in Fig. 4 (left) of the main paper.\n\nB.4 Hyperparameters for ReCo+ training\n\nAs described in Sec. 4.2 (in the main paper), we adopt DeepLabv3+2 [2] with ResNet101 encoder [9]\nfor ReCo+ and train the network with standard data augmentations such as random scaling and\nhorizontal flip following [14, 19]. In detail, for geometric transformations, we use random scaling\nwith a range of [0.5, 2.0], random crop with a crop size 320\u00d7320 pixels, and random horizontal flip\nwith a probability of 0.5. For the photometric augmentations, we apply colour jittering3 with 0.8, 0.8,\n0.8, 0.2, and 0.8 for brightness, contrast, saturation, hue and probability parameters respectively. We\nalso employ Gaussian blurring with a kernel size of 10% of min(H, W ) where min(H, W ) returns\nthe length of the shorter side of an image.\n\n1https://www.cityscapes-dataset.com/license and https://www.image-net.org/download.\n\nphp for Cityscapes and ImageNet1K respectively.\n\n2We use the code for DeepLabv3+ from https://github.com/VainF/DeepLabV3Plus-Pytorch.\n3We use ColorJitter function in torchvision package [15].\n\n3\n\n\fAlgorithm 1 Pseudocode for the core of ReCo (using PyTorch-like syntax)\nInput. a CLIP image encoder \u03c8I, a CLIP text encoder \u03c8T , an image encoder \u03d5I, an image collection\nU, a concept c, the number of co-segmented images k\nOutput. a reference image embedding ref_emb and a prediction of the concept c in a new image\n\n# retrieve images\nimage_emb = l2_normalize(\u03c8I(U), dim=1) # NxC\ntext_emb = l2_normalize(\u03c8T (c), dim=0) # C\nscores = mm(image_emb, text_emb) # N\nindices = argmax(scores)[:k] # k\nimages = U[indices] # kx3xHxW\n\n# co-segment\nF = l2_normalize(\u03d5I(images), dim=1) # kxCxhxw\nF_flat = F.permute(1,0,2,3).view(C,k*h*w) # Cxkhw\nA = mm(F_flat.T, F_flat) # adjacency matrix, khwxkhw\n\ngrid = zeros((k*h*w, k))\nstart_col = 0 # start column index\nfor i in range(k):\n\nend_col = start_col + h*w # end column index\ngrid[:,i] = max(A[:,start_col:start_col+end_col], dim=1)\nstart_col = end_col\n\navg_grid = mean(grid, dim=1) # khw\n\nseed_features = []\nstart_row = 0 # start row index\nfor i in range(k):\n\nend_row = start_row + h*w # end row index\nindex_1d = argmax(avg_grid[start_row:end_row])\nstart_row = end_row\nindex_2d = [index_1d//w,index_1d%w]\nseed_features.append(F[i,:,index_2d[0],index_2d[1]])\n\nseed_features = stack(seed_features, dim=0) # k\u00d7C\nref_emb = l2_normalize(seed_features.mean(dim=0), dim=0) # C\n\n# inference\nF_new = l2_normalize(\u03d5I(new_image), dim=0) # Cxhxw\nprediction = sigmoid(mm(ref_emb, F_new)) # hxw\n\nmm:matrix multiplication.\n\ng\nn\ni\nd\nl\ni\nu\nb\n\nn\no\ns\nr\ne\np\n\nd\na\no\nr\n\ne\ne\nr\nt\n\ny\nk\ns\n\nmIoU\n\n\u2717\n\u2717\n\u2717\n\u2717\n\u2717\n5.7\n\u2717\n\u2717\n\u2713 \u2717\n\u2717\n10.9\n\u2717\n\u2717\n\u2713 \u2713 \u2717\n12.0\n\u2713 \u2713 \u2713 \u2717\n\u2717\n10.8\n\u2713 \u2713 \u2713 \u2713 \u2717\n11.4\n\u2713 \u2713 \u2713 \u2713 \u2713 12.3\n\nparking\n\nvegetation\n\n\u2192parking lot \u2192tree\n\nmIoU\n\n\u2717\n\u2717\n\u2713\n\u2713\n\n\u2717\n\u2713\n\u2717\n\u2713\n\n15.4\n16.0\n18.6\n19.3\n\nTable 1: Effect of context category choices and reducing label ambiguity. Left: We find that\nsuppressing 5 frequently appearing categories brings performance gain on PASCAL-Context [12].\nRight: We observe that specifying the meaning of a class more concretely helps segmentation\nperformance of ReCo on Cityscapes [4]. For both cases, input images are resized and center-cropped\nto 320\u00d7320 following [7].\n\n4\n\n\fModel\n\nCLIP arch.\n\nDenseCLIP arch. mIoU\n\nReCo (ours)\n\nResNet50\nViT-B/32\nViT-L/14@336px ResNet50x16\n\nResNet50\nResNet50\n\n20.3\n20.3\n22.0\n\n# seed pixels mIoU Acc.\n\n1\n5\n10\n50\n100\n\n22.0\n22.2\n22.1\n21.7\n20.4\n\n65.4\n64.9\n62.9\n57.5\n51.0\n\nTable 2: Effect of architecture choices (left) and the number of seed pixels (right) on performance\nof ReCo on the Cityscapes validation set.\n\nC Additional ablation studies\n\nC.1 Choices of context categories\n\nAs described in Sec. 3.3 (main paper), we propose to suppress the commonly appearing categories,\ne.g., sky, which co-occur with other classes, e.g., aeroplanes. To achieve this, we manually pick 5\nfrequently appearing classes in PASCAL-Context dataset [12] and investigate the effect of different\ncombinations of such categories. In Tab. 1 (left) we observe that suppressing the tree and sky\ncategories yields a notable performance gain, while eliminating all five categories performs best. For\nthis reason, we apply the context elimination strategy with these five categories in the main paper.\n\nC.2 Category name rephrasing to reduce ambiguity\n\nWe observe that it is important to specify a concept concretely to obtain retrieved images exhibiting\nsimilar visual appearance. For instance, in Cityscapes dataset, parking and vegetation can be rephrased\nto less ambiguous concepts parking lot and tree respectively based on their descriptions4 in the paper\naccompanying the dataset [4]. As can be seen in Tab. 1 (right), ReCo gains benefits in performance\non Cityscapes by replacing the category names with less abstract concepts. This sensitivity is a\nconsequence of our co-segmentation algorithm, which locates pixels that share similar visual features\nacross multiple images. Thus we use the rephrased label names throughout the experiments in the\npaper.\n\nIn addition to the limitations listed in the main paper, this dependence on concrete/specific concept\nnames can be considered a limitation of our approach (albeit one that is readily mitigated). However,\nwe believe it is a reasonable requirement for methods that operate in the zero-shot transfer setting.\nUnlike fine-tuning methods that learn to associate abstract text descriptions to visual concepts by\nseeing examples from the target distribution, ReCo relies entirely on an adequate text description to\ndisambiguate the concept. Since many computer vision datasets have been constructed with training\nand testing splits with the assumption that methods would make use of the training set, we believe\nit is probable that category names were not designed to be uniquely descriptive (hence the use of\n\u201cparking\u201d as a category in Cityscapes, which could be either a verb or a noun). Indeed, there may\nhave been little perceived need to construct unambiguous category names when examples from the\ntraining set implicitly provide disambiguation of the concept.\n\nC.3 Effect of architectural choices for CLIP and DenseCLIP\n\nFor the experiments in the main paper, we use ViT-L/14@336px and ResNet50x16 for image retrieval\nand DenseCLIP inference respectively. As these models are relatively heavier than commonly\nused architectures such as ResNet50 or ViT-B/32, we evaluate ReCo with lighter encoders on the\nCityscapes validation split in Tab. 2 (left). Specifically we use either ResNet50 or ViT-B/32 for image\nretrieval and ResNet50 for DenseCLIP inference. As can be seen, adopting a lighter model slightly\ndecreases the performance, but still outperforms previous state-of-the-art methods (e.g., 16.3 mIoU\nfor D&S [20]).\n\n4\u201cHorizontal surfaces that are intended for parking and separated from the road, either via elevation or via a\ndifferent texture/material, but not separated merely by markings.\u201d for parking and \u201cTrees, hedges, and all kinds\nof vertically growing vegetation.\u201d for vegetation.\n\n5\n\n\fFigure 1: Additional visualisations of our co-segmentation method used for ReCo. For visualisa-\ntion purpose, we show the top 5 images from 50 retrieved images in each archive for a category. Left:\nSuccessful cases. Right: Typical failure cases. Highlighted regions are shown in red. Best viewed in\ncolour.\n\nC.4 Effect of the number of seed pixels\n\nWhile we pick one seed pixel per image by default as decribed in Sec. 3, we investigate how the\nnumber of seed pixels selected for an image affects performance of ReCo. For this, we evaluate ReCo\non Cityscapes with each reference image embedding computed by averaging 1, 5, 10, 50, and 100\nseed pixel(s) for an image. As can be seen in Tab. 2 (right), using one or five seed pixel(s) per image\nshows the best performance in terms of pixel accuracy and mIoU and picking more pixels tends to\nhurt the performance.\n\nD Additional visualisations\n\nD.1 Co-segmentation with seed pixels\n\nIn Fig. 1, we visualise examples of the co-segmentation with seed pixels on ImageNet2012, which is\nused for an index dataset for ReCo in the main paper. On the left, we show successful cases where the\nco-segmentation highlights regions corresponding to a given concept (i.e., sheep, building, and zebra).\nOn the right, we display examples of two typical failure cases: partial segmentation (i.e., bicycle\nand cup) and highlighting an object commonly co-occurring with a given concept (i.e., baseball\nglove). For bicycles, their frames are less highlighted compared to the wheels. Similarly, the body\nparts of the cups are less likely to be emphasised than the handles. In case of baseball gloves, the\nco-segmentation locates a part of a baseball, which often appears with a baseball glove. We believe\nthese failure cases are caused by the property of our co-segmentation algorithm, which focuses on\nregions with less variance in visual features (e.g., texture and shape) appearing in multiple images of\nan archive.\n\nD.2 Predictions of ReCo and ReCo+\n\nIn Fig. 2, we show more visualisation samples on the COCO-Stuff benchmark. Successful and failure\ncases are shown on the left and right, respectively. We note that ReCo tends to fail in predicting small\nobjects, e.g., people in the bus, and so does ReCo+ which is trained on the ReCo\u2019s predictions as\npseudo-labels. We conjecture that this is related to the stride of the image encoder used for ReCo,\n\n6\n\n\fFigure 2: Additional visualisations on COCO-Stuff. Left: Successful cases. Right: Typical failure\ncases. White pixels denote ignored regions.\n\nwhich is 16\u00d716 for the case of DeiT-S/16-SIN [13]. It could therefore potentially be improved by\nusing an encoder with a smaller stride at the cost of increased computational burden.\n\n", "system_prompts": [{"role": "system", "content": "You are a computer science researcher currently reviewing a paper titled \"ReCo: Retrieve and Co-segment for Zero-shot Transfer\" for the NeurIPS computer science conference. Your goal is to try to be as objective and truthful as possible in your answers about the paper provided. Your reviews will be used for causal reasoning in determining the quality of the paper."}], "prompts": {"1b": {"role": "user", "content": "The following is the conclusion section of the paper you are reviewing:\n\n\nOur work has several limitations: (1) There may be cases for which our first hypothesis does not\nhold\u2014concepts so rare that they do not appear in billion-image scale datasets. In such cases, e.g. \u201ca\npurple elephant with square orange feet wearing an inverted cowboy hat in front of the Doge\u2019s\nPalace\u201d, ReCo will struggle. (2) During inference, we make use of a visual encoder and a pre-trained\nlanguage-image model, i.e., CLIP (a computationally heavy model). Future work could address this\nwith training a light student network which distills knowledge from both models. (3) We employ\nImageNet as the unlabelled image collection for our primary investigations, which is known to exhibit\nan object-centric bias. While qualitative results suggest that ReCo can learn from extremely diverse\ndata [75], a more comprehensive empirical evaluation would strengthen this result. (4) Models like\nCLIP are expensive to train and are consequently updated infrequently. If a new concept emerges\n(e.g. a new product) after CLIP was trained, ReCo will be unable to build an archive to enable\nco-segmentation for this concept. (5) Although our work is unsupervised in the sense that it is\n\n9\n\n\fnot trained on pixel-level annotation, the ablation studies on PASCAL-Context have guided the\ndevelopment of our method, and thus it benefits from a form of indirect supervision. As a result, our\ndesign likely contains choices that subtly favour performance on the evaluation tasks.\n\n6 Broader impact\nSemantic segmentation is a dual use technology. It enables many applications with the potential for\nsignificant positive societal impact across domains in medical imaging, wildlife monitoring, improved\nfault detection in manufacturing processes etc. However, it is also vulnerable to abuse: it may enable\nunlawful surveillance or invasions of privacy, for example. By removing the requirement to collect\npixel masks for concepts of interests, ReCo lowers the barrier to entry for any individual that wish to\nmake practical use of segmentation, but makes no distinction on the ethical implications of the use\ncase, positive or negative.\n\nReCo makes use of large-scale, unlabelled image collections. By their nature, such images are subject\nto minimal curation and sanitisation, and thus may contain not only biases across demographics, but\nalso content that does not align with the ethical values of the user. Consequently, we emphasise that\nReCo represents a research proof of concept that is not appropriate for real-world usage without\nextensive additional analysis of the specific deployment context in which it will be used and, in\nparticular, safeguards to moderate the archive curation process.\n\n7 Conclusion\nIn this work, we introduced the ReCo framework for semantic segmentation zero-shot transfer. By\ndrawing on the strengths of large-scale language-image pre-training and modern visual backbones,\nReCo attains the ability to segment rare concepts and to directly assign names to concepts without\nlabelled examples from the target distribution. In addition, experimental comparisons demonstrate\nthat ReCo strongly outperforms existing zero-shot transfer approaches that forgo pixel supervision.\n\n\n\nBased on the section(s), please answer the following question with yes, no, or n/a and provide a brief justification for your answer.\nQuestion: Do the authors describe the limitations of their work?"}, "1c": {"role": "user", "content": "The following is the conclusion section of the paper you are reviewing:\n\n\nOur work has several limitations: (1) There may be cases for which our first hypothesis does not\nhold\u2014concepts so rare that they do not appear in billion-image scale datasets. In such cases, e.g. \u201ca\npurple elephant with square orange feet wearing an inverted cowboy hat in front of the Doge\u2019s\nPalace\u201d, ReCo will struggle. (2) During inference, we make use of a visual encoder and a pre-trained\nlanguage-image model, i.e., CLIP (a computationally heavy model). Future work could address this\nwith training a light student network which distills knowledge from both models. (3) We employ\nImageNet as the unlabelled image collection for our primary investigations, which is known to exhibit\nan object-centric bias. While qualitative results suggest that ReCo can learn from extremely diverse\ndata [75], a more comprehensive empirical evaluation would strengthen this result. (4) Models like\nCLIP are expensive to train and are consequently updated infrequently. If a new concept emerges\n(e.g. a new product) after CLIP was trained, ReCo will be unable to build an archive to enable\nco-segmentation for this concept. (5) Although our work is unsupervised in the sense that it is\n\n9\n\n\fnot trained on pixel-level annotation, the ablation studies on PASCAL-Context have guided the\ndevelopment of our method, and thus it benefits from a form of indirect supervision. As a result, our\ndesign likely contains choices that subtly favour performance on the evaluation tasks.\n\n6 Broader impact\nSemantic segmentation is a dual use technology. It enables many applications with the potential for\nsignificant positive societal impact across domains in medical imaging, wildlife monitoring, improved\nfault detection in manufacturing processes etc. However, it is also vulnerable to abuse: it may enable\nunlawful surveillance or invasions of privacy, for example. By removing the requirement to collect\npixel masks for concepts of interests, ReCo lowers the barrier to entry for any individual that wish to\nmake practical use of segmentation, but makes no distinction on the ethical implications of the use\ncase, positive or negative.\n\nReCo makes use of large-scale, unlabelled image collections. By their nature, such images are subject\nto minimal curation and sanitisation, and thus may contain not only biases across demographics, but\nalso content that does not align with the ethical values of the user. Consequently, we emphasise that\nReCo represents a research proof of concept that is not appropriate for real-world usage without\nextensive additional analysis of the specific deployment context in which it will be used and, in\nparticular, safeguards to moderate the archive curation process.\n\n7 Conclusion\nIn this work, we introduced the ReCo framework for semantic segmentation zero-shot transfer. By\ndrawing on the strengths of large-scale language-image pre-training and modern visual backbones,\nReCo attains the ability to segment rare concepts and to directly assign names to concepts without\nlabelled examples from the target distribution. In addition, experimental comparisons demonstrate\nthat ReCo strongly outperforms existing zero-shot transfer approaches that forgo pixel supervision.\n\n\n\nBased on the section(s), please answer the following question with yes, no, or n/a and provide a brief justification for your answer.\nQuestion: Do the authors discuss any potential negative societal impacts of their work?"}, "3b": {"role": "user", "content": "The following is the experiments section of the paper you are reviewing:\n\nIn this section, we start by describing the datasets used for our experiments (Sec. 4.1) and implemen-\ntation details (Sec. 4.2). Then, we conduct an ablation study (Sec. 4.3) and compare our model to\nstate-of-the-art methods for unsupervised semantic segmentation with and without language-image\npre-training (Sec. 4.4). Finally, we showcase our model\u2019s ability to segment rare-category objects\n(Sec. 4.5).\n\n4.1 Datasets\nFor our ablation study, we use the ImageNet1K [16] validation set to curate archive for concepts\nof interest. The dataset covers 1K classes with 50 images for each class. To measure segmentation\nperformance in the zero-shot transfer setting, we use the PASCAL-Context [58] validation set for\nevaluation, which has 5,104 images of 59 categories excluding the background class.\n\nTo compare with previous unsupervised segmentation methods, we use the ImageNet1K training set\nto construct the reference embedding for the concept we wish to segment. The dataset has 1K classes\nwith 1.2M images. We evaluate on standard benchmarks including the Cityscapes [14] validation\n\n6\n\n\fsplit, which has 500 urban scene images of 27 categories, KITTI-STEP [87] validation set, which\nis composed of 2,981 urban scene images of 19 categories, and COCO-Stuff [6] validation split,\nwhich has 4,172 images of 171 low-level thing and stuff categories excluding background class.\nFollowing [12, 24], we use the 27 mid-level categories for evaluation. For unsupervised adaptation\nwith ReCo+ (Sec. 3.2), we train on ReCo pseudo-labels on the Cityscapes training set with 2,975\nimages, KITTI-STEP training set which contains 5,027 images, and the COCO-Stuff10K subset\nwhich has 9,000 images for each respective benchmark. We emphasise that no ground-truth labels\nare used for training.\n\nFinally, to demonstrate our model\u2019s ability to segment rare concepts, we use the LAION-5B\ndataset [75] with 5 billion images as a large collection of images that we expect to satisfy our\nfirst hypothesis, namely that it will have coverage of rare concepts. To assess performance, we use\nthe FireNet dataset [62] which has 1,452 images spanning rare fire safety-related classes. For our\nexperiment, we select the fire extinguisher class as an example of a concept that is important but\nrare in vision datasets (it is not contained in ImageNet1K [74], for example) and evaluate ReCo on\n263 images containing at least one instance of the category. As a further proof of concept, we also\ndemonstrate co-segmentations of the Antikythera mechanism (a historical item that does not appear\nin WordNet [57], or any labelled vision datasets that we are aware of).\n\nImplementation details\n\n4.2\nHere, we describe the hyperparameters used to train ReCo+, inference details and evaluation metrics.\nOur implementation is based on the PyTorch library [66] and made publicly available.2\n\nReCo+ Training. While ReCo does not require training, we train ReCo+ based on the\nDeepLabv3+ [10] segmentation architecture with a ResNet101 [26] backone on the predictions\nfrom ReCo as described in 3.2. All training images are resized and center-cropped to 320\u00d7320 pixels\nand data augmentations such as random scaling, cropping, and horizontal flipping are applied with\nrandom color jittering and Gaussian blurring. We use the Adam optimiser [43] with an initial learning\nrate of 5 \u00d7 10\u22124 and a weight decay of 2 \u00d7 10\u22124 with the Poly learning rate schedule as in [50, 10].\nTraining consists of 20K gradient iterations with a batch size of 8 and takes about 5 hours on a single\n24GB NVIDIA P40 GPU.\n\nInference. For each benchmark, we pre-compute reference image embeddings for a list of categories\nfor the benchmark and store the embeddings to form a classifier. Whenever DenseCLIP is employed,\nwe use the ResNet50x16 model (following [96]) to construct a saliency map for each image. Unless\notherwise stated, for the COCO-Stuff and Cityscapes benchmarks, we resize and center crop the\ninput images to 320\u00d7320 pixels as in [24]. For the KITTI-STEP validation set, we use the original\nresolution of each image as in [41]. For the FireNet benchmark, we resize the shorter side of images\nto 512 pixels and predict a single class of fire extinguisher by thresholding the predicted heatmap\nwith probability of 0.5.\n\nEvaluation metrics. Following the common practice [34, 12, 24], we report pixel accuracy (Acc.)\nand mean intersection-over-union (mIoU).\n\n4.3 Ablation studies\n\nAbility of CLIP to curate archives. We begin by assessing the validity of our second hypothesis\u2014\nnamely that CLIP is capable of achieving high purity archives from unlabelled images. To this end,\nwe evaluate the retrieval performance of different CLIP models on the ImageNet1K validation set\nwhen constructing different archive sizes. In detail, for each archive size, k, we compute the precision\nof the top-k retrieved images based on whether the the ground-truth image-labels match the query\ntext. As can be seen in Fig. 4 (left), all CLIP models achieve solid retrieval performance, suggesting\ntheir potential for curating high purity archives as part of ReCo. Since ViT-L/14@336px performs\nbest, we employ this as our retrieval model in the remaining experiments.\n\nInfluence of archive size and visual encoder used for co-segmentation. In Fig. 4 (right) we illustrate\nthe effect of using different pre-trained architectures, e.g. MoCov2 [25], DINO [7], CLIP [71], DeiT-\nSIN [60], as the archive size (and thus the number of images used for co-segmentation) changes. The\ny-axis depicts segmentation performance for ReCo with these configurations on the PASCAL-Context\nbenchmark. We observe that using larger archives tends to improve performance (likely due to their\nreasonably high purity) albeit non-monotonically, and that features from DeiT-SIN perform best. We\n\n2Code available at https://github.com/NoelShin/reco\n\n7\n\n\fFigure 4: Ablation studies. Left: Image retrieval performance of different CLIP models on the\nImageNet1K validation set with k ranging from 5 to 50. ViT-L/14@336px performs particularly\nstrongly, suggesting the ability to curate archives of high purity. Right: Co-segmentation performance\non PASCAL-Context validation set as we vary the archive size and choice of visual encoders. We\nobserve a general trend towards improved performance with increasing archive size for all encoders.\n\nDenseCLIP LGC CE CRF Acc. mIoU\n\u2717\n\u2717\n\u2717\n\u2713\n\u2713\n\u2713\n\n16.8\n41.1\n43.1\n49.7\n50.9\n51.6\n\n5.7\n21.8\n23.1\n26.0\n26.6\n27.2\n\n\u2717\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\n\u2717\n\u2717\n\u2713\n\u2717\n\u2713\n\u2713\n\n\u2717\n\u2717\n\u2717\n\u2717\n\u2717\n\u2713\n\nTable 1: Influence of ReCo components for zero-shot transfer on PASCAL-Context [58]. We\nobserve that integrating DenseCLIP during inference, Language-guided co-segmentation (LGC),\nContext elimination (CE), and CRF [44] post-processing each contribute to improved performance.\nAll comparisons use a DeiT-SIN visual backbone for co-segmentation and ViT-L/14@336px for\narchive curation.\n\ntherefore use DeiT-SIN with an archive size of k=50 for the remaining experiments unless otherwise\nstated. See Fig. 3 for qualitative examples of our co-segmentation results.\n\nInfluence of ReCo framework components. We next assess the effect of employing DenseCLIP\nduring inference, the language-guided co-segmentation and context elimination components of ReCo\nwhich seek to improve the quality of co-segmentation achieved across each archive to boost down-\nstream segmentation performance. When applying context elimination, we select tree, sky, building,\nroad, and person as common background concepts appearing in natural images to be suppressed.\nIn Tab. 1, we show the effect of three of these strategies, together with the effect of applying a\nCRF [44] as post-processing. We observe that integrating DenseCLIP into the inference procedure\nbrings a significant gain in performance which we believe is driven by the notable robustness of CLIP\nfeatures under zero-shot transfer [71]. In addition, language-guided co-segmentation and context\nelimination further boost co-segmentation performance, while the CRF brings a small gain. We\ntherefore use each of these strategies (including CRF post-processing) in the remaining experiments.\n4.4 Comparison to state-of-the-art unsupervised methods\nWe compare ReCo and ReCo+ to state-of-the-art unsupervised semantic segmentation models\nwith and without vision-language pre-training on standard benchmarks, including COCO-Stuff [6],\nCityscapes [14] and KITTI-STEP [87] under both zero-shot transfer and unsupervised adaptation\n(training without labels on the target distribution). For COCO-Stuff, we observe that the mid-level\ncategories used for evaluation are somewhat abstract for retrieval (for instance, one mid-level category\nis \u201coutdoor objects\u201d, which may include many low-level categories beyond the target hierarchy). To\navoid introducing ambiguity to the co-segmentation procedure, we instead directly use the low-level\ncategories and then merge the predictions into the mid-level categories. Additionally, we rephrase\ntwo category names to reduce ambiguity (parking to parking lot and vegetation to tree) in Cityscapes\nand KITTI-STEP based on the descriptions found in [14]. A detailed discussion can be found in the\nsupplementary material.\n\nAs shown in Tab. 2, ReCo strongly outperforms prior models on all benchmarks for zero-shot transfer.\nUnder an unsupervised adaptation protocol, ReCo+ outperforms the state-of-the-art by a large margin\non the Cityscapes and KITTI-STEP. On COCO-Stuff, ReCo+ achieves slightly lower pixel accuracy\n\n8\n\n1020304050Top k0.20.30.40.50.60.70.8PrecisionRN50RN50x16RN50x64ViT-B/32ViT-B/16ViT-L/14ViT-L/14@336px1020304050archive size (k)0.200.220.240.260.28mIoURN50-CLIPRN50-MoCov2DeiT-S/16-SINViT-S/16-DINOViT-B/16-DINODenseCLIP\fModel\n\nAcc. mIoU\n\nModel\n\nAcc. mIoU\n\nModel\n\nAcc. mIoU\n\nZero-shot transfer\nDenseCLIP\u2021 [96] 32.3\nReCo\u2021 (Ours)\n46.6\n\nUnsupervised adaptation\nIIC [34]\nMDC [12]\nPiCIE [12]\nPiCIE + H [12]\nSTEGO [24]\nReCo+\u2021 (Ours)\n\n21.8\n32.2\n48.1\n50.0\n56.9\n54.5\n\n19.8\n27.2\n\n6.7\n9.8\n13.8\n14.4\n28.2\n33.0\n\nZero-shot transfer\nDenseCLIP\u22c6\u2021 [96] 35.9\nMDC\u22c6\u2020 [12]\nPiCIE\u22c6\u2020 [12]\nD&S\u22c6\u2020 [85]\nReCo\u22c6\u2021 (Ours)\n\n-\n-\n-\n65.4\n\nUnsupervised adaptation\n47.9\nIIC [34]\n40.7\nMDC [12]\n65.5\nPiCIE [12]\n73.2\nSTEGO [24]\nReCo+\u2021 (Ours)\n83.7\n\n10.0\n7.0\n9.7\n16.2\n22.0\n\n6.4\n7.1\n12.3\n21.0\n24.2\n\nZero-shot transfer\nDenseCLIP\u2021 [96] 34.1\nReCo\u2021 (Ours)\n70.6\n\nUnsupervised adaptation\nSegSort [33]\nHSG [41]\nReCo+\u2021 (Ours)\n\n69.8\n73.8\n75.3\n\n15.3\n29.8\n\n19.2\n21.7\n31.9\n\nTable 2: Comparison to state-of-the art approaches on COCO-Stuff (left), Cityscapes (middle),\nand KITTI-STEP (right) validation sets. \u22c6Evaluated at the original resolution. \u2020Models trained\non Waymo Open [81] (reported from [85]). \u2021Models that leverage a language-image pre-training\nmodel to assign a concept name to a prediction. The best score for each metric under each protocol is\nhighlighted in bold. We observe that ReCo and ReCo+ perform strongly relative to prior work under\nzero-shot transfer and unsupervised adaptation protocols, respectively.\n\nFigure 5: Co-segmentations of rare concepts. Left: fire extinguisher. Right: Antikythera mech-\nanism. We show selected samples from ReCo archives for each concept, together with their co-\nsegmentations. In each case, ReCo successfully identifies the regions associated with the concept.\n\ncompared to [24] but a considerably higher mIoU. In Fig. 1, we visualise the sample predictions of\nour models on COCO-Stuff.\n\n4.5 Segmenting rare concepts\nBy virtue of inheriting CLIP\u2019s diverse knowledge of nameable visual concepts, ReCo exhibits the\nability to segment rare categories. We first demonstrate this ability for fire extinguisher objects,\nwhich have important fire-safety implications but seldom appear in popular semantic segmentation\nbenchmarks. To assess performance, we evaluate segmentation quality on the FireNet dataset (as\ndescribed in Sec. 4.1) and achieve reasonable performance on pixel accuracy (93.3) and IoU (44.9)\nmetrics. In Fig. 5 (left) we visualise the co-segmentation produced by ReCo across sample images\ncontainer fire extinguishers. As an additional demonstration (Fig. 5, right), we also show co-\nsegmentations for images containing the views of the (unique) Antikythera mechanism. In both cases,\nwe observe that ReCo is capable of co-segmenting the concept of interest without labelled examples.\n\n\n\nBased on the section(s), please answer the following question with yes, no, or n/a and provide a brief justification for your answer.\nQuestion: If the authors ran experiments, do the authors specify all the training details (e.g., data splits, hyperparameters, how they were chosen)?"}, "3d": {"role": "user", "content": "The following is the experiments section of the paper you are reviewing:\n\nIn this section, we start by describing the datasets used for our experiments (Sec. 4.1) and implemen-\ntation details (Sec. 4.2). Then, we conduct an ablation study (Sec. 4.3) and compare our model to\nstate-of-the-art methods for unsupervised semantic segmentation with and without language-image\npre-training (Sec. 4.4). Finally, we showcase our model\u2019s ability to segment rare-category objects\n(Sec. 4.5).\n\n4.1 Datasets\nFor our ablation study, we use the ImageNet1K [16] validation set to curate archive for concepts\nof interest. The dataset covers 1K classes with 50 images for each class. To measure segmentation\nperformance in the zero-shot transfer setting, we use the PASCAL-Context [58] validation set for\nevaluation, which has 5,104 images of 59 categories excluding the background class.\n\nTo compare with previous unsupervised segmentation methods, we use the ImageNet1K training set\nto construct the reference embedding for the concept we wish to segment. The dataset has 1K classes\nwith 1.2M images. We evaluate on standard benchmarks including the Cityscapes [14] validation\n\n6\n\n\fsplit, which has 500 urban scene images of 27 categories, KITTI-STEP [87] validation set, which\nis composed of 2,981 urban scene images of 19 categories, and COCO-Stuff [6] validation split,\nwhich has 4,172 images of 171 low-level thing and stuff categories excluding background class.\nFollowing [12, 24], we use the 27 mid-level categories for evaluation. For unsupervised adaptation\nwith ReCo+ (Sec. 3.2), we train on ReCo pseudo-labels on the Cityscapes training set with 2,975\nimages, KITTI-STEP training set which contains 5,027 images, and the COCO-Stuff10K subset\nwhich has 9,000 images for each respective benchmark. We emphasise that no ground-truth labels\nare used for training.\n\nFinally, to demonstrate our model\u2019s ability to segment rare concepts, we use the LAION-5B\ndataset [75] with 5 billion images as a large collection of images that we expect to satisfy our\nfirst hypothesis, namely that it will have coverage of rare concepts. To assess performance, we use\nthe FireNet dataset [62] which has 1,452 images spanning rare fire safety-related classes. For our\nexperiment, we select the fire extinguisher class as an example of a concept that is important but\nrare in vision datasets (it is not contained in ImageNet1K [74], for example) and evaluate ReCo on\n263 images containing at least one instance of the category. As a further proof of concept, we also\ndemonstrate co-segmentations of the Antikythera mechanism (a historical item that does not appear\nin WordNet [57], or any labelled vision datasets that we are aware of).\n\nImplementation details\n\n4.2\nHere, we describe the hyperparameters used to train ReCo+, inference details and evaluation metrics.\nOur implementation is based on the PyTorch library [66] and made publicly available.2\n\nReCo+ Training. While ReCo does not require training, we train ReCo+ based on the\nDeepLabv3+ [10] segmentation architecture with a ResNet101 [26] backone on the predictions\nfrom ReCo as described in 3.2. All training images are resized and center-cropped to 320\u00d7320 pixels\nand data augmentations such as random scaling, cropping, and horizontal flipping are applied with\nrandom color jittering and Gaussian blurring. We use the Adam optimiser [43] with an initial learning\nrate of 5 \u00d7 10\u22124 and a weight decay of 2 \u00d7 10\u22124 with the Poly learning rate schedule as in [50, 10].\nTraining consists of 20K gradient iterations with a batch size of 8 and takes about 5 hours on a single\n24GB NVIDIA P40 GPU.\n\nInference. For each benchmark, we pre-compute reference image embeddings for a list of categories\nfor the benchmark and store the embeddings to form a classifier. Whenever DenseCLIP is employed,\nwe use the ResNet50x16 model (following [96]) to construct a saliency map for each image. Unless\notherwise stated, for the COCO-Stuff and Cityscapes benchmarks, we resize and center crop the\ninput images to 320\u00d7320 pixels as in [24]. For the KITTI-STEP validation set, we use the original\nresolution of each image as in [41]. For the FireNet benchmark, we resize the shorter side of images\nto 512 pixels and predict a single class of fire extinguisher by thresholding the predicted heatmap\nwith probability of 0.5.\n\nEvaluation metrics. Following the common practice [34, 12, 24], we report pixel accuracy (Acc.)\nand mean intersection-over-union (mIoU).\n\n4.3 Ablation studies\n\nAbility of CLIP to curate archives. We begin by assessing the validity of our second hypothesis\u2014\nnamely that CLIP is capable of achieving high purity archives from unlabelled images. To this end,\nwe evaluate the retrieval performance of different CLIP models on the ImageNet1K validation set\nwhen constructing different archive sizes. In detail, for each archive size, k, we compute the precision\nof the top-k retrieved images based on whether the the ground-truth image-labels match the query\ntext. As can be seen in Fig. 4 (left), all CLIP models achieve solid retrieval performance, suggesting\ntheir potential for curating high purity archives as part of ReCo. Since ViT-L/14@336px performs\nbest, we employ this as our retrieval model in the remaining experiments.\n\nInfluence of archive size and visual encoder used for co-segmentation. In Fig. 4 (right) we illustrate\nthe effect of using different pre-trained architectures, e.g. MoCov2 [25], DINO [7], CLIP [71], DeiT-\nSIN [60], as the archive size (and thus the number of images used for co-segmentation) changes. The\ny-axis depicts segmentation performance for ReCo with these configurations on the PASCAL-Context\nbenchmark. We observe that using larger archives tends to improve performance (likely due to their\nreasonably high purity) albeit non-monotonically, and that features from DeiT-SIN perform best. We\n\n2Code available at https://github.com/NoelShin/reco\n\n7\n\n\fFigure 4: Ablation studies. Left: Image retrieval performance of different CLIP models on the\nImageNet1K validation set with k ranging from 5 to 50. ViT-L/14@336px performs particularly\nstrongly, suggesting the ability to curate archives of high purity. Right: Co-segmentation performance\non PASCAL-Context validation set as we vary the archive size and choice of visual encoders. We\nobserve a general trend towards improved performance with increasing archive size for all encoders.\n\nDenseCLIP LGC CE CRF Acc. mIoU\n\u2717\n\u2717\n\u2717\n\u2713\n\u2713\n\u2713\n\n16.8\n41.1\n43.1\n49.7\n50.9\n51.6\n\n5.7\n21.8\n23.1\n26.0\n26.6\n27.2\n\n\u2717\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\n\u2717\n\u2717\n\u2713\n\u2717\n\u2713\n\u2713\n\n\u2717\n\u2717\n\u2717\n\u2717\n\u2717\n\u2713\n\nTable 1: Influence of ReCo components for zero-shot transfer on PASCAL-Context [58]. We\nobserve that integrating DenseCLIP during inference, Language-guided co-segmentation (LGC),\nContext elimination (CE), and CRF [44] post-processing each contribute to improved performance.\nAll comparisons use a DeiT-SIN visual backbone for co-segmentation and ViT-L/14@336px for\narchive curation.\n\ntherefore use DeiT-SIN with an archive size of k=50 for the remaining experiments unless otherwise\nstated. See Fig. 3 for qualitative examples of our co-segmentation results.\n\nInfluence of ReCo framework components. We next assess the effect of employing DenseCLIP\nduring inference, the language-guided co-segmentation and context elimination components of ReCo\nwhich seek to improve the quality of co-segmentation achieved across each archive to boost down-\nstream segmentation performance. When applying context elimination, we select tree, sky, building,\nroad, and person as common background concepts appearing in natural images to be suppressed.\nIn Tab. 1, we show the effect of three of these strategies, together with the effect of applying a\nCRF [44] as post-processing. We observe that integrating DenseCLIP into the inference procedure\nbrings a significant gain in performance which we believe is driven by the notable robustness of CLIP\nfeatures under zero-shot transfer [71]. In addition, language-guided co-segmentation and context\nelimination further boost co-segmentation performance, while the CRF brings a small gain. We\ntherefore use each of these strategies (including CRF post-processing) in the remaining experiments.\n4.4 Comparison to state-of-the-art unsupervised methods\nWe compare ReCo and ReCo+ to state-of-the-art unsupervised semantic segmentation models\nwith and without vision-language pre-training on standard benchmarks, including COCO-Stuff [6],\nCityscapes [14] and KITTI-STEP [87] under both zero-shot transfer and unsupervised adaptation\n(training without labels on the target distribution). For COCO-Stuff, we observe that the mid-level\ncategories used for evaluation are somewhat abstract for retrieval (for instance, one mid-level category\nis \u201coutdoor objects\u201d, which may include many low-level categories beyond the target hierarchy). To\navoid introducing ambiguity to the co-segmentation procedure, we instead directly use the low-level\ncategories and then merge the predictions into the mid-level categories. Additionally, we rephrase\ntwo category names to reduce ambiguity (parking to parking lot and vegetation to tree) in Cityscapes\nand KITTI-STEP based on the descriptions found in [14]. A detailed discussion can be found in the\nsupplementary material.\n\nAs shown in Tab. 2, ReCo strongly outperforms prior models on all benchmarks for zero-shot transfer.\nUnder an unsupervised adaptation protocol, ReCo+ outperforms the state-of-the-art by a large margin\non the Cityscapes and KITTI-STEP. On COCO-Stuff, ReCo+ achieves slightly lower pixel accuracy\n\n8\n\n1020304050Top k0.20.30.40.50.60.70.8PrecisionRN50RN50x16RN50x64ViT-B/32ViT-B/16ViT-L/14ViT-L/14@336px1020304050archive size (k)0.200.220.240.260.28mIoURN50-CLIPRN50-MoCov2DeiT-S/16-SINViT-S/16-DINOViT-B/16-DINODenseCLIP\fModel\n\nAcc. mIoU\n\nModel\n\nAcc. mIoU\n\nModel\n\nAcc. mIoU\n\nZero-shot transfer\nDenseCLIP\u2021 [96] 32.3\nReCo\u2021 (Ours)\n46.6\n\nUnsupervised adaptation\nIIC [34]\nMDC [12]\nPiCIE [12]\nPiCIE + H [12]\nSTEGO [24]\nReCo+\u2021 (Ours)\n\n21.8\n32.2\n48.1\n50.0\n56.9\n54.5\n\n19.8\n27.2\n\n6.7\n9.8\n13.8\n14.4\n28.2\n33.0\n\nZero-shot transfer\nDenseCLIP\u22c6\u2021 [96] 35.9\nMDC\u22c6\u2020 [12]\nPiCIE\u22c6\u2020 [12]\nD&S\u22c6\u2020 [85]\nReCo\u22c6\u2021 (Ours)\n\n-\n-\n-\n65.4\n\nUnsupervised adaptation\n47.9\nIIC [34]\n40.7\nMDC [12]\n65.5\nPiCIE [12]\n73.2\nSTEGO [24]\nReCo+\u2021 (Ours)\n83.7\n\n10.0\n7.0\n9.7\n16.2\n22.0\n\n6.4\n7.1\n12.3\n21.0\n24.2\n\nZero-shot transfer\nDenseCLIP\u2021 [96] 34.1\nReCo\u2021 (Ours)\n70.6\n\nUnsupervised adaptation\nSegSort [33]\nHSG [41]\nReCo+\u2021 (Ours)\n\n69.8\n73.8\n75.3\n\n15.3\n29.8\n\n19.2\n21.7\n31.9\n\nTable 2: Comparison to state-of-the art approaches on COCO-Stuff (left), Cityscapes (middle),\nand KITTI-STEP (right) validation sets. \u22c6Evaluated at the original resolution. \u2020Models trained\non Waymo Open [81] (reported from [85]). \u2021Models that leverage a language-image pre-training\nmodel to assign a concept name to a prediction. The best score for each metric under each protocol is\nhighlighted in bold. We observe that ReCo and ReCo+ perform strongly relative to prior work under\nzero-shot transfer and unsupervised adaptation protocols, respectively.\n\nFigure 5: Co-segmentations of rare concepts. Left: fire extinguisher. Right: Antikythera mech-\nanism. We show selected samples from ReCo archives for each concept, together with their co-\nsegmentations. In each case, ReCo successfully identifies the regions associated with the concept.\n\ncompared to [24] but a considerably higher mIoU. In Fig. 1, we visualise the sample predictions of\nour models on COCO-Stuff.\n\n4.5 Segmenting rare concepts\nBy virtue of inheriting CLIP\u2019s diverse knowledge of nameable visual concepts, ReCo exhibits the\nability to segment rare categories. We first demonstrate this ability for fire extinguisher objects,\nwhich have important fire-safety implications but seldom appear in popular semantic segmentation\nbenchmarks. To assess performance, we evaluate segmentation quality on the FireNet dataset (as\ndescribed in Sec. 4.1) and achieve reasonable performance on pixel accuracy (93.3) and IoU (44.9)\nmetrics. In Fig. 5 (left) we visualise the co-segmentation produced by ReCo across sample images\ncontainer fire extinguishers. As an additional demonstration (Fig. 5, right), we also show co-\nsegmentations for images containing the views of the (unique) Antikythera mechanism. In both cases,\nwe observe that ReCo is capable of co-segmenting the concept of interest without labelled examples.\n\n\n\nBased on the section(s), please answer the following question with yes, no, or n/a and provide a brief justification for your answer.\nQuestion: If the authors ran experiments, do the authors include the total amount of compute and the type of resources used (e.g., type of GPUs, internal cluster, or cloud provider)?"}, "4a": {"role": "user", "content": "The following is the experiments section of the paper you are reviewing:\n\nIn this section, we start by describing the datasets used for our experiments (Sec. 4.1) and implemen-\ntation details (Sec. 4.2). Then, we conduct an ablation study (Sec. 4.3) and compare our model to\nstate-of-the-art methods for unsupervised semantic segmentation with and without language-image\npre-training (Sec. 4.4). Finally, we showcase our model\u2019s ability to segment rare-category objects\n(Sec. 4.5).\n\n4.1 Datasets\nFor our ablation study, we use the ImageNet1K [16] validation set to curate archive for concepts\nof interest. The dataset covers 1K classes with 50 images for each class. To measure segmentation\nperformance in the zero-shot transfer setting, we use the PASCAL-Context [58] validation set for\nevaluation, which has 5,104 images of 59 categories excluding the background class.\n\nTo compare with previous unsupervised segmentation methods, we use the ImageNet1K training set\nto construct the reference embedding for the concept we wish to segment. The dataset has 1K classes\nwith 1.2M images. We evaluate on standard benchmarks including the Cityscapes [14] validation\n\n6\n\n\fsplit, which has 500 urban scene images of 27 categories, KITTI-STEP [87] validation set, which\nis composed of 2,981 urban scene images of 19 categories, and COCO-Stuff [6] validation split,\nwhich has 4,172 images of 171 low-level thing and stuff categories excluding background class.\nFollowing [12, 24], we use the 27 mid-level categories for evaluation. For unsupervised adaptation\nwith ReCo+ (Sec. 3.2), we train on ReCo pseudo-labels on the Cityscapes training set with 2,975\nimages, KITTI-STEP training set which contains 5,027 images, and the COCO-Stuff10K subset\nwhich has 9,000 images for each respective benchmark. We emphasise that no ground-truth labels\nare used for training.\n\nFinally, to demonstrate our model\u2019s ability to segment rare concepts, we use the LAION-5B\ndataset [75] with 5 billion images as a large collection of images that we expect to satisfy our\nfirst hypothesis, namely that it will have coverage of rare concepts. To assess performance, we use\nthe FireNet dataset [62] which has 1,452 images spanning rare fire safety-related classes. For our\nexperiment, we select the fire extinguisher class as an example of a concept that is important but\nrare in vision datasets (it is not contained in ImageNet1K [74], for example) and evaluate ReCo on\n263 images containing at least one instance of the category. As a further proof of concept, we also\ndemonstrate co-segmentations of the Antikythera mechanism (a historical item that does not appear\nin WordNet [57], or any labelled vision datasets that we are aware of).\n\nImplementation details\n\n4.2\nHere, we describe the hyperparameters used to train ReCo+, inference details and evaluation metrics.\nOur implementation is based on the PyTorch library [66] and made publicly available.2\n\nReCo+ Training. While ReCo does not require training, we train ReCo+ based on the\nDeepLabv3+ [10] segmentation architecture with a ResNet101 [26] backone on the predictions\nfrom ReCo as described in 3.2. All training images are resized and center-cropped to 320\u00d7320 pixels\nand data augmentations such as random scaling, cropping, and horizontal flipping are applied with\nrandom color jittering and Gaussian blurring. We use the Adam optimiser [43] with an initial learning\nrate of 5 \u00d7 10\u22124 and a weight decay of 2 \u00d7 10\u22124 with the Poly learning rate schedule as in [50, 10].\nTraining consists of 20K gradient iterations with a batch size of 8 and takes about 5 hours on a single\n24GB NVIDIA P40 GPU.\n\nInference. For each benchmark, we pre-compute reference image embeddings for a list of categories\nfor the benchmark and store the embeddings to form a classifier. Whenever DenseCLIP is employed,\nwe use the ResNet50x16 model (following [96]) to construct a saliency map for each image. Unless\notherwise stated, for the COCO-Stuff and Cityscapes benchmarks, we resize and center crop the\ninput images to 320\u00d7320 pixels as in [24]. For the KITTI-STEP validation set, we use the original\nresolution of each image as in [41]. For the FireNet benchmark, we resize the shorter side of images\nto 512 pixels and predict a single class of fire extinguisher by thresholding the predicted heatmap\nwith probability of 0.5.\n\nEvaluation metrics. Following the common practice [34, 12, 24], we report pixel accuracy (Acc.)\nand mean intersection-over-union (mIoU).\n\n4.3 Ablation studies\n\nAbility of CLIP to curate archives. We begin by assessing the validity of our second hypothesis\u2014\nnamely that CLIP is capable of achieving high purity archives from unlabelled images. To this end,\nwe evaluate the retrieval performance of different CLIP models on the ImageNet1K validation set\nwhen constructing different archive sizes. In detail, for each archive size, k, we compute the precision\nof the top-k retrieved images based on whether the the ground-truth image-labels match the query\ntext. As can be seen in Fig. 4 (left), all CLIP models achieve solid retrieval performance, suggesting\ntheir potential for curating high purity archives as part of ReCo. Since ViT-L/14@336px performs\nbest, we employ this as our retrieval model in the remaining experiments.\n\nInfluence of archive size and visual encoder used for co-segmentation. In Fig. 4 (right) we illustrate\nthe effect of using different pre-trained architectures, e.g. MoCov2 [25], DINO [7], CLIP [71], DeiT-\nSIN [60], as the archive size (and thus the number of images used for co-segmentation) changes. The\ny-axis depicts segmentation performance for ReCo with these configurations on the PASCAL-Context\nbenchmark. We observe that using larger archives tends to improve performance (likely due to their\nreasonably high purity) albeit non-monotonically, and that features from DeiT-SIN perform best. We\n\n2Code available at https://github.com/NoelShin/reco\n\n7\n\n\fFigure 4: Ablation studies. Left: Image retrieval performance of different CLIP models on the\nImageNet1K validation set with k ranging from 5 to 50. ViT-L/14@336px performs particularly\nstrongly, suggesting the ability to curate archives of high purity. Right: Co-segmentation performance\non PASCAL-Context validation set as we vary the archive size and choice of visual encoders. We\nobserve a general trend towards improved performance with increasing archive size for all encoders.\n\nDenseCLIP LGC CE CRF Acc. mIoU\n\u2717\n\u2717\n\u2717\n\u2713\n\u2713\n\u2713\n\n16.8\n41.1\n43.1\n49.7\n50.9\n51.6\n\n5.7\n21.8\n23.1\n26.0\n26.6\n27.2\n\n\u2717\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\n\u2717\n\u2717\n\u2713\n\u2717\n\u2713\n\u2713\n\n\u2717\n\u2717\n\u2717\n\u2717\n\u2717\n\u2713\n\nTable 1: Influence of ReCo components for zero-shot transfer on PASCAL-Context [58]. We\nobserve that integrating DenseCLIP during inference, Language-guided co-segmentation (LGC),\nContext elimination (CE), and CRF [44] post-processing each contribute to improved performance.\nAll comparisons use a DeiT-SIN visual backbone for co-segmentation and ViT-L/14@336px for\narchive curation.\n\ntherefore use DeiT-SIN with an archive size of k=50 for the remaining experiments unless otherwise\nstated. See Fig. 3 for qualitative examples of our co-segmentation results.\n\nInfluence of ReCo framework components. We next assess the effect of employing DenseCLIP\nduring inference, the language-guided co-segmentation and context elimination components of ReCo\nwhich seek to improve the quality of co-segmentation achieved across each archive to boost down-\nstream segmentation performance. When applying context elimination, we select tree, sky, building,\nroad, and person as common background concepts appearing in natural images to be suppressed.\nIn Tab. 1, we show the effect of three of these strategies, together with the effect of applying a\nCRF [44] as post-processing. We observe that integrating DenseCLIP into the inference procedure\nbrings a significant gain in performance which we believe is driven by the notable robustness of CLIP\nfeatures under zero-shot transfer [71]. In addition, language-guided co-segmentation and context\nelimination further boost co-segmentation performance, while the CRF brings a small gain. We\ntherefore use each of these strategies (including CRF post-processing) in the remaining experiments.\n4.4 Comparison to state-of-the-art unsupervised methods\nWe compare ReCo and ReCo+ to state-of-the-art unsupervised semantic segmentation models\nwith and without vision-language pre-training on standard benchmarks, including COCO-Stuff [6],\nCityscapes [14] and KITTI-STEP [87] under both zero-shot transfer and unsupervised adaptation\n(training without labels on the target distribution). For COCO-Stuff, we observe that the mid-level\ncategories used for evaluation are somewhat abstract for retrieval (for instance, one mid-level category\nis \u201coutdoor objects\u201d, which may include many low-level categories beyond the target hierarchy). To\navoid introducing ambiguity to the co-segmentation procedure, we instead directly use the low-level\ncategories and then merge the predictions into the mid-level categories. Additionally, we rephrase\ntwo category names to reduce ambiguity (parking to parking lot and vegetation to tree) in Cityscapes\nand KITTI-STEP based on the descriptions found in [14]. A detailed discussion can be found in the\nsupplementary material.\n\nAs shown in Tab. 2, ReCo strongly outperforms prior models on all benchmarks for zero-shot transfer.\nUnder an unsupervised adaptation protocol, ReCo+ outperforms the state-of-the-art by a large margin\non the Cityscapes and KITTI-STEP. On COCO-Stuff, ReCo+ achieves slightly lower pixel accuracy\n\n8\n\n1020304050Top k0.20.30.40.50.60.70.8PrecisionRN50RN50x16RN50x64ViT-B/32ViT-B/16ViT-L/14ViT-L/14@336px1020304050archive size (k)0.200.220.240.260.28mIoURN50-CLIPRN50-MoCov2DeiT-S/16-SINViT-S/16-DINOViT-B/16-DINODenseCLIP\fModel\n\nAcc. mIoU\n\nModel\n\nAcc. mIoU\n\nModel\n\nAcc. mIoU\n\nZero-shot transfer\nDenseCLIP\u2021 [96] 32.3\nReCo\u2021 (Ours)\n46.6\n\nUnsupervised adaptation\nIIC [34]\nMDC [12]\nPiCIE [12]\nPiCIE + H [12]\nSTEGO [24]\nReCo+\u2021 (Ours)\n\n21.8\n32.2\n48.1\n50.0\n56.9\n54.5\n\n19.8\n27.2\n\n6.7\n9.8\n13.8\n14.4\n28.2\n33.0\n\nZero-shot transfer\nDenseCLIP\u22c6\u2021 [96] 35.9\nMDC\u22c6\u2020 [12]\nPiCIE\u22c6\u2020 [12]\nD&S\u22c6\u2020 [85]\nReCo\u22c6\u2021 (Ours)\n\n-\n-\n-\n65.4\n\nUnsupervised adaptation\n47.9\nIIC [34]\n40.7\nMDC [12]\n65.5\nPiCIE [12]\n73.2\nSTEGO [24]\nReCo+\u2021 (Ours)\n83.7\n\n10.0\n7.0\n9.7\n16.2\n22.0\n\n6.4\n7.1\n12.3\n21.0\n24.2\n\nZero-shot transfer\nDenseCLIP\u2021 [96] 34.1\nReCo\u2021 (Ours)\n70.6\n\nUnsupervised adaptation\nSegSort [33]\nHSG [41]\nReCo+\u2021 (Ours)\n\n69.8\n73.8\n75.3\n\n15.3\n29.8\n\n19.2\n21.7\n31.9\n\nTable 2: Comparison to state-of-the art approaches on COCO-Stuff (left), Cityscapes (middle),\nand KITTI-STEP (right) validation sets. \u22c6Evaluated at the original resolution. \u2020Models trained\non Waymo Open [81] (reported from [85]). \u2021Models that leverage a language-image pre-training\nmodel to assign a concept name to a prediction. The best score for each metric under each protocol is\nhighlighted in bold. We observe that ReCo and ReCo+ perform strongly relative to prior work under\nzero-shot transfer and unsupervised adaptation protocols, respectively.\n\nFigure 5: Co-segmentations of rare concepts. Left: fire extinguisher. Right: Antikythera mech-\nanism. We show selected samples from ReCo archives for each concept, together with their co-\nsegmentations. In each case, ReCo successfully identifies the regions associated with the concept.\n\ncompared to [24] but a considerably higher mIoU. In Fig. 1, we visualise the sample predictions of\nour models on COCO-Stuff.\n\n4.5 Segmenting rare concepts\nBy virtue of inheriting CLIP\u2019s diverse knowledge of nameable visual concepts, ReCo exhibits the\nability to segment rare categories. We first demonstrate this ability for fire extinguisher objects,\nwhich have important fire-safety implications but seldom appear in popular semantic segmentation\nbenchmarks. To assess performance, we evaluate segmentation quality on the FireNet dataset (as\ndescribed in Sec. 4.1) and achieve reasonable performance on pixel accuracy (93.3) and IoU (44.9)\nmetrics. In Fig. 5 (left) we visualise the co-segmentation produced by ReCo across sample images\ncontainer fire extinguishers. As an additional demonstration (Fig. 5, right), we also show co-\nsegmentations for images containing the views of the (unique) Antikythera mechanism. In both cases,\nwe observe that ReCo is capable of co-segmenting the concept of interest without labelled examples.\n\n\n\nBased on the section(s), please answer the following question with yes, no, or n/a and provide a brief justification for your answer.\nQuestion: If the authors use existing assets (e.g., code, data, models), do the authors cite the creators?"}, "4b": {"role": "user", "content": "The following is the experiments section of the paper you are reviewing:\n\nIn this section, we start by describing the datasets used for our experiments (Sec. 4.1) and implemen-\ntation details (Sec. 4.2). Then, we conduct an ablation study (Sec. 4.3) and compare our model to\nstate-of-the-art methods for unsupervised semantic segmentation with and without language-image\npre-training (Sec. 4.4). Finally, we showcase our model\u2019s ability to segment rare-category objects\n(Sec. 4.5).\n\n4.1 Datasets\nFor our ablation study, we use the ImageNet1K [16] validation set to curate archive for concepts\nof interest. The dataset covers 1K classes with 50 images for each class. To measure segmentation\nperformance in the zero-shot transfer setting, we use the PASCAL-Context [58] validation set for\nevaluation, which has 5,104 images of 59 categories excluding the background class.\n\nTo compare with previous unsupervised segmentation methods, we use the ImageNet1K training set\nto construct the reference embedding for the concept we wish to segment. The dataset has 1K classes\nwith 1.2M images. We evaluate on standard benchmarks including the Cityscapes [14] validation\n\n6\n\n\fsplit, which has 500 urban scene images of 27 categories, KITTI-STEP [87] validation set, which\nis composed of 2,981 urban scene images of 19 categories, and COCO-Stuff [6] validation split,\nwhich has 4,172 images of 171 low-level thing and stuff categories excluding background class.\nFollowing [12, 24], we use the 27 mid-level categories for evaluation. For unsupervised adaptation\nwith ReCo+ (Sec. 3.2), we train on ReCo pseudo-labels on the Cityscapes training set with 2,975\nimages, KITTI-STEP training set which contains 5,027 images, and the COCO-Stuff10K subset\nwhich has 9,000 images for each respective benchmark. We emphasise that no ground-truth labels\nare used for training.\n\nFinally, to demonstrate our model\u2019s ability to segment rare concepts, we use the LAION-5B\ndataset [75] with 5 billion images as a large collection of images that we expect to satisfy our\nfirst hypothesis, namely that it will have coverage of rare concepts. To assess performance, we use\nthe FireNet dataset [62] which has 1,452 images spanning rare fire safety-related classes. For our\nexperiment, we select the fire extinguisher class as an example of a concept that is important but\nrare in vision datasets (it is not contained in ImageNet1K [74], for example) and evaluate ReCo on\n263 images containing at least one instance of the category. As a further proof of concept, we also\ndemonstrate co-segmentations of the Antikythera mechanism (a historical item that does not appear\nin WordNet [57], or any labelled vision datasets that we are aware of).\n\nImplementation details\n\n4.2\nHere, we describe the hyperparameters used to train ReCo+, inference details and evaluation metrics.\nOur implementation is based on the PyTorch library [66] and made publicly available.2\n\nReCo+ Training. While ReCo does not require training, we train ReCo+ based on the\nDeepLabv3+ [10] segmentation architecture with a ResNet101 [26] backone on the predictions\nfrom ReCo as described in 3.2. All training images are resized and center-cropped to 320\u00d7320 pixels\nand data augmentations such as random scaling, cropping, and horizontal flipping are applied with\nrandom color jittering and Gaussian blurring. We use the Adam optimiser [43] with an initial learning\nrate of 5 \u00d7 10\u22124 and a weight decay of 2 \u00d7 10\u22124 with the Poly learning rate schedule as in [50, 10].\nTraining consists of 20K gradient iterations with a batch size of 8 and takes about 5 hours on a single\n24GB NVIDIA P40 GPU.\n\nInference. For each benchmark, we pre-compute reference image embeddings for a list of categories\nfor the benchmark and store the embeddings to form a classifier. Whenever DenseCLIP is employed,\nwe use the ResNet50x16 model (following [96]) to construct a saliency map for each image. Unless\notherwise stated, for the COCO-Stuff and Cityscapes benchmarks, we resize and center crop the\ninput images to 320\u00d7320 pixels as in [24]. For the KITTI-STEP validation set, we use the original\nresolution of each image as in [41]. For the FireNet benchmark, we resize the shorter side of images\nto 512 pixels and predict a single class of fire extinguisher by thresholding the predicted heatmap\nwith probability of 0.5.\n\nEvaluation metrics. Following the common practice [34, 12, 24], we report pixel accuracy (Acc.)\nand mean intersection-over-union (mIoU).\n\n4.3 Ablation studies\n\nAbility of CLIP to curate archives. We begin by assessing the validity of our second hypothesis\u2014\nnamely that CLIP is capable of achieving high purity archives from unlabelled images. To this end,\nwe evaluate the retrieval performance of different CLIP models on the ImageNet1K validation set\nwhen constructing different archive sizes. In detail, for each archive size, k, we compute the precision\nof the top-k retrieved images based on whether the the ground-truth image-labels match the query\ntext. As can be seen in Fig. 4 (left), all CLIP models achieve solid retrieval performance, suggesting\ntheir potential for curating high purity archives as part of ReCo. Since ViT-L/14@336px performs\nbest, we employ this as our retrieval model in the remaining experiments.\n\nInfluence of archive size and visual encoder used for co-segmentation. In Fig. 4 (right) we illustrate\nthe effect of using different pre-trained architectures, e.g. MoCov2 [25], DINO [7], CLIP [71], DeiT-\nSIN [60], as the archive size (and thus the number of images used for co-segmentation) changes. The\ny-axis depicts segmentation performance for ReCo with these configurations on the PASCAL-Context\nbenchmark. We observe that using larger archives tends to improve performance (likely due to their\nreasonably high purity) albeit non-monotonically, and that features from DeiT-SIN perform best. We\n\n2Code available at https://github.com/NoelShin/reco\n\n7\n\n\fFigure 4: Ablation studies. Left: Image retrieval performance of different CLIP models on the\nImageNet1K validation set with k ranging from 5 to 50. ViT-L/14@336px performs particularly\nstrongly, suggesting the ability to curate archives of high purity. Right: Co-segmentation performance\non PASCAL-Context validation set as we vary the archive size and choice of visual encoders. We\nobserve a general trend towards improved performance with increasing archive size for all encoders.\n\nDenseCLIP LGC CE CRF Acc. mIoU\n\u2717\n\u2717\n\u2717\n\u2713\n\u2713\n\u2713\n\n16.8\n41.1\n43.1\n49.7\n50.9\n51.6\n\n5.7\n21.8\n23.1\n26.0\n26.6\n27.2\n\n\u2717\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\n\u2717\n\u2717\n\u2713\n\u2717\n\u2713\n\u2713\n\n\u2717\n\u2717\n\u2717\n\u2717\n\u2717\n\u2713\n\nTable 1: Influence of ReCo components for zero-shot transfer on PASCAL-Context [58]. We\nobserve that integrating DenseCLIP during inference, Language-guided co-segmentation (LGC),\nContext elimination (CE), and CRF [44] post-processing each contribute to improved performance.\nAll comparisons use a DeiT-SIN visual backbone for co-segmentation and ViT-L/14@336px for\narchive curation.\n\ntherefore use DeiT-SIN with an archive size of k=50 for the remaining experiments unless otherwise\nstated. See Fig. 3 for qualitative examples of our co-segmentation results.\n\nInfluence of ReCo framework components. We next assess the effect of employing DenseCLIP\nduring inference, the language-guided co-segmentation and context elimination components of ReCo\nwhich seek to improve the quality of co-segmentation achieved across each archive to boost down-\nstream segmentation performance. When applying context elimination, we select tree, sky, building,\nroad, and person as common background concepts appearing in natural images to be suppressed.\nIn Tab. 1, we show the effect of three of these strategies, together with the effect of applying a\nCRF [44] as post-processing. We observe that integrating DenseCLIP into the inference procedure\nbrings a significant gain in performance which we believe is driven by the notable robustness of CLIP\nfeatures under zero-shot transfer [71]. In addition, language-guided co-segmentation and context\nelimination further boost co-segmentation performance, while the CRF brings a small gain. We\ntherefore use each of these strategies (including CRF post-processing) in the remaining experiments.\n4.4 Comparison to state-of-the-art unsupervised methods\nWe compare ReCo and ReCo+ to state-of-the-art unsupervised semantic segmentation models\nwith and without vision-language pre-training on standard benchmarks, including COCO-Stuff [6],\nCityscapes [14] and KITTI-STEP [87] under both zero-shot transfer and unsupervised adaptation\n(training without labels on the target distribution). For COCO-Stuff, we observe that the mid-level\ncategories used for evaluation are somewhat abstract for retrieval (for instance, one mid-level category\nis \u201coutdoor objects\u201d, which may include many low-level categories beyond the target hierarchy). To\navoid introducing ambiguity to the co-segmentation procedure, we instead directly use the low-level\ncategories and then merge the predictions into the mid-level categories. Additionally, we rephrase\ntwo category names to reduce ambiguity (parking to parking lot and vegetation to tree) in Cityscapes\nand KITTI-STEP based on the descriptions found in [14]. A detailed discussion can be found in the\nsupplementary material.\n\nAs shown in Tab. 2, ReCo strongly outperforms prior models on all benchmarks for zero-shot transfer.\nUnder an unsupervised adaptation protocol, ReCo+ outperforms the state-of-the-art by a large margin\non the Cityscapes and KITTI-STEP. On COCO-Stuff, ReCo+ achieves slightly lower pixel accuracy\n\n8\n\n1020304050Top k0.20.30.40.50.60.70.8PrecisionRN50RN50x16RN50x64ViT-B/32ViT-B/16ViT-L/14ViT-L/14@336px1020304050archive size (k)0.200.220.240.260.28mIoURN50-CLIPRN50-MoCov2DeiT-S/16-SINViT-S/16-DINOViT-B/16-DINODenseCLIP\fModel\n\nAcc. mIoU\n\nModel\n\nAcc. mIoU\n\nModel\n\nAcc. mIoU\n\nZero-shot transfer\nDenseCLIP\u2021 [96] 32.3\nReCo\u2021 (Ours)\n46.6\n\nUnsupervised adaptation\nIIC [34]\nMDC [12]\nPiCIE [12]\nPiCIE + H [12]\nSTEGO [24]\nReCo+\u2021 (Ours)\n\n21.8\n32.2\n48.1\n50.0\n56.9\n54.5\n\n19.8\n27.2\n\n6.7\n9.8\n13.8\n14.4\n28.2\n33.0\n\nZero-shot transfer\nDenseCLIP\u22c6\u2021 [96] 35.9\nMDC\u22c6\u2020 [12]\nPiCIE\u22c6\u2020 [12]\nD&S\u22c6\u2020 [85]\nReCo\u22c6\u2021 (Ours)\n\n-\n-\n-\n65.4\n\nUnsupervised adaptation\n47.9\nIIC [34]\n40.7\nMDC [12]\n65.5\nPiCIE [12]\n73.2\nSTEGO [24]\nReCo+\u2021 (Ours)\n83.7\n\n10.0\n7.0\n9.7\n16.2\n22.0\n\n6.4\n7.1\n12.3\n21.0\n24.2\n\nZero-shot transfer\nDenseCLIP\u2021 [96] 34.1\nReCo\u2021 (Ours)\n70.6\n\nUnsupervised adaptation\nSegSort [33]\nHSG [41]\nReCo+\u2021 (Ours)\n\n69.8\n73.8\n75.3\n\n15.3\n29.8\n\n19.2\n21.7\n31.9\n\nTable 2: Comparison to state-of-the art approaches on COCO-Stuff (left), Cityscapes (middle),\nand KITTI-STEP (right) validation sets. \u22c6Evaluated at the original resolution. \u2020Models trained\non Waymo Open [81] (reported from [85]). \u2021Models that leverage a language-image pre-training\nmodel to assign a concept name to a prediction. The best score for each metric under each protocol is\nhighlighted in bold. We observe that ReCo and ReCo+ perform strongly relative to prior work under\nzero-shot transfer and unsupervised adaptation protocols, respectively.\n\nFigure 5: Co-segmentations of rare concepts. Left: fire extinguisher. Right: Antikythera mech-\nanism. We show selected samples from ReCo archives for each concept, together with their co-\nsegmentations. In each case, ReCo successfully identifies the regions associated with the concept.\n\ncompared to [24] but a considerably higher mIoU. In Fig. 1, we visualise the sample predictions of\nour models on COCO-Stuff.\n\n4.5 Segmenting rare concepts\nBy virtue of inheriting CLIP\u2019s diverse knowledge of nameable visual concepts, ReCo exhibits the\nability to segment rare categories. We first demonstrate this ability for fire extinguisher objects,\nwhich have important fire-safety implications but seldom appear in popular semantic segmentation\nbenchmarks. To assess performance, we evaluate segmentation quality on the FireNet dataset (as\ndescribed in Sec. 4.1) and achieve reasonable performance on pixel accuracy (93.3) and IoU (44.9)\nmetrics. In Fig. 5 (left) we visualise the co-segmentation produced by ReCo across sample images\ncontainer fire extinguishers. As an additional demonstration (Fig. 5, right), we also show co-\nsegmentations for images containing the views of the (unique) Antikythera mechanism. In both cases,\nwe observe that ReCo is capable of co-segmenting the concept of interest without labelled examples.\n\n\n\nThe following is the appendix section of the paper you are reviewing:\nwe first discuss the role of supervisory signals for ReCo and alternative\napproaches, and the datasets used in our work (Appendix A). Next we provide further details\nof experiments conducted in the main paper together with hyperparameters for training ReCo+\nin Appendix B. We then report additional ablations investigating the influence of common category\nselection for context elimination, reducing ambiguity in category name, architectural chioces for\nCLIP and DenseCLIP, and the number of seed pixels considered for computing a reference image\nembedding in Appendix C. Finally, we provide additional qualitative results in Sec. D to illustrate\nboth successful and failure cases for our method.\n\nA Discussion of supervision and data\n\nA.1 Supervisory signals for ReCo and prior work\n\nIn the main paper, we compare to previous methods that are typically described as unsupervised. In\npractice, however, many methods (including ours) either implicitly or explicitly engage humans in\nthe data curation process at some stage.\n\nSupervision used by ReCo. (1) Similarly to prior work, our experiments make use of datasets\nconstructed from photographs taken by humans for both training and evaluation. These photographs\nare naturally biased towards content that humans find interesting and are typically well-framed (with\na concept of interest featuring prominently) or taken from a vantage point that offers a convenient\nscene overview (e.g. a roof-mounted camera on a vehicle driving on public roads). By training\nand evaluating on such data, our experimental results likely provide an optimistic assessment of\nperformance when contrasted with other distributions (e.g. the video feed received by an autonomous\nmobile robot). (2) For our comparisons to prior work, we use the ImageNet training subset without\nlabels to curate archives. However, in practice, this dataset is not free from human involvement: it\nwas curated by human workers who were asked to verify that each image contains a particular synset\ncategory (see [5] for a discussion of the collection process). While annotators were encouraged to\nselect images regardless of occlusions, the number of objects and clutter in the scene, this process\nnevertheless produced a relatively clean dataset with fairly object-centric images. (3) Several of\nour experiments make use of DeiT-SIN [13], which is trained on stylised ImageNet [6] with labels.\nWe don\u2019t believe that this supervision is critical, since in Fig. 4 of the main paper, we showed that\nResNet50-MoCov2 [8] which does not use labels achieves similar performance (less than 1 mIoU\ndifference on PASCAL-Context [12]). Moreover, we note that previous unsupervised methods to\nwhich we compare (e.g. [10, 3]) initialise their approach from supervised ImageNet training with the\nconvention that unsupervised in this context denotes the fact that no pixel-level supervision is used.\n(4) By using CLIP [17], we also make use of a different kind of supervision, namely images paired\nwith alt-text scraped from the web. Empirically, this data source has been shown to be extremely\nscalable and to enable generalisation to very large numbers of concepts [17, 11, 16]. Nevertheless,\nthe creation of the original alt-text image descriptions (mostly) derives from a human source, and\ntherefore provides a form of human supervision. In contrast to using ImageNet classification labels,\nthis source of supervision is indispensable to ReCo.\n\nWe believe that the key factors to be considered when discussing the question of supervision are\nscalability and generalisation. We are typically not interested in unsupervised methods for their own\nsake, but rather because they offer the ability to cheaply scale up machine learning to larger training\ndata sets that improve performance, and to build methods that go beyond the functionality afforded by\nlabelled datasets (e.g. new classes, new tasks etc.). Subject to the caveats (e.g. human photographer\nbias) outlined above, we believe ReCo has the flexibility to scale up far beyond the experimental\ncomparisons conducted in this work without requiring any changes to the underlying framework.\n\nA.2 Discussion of consent in used datasets\n\nIn this work, we work primarily with widely used Computer Vision benchmarks: ImageNet [5],\nPASCAL-Context [12], Cityscapes [4], COCO-Stuff [1], KITTI-STEP [21]. For these datasets, we\ndo not conduct an independent investigation of consent beyond the considerations of the authors that\nreleased these datasets. For our final exploratory studies which make use of LAION-5B [18], we\nmanually verified that no humans were present in the archives that were curated by ReCo.\n\n2\n\n\fA.3 Discussion on whether data contains personally identifiable information or offensive\n\ncontent\n\nWe do not release any data as part of this work. By working with widely used Computer Vision\nbenchmarks, we also restrict ourselves to imagery that is available in the public domain. We therefore\nbelieve that the risk that our work builds on harmful content or contributes to the leakage of personal\ninformation is low.\n\nOne exception to this is our use of the LAION-5B dataset for qualitative studies. We manually\nverified that no personally identifiable information or harmful content (as judged by the authors) was\npresent in the archives curated by ReCo.\n\nA.4 Dataset licenses\n\nHere we describe the terms/licenses of datasets used in our paper. For images in PASCAL-Context\nand COCO dataset, we comply with the Flickr Terms of Use and the Creative Commons Attribution\n4.0 License for the COCO-Stuff annotations. For Cityscapes and ImageNet1K, we follow the terms\nstated on their official website1 and the Attribution-NonCommercial-ShareAlike 3.0 Unported (CC\nBY-NC-SA 3.0) licence for KITTI-STEP.\n\nB Experiment details\n\nHere we provide pseudocode for ReCo and details of experiments conducted in the main paper.\n\nB.1 Pseudocode for ReCo\n\nIn Alg. 1, we describe the pseudocode for the core of ReCo (to maintain readability, language-guided\nco-segmentation and context elimination are omitted since these follow a similar structure).\n\nB.2 Prompt engineering\n\nTo obtain the text embedding for a concept, we ensemble the textual features from 85 templates,\ne.g., \u201ca photo of the {concept}\u201d and \u201cthere is a {concept} in the scene\u201d\nfollowing [22].\n\nB.3 Details of ablation study to assess CLIP retrieval performance\n\nWe observe that two of the ImageNet1K class labels are not unique\u2014they occur twice with different\nmeanings (e.g., \u201ccrane\u201d is used to represent both bird and machine), which makes retrieval inference\nand evaluation ambiguous. Therefore, we exclude those classes and use the remaining 996 categories\nfor the experimental results reported in Fig. 4 (left) of the main paper.\n\nB.4 Hyperparameters for ReCo+ training\n\nAs described in Sec. 4.2 (in the main paper), we adopt DeepLabv3+2 [2] with ResNet101 encoder [9]\nfor ReCo+ and train the network with standard data augmentations such as random scaling and\nhorizontal flip following [14, 19]. In detail, for geometric transformations, we use random scaling\nwith a range of [0.5, 2.0], random crop with a crop size 320\u00d7320 pixels, and random horizontal flip\nwith a probability of 0.5. For the photometric augmentations, we apply colour jittering3 with 0.8, 0.8,\n0.8, 0.2, and 0.8 for brightness, contrast, saturation, hue and probability parameters respectively. We\nalso employ Gaussian blurring with a kernel size of 10% of min(H, W ) where min(H, W ) returns\nthe length of the shorter side of an image.\n\n1https://www.cityscapes-dataset.com/license and https://www.image-net.org/download.\n\nphp for Cityscapes and ImageNet1K respectively.\n\n2We use the code for DeepLabv3+ from https://github.com/VainF/DeepLabV3Plus-Pytorch.\n3We use ColorJitter function in torchvision package [15].\n\n3\n\n\fAlgorithm 1 Pseudocode for the core of ReCo (using PyTorch-like syntax)\nInput. a CLIP image encoder \u03c8I, a CLIP text encoder \u03c8T , an image encoder \u03d5I, an image collection\nU, a concept c, the number of co-segmented images k\nOutput. a reference image embedding ref_emb and a prediction of the concept c in a new image\n\n# retrieve images\nimage_emb = l2_normalize(\u03c8I(U), dim=1) # NxC\ntext_emb = l2_normalize(\u03c8T (c), dim=0) # C\nscores = mm(image_emb, text_emb) # N\nindices = argmax(scores)[:k] # k\nimages = U[indices] # kx3xHxW\n\n# co-segment\nF = l2_normalize(\u03d5I(images), dim=1) # kxCxhxw\nF_flat = F.permute(1,0,2,3).view(C,k*h*w) # Cxkhw\nA = mm(F_flat.T, F_flat) # adjacency matrix, khwxkhw\n\ngrid = zeros((k*h*w, k))\nstart_col = 0 # start column index\nfor i in range(k):\n\nend_col = start_col + h*w # end column index\ngrid[:,i] = max(A[:,start_col:start_col+end_col], dim=1)\nstart_col = end_col\n\navg_grid = mean(grid, dim=1) # khw\n\nseed_features = []\nstart_row = 0 # start row index\nfor i in range(k):\n\nend_row = start_row + h*w # end row index\nindex_1d = argmax(avg_grid[start_row:end_row])\nstart_row = end_row\nindex_2d = [index_1d//w,index_1d%w]\nseed_features.append(F[i,:,index_2d[0],index_2d[1]])\n\nseed_features = stack(seed_features, dim=0) # k\u00d7C\nref_emb = l2_normalize(seed_features.mean(dim=0), dim=0) # C\n\n# inference\nF_new = l2_normalize(\u03d5I(new_image), dim=0) # Cxhxw\nprediction = sigmoid(mm(ref_emb, F_new)) # hxw\n\nmm:matrix multiplication.\n\ng\nn\ni\nd\nl\ni\nu\nb\n\nn\no\ns\nr\ne\np\n\nd\na\no\nr\n\ne\ne\nr\nt\n\ny\nk\ns\n\nmIoU\n\n\u2717\n\u2717\n\u2717\n\u2717\n\u2717\n5.7\n\u2717\n\u2717\n\u2713 \u2717\n\u2717\n10.9\n\u2717\n\u2717\n\u2713 \u2713 \u2717\n12.0\n\u2713 \u2713 \u2713 \u2717\n\u2717\n10.8\n\u2713 \u2713 \u2713 \u2713 \u2717\n11.4\n\u2713 \u2713 \u2713 \u2713 \u2713 12.3\n\nparking\n\nvegetation\n\n\u2192parking lot \u2192tree\n\nmIoU\n\n\u2717\n\u2717\n\u2713\n\u2713\n\n\u2717\n\u2713\n\u2717\n\u2713\n\n15.4\n16.0\n18.6\n19.3\n\nTable 1: Effect of context category choices and reducing label ambiguity. Left: We find that\nsuppressing 5 frequently appearing categories brings performance gain on PASCAL-Context [12].\nRight: We observe that specifying the meaning of a class more concretely helps segmentation\nperformance of ReCo on Cityscapes [4]. For both cases, input images are resized and center-cropped\nto 320\u00d7320 following [7].\n\n4\n\n\fModel\n\nCLIP arch.\n\nDenseCLIP arch. mIoU\n\nReCo (ours)\n\nResNet50\nViT-B/32\nViT-L/14@336px ResNet50x16\n\nResNet50\nResNet50\n\n20.3\n20.3\n22.0\n\n# seed pixels mIoU Acc.\n\n1\n5\n10\n50\n100\n\n22.0\n22.2\n22.1\n21.7\n20.4\n\n65.4\n64.9\n62.9\n57.5\n51.0\n\nTable 2: Effect of architecture choices (left) and the number of seed pixels (right) on performance\nof ReCo on the Cityscapes validation set.\n\nC Additional ablation studies\n\nC.1 Choices of context categories\n\nAs described in Sec. 3.3 (main paper), we propose to suppress the commonly appearing categories,\ne.g., sky, which co-occur with other classes, e.g., aeroplanes. To achieve this, we manually pick 5\nfrequently appearing classes in PASCAL-Context dataset [12] and investigate the effect of different\ncombinations of such categories. In Tab. 1 (left) we observe that suppressing the tree and sky\ncategories yields a notable performance gain, while eliminating all five categories performs best. For\nthis reason, we apply the context elimination strategy with these five categories in the main paper.\n\nC.2 Category name rephrasing to reduce ambiguity\n\nWe observe that it is important to specify a concept concretely to obtain retrieved images exhibiting\nsimilar visual appearance. For instance, in Cityscapes dataset, parking and vegetation can be rephrased\nto less ambiguous concepts parking lot and tree respectively based on their descriptions4 in the paper\naccompanying the dataset [4]. As can be seen in Tab. 1 (right), ReCo gains benefits in performance\non Cityscapes by replacing the category names with less abstract concepts. This sensitivity is a\nconsequence of our co-segmentation algorithm, which locates pixels that share similar visual features\nacross multiple images. Thus we use the rephrased label names throughout the experiments in the\npaper.\n\nIn addition to the limitations listed in the main paper, this dependence on concrete/specific concept\nnames can be considered a limitation of our approach (albeit one that is readily mitigated). However,\nwe believe it is a reasonable requirement for methods that operate in the zero-shot transfer setting.\nUnlike fine-tuning methods that learn to associate abstract text descriptions to visual concepts by\nseeing examples from the target distribution, ReCo relies entirely on an adequate text description to\ndisambiguate the concept. Since many computer vision datasets have been constructed with training\nand testing splits with the assumption that methods would make use of the training set, we believe\nit is probable that category names were not designed to be uniquely descriptive (hence the use of\n\u201cparking\u201d as a category in Cityscapes, which could be either a verb or a noun). Indeed, there may\nhave been little perceived need to construct unambiguous category names when examples from the\ntraining set implicitly provide disambiguation of the concept.\n\nC.3 Effect of architectural choices for CLIP and DenseCLIP\n\nFor the experiments in the main paper, we use ViT-L/14@336px and ResNet50x16 for image retrieval\nand DenseCLIP inference respectively. As these models are relatively heavier than commonly\nused architectures such as ResNet50 or ViT-B/32, we evaluate ReCo with lighter encoders on the\nCityscapes validation split in Tab. 2 (left). Specifically we use either ResNet50 or ViT-B/32 for image\nretrieval and ResNet50 for DenseCLIP inference. As can be seen, adopting a lighter model slightly\ndecreases the performance, but still outperforms previous state-of-the-art methods (e.g., 16.3 mIoU\nfor D&S [20]).\n\n4\u201cHorizontal surfaces that are intended for parking and separated from the road, either via elevation or via a\ndifferent texture/material, but not separated merely by markings.\u201d for parking and \u201cTrees, hedges, and all kinds\nof vertically growing vegetation.\u201d for vegetation.\n\n5\n\n\fFigure 1: Additional visualisations of our co-segmentation method used for ReCo. For visualisa-\ntion purpose, we show the top 5 images from 50 retrieved images in each archive for a category. Left:\nSuccessful cases. Right: Typical failure cases. Highlighted regions are shown in red. Best viewed in\ncolour.\n\nC.4 Effect of the number of seed pixels\n\nWhile we pick one seed pixel per image by default as decribed in Sec. 3, we investigate how the\nnumber of seed pixels selected for an image affects performance of ReCo. For this, we evaluate ReCo\non Cityscapes with each reference image embedding computed by averaging 1, 5, 10, 50, and 100\nseed pixel(s) for an image. As can be seen in Tab. 2 (right), using one or five seed pixel(s) per image\nshows the best performance in terms of pixel accuracy and mIoU and picking more pixels tends to\nhurt the performance.\n\nD Additional visualisations\n\nD.1 Co-segmentation with seed pixels\n\nIn Fig. 1, we visualise examples of the co-segmentation with seed pixels on ImageNet2012, which is\nused for an index dataset for ReCo in the main paper. On the left, we show successful cases where the\nco-segmentation highlights regions corresponding to a given concept (i.e., sheep, building, and zebra).\nOn the right, we display examples of two typical failure cases: partial segmentation (i.e., bicycle\nand cup) and highlighting an object commonly co-occurring with a given concept (i.e., baseball\nglove). For bicycles, their frames are less highlighted compared to the wheels. Similarly, the body\nparts of the cups are less likely to be emphasised than the handles. In case of baseball gloves, the\nco-segmentation locates a part of a baseball, which often appears with a baseball glove. We believe\nthese failure cases are caused by the property of our co-segmentation algorithm, which focuses on\nregions with less variance in visual features (e.g., texture and shape) appearing in multiple images of\nan archive.\n\nD.2 Predictions of ReCo and ReCo+\n\nIn Fig. 2, we show more visualisation samples on the COCO-Stuff benchmark. Successful and failure\ncases are shown on the left and right, respectively. We note that ReCo tends to fail in predicting small\nobjects, e.g., people in the bus, and so does ReCo+ which is trained on the ReCo\u2019s predictions as\npseudo-labels. We conjecture that this is related to the stride of the image encoder used for ReCo,\n\n6\n\n\fFigure 2: Additional visualisations on COCO-Stuff. Left: Successful cases. Right: Typical failure\ncases. White pixels denote ignored regions.\n\nwhich is 16\u00d716 for the case of DeiT-S/16-SIN [13]. It could therefore potentially be improved by\nusing an encoder with a smaller stride at the cost of increased computational burden.\n\n\n\nBased on the section(s), please answer the following question with yes, no, or n/a and provide a brief justification for your answer.\nQuestion: If the authors use existing assets (e.g., code, data, models) or curate/release new assets, do the authors mention the license of the assets?"}, "4c": {"role": "user", "content": "The following is the experiments section of the paper you are reviewing:\n\nIn this section, we start by describing the datasets used for our experiments (Sec. 4.1) and implemen-\ntation details (Sec. 4.2). Then, we conduct an ablation study (Sec. 4.3) and compare our model to\nstate-of-the-art methods for unsupervised semantic segmentation with and without language-image\npre-training (Sec. 4.4). Finally, we showcase our model\u2019s ability to segment rare-category objects\n(Sec. 4.5).\n\n4.1 Datasets\nFor our ablation study, we use the ImageNet1K [16] validation set to curate archive for concepts\nof interest. The dataset covers 1K classes with 50 images for each class. To measure segmentation\nperformance in the zero-shot transfer setting, we use the PASCAL-Context [58] validation set for\nevaluation, which has 5,104 images of 59 categories excluding the background class.\n\nTo compare with previous unsupervised segmentation methods, we use the ImageNet1K training set\nto construct the reference embedding for the concept we wish to segment. The dataset has 1K classes\nwith 1.2M images. We evaluate on standard benchmarks including the Cityscapes [14] validation\n\n6\n\n\fsplit, which has 500 urban scene images of 27 categories, KITTI-STEP [87] validation set, which\nis composed of 2,981 urban scene images of 19 categories, and COCO-Stuff [6] validation split,\nwhich has 4,172 images of 171 low-level thing and stuff categories excluding background class.\nFollowing [12, 24], we use the 27 mid-level categories for evaluation. For unsupervised adaptation\nwith ReCo+ (Sec. 3.2), we train on ReCo pseudo-labels on the Cityscapes training set with 2,975\nimages, KITTI-STEP training set which contains 5,027 images, and the COCO-Stuff10K subset\nwhich has 9,000 images for each respective benchmark. We emphasise that no ground-truth labels\nare used for training.\n\nFinally, to demonstrate our model\u2019s ability to segment rare concepts, we use the LAION-5B\ndataset [75] with 5 billion images as a large collection of images that we expect to satisfy our\nfirst hypothesis, namely that it will have coverage of rare concepts. To assess performance, we use\nthe FireNet dataset [62] which has 1,452 images spanning rare fire safety-related classes. For our\nexperiment, we select the fire extinguisher class as an example of a concept that is important but\nrare in vision datasets (it is not contained in ImageNet1K [74], for example) and evaluate ReCo on\n263 images containing at least one instance of the category. As a further proof of concept, we also\ndemonstrate co-segmentations of the Antikythera mechanism (a historical item that does not appear\nin WordNet [57], or any labelled vision datasets that we are aware of).\n\nImplementation details\n\n4.2\nHere, we describe the hyperparameters used to train ReCo+, inference details and evaluation metrics.\nOur implementation is based on the PyTorch library [66] and made publicly available.2\n\nReCo+ Training. While ReCo does not require training, we train ReCo+ based on the\nDeepLabv3+ [10] segmentation architecture with a ResNet101 [26] backone on the predictions\nfrom ReCo as described in 3.2. All training images are resized and center-cropped to 320\u00d7320 pixels\nand data augmentations such as random scaling, cropping, and horizontal flipping are applied with\nrandom color jittering and Gaussian blurring. We use the Adam optimiser [43] with an initial learning\nrate of 5 \u00d7 10\u22124 and a weight decay of 2 \u00d7 10\u22124 with the Poly learning rate schedule as in [50, 10].\nTraining consists of 20K gradient iterations with a batch size of 8 and takes about 5 hours on a single\n24GB NVIDIA P40 GPU.\n\nInference. For each benchmark, we pre-compute reference image embeddings for a list of categories\nfor the benchmark and store the embeddings to form a classifier. Whenever DenseCLIP is employed,\nwe use the ResNet50x16 model (following [96]) to construct a saliency map for each image. Unless\notherwise stated, for the COCO-Stuff and Cityscapes benchmarks, we resize and center crop the\ninput images to 320\u00d7320 pixels as in [24]. For the KITTI-STEP validation set, we use the original\nresolution of each image as in [41]. For the FireNet benchmark, we resize the shorter side of images\nto 512 pixels and predict a single class of fire extinguisher by thresholding the predicted heatmap\nwith probability of 0.5.\n\nEvaluation metrics. Following the common practice [34, 12, 24], we report pixel accuracy (Acc.)\nand mean intersection-over-union (mIoU).\n\n4.3 Ablation studies\n\nAbility of CLIP to curate archives. We begin by assessing the validity of our second hypothesis\u2014\nnamely that CLIP is capable of achieving high purity archives from unlabelled images. To this end,\nwe evaluate the retrieval performance of different CLIP models on the ImageNet1K validation set\nwhen constructing different archive sizes. In detail, for each archive size, k, we compute the precision\nof the top-k retrieved images based on whether the the ground-truth image-labels match the query\ntext. As can be seen in Fig. 4 (left), all CLIP models achieve solid retrieval performance, suggesting\ntheir potential for curating high purity archives as part of ReCo. Since ViT-L/14@336px performs\nbest, we employ this as our retrieval model in the remaining experiments.\n\nInfluence of archive size and visual encoder used for co-segmentation. In Fig. 4 (right) we illustrate\nthe effect of using different pre-trained architectures, e.g. MoCov2 [25], DINO [7], CLIP [71], DeiT-\nSIN [60], as the archive size (and thus the number of images used for co-segmentation) changes. The\ny-axis depicts segmentation performance for ReCo with these configurations on the PASCAL-Context\nbenchmark. We observe that using larger archives tends to improve performance (likely due to their\nreasonably high purity) albeit non-monotonically, and that features from DeiT-SIN perform best. We\n\n2Code available at https://github.com/NoelShin/reco\n\n7\n\n\fFigure 4: Ablation studies. Left: Image retrieval performance of different CLIP models on the\nImageNet1K validation set with k ranging from 5 to 50. ViT-L/14@336px performs particularly\nstrongly, suggesting the ability to curate archives of high purity. Right: Co-segmentation performance\non PASCAL-Context validation set as we vary the archive size and choice of visual encoders. We\nobserve a general trend towards improved performance with increasing archive size for all encoders.\n\nDenseCLIP LGC CE CRF Acc. mIoU\n\u2717\n\u2717\n\u2717\n\u2713\n\u2713\n\u2713\n\n16.8\n41.1\n43.1\n49.7\n50.9\n51.6\n\n5.7\n21.8\n23.1\n26.0\n26.6\n27.2\n\n\u2717\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\n\u2717\n\u2717\n\u2713\n\u2717\n\u2713\n\u2713\n\n\u2717\n\u2717\n\u2717\n\u2717\n\u2717\n\u2713\n\nTable 1: Influence of ReCo components for zero-shot transfer on PASCAL-Context [58]. We\nobserve that integrating DenseCLIP during inference, Language-guided co-segmentation (LGC),\nContext elimination (CE), and CRF [44] post-processing each contribute to improved performance.\nAll comparisons use a DeiT-SIN visual backbone for co-segmentation and ViT-L/14@336px for\narchive curation.\n\ntherefore use DeiT-SIN with an archive size of k=50 for the remaining experiments unless otherwise\nstated. See Fig. 3 for qualitative examples of our co-segmentation results.\n\nInfluence of ReCo framework components. We next assess the effect of employing DenseCLIP\nduring inference, the language-guided co-segmentation and context elimination components of ReCo\nwhich seek to improve the quality of co-segmentation achieved across each archive to boost down-\nstream segmentation performance. When applying context elimination, we select tree, sky, building,\nroad, and person as common background concepts appearing in natural images to be suppressed.\nIn Tab. 1, we show the effect of three of these strategies, together with the effect of applying a\nCRF [44] as post-processing. We observe that integrating DenseCLIP into the inference procedure\nbrings a significant gain in performance which we believe is driven by the notable robustness of CLIP\nfeatures under zero-shot transfer [71]. In addition, language-guided co-segmentation and context\nelimination further boost co-segmentation performance, while the CRF brings a small gain. We\ntherefore use each of these strategies (including CRF post-processing) in the remaining experiments.\n4.4 Comparison to state-of-the-art unsupervised methods\nWe compare ReCo and ReCo+ to state-of-the-art unsupervised semantic segmentation models\nwith and without vision-language pre-training on standard benchmarks, including COCO-Stuff [6],\nCityscapes [14] and KITTI-STEP [87] under both zero-shot transfer and unsupervised adaptation\n(training without labels on the target distribution). For COCO-Stuff, we observe that the mid-level\ncategories used for evaluation are somewhat abstract for retrieval (for instance, one mid-level category\nis \u201coutdoor objects\u201d, which may include many low-level categories beyond the target hierarchy). To\navoid introducing ambiguity to the co-segmentation procedure, we instead directly use the low-level\ncategories and then merge the predictions into the mid-level categories. Additionally, we rephrase\ntwo category names to reduce ambiguity (parking to parking lot and vegetation to tree) in Cityscapes\nand KITTI-STEP based on the descriptions found in [14]. A detailed discussion can be found in the\nsupplementary material.\n\nAs shown in Tab. 2, ReCo strongly outperforms prior models on all benchmarks for zero-shot transfer.\nUnder an unsupervised adaptation protocol, ReCo+ outperforms the state-of-the-art by a large margin\non the Cityscapes and KITTI-STEP. On COCO-Stuff, ReCo+ achieves slightly lower pixel accuracy\n\n8\n\n1020304050Top k0.20.30.40.50.60.70.8PrecisionRN50RN50x16RN50x64ViT-B/32ViT-B/16ViT-L/14ViT-L/14@336px1020304050archive size (k)0.200.220.240.260.28mIoURN50-CLIPRN50-MoCov2DeiT-S/16-SINViT-S/16-DINOViT-B/16-DINODenseCLIP\fModel\n\nAcc. mIoU\n\nModel\n\nAcc. mIoU\n\nModel\n\nAcc. mIoU\n\nZero-shot transfer\nDenseCLIP\u2021 [96] 32.3\nReCo\u2021 (Ours)\n46.6\n\nUnsupervised adaptation\nIIC [34]\nMDC [12]\nPiCIE [12]\nPiCIE + H [12]\nSTEGO [24]\nReCo+\u2021 (Ours)\n\n21.8\n32.2\n48.1\n50.0\n56.9\n54.5\n\n19.8\n27.2\n\n6.7\n9.8\n13.8\n14.4\n28.2\n33.0\n\nZero-shot transfer\nDenseCLIP\u22c6\u2021 [96] 35.9\nMDC\u22c6\u2020 [12]\nPiCIE\u22c6\u2020 [12]\nD&S\u22c6\u2020 [85]\nReCo\u22c6\u2021 (Ours)\n\n-\n-\n-\n65.4\n\nUnsupervised adaptation\n47.9\nIIC [34]\n40.7\nMDC [12]\n65.5\nPiCIE [12]\n73.2\nSTEGO [24]\nReCo+\u2021 (Ours)\n83.7\n\n10.0\n7.0\n9.7\n16.2\n22.0\n\n6.4\n7.1\n12.3\n21.0\n24.2\n\nZero-shot transfer\nDenseCLIP\u2021 [96] 34.1\nReCo\u2021 (Ours)\n70.6\n\nUnsupervised adaptation\nSegSort [33]\nHSG [41]\nReCo+\u2021 (Ours)\n\n69.8\n73.8\n75.3\n\n15.3\n29.8\n\n19.2\n21.7\n31.9\n\nTable 2: Comparison to state-of-the art approaches on COCO-Stuff (left), Cityscapes (middle),\nand KITTI-STEP (right) validation sets. \u22c6Evaluated at the original resolution. \u2020Models trained\non Waymo Open [81] (reported from [85]). \u2021Models that leverage a language-image pre-training\nmodel to assign a concept name to a prediction. The best score for each metric under each protocol is\nhighlighted in bold. We observe that ReCo and ReCo+ perform strongly relative to prior work under\nzero-shot transfer and unsupervised adaptation protocols, respectively.\n\nFigure 5: Co-segmentations of rare concepts. Left: fire extinguisher. Right: Antikythera mech-\nanism. We show selected samples from ReCo archives for each concept, together with their co-\nsegmentations. In each case, ReCo successfully identifies the regions associated with the concept.\n\ncompared to [24] but a considerably higher mIoU. In Fig. 1, we visualise the sample predictions of\nour models on COCO-Stuff.\n\n4.5 Segmenting rare concepts\nBy virtue of inheriting CLIP\u2019s diverse knowledge of nameable visual concepts, ReCo exhibits the\nability to segment rare categories. We first demonstrate this ability for fire extinguisher objects,\nwhich have important fire-safety implications but seldom appear in popular semantic segmentation\nbenchmarks. To assess performance, we evaluate segmentation quality on the FireNet dataset (as\ndescribed in Sec. 4.1) and achieve reasonable performance on pixel accuracy (93.3) and IoU (44.9)\nmetrics. In Fig. 5 (left) we visualise the co-segmentation produced by ReCo across sample images\ncontainer fire extinguishers. As an additional demonstration (Fig. 5, right), we also show co-\nsegmentations for images containing the views of the (unique) Antikythera mechanism. In both cases,\nwe observe that ReCo is capable of co-segmenting the concept of interest without labelled examples.\n\n\n\nBased on the section(s), please answer the following question with yes, no, or n/a and provide a brief justification for your answer.\nQuestion: If the authors curate/release new assets (e.g., code, data, models), do the authors include any new assets either in the supplemental material or as a URL?"}, "4d": {"role": "user", "content": "The following is the experiments section of the paper you are reviewing:\n\nIn this section, we start by describing the datasets used for our experiments (Sec. 4.1) and implemen-\ntation details (Sec. 4.2). Then, we conduct an ablation study (Sec. 4.3) and compare our model to\nstate-of-the-art methods for unsupervised semantic segmentation with and without language-image\npre-training (Sec. 4.4). Finally, we showcase our model\u2019s ability to segment rare-category objects\n(Sec. 4.5).\n\n4.1 Datasets\nFor our ablation study, we use the ImageNet1K [16] validation set to curate archive for concepts\nof interest. The dataset covers 1K classes with 50 images for each class. To measure segmentation\nperformance in the zero-shot transfer setting, we use the PASCAL-Context [58] validation set for\nevaluation, which has 5,104 images of 59 categories excluding the background class.\n\nTo compare with previous unsupervised segmentation methods, we use the ImageNet1K training set\nto construct the reference embedding for the concept we wish to segment. The dataset has 1K classes\nwith 1.2M images. We evaluate on standard benchmarks including the Cityscapes [14] validation\n\n6\n\n\fsplit, which has 500 urban scene images of 27 categories, KITTI-STEP [87] validation set, which\nis composed of 2,981 urban scene images of 19 categories, and COCO-Stuff [6] validation split,\nwhich has 4,172 images of 171 low-level thing and stuff categories excluding background class.\nFollowing [12, 24], we use the 27 mid-level categories for evaluation. For unsupervised adaptation\nwith ReCo+ (Sec. 3.2), we train on ReCo pseudo-labels on the Cityscapes training set with 2,975\nimages, KITTI-STEP training set which contains 5,027 images, and the COCO-Stuff10K subset\nwhich has 9,000 images for each respective benchmark. We emphasise that no ground-truth labels\nare used for training.\n\nFinally, to demonstrate our model\u2019s ability to segment rare concepts, we use the LAION-5B\ndataset [75] with 5 billion images as a large collection of images that we expect to satisfy our\nfirst hypothesis, namely that it will have coverage of rare concepts. To assess performance, we use\nthe FireNet dataset [62] which has 1,452 images spanning rare fire safety-related classes. For our\nexperiment, we select the fire extinguisher class as an example of a concept that is important but\nrare in vision datasets (it is not contained in ImageNet1K [74], for example) and evaluate ReCo on\n263 images containing at least one instance of the category. As a further proof of concept, we also\ndemonstrate co-segmentations of the Antikythera mechanism (a historical item that does not appear\nin WordNet [57], or any labelled vision datasets that we are aware of).\n\nImplementation details\n\n4.2\nHere, we describe the hyperparameters used to train ReCo+, inference details and evaluation metrics.\nOur implementation is based on the PyTorch library [66] and made publicly available.2\n\nReCo+ Training. While ReCo does not require training, we train ReCo+ based on the\nDeepLabv3+ [10] segmentation architecture with a ResNet101 [26] backone on the predictions\nfrom ReCo as described in 3.2. All training images are resized and center-cropped to 320\u00d7320 pixels\nand data augmentations such as random scaling, cropping, and horizontal flipping are applied with\nrandom color jittering and Gaussian blurring. We use the Adam optimiser [43] with an initial learning\nrate of 5 \u00d7 10\u22124 and a weight decay of 2 \u00d7 10\u22124 with the Poly learning rate schedule as in [50, 10].\nTraining consists of 20K gradient iterations with a batch size of 8 and takes about 5 hours on a single\n24GB NVIDIA P40 GPU.\n\nInference. For each benchmark, we pre-compute reference image embeddings for a list of categories\nfor the benchmark and store the embeddings to form a classifier. Whenever DenseCLIP is employed,\nwe use the ResNet50x16 model (following [96]) to construct a saliency map for each image. Unless\notherwise stated, for the COCO-Stuff and Cityscapes benchmarks, we resize and center crop the\ninput images to 320\u00d7320 pixels as in [24]. For the KITTI-STEP validation set, we use the original\nresolution of each image as in [41]. For the FireNet benchmark, we resize the shorter side of images\nto 512 pixels and predict a single class of fire extinguisher by thresholding the predicted heatmap\nwith probability of 0.5.\n\nEvaluation metrics. Following the common practice [34, 12, 24], we report pixel accuracy (Acc.)\nand mean intersection-over-union (mIoU).\n\n4.3 Ablation studies\n\nAbility of CLIP to curate archives. We begin by assessing the validity of our second hypothesis\u2014\nnamely that CLIP is capable of achieving high purity archives from unlabelled images. To this end,\nwe evaluate the retrieval performance of different CLIP models on the ImageNet1K validation set\nwhen constructing different archive sizes. In detail, for each archive size, k, we compute the precision\nof the top-k retrieved images based on whether the the ground-truth image-labels match the query\ntext. As can be seen in Fig. 4 (left), all CLIP models achieve solid retrieval performance, suggesting\ntheir potential for curating high purity archives as part of ReCo. Since ViT-L/14@336px performs\nbest, we employ this as our retrieval model in the remaining experiments.\n\nInfluence of archive size and visual encoder used for co-segmentation. In Fig. 4 (right) we illustrate\nthe effect of using different pre-trained architectures, e.g. MoCov2 [25], DINO [7], CLIP [71], DeiT-\nSIN [60], as the archive size (and thus the number of images used for co-segmentation) changes. The\ny-axis depicts segmentation performance for ReCo with these configurations on the PASCAL-Context\nbenchmark. We observe that using larger archives tends to improve performance (likely due to their\nreasonably high purity) albeit non-monotonically, and that features from DeiT-SIN perform best. We\n\n2Code available at https://github.com/NoelShin/reco\n\n7\n\n\fFigure 4: Ablation studies. Left: Image retrieval performance of different CLIP models on the\nImageNet1K validation set with k ranging from 5 to 50. ViT-L/14@336px performs particularly\nstrongly, suggesting the ability to curate archives of high purity. Right: Co-segmentation performance\non PASCAL-Context validation set as we vary the archive size and choice of visual encoders. We\nobserve a general trend towards improved performance with increasing archive size for all encoders.\n\nDenseCLIP LGC CE CRF Acc. mIoU\n\u2717\n\u2717\n\u2717\n\u2713\n\u2713\n\u2713\n\n16.8\n41.1\n43.1\n49.7\n50.9\n51.6\n\n5.7\n21.8\n23.1\n26.0\n26.6\n27.2\n\n\u2717\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\n\u2717\n\u2717\n\u2713\n\u2717\n\u2713\n\u2713\n\n\u2717\n\u2717\n\u2717\n\u2717\n\u2717\n\u2713\n\nTable 1: Influence of ReCo components for zero-shot transfer on PASCAL-Context [58]. We\nobserve that integrating DenseCLIP during inference, Language-guided co-segmentation (LGC),\nContext elimination (CE), and CRF [44] post-processing each contribute to improved performance.\nAll comparisons use a DeiT-SIN visual backbone for co-segmentation and ViT-L/14@336px for\narchive curation.\n\ntherefore use DeiT-SIN with an archive size of k=50 for the remaining experiments unless otherwise\nstated. See Fig. 3 for qualitative examples of our co-segmentation results.\n\nInfluence of ReCo framework components. We next assess the effect of employing DenseCLIP\nduring inference, the language-guided co-segmentation and context elimination components of ReCo\nwhich seek to improve the quality of co-segmentation achieved across each archive to boost down-\nstream segmentation performance. When applying context elimination, we select tree, sky, building,\nroad, and person as common background concepts appearing in natural images to be suppressed.\nIn Tab. 1, we show the effect of three of these strategies, together with the effect of applying a\nCRF [44] as post-processing. We observe that integrating DenseCLIP into the inference procedure\nbrings a significant gain in performance which we believe is driven by the notable robustness of CLIP\nfeatures under zero-shot transfer [71]. In addition, language-guided co-segmentation and context\nelimination further boost co-segmentation performance, while the CRF brings a small gain. We\ntherefore use each of these strategies (including CRF post-processing) in the remaining experiments.\n4.4 Comparison to state-of-the-art unsupervised methods\nWe compare ReCo and ReCo+ to state-of-the-art unsupervised semantic segmentation models\nwith and without vision-language pre-training on standard benchmarks, including COCO-Stuff [6],\nCityscapes [14] and KITTI-STEP [87] under both zero-shot transfer and unsupervised adaptation\n(training without labels on the target distribution). For COCO-Stuff, we observe that the mid-level\ncategories used for evaluation are somewhat abstract for retrieval (for instance, one mid-level category\nis \u201coutdoor objects\u201d, which may include many low-level categories beyond the target hierarchy). To\navoid introducing ambiguity to the co-segmentation procedure, we instead directly use the low-level\ncategories and then merge the predictions into the mid-level categories. Additionally, we rephrase\ntwo category names to reduce ambiguity (parking to parking lot and vegetation to tree) in Cityscapes\nand KITTI-STEP based on the descriptions found in [14]. A detailed discussion can be found in the\nsupplementary material.\n\nAs shown in Tab. 2, ReCo strongly outperforms prior models on all benchmarks for zero-shot transfer.\nUnder an unsupervised adaptation protocol, ReCo+ outperforms the state-of-the-art by a large margin\non the Cityscapes and KITTI-STEP. On COCO-Stuff, ReCo+ achieves slightly lower pixel accuracy\n\n8\n\n1020304050Top k0.20.30.40.50.60.70.8PrecisionRN50RN50x16RN50x64ViT-B/32ViT-B/16ViT-L/14ViT-L/14@336px1020304050archive size (k)0.200.220.240.260.28mIoURN50-CLIPRN50-MoCov2DeiT-S/16-SINViT-S/16-DINOViT-B/16-DINODenseCLIP\fModel\n\nAcc. mIoU\n\nModel\n\nAcc. mIoU\n\nModel\n\nAcc. mIoU\n\nZero-shot transfer\nDenseCLIP\u2021 [96] 32.3\nReCo\u2021 (Ours)\n46.6\n\nUnsupervised adaptation\nIIC [34]\nMDC [12]\nPiCIE [12]\nPiCIE + H [12]\nSTEGO [24]\nReCo+\u2021 (Ours)\n\n21.8\n32.2\n48.1\n50.0\n56.9\n54.5\n\n19.8\n27.2\n\n6.7\n9.8\n13.8\n14.4\n28.2\n33.0\n\nZero-shot transfer\nDenseCLIP\u22c6\u2021 [96] 35.9\nMDC\u22c6\u2020 [12]\nPiCIE\u22c6\u2020 [12]\nD&S\u22c6\u2020 [85]\nReCo\u22c6\u2021 (Ours)\n\n-\n-\n-\n65.4\n\nUnsupervised adaptation\n47.9\nIIC [34]\n40.7\nMDC [12]\n65.5\nPiCIE [12]\n73.2\nSTEGO [24]\nReCo+\u2021 (Ours)\n83.7\n\n10.0\n7.0\n9.7\n16.2\n22.0\n\n6.4\n7.1\n12.3\n21.0\n24.2\n\nZero-shot transfer\nDenseCLIP\u2021 [96] 34.1\nReCo\u2021 (Ours)\n70.6\n\nUnsupervised adaptation\nSegSort [33]\nHSG [41]\nReCo+\u2021 (Ours)\n\n69.8\n73.8\n75.3\n\n15.3\n29.8\n\n19.2\n21.7\n31.9\n\nTable 2: Comparison to state-of-the art approaches on COCO-Stuff (left), Cityscapes (middle),\nand KITTI-STEP (right) validation sets. \u22c6Evaluated at the original resolution. \u2020Models trained\non Waymo Open [81] (reported from [85]). \u2021Models that leverage a language-image pre-training\nmodel to assign a concept name to a prediction. The best score for each metric under each protocol is\nhighlighted in bold. We observe that ReCo and ReCo+ perform strongly relative to prior work under\nzero-shot transfer and unsupervised adaptation protocols, respectively.\n\nFigure 5: Co-segmentations of rare concepts. Left: fire extinguisher. Right: Antikythera mech-\nanism. We show selected samples from ReCo archives for each concept, together with their co-\nsegmentations. In each case, ReCo successfully identifies the regions associated with the concept.\n\ncompared to [24] but a considerably higher mIoU. In Fig. 1, we visualise the sample predictions of\nour models on COCO-Stuff.\n\n4.5 Segmenting rare concepts\nBy virtue of inheriting CLIP\u2019s diverse knowledge of nameable visual concepts, ReCo exhibits the\nability to segment rare categories. We first demonstrate this ability for fire extinguisher objects,\nwhich have important fire-safety implications but seldom appear in popular semantic segmentation\nbenchmarks. To assess performance, we evaluate segmentation quality on the FireNet dataset (as\ndescribed in Sec. 4.1) and achieve reasonable performance on pixel accuracy (93.3) and IoU (44.9)\nmetrics. In Fig. 5 (left) we visualise the co-segmentation produced by ReCo across sample images\ncontainer fire extinguishers. As an additional demonstration (Fig. 5, right), we also show co-\nsegmentations for images containing the views of the (unique) Antikythera mechanism. In both cases,\nwe observe that ReCo is capable of co-segmenting the concept of interest without labelled examples.\n\n\n\nThe following is the appendix section of the paper you are reviewing:\nwe first discuss the role of supervisory signals for ReCo and alternative\napproaches, and the datasets used in our work (Appendix A). Next we provide further details\nof experiments conducted in the main paper together with hyperparameters for training ReCo+\nin Appendix B. We then report additional ablations investigating the influence of common category\nselection for context elimination, reducing ambiguity in category name, architectural chioces for\nCLIP and DenseCLIP, and the number of seed pixels considered for computing a reference image\nembedding in Appendix C. Finally, we provide additional qualitative results in Sec. D to illustrate\nboth successful and failure cases for our method.\n\nA Discussion of supervision and data\n\nA.1 Supervisory signals for ReCo and prior work\n\nIn the main paper, we compare to previous methods that are typically described as unsupervised. In\npractice, however, many methods (including ours) either implicitly or explicitly engage humans in\nthe data curation process at some stage.\n\nSupervision used by ReCo. (1) Similarly to prior work, our experiments make use of datasets\nconstructed from photographs taken by humans for both training and evaluation. These photographs\nare naturally biased towards content that humans find interesting and are typically well-framed (with\na concept of interest featuring prominently) or taken from a vantage point that offers a convenient\nscene overview (e.g. a roof-mounted camera on a vehicle driving on public roads). By training\nand evaluating on such data, our experimental results likely provide an optimistic assessment of\nperformance when contrasted with other distributions (e.g. the video feed received by an autonomous\nmobile robot). (2) For our comparisons to prior work, we use the ImageNet training subset without\nlabels to curate archives. However, in practice, this dataset is not free from human involvement: it\nwas curated by human workers who were asked to verify that each image contains a particular synset\ncategory (see [5] for a discussion of the collection process). While annotators were encouraged to\nselect images regardless of occlusions, the number of objects and clutter in the scene, this process\nnevertheless produced a relatively clean dataset with fairly object-centric images. (3) Several of\nour experiments make use of DeiT-SIN [13], which is trained on stylised ImageNet [6] with labels.\nWe don\u2019t believe that this supervision is critical, since in Fig. 4 of the main paper, we showed that\nResNet50-MoCov2 [8] which does not use labels achieves similar performance (less than 1 mIoU\ndifference on PASCAL-Context [12]). Moreover, we note that previous unsupervised methods to\nwhich we compare (e.g. [10, 3]) initialise their approach from supervised ImageNet training with the\nconvention that unsupervised in this context denotes the fact that no pixel-level supervision is used.\n(4) By using CLIP [17], we also make use of a different kind of supervision, namely images paired\nwith alt-text scraped from the web. Empirically, this data source has been shown to be extremely\nscalable and to enable generalisation to very large numbers of concepts [17, 11, 16]. Nevertheless,\nthe creation of the original alt-text image descriptions (mostly) derives from a human source, and\ntherefore provides a form of human supervision. In contrast to using ImageNet classification labels,\nthis source of supervision is indispensable to ReCo.\n\nWe believe that the key factors to be considered when discussing the question of supervision are\nscalability and generalisation. We are typically not interested in unsupervised methods for their own\nsake, but rather because they offer the ability to cheaply scale up machine learning to larger training\ndata sets that improve performance, and to build methods that go beyond the functionality afforded by\nlabelled datasets (e.g. new classes, new tasks etc.). Subject to the caveats (e.g. human photographer\nbias) outlined above, we believe ReCo has the flexibility to scale up far beyond the experimental\ncomparisons conducted in this work without requiring any changes to the underlying framework.\n\nA.2 Discussion of consent in used datasets\n\nIn this work, we work primarily with widely used Computer Vision benchmarks: ImageNet [5],\nPASCAL-Context [12], Cityscapes [4], COCO-Stuff [1], KITTI-STEP [21]. For these datasets, we\ndo not conduct an independent investigation of consent beyond the considerations of the authors that\nreleased these datasets. For our final exploratory studies which make use of LAION-5B [18], we\nmanually verified that no humans were present in the archives that were curated by ReCo.\n\n2\n\n\fA.3 Discussion on whether data contains personally identifiable information or offensive\n\ncontent\n\nWe do not release any data as part of this work. By working with widely used Computer Vision\nbenchmarks, we also restrict ourselves to imagery that is available in the public domain. We therefore\nbelieve that the risk that our work builds on harmful content or contributes to the leakage of personal\ninformation is low.\n\nOne exception to this is our use of the LAION-5B dataset for qualitative studies. We manually\nverified that no personally identifiable information or harmful content (as judged by the authors) was\npresent in the archives curated by ReCo.\n\nA.4 Dataset licenses\n\nHere we describe the terms/licenses of datasets used in our paper. For images in PASCAL-Context\nand COCO dataset, we comply with the Flickr Terms of Use and the Creative Commons Attribution\n4.0 License for the COCO-Stuff annotations. For Cityscapes and ImageNet1K, we follow the terms\nstated on their official website1 and the Attribution-NonCommercial-ShareAlike 3.0 Unported (CC\nBY-NC-SA 3.0) licence for KITTI-STEP.\n\nB Experiment details\n\nHere we provide pseudocode for ReCo and details of experiments conducted in the main paper.\n\nB.1 Pseudocode for ReCo\n\nIn Alg. 1, we describe the pseudocode for the core of ReCo (to maintain readability, language-guided\nco-segmentation and context elimination are omitted since these follow a similar structure).\n\nB.2 Prompt engineering\n\nTo obtain the text embedding for a concept, we ensemble the textual features from 85 templates,\ne.g., \u201ca photo of the {concept}\u201d and \u201cthere is a {concept} in the scene\u201d\nfollowing [22].\n\nB.3 Details of ablation study to assess CLIP retrieval performance\n\nWe observe that two of the ImageNet1K class labels are not unique\u2014they occur twice with different\nmeanings (e.g., \u201ccrane\u201d is used to represent both bird and machine), which makes retrieval inference\nand evaluation ambiguous. Therefore, we exclude those classes and use the remaining 996 categories\nfor the experimental results reported in Fig. 4 (left) of the main paper.\n\nB.4 Hyperparameters for ReCo+ training\n\nAs described in Sec. 4.2 (in the main paper), we adopt DeepLabv3+2 [2] with ResNet101 encoder [9]\nfor ReCo+ and train the network with standard data augmentations such as random scaling and\nhorizontal flip following [14, 19]. In detail, for geometric transformations, we use random scaling\nwith a range of [0.5, 2.0], random crop with a crop size 320\u00d7320 pixels, and random horizontal flip\nwith a probability of 0.5. For the photometric augmentations, we apply colour jittering3 with 0.8, 0.8,\n0.8, 0.2, and 0.8 for brightness, contrast, saturation, hue and probability parameters respectively. We\nalso employ Gaussian blurring with a kernel size of 10% of min(H, W ) where min(H, W ) returns\nthe length of the shorter side of an image.\n\n1https://www.cityscapes-dataset.com/license and https://www.image-net.org/download.\n\nphp for Cityscapes and ImageNet1K respectively.\n\n2We use the code for DeepLabv3+ from https://github.com/VainF/DeepLabV3Plus-Pytorch.\n3We use ColorJitter function in torchvision package [15].\n\n3\n\n\fAlgorithm 1 Pseudocode for the core of ReCo (using PyTorch-like syntax)\nInput. a CLIP image encoder \u03c8I, a CLIP text encoder \u03c8T , an image encoder \u03d5I, an image collection\nU, a concept c, the number of co-segmented images k\nOutput. a reference image embedding ref_emb and a prediction of the concept c in a new image\n\n# retrieve images\nimage_emb = l2_normalize(\u03c8I(U), dim=1) # NxC\ntext_emb = l2_normalize(\u03c8T (c), dim=0) # C\nscores = mm(image_emb, text_emb) # N\nindices = argmax(scores)[:k] # k\nimages = U[indices] # kx3xHxW\n\n# co-segment\nF = l2_normalize(\u03d5I(images), dim=1) # kxCxhxw\nF_flat = F.permute(1,0,2,3).view(C,k*h*w) # Cxkhw\nA = mm(F_flat.T, F_flat) # adjacency matrix, khwxkhw\n\ngrid = zeros((k*h*w, k))\nstart_col = 0 # start column index\nfor i in range(k):\n\nend_col = start_col + h*w # end column index\ngrid[:,i] = max(A[:,start_col:start_col+end_col], dim=1)\nstart_col = end_col\n\navg_grid = mean(grid, dim=1) # khw\n\nseed_features = []\nstart_row = 0 # start row index\nfor i in range(k):\n\nend_row = start_row + h*w # end row index\nindex_1d = argmax(avg_grid[start_row:end_row])\nstart_row = end_row\nindex_2d = [index_1d//w,index_1d%w]\nseed_features.append(F[i,:,index_2d[0],index_2d[1]])\n\nseed_features = stack(seed_features, dim=0) # k\u00d7C\nref_emb = l2_normalize(seed_features.mean(dim=0), dim=0) # C\n\n# inference\nF_new = l2_normalize(\u03d5I(new_image), dim=0) # Cxhxw\nprediction = sigmoid(mm(ref_emb, F_new)) # hxw\n\nmm:matrix multiplication.\n\ng\nn\ni\nd\nl\ni\nu\nb\n\nn\no\ns\nr\ne\np\n\nd\na\no\nr\n\ne\ne\nr\nt\n\ny\nk\ns\n\nmIoU\n\n\u2717\n\u2717\n\u2717\n\u2717\n\u2717\n5.7\n\u2717\n\u2717\n\u2713 \u2717\n\u2717\n10.9\n\u2717\n\u2717\n\u2713 \u2713 \u2717\n12.0\n\u2713 \u2713 \u2713 \u2717\n\u2717\n10.8\n\u2713 \u2713 \u2713 \u2713 \u2717\n11.4\n\u2713 \u2713 \u2713 \u2713 \u2713 12.3\n\nparking\n\nvegetation\n\n\u2192parking lot \u2192tree\n\nmIoU\n\n\u2717\n\u2717\n\u2713\n\u2713\n\n\u2717\n\u2713\n\u2717\n\u2713\n\n15.4\n16.0\n18.6\n19.3\n\nTable 1: Effect of context category choices and reducing label ambiguity. Left: We find that\nsuppressing 5 frequently appearing categories brings performance gain on PASCAL-Context [12].\nRight: We observe that specifying the meaning of a class more concretely helps segmentation\nperformance of ReCo on Cityscapes [4]. For both cases, input images are resized and center-cropped\nto 320\u00d7320 following [7].\n\n4\n\n\fModel\n\nCLIP arch.\n\nDenseCLIP arch. mIoU\n\nReCo (ours)\n\nResNet50\nViT-B/32\nViT-L/14@336px ResNet50x16\n\nResNet50\nResNet50\n\n20.3\n20.3\n22.0\n\n# seed pixels mIoU Acc.\n\n1\n5\n10\n50\n100\n\n22.0\n22.2\n22.1\n21.7\n20.4\n\n65.4\n64.9\n62.9\n57.5\n51.0\n\nTable 2: Effect of architecture choices (left) and the number of seed pixels (right) on performance\nof ReCo on the Cityscapes validation set.\n\nC Additional ablation studies\n\nC.1 Choices of context categories\n\nAs described in Sec. 3.3 (main paper), we propose to suppress the commonly appearing categories,\ne.g., sky, which co-occur with other classes, e.g., aeroplanes. To achieve this, we manually pick 5\nfrequently appearing classes in PASCAL-Context dataset [12] and investigate the effect of different\ncombinations of such categories. In Tab. 1 (left) we observe that suppressing the tree and sky\ncategories yields a notable performance gain, while eliminating all five categories performs best. For\nthis reason, we apply the context elimination strategy with these five categories in the main paper.\n\nC.2 Category name rephrasing to reduce ambiguity\n\nWe observe that it is important to specify a concept concretely to obtain retrieved images exhibiting\nsimilar visual appearance. For instance, in Cityscapes dataset, parking and vegetation can be rephrased\nto less ambiguous concepts parking lot and tree respectively based on their descriptions4 in the paper\naccompanying the dataset [4]. As can be seen in Tab. 1 (right), ReCo gains benefits in performance\non Cityscapes by replacing the category names with less abstract concepts. This sensitivity is a\nconsequence of our co-segmentation algorithm, which locates pixels that share similar visual features\nacross multiple images. Thus we use the rephrased label names throughout the experiments in the\npaper.\n\nIn addition to the limitations listed in the main paper, this dependence on concrete/specific concept\nnames can be considered a limitation of our approach (albeit one that is readily mitigated). However,\nwe believe it is a reasonable requirement for methods that operate in the zero-shot transfer setting.\nUnlike fine-tuning methods that learn to associate abstract text descriptions to visual concepts by\nseeing examples from the target distribution, ReCo relies entirely on an adequate text description to\ndisambiguate the concept. Since many computer vision datasets have been constructed with training\nand testing splits with the assumption that methods would make use of the training set, we believe\nit is probable that category names were not designed to be uniquely descriptive (hence the use of\n\u201cparking\u201d as a category in Cityscapes, which could be either a verb or a noun). Indeed, there may\nhave been little perceived need to construct unambiguous category names when examples from the\ntraining set implicitly provide disambiguation of the concept.\n\nC.3 Effect of architectural choices for CLIP and DenseCLIP\n\nFor the experiments in the main paper, we use ViT-L/14@336px and ResNet50x16 for image retrieval\nand DenseCLIP inference respectively. As these models are relatively heavier than commonly\nused architectures such as ResNet50 or ViT-B/32, we evaluate ReCo with lighter encoders on the\nCityscapes validation split in Tab. 2 (left). Specifically we use either ResNet50 or ViT-B/32 for image\nretrieval and ResNet50 for DenseCLIP inference. As can be seen, adopting a lighter model slightly\ndecreases the performance, but still outperforms previous state-of-the-art methods (e.g., 16.3 mIoU\nfor D&S [20]).\n\n4\u201cHorizontal surfaces that are intended for parking and separated from the road, either via elevation or via a\ndifferent texture/material, but not separated merely by markings.\u201d for parking and \u201cTrees, hedges, and all kinds\nof vertically growing vegetation.\u201d for vegetation.\n\n5\n\n\fFigure 1: Additional visualisations of our co-segmentation method used for ReCo. For visualisa-\ntion purpose, we show the top 5 images from 50 retrieved images in each archive for a category. Left:\nSuccessful cases. Right: Typical failure cases. Highlighted regions are shown in red. Best viewed in\ncolour.\n\nC.4 Effect of the number of seed pixels\n\nWhile we pick one seed pixel per image by default as decribed in Sec. 3, we investigate how the\nnumber of seed pixels selected for an image affects performance of ReCo. For this, we evaluate ReCo\non Cityscapes with each reference image embedding computed by averaging 1, 5, 10, 50, and 100\nseed pixel(s) for an image. As can be seen in Tab. 2 (right), using one or five seed pixel(s) per image\nshows the best performance in terms of pixel accuracy and mIoU and picking more pixels tends to\nhurt the performance.\n\nD Additional visualisations\n\nD.1 Co-segmentation with seed pixels\n\nIn Fig. 1, we visualise examples of the co-segmentation with seed pixels on ImageNet2012, which is\nused for an index dataset for ReCo in the main paper. On the left, we show successful cases where the\nco-segmentation highlights regions corresponding to a given concept (i.e., sheep, building, and zebra).\nOn the right, we display examples of two typical failure cases: partial segmentation (i.e., bicycle\nand cup) and highlighting an object commonly co-occurring with a given concept (i.e., baseball\nglove). For bicycles, their frames are less highlighted compared to the wheels. Similarly, the body\nparts of the cups are less likely to be emphasised than the handles. In case of baseball gloves, the\nco-segmentation locates a part of a baseball, which often appears with a baseball glove. We believe\nthese failure cases are caused by the property of our co-segmentation algorithm, which focuses on\nregions with less variance in visual features (e.g., texture and shape) appearing in multiple images of\nan archive.\n\nD.2 Predictions of ReCo and ReCo+\n\nIn Fig. 2, we show more visualisation samples on the COCO-Stuff benchmark. Successful and failure\ncases are shown on the left and right, respectively. We note that ReCo tends to fail in predicting small\nobjects, e.g., people in the bus, and so does ReCo+ which is trained on the ReCo\u2019s predictions as\npseudo-labels. We conjecture that this is related to the stride of the image encoder used for ReCo,\n\n6\n\n\fFigure 2: Additional visualisations on COCO-Stuff. Left: Successful cases. Right: Typical failure\ncases. White pixels denote ignored regions.\n\nwhich is 16\u00d716 for the case of DeiT-S/16-SIN [13]. It could therefore potentially be improved by\nusing an encoder with a smaller stride at the cost of increased computational burden.\n\n\n\nBased on the section(s), please answer the following question with yes, no, or n/a and provide a brief justification for your answer.\nQuestion: If the authors curate/release new assets (e.g., code, data, models), do the authors discuss whether and how consent was obtained from people whose data they are using/curating?"}, "4e": {"role": "user", "content": "The following is the experiments section of the paper you are reviewing:\n\nIn this section, we start by describing the datasets used for our experiments (Sec. 4.1) and implemen-\ntation details (Sec. 4.2). Then, we conduct an ablation study (Sec. 4.3) and compare our model to\nstate-of-the-art methods for unsupervised semantic segmentation with and without language-image\npre-training (Sec. 4.4). Finally, we showcase our model\u2019s ability to segment rare-category objects\n(Sec. 4.5).\n\n4.1 Datasets\nFor our ablation study, we use the ImageNet1K [16] validation set to curate archive for concepts\nof interest. The dataset covers 1K classes with 50 images for each class. To measure segmentation\nperformance in the zero-shot transfer setting, we use the PASCAL-Context [58] validation set for\nevaluation, which has 5,104 images of 59 categories excluding the background class.\n\nTo compare with previous unsupervised segmentation methods, we use the ImageNet1K training set\nto construct the reference embedding for the concept we wish to segment. The dataset has 1K classes\nwith 1.2M images. We evaluate on standard benchmarks including the Cityscapes [14] validation\n\n6\n\n\fsplit, which has 500 urban scene images of 27 categories, KITTI-STEP [87] validation set, which\nis composed of 2,981 urban scene images of 19 categories, and COCO-Stuff [6] validation split,\nwhich has 4,172 images of 171 low-level thing and stuff categories excluding background class.\nFollowing [12, 24], we use the 27 mid-level categories for evaluation. For unsupervised adaptation\nwith ReCo+ (Sec. 3.2), we train on ReCo pseudo-labels on the Cityscapes training set with 2,975\nimages, KITTI-STEP training set which contains 5,027 images, and the COCO-Stuff10K subset\nwhich has 9,000 images for each respective benchmark. We emphasise that no ground-truth labels\nare used for training.\n\nFinally, to demonstrate our model\u2019s ability to segment rare concepts, we use the LAION-5B\ndataset [75] with 5 billion images as a large collection of images that we expect to satisfy our\nfirst hypothesis, namely that it will have coverage of rare concepts. To assess performance, we use\nthe FireNet dataset [62] which has 1,452 images spanning rare fire safety-related classes. For our\nexperiment, we select the fire extinguisher class as an example of a concept that is important but\nrare in vision datasets (it is not contained in ImageNet1K [74], for example) and evaluate ReCo on\n263 images containing at least one instance of the category. As a further proof of concept, we also\ndemonstrate co-segmentations of the Antikythera mechanism (a historical item that does not appear\nin WordNet [57], or any labelled vision datasets that we are aware of).\n\nImplementation details\n\n4.2\nHere, we describe the hyperparameters used to train ReCo+, inference details and evaluation metrics.\nOur implementation is based on the PyTorch library [66] and made publicly available.2\n\nReCo+ Training. While ReCo does not require training, we train ReCo+ based on the\nDeepLabv3+ [10] segmentation architecture with a ResNet101 [26] backone on the predictions\nfrom ReCo as described in 3.2. All training images are resized and center-cropped to 320\u00d7320 pixels\nand data augmentations such as random scaling, cropping, and horizontal flipping are applied with\nrandom color jittering and Gaussian blurring. We use the Adam optimiser [43] with an initial learning\nrate of 5 \u00d7 10\u22124 and a weight decay of 2 \u00d7 10\u22124 with the Poly learning rate schedule as in [50, 10].\nTraining consists of 20K gradient iterations with a batch size of 8 and takes about 5 hours on a single\n24GB NVIDIA P40 GPU.\n\nInference. For each benchmark, we pre-compute reference image embeddings for a list of categories\nfor the benchmark and store the embeddings to form a classifier. Whenever DenseCLIP is employed,\nwe use the ResNet50x16 model (following [96]) to construct a saliency map for each image. Unless\notherwise stated, for the COCO-Stuff and Cityscapes benchmarks, we resize and center crop the\ninput images to 320\u00d7320 pixels as in [24]. For the KITTI-STEP validation set, we use the original\nresolution of each image as in [41]. For the FireNet benchmark, we resize the shorter side of images\nto 512 pixels and predict a single class of fire extinguisher by thresholding the predicted heatmap\nwith probability of 0.5.\n\nEvaluation metrics. Following the common practice [34, 12, 24], we report pixel accuracy (Acc.)\nand mean intersection-over-union (mIoU).\n\n4.3 Ablation studies\n\nAbility of CLIP to curate archives. We begin by assessing the validity of our second hypothesis\u2014\nnamely that CLIP is capable of achieving high purity archives from unlabelled images. To this end,\nwe evaluate the retrieval performance of different CLIP models on the ImageNet1K validation set\nwhen constructing different archive sizes. In detail, for each archive size, k, we compute the precision\nof the top-k retrieved images based on whether the the ground-truth image-labels match the query\ntext. As can be seen in Fig. 4 (left), all CLIP models achieve solid retrieval performance, suggesting\ntheir potential for curating high purity archives as part of ReCo. Since ViT-L/14@336px performs\nbest, we employ this as our retrieval model in the remaining experiments.\n\nInfluence of archive size and visual encoder used for co-segmentation. In Fig. 4 (right) we illustrate\nthe effect of using different pre-trained architectures, e.g. MoCov2 [25], DINO [7], CLIP [71], DeiT-\nSIN [60], as the archive size (and thus the number of images used for co-segmentation) changes. The\ny-axis depicts segmentation performance for ReCo with these configurations on the PASCAL-Context\nbenchmark. We observe that using larger archives tends to improve performance (likely due to their\nreasonably high purity) albeit non-monotonically, and that features from DeiT-SIN perform best. We\n\n2Code available at https://github.com/NoelShin/reco\n\n7\n\n\fFigure 4: Ablation studies. Left: Image retrieval performance of different CLIP models on the\nImageNet1K validation set with k ranging from 5 to 50. ViT-L/14@336px performs particularly\nstrongly, suggesting the ability to curate archives of high purity. Right: Co-segmentation performance\non PASCAL-Context validation set as we vary the archive size and choice of visual encoders. We\nobserve a general trend towards improved performance with increasing archive size for all encoders.\n\nDenseCLIP LGC CE CRF Acc. mIoU\n\u2717\n\u2717\n\u2717\n\u2713\n\u2713\n\u2713\n\n16.8\n41.1\n43.1\n49.7\n50.9\n51.6\n\n5.7\n21.8\n23.1\n26.0\n26.6\n27.2\n\n\u2717\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\n\u2717\n\u2717\n\u2713\n\u2717\n\u2713\n\u2713\n\n\u2717\n\u2717\n\u2717\n\u2717\n\u2717\n\u2713\n\nTable 1: Influence of ReCo components for zero-shot transfer on PASCAL-Context [58]. We\nobserve that integrating DenseCLIP during inference, Language-guided co-segmentation (LGC),\nContext elimination (CE), and CRF [44] post-processing each contribute to improved performance.\nAll comparisons use a DeiT-SIN visual backbone for co-segmentation and ViT-L/14@336px for\narchive curation.\n\ntherefore use DeiT-SIN with an archive size of k=50 for the remaining experiments unless otherwise\nstated. See Fig. 3 for qualitative examples of our co-segmentation results.\n\nInfluence of ReCo framework components. We next assess the effect of employing DenseCLIP\nduring inference, the language-guided co-segmentation and context elimination components of ReCo\nwhich seek to improve the quality of co-segmentation achieved across each archive to boost down-\nstream segmentation performance. When applying context elimination, we select tree, sky, building,\nroad, and person as common background concepts appearing in natural images to be suppressed.\nIn Tab. 1, we show the effect of three of these strategies, together with the effect of applying a\nCRF [44] as post-processing. We observe that integrating DenseCLIP into the inference procedure\nbrings a significant gain in performance which we believe is driven by the notable robustness of CLIP\nfeatures under zero-shot transfer [71]. In addition, language-guided co-segmentation and context\nelimination further boost co-segmentation performance, while the CRF brings a small gain. We\ntherefore use each of these strategies (including CRF post-processing) in the remaining experiments.\n4.4 Comparison to state-of-the-art unsupervised methods\nWe compare ReCo and ReCo+ to state-of-the-art unsupervised semantic segmentation models\nwith and without vision-language pre-training on standard benchmarks, including COCO-Stuff [6],\nCityscapes [14] and KITTI-STEP [87] under both zero-shot transfer and unsupervised adaptation\n(training without labels on the target distribution). For COCO-Stuff, we observe that the mid-level\ncategories used for evaluation are somewhat abstract for retrieval (for instance, one mid-level category\nis \u201coutdoor objects\u201d, which may include many low-level categories beyond the target hierarchy). To\navoid introducing ambiguity to the co-segmentation procedure, we instead directly use the low-level\ncategories and then merge the predictions into the mid-level categories. Additionally, we rephrase\ntwo category names to reduce ambiguity (parking to parking lot and vegetation to tree) in Cityscapes\nand KITTI-STEP based on the descriptions found in [14]. A detailed discussion can be found in the\nsupplementary material.\n\nAs shown in Tab. 2, ReCo strongly outperforms prior models on all benchmarks for zero-shot transfer.\nUnder an unsupervised adaptation protocol, ReCo+ outperforms the state-of-the-art by a large margin\non the Cityscapes and KITTI-STEP. On COCO-Stuff, ReCo+ achieves slightly lower pixel accuracy\n\n8\n\n1020304050Top k0.20.30.40.50.60.70.8PrecisionRN50RN50x16RN50x64ViT-B/32ViT-B/16ViT-L/14ViT-L/14@336px1020304050archive size (k)0.200.220.240.260.28mIoURN50-CLIPRN50-MoCov2DeiT-S/16-SINViT-S/16-DINOViT-B/16-DINODenseCLIP\fModel\n\nAcc. mIoU\n\nModel\n\nAcc. mIoU\n\nModel\n\nAcc. mIoU\n\nZero-shot transfer\nDenseCLIP\u2021 [96] 32.3\nReCo\u2021 (Ours)\n46.6\n\nUnsupervised adaptation\nIIC [34]\nMDC [12]\nPiCIE [12]\nPiCIE + H [12]\nSTEGO [24]\nReCo+\u2021 (Ours)\n\n21.8\n32.2\n48.1\n50.0\n56.9\n54.5\n\n19.8\n27.2\n\n6.7\n9.8\n13.8\n14.4\n28.2\n33.0\n\nZero-shot transfer\nDenseCLIP\u22c6\u2021 [96] 35.9\nMDC\u22c6\u2020 [12]\nPiCIE\u22c6\u2020 [12]\nD&S\u22c6\u2020 [85]\nReCo\u22c6\u2021 (Ours)\n\n-\n-\n-\n65.4\n\nUnsupervised adaptation\n47.9\nIIC [34]\n40.7\nMDC [12]\n65.5\nPiCIE [12]\n73.2\nSTEGO [24]\nReCo+\u2021 (Ours)\n83.7\n\n10.0\n7.0\n9.7\n16.2\n22.0\n\n6.4\n7.1\n12.3\n21.0\n24.2\n\nZero-shot transfer\nDenseCLIP\u2021 [96] 34.1\nReCo\u2021 (Ours)\n70.6\n\nUnsupervised adaptation\nSegSort [33]\nHSG [41]\nReCo+\u2021 (Ours)\n\n69.8\n73.8\n75.3\n\n15.3\n29.8\n\n19.2\n21.7\n31.9\n\nTable 2: Comparison to state-of-the art approaches on COCO-Stuff (left), Cityscapes (middle),\nand KITTI-STEP (right) validation sets. \u22c6Evaluated at the original resolution. \u2020Models trained\non Waymo Open [81] (reported from [85]). \u2021Models that leverage a language-image pre-training\nmodel to assign a concept name to a prediction. The best score for each metric under each protocol is\nhighlighted in bold. We observe that ReCo and ReCo+ perform strongly relative to prior work under\nzero-shot transfer and unsupervised adaptation protocols, respectively.\n\nFigure 5: Co-segmentations of rare concepts. Left: fire extinguisher. Right: Antikythera mech-\nanism. We show selected samples from ReCo archives for each concept, together with their co-\nsegmentations. In each case, ReCo successfully identifies the regions associated with the concept.\n\ncompared to [24] but a considerably higher mIoU. In Fig. 1, we visualise the sample predictions of\nour models on COCO-Stuff.\n\n4.5 Segmenting rare concepts\nBy virtue of inheriting CLIP\u2019s diverse knowledge of nameable visual concepts, ReCo exhibits the\nability to segment rare categories. We first demonstrate this ability for fire extinguisher objects,\nwhich have important fire-safety implications but seldom appear in popular semantic segmentation\nbenchmarks. To assess performance, we evaluate segmentation quality on the FireNet dataset (as\ndescribed in Sec. 4.1) and achieve reasonable performance on pixel accuracy (93.3) and IoU (44.9)\nmetrics. In Fig. 5 (left) we visualise the co-segmentation produced by ReCo across sample images\ncontainer fire extinguishers. As an additional demonstration (Fig. 5, right), we also show co-\nsegmentations for images containing the views of the (unique) Antikythera mechanism. In both cases,\nwe observe that ReCo is capable of co-segmenting the concept of interest without labelled examples.\n\n\n\nThe following is the appendix section of the paper you are reviewing:\nwe first discuss the role of supervisory signals for ReCo and alternative\napproaches, and the datasets used in our work (Appendix A). Next we provide further details\nof experiments conducted in the main paper together with hyperparameters for training ReCo+\nin Appendix B. We then report additional ablations investigating the influence of common category\nselection for context elimination, reducing ambiguity in category name, architectural chioces for\nCLIP and DenseCLIP, and the number of seed pixels considered for computing a reference image\nembedding in Appendix C. Finally, we provide additional qualitative results in Sec. D to illustrate\nboth successful and failure cases for our method.\n\nA Discussion of supervision and data\n\nA.1 Supervisory signals for ReCo and prior work\n\nIn the main paper, we compare to previous methods that are typically described as unsupervised. In\npractice, however, many methods (including ours) either implicitly or explicitly engage humans in\nthe data curation process at some stage.\n\nSupervision used by ReCo. (1) Similarly to prior work, our experiments make use of datasets\nconstructed from photographs taken by humans for both training and evaluation. These photographs\nare naturally biased towards content that humans find interesting and are typically well-framed (with\na concept of interest featuring prominently) or taken from a vantage point that offers a convenient\nscene overview (e.g. a roof-mounted camera on a vehicle driving on public roads). By training\nand evaluating on such data, our experimental results likely provide an optimistic assessment of\nperformance when contrasted with other distributions (e.g. the video feed received by an autonomous\nmobile robot). (2) For our comparisons to prior work, we use the ImageNet training subset without\nlabels to curate archives. However, in practice, this dataset is not free from human involvement: it\nwas curated by human workers who were asked to verify that each image contains a particular synset\ncategory (see [5] for a discussion of the collection process). While annotators were encouraged to\nselect images regardless of occlusions, the number of objects and clutter in the scene, this process\nnevertheless produced a relatively clean dataset with fairly object-centric images. (3) Several of\nour experiments make use of DeiT-SIN [13], which is trained on stylised ImageNet [6] with labels.\nWe don\u2019t believe that this supervision is critical, since in Fig. 4 of the main paper, we showed that\nResNet50-MoCov2 [8] which does not use labels achieves similar performance (less than 1 mIoU\ndifference on PASCAL-Context [12]). Moreover, we note that previous unsupervised methods to\nwhich we compare (e.g. [10, 3]) initialise their approach from supervised ImageNet training with the\nconvention that unsupervised in this context denotes the fact that no pixel-level supervision is used.\n(4) By using CLIP [17], we also make use of a different kind of supervision, namely images paired\nwith alt-text scraped from the web. Empirically, this data source has been shown to be extremely\nscalable and to enable generalisation to very large numbers of concepts [17, 11, 16]. Nevertheless,\nthe creation of the original alt-text image descriptions (mostly) derives from a human source, and\ntherefore provides a form of human supervision. In contrast to using ImageNet classification labels,\nthis source of supervision is indispensable to ReCo.\n\nWe believe that the key factors to be considered when discussing the question of supervision are\nscalability and generalisation. We are typically not interested in unsupervised methods for their own\nsake, but rather because they offer the ability to cheaply scale up machine learning to larger training\ndata sets that improve performance, and to build methods that go beyond the functionality afforded by\nlabelled datasets (e.g. new classes, new tasks etc.). Subject to the caveats (e.g. human photographer\nbias) outlined above, we believe ReCo has the flexibility to scale up far beyond the experimental\ncomparisons conducted in this work without requiring any changes to the underlying framework.\n\nA.2 Discussion of consent in used datasets\n\nIn this work, we work primarily with widely used Computer Vision benchmarks: ImageNet [5],\nPASCAL-Context [12], Cityscapes [4], COCO-Stuff [1], KITTI-STEP [21]. For these datasets, we\ndo not conduct an independent investigation of consent beyond the considerations of the authors that\nreleased these datasets. For our final exploratory studies which make use of LAION-5B [18], we\nmanually verified that no humans were present in the archives that were curated by ReCo.\n\n2\n\n\fA.3 Discussion on whether data contains personally identifiable information or offensive\n\ncontent\n\nWe do not release any data as part of this work. By working with widely used Computer Vision\nbenchmarks, we also restrict ourselves to imagery that is available in the public domain. We therefore\nbelieve that the risk that our work builds on harmful content or contributes to the leakage of personal\ninformation is low.\n\nOne exception to this is our use of the LAION-5B dataset for qualitative studies. We manually\nverified that no personally identifiable information or harmful content (as judged by the authors) was\npresent in the archives curated by ReCo.\n\nA.4 Dataset licenses\n\nHere we describe the terms/licenses of datasets used in our paper. For images in PASCAL-Context\nand COCO dataset, we comply with the Flickr Terms of Use and the Creative Commons Attribution\n4.0 License for the COCO-Stuff annotations. For Cityscapes and ImageNet1K, we follow the terms\nstated on their official website1 and the Attribution-NonCommercial-ShareAlike 3.0 Unported (CC\nBY-NC-SA 3.0) licence for KITTI-STEP.\n\nB Experiment details\n\nHere we provide pseudocode for ReCo and details of experiments conducted in the main paper.\n\nB.1 Pseudocode for ReCo\n\nIn Alg. 1, we describe the pseudocode for the core of ReCo (to maintain readability, language-guided\nco-segmentation and context elimination are omitted since these follow a similar structure).\n\nB.2 Prompt engineering\n\nTo obtain the text embedding for a concept, we ensemble the textual features from 85 templates,\ne.g., \u201ca photo of the {concept}\u201d and \u201cthere is a {concept} in the scene\u201d\nfollowing [22].\n\nB.3 Details of ablation study to assess CLIP retrieval performance\n\nWe observe that two of the ImageNet1K class labels are not unique\u2014they occur twice with different\nmeanings (e.g., \u201ccrane\u201d is used to represent both bird and machine), which makes retrieval inference\nand evaluation ambiguous. Therefore, we exclude those classes and use the remaining 996 categories\nfor the experimental results reported in Fig. 4 (left) of the main paper.\n\nB.4 Hyperparameters for ReCo+ training\n\nAs described in Sec. 4.2 (in the main paper), we adopt DeepLabv3+2 [2] with ResNet101 encoder [9]\nfor ReCo+ and train the network with standard data augmentations such as random scaling and\nhorizontal flip following [14, 19]. In detail, for geometric transformations, we use random scaling\nwith a range of [0.5, 2.0], random crop with a crop size 320\u00d7320 pixels, and random horizontal flip\nwith a probability of 0.5. For the photometric augmentations, we apply colour jittering3 with 0.8, 0.8,\n0.8, 0.2, and 0.8 for brightness, contrast, saturation, hue and probability parameters respectively. We\nalso employ Gaussian blurring with a kernel size of 10% of min(H, W ) where min(H, W ) returns\nthe length of the shorter side of an image.\n\n1https://www.cityscapes-dataset.com/license and https://www.image-net.org/download.\n\nphp for Cityscapes and ImageNet1K respectively.\n\n2We use the code for DeepLabv3+ from https://github.com/VainF/DeepLabV3Plus-Pytorch.\n3We use ColorJitter function in torchvision package [15].\n\n3\n\n\fAlgorithm 1 Pseudocode for the core of ReCo (using PyTorch-like syntax)\nInput. a CLIP image encoder \u03c8I, a CLIP text encoder \u03c8T , an image encoder \u03d5I, an image collection\nU, a concept c, the number of co-segmented images k\nOutput. a reference image embedding ref_emb and a prediction of the concept c in a new image\n\n# retrieve images\nimage_emb = l2_normalize(\u03c8I(U), dim=1) # NxC\ntext_emb = l2_normalize(\u03c8T (c), dim=0) # C\nscores = mm(image_emb, text_emb) # N\nindices = argmax(scores)[:k] # k\nimages = U[indices] # kx3xHxW\n\n# co-segment\nF = l2_normalize(\u03d5I(images), dim=1) # kxCxhxw\nF_flat = F.permute(1,0,2,3).view(C,k*h*w) # Cxkhw\nA = mm(F_flat.T, F_flat) # adjacency matrix, khwxkhw\n\ngrid = zeros((k*h*w, k))\nstart_col = 0 # start column index\nfor i in range(k):\n\nend_col = start_col + h*w # end column index\ngrid[:,i] = max(A[:,start_col:start_col+end_col], dim=1)\nstart_col = end_col\n\navg_grid = mean(grid, dim=1) # khw\n\nseed_features = []\nstart_row = 0 # start row index\nfor i in range(k):\n\nend_row = start_row + h*w # end row index\nindex_1d = argmax(avg_grid[start_row:end_row])\nstart_row = end_row\nindex_2d = [index_1d//w,index_1d%w]\nseed_features.append(F[i,:,index_2d[0],index_2d[1]])\n\nseed_features = stack(seed_features, dim=0) # k\u00d7C\nref_emb = l2_normalize(seed_features.mean(dim=0), dim=0) # C\n\n# inference\nF_new = l2_normalize(\u03d5I(new_image), dim=0) # Cxhxw\nprediction = sigmoid(mm(ref_emb, F_new)) # hxw\n\nmm:matrix multiplication.\n\ng\nn\ni\nd\nl\ni\nu\nb\n\nn\no\ns\nr\ne\np\n\nd\na\no\nr\n\ne\ne\nr\nt\n\ny\nk\ns\n\nmIoU\n\n\u2717\n\u2717\n\u2717\n\u2717\n\u2717\n5.7\n\u2717\n\u2717\n\u2713 \u2717\n\u2717\n10.9\n\u2717\n\u2717\n\u2713 \u2713 \u2717\n12.0\n\u2713 \u2713 \u2713 \u2717\n\u2717\n10.8\n\u2713 \u2713 \u2713 \u2713 \u2717\n11.4\n\u2713 \u2713 \u2713 \u2713 \u2713 12.3\n\nparking\n\nvegetation\n\n\u2192parking lot \u2192tree\n\nmIoU\n\n\u2717\n\u2717\n\u2713\n\u2713\n\n\u2717\n\u2713\n\u2717\n\u2713\n\n15.4\n16.0\n18.6\n19.3\n\nTable 1: Effect of context category choices and reducing label ambiguity. Left: We find that\nsuppressing 5 frequently appearing categories brings performance gain on PASCAL-Context [12].\nRight: We observe that specifying the meaning of a class more concretely helps segmentation\nperformance of ReCo on Cityscapes [4]. For both cases, input images are resized and center-cropped\nto 320\u00d7320 following [7].\n\n4\n\n\fModel\n\nCLIP arch.\n\nDenseCLIP arch. mIoU\n\nReCo (ours)\n\nResNet50\nViT-B/32\nViT-L/14@336px ResNet50x16\n\nResNet50\nResNet50\n\n20.3\n20.3\n22.0\n\n# seed pixels mIoU Acc.\n\n1\n5\n10\n50\n100\n\n22.0\n22.2\n22.1\n21.7\n20.4\n\n65.4\n64.9\n62.9\n57.5\n51.0\n\nTable 2: Effect of architecture choices (left) and the number of seed pixels (right) on performance\nof ReCo on the Cityscapes validation set.\n\nC Additional ablation studies\n\nC.1 Choices of context categories\n\nAs described in Sec. 3.3 (main paper), we propose to suppress the commonly appearing categories,\ne.g., sky, which co-occur with other classes, e.g., aeroplanes. To achieve this, we manually pick 5\nfrequently appearing classes in PASCAL-Context dataset [12] and investigate the effect of different\ncombinations of such categories. In Tab. 1 (left) we observe that suppressing the tree and sky\ncategories yields a notable performance gain, while eliminating all five categories performs best. For\nthis reason, we apply the context elimination strategy with these five categories in the main paper.\n\nC.2 Category name rephrasing to reduce ambiguity\n\nWe observe that it is important to specify a concept concretely to obtain retrieved images exhibiting\nsimilar visual appearance. For instance, in Cityscapes dataset, parking and vegetation can be rephrased\nto less ambiguous concepts parking lot and tree respectively based on their descriptions4 in the paper\naccompanying the dataset [4]. As can be seen in Tab. 1 (right), ReCo gains benefits in performance\non Cityscapes by replacing the category names with less abstract concepts. This sensitivity is a\nconsequence of our co-segmentation algorithm, which locates pixels that share similar visual features\nacross multiple images. Thus we use the rephrased label names throughout the experiments in the\npaper.\n\nIn addition to the limitations listed in the main paper, this dependence on concrete/specific concept\nnames can be considered a limitation of our approach (albeit one that is readily mitigated). However,\nwe believe it is a reasonable requirement for methods that operate in the zero-shot transfer setting.\nUnlike fine-tuning methods that learn to associate abstract text descriptions to visual concepts by\nseeing examples from the target distribution, ReCo relies entirely on an adequate text description to\ndisambiguate the concept. Since many computer vision datasets have been constructed with training\nand testing splits with the assumption that methods would make use of the training set, we believe\nit is probable that category names were not designed to be uniquely descriptive (hence the use of\n\u201cparking\u201d as a category in Cityscapes, which could be either a verb or a noun). Indeed, there may\nhave been little perceived need to construct unambiguous category names when examples from the\ntraining set implicitly provide disambiguation of the concept.\n\nC.3 Effect of architectural choices for CLIP and DenseCLIP\n\nFor the experiments in the main paper, we use ViT-L/14@336px and ResNet50x16 for image retrieval\nand DenseCLIP inference respectively. As these models are relatively heavier than commonly\nused architectures such as ResNet50 or ViT-B/32, we evaluate ReCo with lighter encoders on the\nCityscapes validation split in Tab. 2 (left). Specifically we use either ResNet50 or ViT-B/32 for image\nretrieval and ResNet50 for DenseCLIP inference. As can be seen, adopting a lighter model slightly\ndecreases the performance, but still outperforms previous state-of-the-art methods (e.g., 16.3 mIoU\nfor D&S [20]).\n\n4\u201cHorizontal surfaces that are intended for parking and separated from the road, either via elevation or via a\ndifferent texture/material, but not separated merely by markings.\u201d for parking and \u201cTrees, hedges, and all kinds\nof vertically growing vegetation.\u201d for vegetation.\n\n5\n\n\fFigure 1: Additional visualisations of our co-segmentation method used for ReCo. For visualisa-\ntion purpose, we show the top 5 images from 50 retrieved images in each archive for a category. Left:\nSuccessful cases. Right: Typical failure cases. Highlighted regions are shown in red. Best viewed in\ncolour.\n\nC.4 Effect of the number of seed pixels\n\nWhile we pick one seed pixel per image by default as decribed in Sec. 3, we investigate how the\nnumber of seed pixels selected for an image affects performance of ReCo. For this, we evaluate ReCo\non Cityscapes with each reference image embedding computed by averaging 1, 5, 10, 50, and 100\nseed pixel(s) for an image. As can be seen in Tab. 2 (right), using one or five seed pixel(s) per image\nshows the best performance in terms of pixel accuracy and mIoU and picking more pixels tends to\nhurt the performance.\n\nD Additional visualisations\n\nD.1 Co-segmentation with seed pixels\n\nIn Fig. 1, we visualise examples of the co-segmentation with seed pixels on ImageNet2012, which is\nused for an index dataset for ReCo in the main paper. On the left, we show successful cases where the\nco-segmentation highlights regions corresponding to a given concept (i.e., sheep, building, and zebra).\nOn the right, we display examples of two typical failure cases: partial segmentation (i.e., bicycle\nand cup) and highlighting an object commonly co-occurring with a given concept (i.e., baseball\nglove). For bicycles, their frames are less highlighted compared to the wheels. Similarly, the body\nparts of the cups are less likely to be emphasised than the handles. In case of baseball gloves, the\nco-segmentation locates a part of a baseball, which often appears with a baseball glove. We believe\nthese failure cases are caused by the property of our co-segmentation algorithm, which focuses on\nregions with less variance in visual features (e.g., texture and shape) appearing in multiple images of\nan archive.\n\nD.2 Predictions of ReCo and ReCo+\n\nIn Fig. 2, we show more visualisation samples on the COCO-Stuff benchmark. Successful and failure\ncases are shown on the left and right, respectively. We note that ReCo tends to fail in predicting small\nobjects, e.g., people in the bus, and so does ReCo+ which is trained on the ReCo\u2019s predictions as\npseudo-labels. We conjecture that this is related to the stride of the image encoder used for ReCo,\n\n6\n\n\fFigure 2: Additional visualisations on COCO-Stuff. Left: Successful cases. Right: Typical failure\ncases. White pixels denote ignored regions.\n\nwhich is 16\u00d716 for the case of DeiT-S/16-SIN [13]. It could therefore potentially be improved by\nusing an encoder with a smaller stride at the cost of increased computational burden.\n\n\n\nBased on the section(s), please answer the following question with yes, no, or n/a and provide a brief justification for your answer.\nQuestion: If the authors curate/release new assets (e.g., code, data, models), do the authors discuss whether the data they are using/curating contains personally identifiable information or offensive content?"}}}