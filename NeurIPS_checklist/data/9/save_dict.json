{"paper_index": 9, "title": "Bridging Central and Local Differential Privacy in Data Acquisition Mechanisms", "abstract": "\n\nWe study the design of optimal Bayesian data acquisition mechanisms for a plat-\nform interested in estimating the mean of a distribution by collecting data from\nprivacy-conscious users. In our setting, users have heterogeneous sensitivities for\ntwo types of privacy losses corresponding to local and central differential privacy\nmeasures. The local privacy loss is due to the leakage of a user\u2019s information when\nshe shares her data with the platform, and the central privacy loss is due to the re-\nleased estimate by the platform to the public. The users share their data in exchange\nfor a payment (e.g., through monetary transfers or services) that compensates for\ntheir privacy losses. The platform knows the distribution of privacy sensitivities\nbut not their realizations, and must design a mechanism to solicit their preferences\nand then deliver both local and central privacy guarantees while minimizing the\nestimation error plus the expected payment to users. We first establish minimax\nlower bounds for the estimation error, given a vector of privacy guarantees, and\nshow that a linear estimator is (near) optimal. We then turn to our main goal:\ndesigning an optimal data acquisition mechanism. We establish that the design of\nsuch mechanisms in a Bayesian setting (where the platform knows the distribu-\ntion of users\u2019 sensitivities and not their realizations) can be cast as a nonconvex\noptimization problem. Additionally, for the class of linear estimators, we prove\nthat finding the optimal mechanism admits a Polynomial Time Approximation\nScheme.\n\n1\n\n", "introduction": "\n\nUsers\u2019 personal data are currently being utilized for personalized advertising, medical trials, targeted\nadvertising, and recommendation systems, among others. The transaction of individual data is set to\ngrow exponentially in the coming years, with more widespread applications of artificial intelligence\n(AI) and machine learning techniques. Even though it is widely accepted that users need to own their\ndata (see, e.g., Posner and Weyl [2019], Kushmaro [2021], and WILL.I.AM [2019]), the impact of\ndifferent market architectures on the design and operation of data markets are not clear: some users\nprefer to protect their own raw data while others expect companies to protect their data proactively\n(see, e.g., GDMA [2018]). In this paper, we consider the design of data acquisition mechanisms when\nusers have heterogeneous privacy concerns and ask the following question:\n\n36th Conference on Neural Information Processing Systems (NeurIPS 2022).\n\n\fWhat is the optimal data acquisition mechanism when users have heterogeneous\nprivacy concerns regarding access to their raw data and the outcome of the\nplatform\u2019s processing?\n\nWe use differential privacy to measure the two types of privacy losses. Informally, an estimator is\ncalled differentially private if its distribution over outputs is insensitive to the changes in a user\u2019s data.\n\nIn particular, we consider a platform whose goal is to estimate an underlying parameter of interest\nby collecting data from a set of users N = {1, . . . , n} who own a noisy version of the underlying\nparameter. For instance, consider a medical trial in which a hospital wants to collect users\u2019 data to\nestimate the efficacy of a drug. Each user has two types of privacy concerns: (i) local privacy concern\nthat captures how much information their shared data reveal about their raw data, and (ii) central\nprivacy concern that captures how much information the platform\u2019s output reveal about their raw\ndata. We adopt local and central R\u00e9nyi differential privacy to measure these two types of privacy\nlosses. The reason for choosing R\u00e9nyi differential privacy is twofold. First, our framework can cover\na wide range of information measures by varying the R\u00e9nyi divergence parameter. Second, it can be\nachieved by a Gaussian mechanism, which simplifies our analysis while capturing the main tradeoffs\nin the design of two-part data acquisition mechanisms.\n\nBefore formulating the platform\u2019s data acquisition problem, we derive optimal estimators for a given\nvector of heterogeneous local privacy loss levels. In particular, we establish a minimax lower bound\nfor the estimation error and prove that first privatizing users\u2019 data by adding a properly designed\nGaussian noise to them and then using a properly designed weighted average of these privatized data\npoints achieves this lower bound. This result motivates us to consider the design of the optimal data\nacquisition mechanism for the class of linear estimators.\n\nWe then turn to our mechanism design problem. Each user has a heterogeneous preference regarding\nthe importance of the above two privacy concerns. For instance, if a user fully trusts the platform,\nthen the first type of concern lessens, and the main concern would be about the information revealed\nfrom the platform\u2019s estimate. On the other hand, if a user does not trust the platform at all, the first\ntype of concern would be more than the second one. We model such a setting by assuming each user\ni has a privacy sensitivity ci \u2208 [0, 1] that determines the relative weight she puts on the local privacy\nconcern (1 \u2212 ci is the weight she puts on the central privacy concern). The utility of user i is the\npayment she receives from the platform (in exchange for sharing her data), minus ci times her local\nprivacy loss, and again, minus 1 \u2212 ci times her central privacy loss. The platform does not know the\nvalue of ci and (knowing its distribution) must design a (Bayesian) data acquisition mechanism to\nelicit the true privacy sensitivities (that guide the optimal choice of local and central privacy losses\ndelivered to each user) and optimize its objective.\n\nIn particular, the platform designs a two-part data acquisition mechanism that comprises a payment\nscheme, a local privacy guarantee, and a central privacy guarantee as a function of the reported\nprivacy sensitivity of users. The platform\u2019s goal is to minimize the sum of the mean estimation error\nand the expected total payment to users while satisfying the incentive compatibility and individual\nrationality constraints. Incentive compatibility ensures that users have no incentive to misreport their\nprivacy sensitivity, and individual rationality ensures that the payment to users (and the delivered\nprivacy guarantees) are such that users are willing to share their data with the platform.\n\nThe platform\u2019s problem is a functional optimization over three functions of the reported privacy\nsensitivities: payments, local privacy guarantees, and central privacy guarantees. We first find\nthe payment function in terms of the local and central privacy guarantees by using the incentive\ncompatibility and individual rationality constraints. This reduces the space of the platform\u2019s decision\nvariables. We then focus on the Gaussian mechanisms and linear estimators (motivated by our\nminimax optimality result) and show that the platform\u2019s problem can be cast as an optimization\nproblem that minimizes a non-convex objective (which depends on the virtual cost of users) for\nany reported vector of privacy sensitivities. This reformulation significantly reduces the space of\ndecision variables that the platform needs to optimize. However, it still involves solving a non-convex\noptimization problem. We further use the structural properties of this non-convex optimization and\nuse duality theory to develop a polynomial time algorithm to approximate the platform\u2019s problem.\nMore precisely, we prove that the design of the optimal two-part data acquisition mechanism admits\na Polynomial Time Approximation Scheme (PTAS).\n\n2\n\n\fThe contribution of our work is threefold. First, we develop a minimax lower bound when users have\nheterogeneous local privacy losses and establish that a linear estimator (approximately) achieves\nthis bound. Second, we develop a modeling framework for data acquisition mechanisms when users\nhave heterogeneous concerns for both local and central privacy losses. Third, for any estimator and\nmechanism to deliver privacy guarantees, we characterize the design of the optimal two-part data\nacquisition mechanism as the solution to a point-wise optimization problem. Additionally, for the\nclass of Gaussian mechanisms to deliver privacy guarantees and linear estimators, we develop an\nalgorithm to approximately find the optimal data acquisition mechanism (despite the fact that the\ncorresponding optimization is non-convex).\n\nRelated literature: Our paper relates to the literature on optimal data acquisition from privacy\nconcerned users. There is a large body of work that use differential privacy to measure the privacy loss\nof users Ghosh and Roth [2011], Nissim et al. [2012], Nissim et al. [2014]. One of the earliest papers\nin the literature is Ghosh and Roth [2011], which study the design of mechanisms for collecting users\u2019\ndata when users incur some privacy cost from sharing their data. More specifically, Ghosh and Roth\n[2011] consider binary data (bit) with the platform\u2019s goal being to estimate the sum of user\u2019s data by\nusing a differentially private and dominant strategy truthful mechanism. They study both the case\nwhen the user data and privacy parameter are independent (similar to our paper) and when they are\ncorrelated. In the independent case, they propose a mechanism that delivers a single privacy level to\nall users (as opposed to our setting that delivers heterogeneous privacy levels). For the correlated case,\nthey prove an impossibility result for the existence of a truthful and individually rational mechanism.\n\nSeveral works build on Ghosh and Roth [2011], extending it to take it or leave it offers Ligett and Roth\n[2012], strengthening the impossibility results Nissim et al. [2014], and studying the open question\nposed by Ghosh and Roth [2011] on whether a model with distributional assumption on users\u2019 costs\nand Bayesian mechanism design approach could be used to develop the optimal mechanism for\ncollecting data with privacy guarantees (see, e.g., Liao et al. [2021] and Fallah et al. [2022]). In\nparticular, Roth and Schoenebeck [2012], Chen et al. [2018], and Chen and Zheng [2019] tackle this\nproblem by developing a randomized mechanism in which user\u2019s data is randomly included in the\nfinal estimator where the inclusion probability depends on the reported privacy costs of the users\n(as opposed to our setting in which the payments and privacy guarantees depend on the reported\nprivacy sensitivity of all users). These papers do not use differential privacy to model privacy costs\nand instead use a menu of probability-price pairs to tune the privacy loss and the payment to each user\n(see also Pai and Roth [2013] for a survey). Similar to the above paper, we consider a setting in which\nthe platform can verify the data of users. For instance, in the context of medical trials, this means that\nthe users decide whether to participate in the medical trial and cannot change the samples they share.\nA different stream of the literature explores settings in which users can misreport their information\nPerote and Perote-Pena [2003], Dekel et al. [2010], Meir et al. [2012], Ghosh et al. [2014], Cai et al.\n[2015], Liu and Chen [2016, 2017].\n\nOur paper differs from these works in three main ways. First, we assume prior information on\nuser privacy sensitivities and focus on characterizing the optimal Bayesian incentive compatible\nmechanism. Second, we model a setting in which users have both local and central privacy concerns\nand explore the different privacy guarantees of these two types delivered by an optimal mechanism.\nThird, we assume that user data are drawn from the same underlying distribution that allows the\nplatform to put differing weights on the data of users depending on their privacy sensitivity, leading\nto different privacy levels for participating users.\n\nFinally, our paper relates to the literature on differential privacy. Pioneered by the seminal work of\nDwork et al. [2006a,b], differential privacy has emerged as a prevalent framework for characterizing\nthe privacy leakage of data oriented algorithms. More specifically, our paper is related to the private\nmean estimation considered by Duchi et al. [2013], Barber and Duchi [2014], Karwa and Vadhan\n[2017], Asoodeh et al. [2021], Kamath et al. [2019, 2020], Cummings et al. [2021], and Acharya\net al. [2021]. Additionally, our paper relates to the stream of differential privacy literature that studies\nR\u00e9nyi differential privacy (RDP) introduced by Bun and Steinke [2016] and Mironov [2017].\n\n", "methods": "\n\nWe consider a platform interested in estimating a parameter \u03b8 \u2208 R by collecting data of n users,\nindexed by N = {1, \u00b7 \u00b7 \u00b7 , n}. For any i \u2208 N , we denote user i\u2019s data by Xi \u2208 X and we assume Xi\n\n3\n\n\fis given by Xi = \u03b8 + Zi where Z1, \u00b7 \u00b7 \u00b7 , Zn are independent and identically distributed zero-mean\nrandom variables with variance VAR. To simplify the exposition, we further assume |Zi| \u2264 1/2 for\nany i \u2208 N .1 Throughout, we use lower case letters to denote the realization of random variables. The\nplatform\u2019s goal is to minimize the estimate\u2019s error by collecting data from privacy-concerned users.\nTherefore, the platform needs to incentivize them to share their data.\n\n2.1 Local and central privacy losses\n\nBefore formalizing the utilities/objectives of the platform and the users, we define the notions of\nprivacy losses that we adopt in this paper. In particular, we consider two different types of privacy\nlosses that users suffer from. The first one is the privacy loss of a user when she shares her data (only)\nwith the platform, and the second one is the privacy loss through the released estimate (to the public)\nby the platform. Depending on how different users trust the platform, they might care differently\nabout these two privacy losses. For instance, if a user fully trusts the platform, then her main privacy\nconcern would be the second one, while a user who does not trust the platform at all would be more\nconcerned with the first one as the public only observes the aggregated estimate, as opposed to the\nplatform which observes each user\u2019s (shared) data separately.\n\nWe use the differential privacy framework to quantify these privacy losses. Since differential privacy\nwas introduced by Dwork et al., several variants of it have been also proposed. In particular, a popular\none in the machine learning literature is R\u00e9nyi differential privacy (RDP), introduced by Mironov\n[2017], which we also adopt in this paper. Let us first recall the definition of R\u00e9nyi divergence.\nDefinition 1. Let P and Q be two distributions over R with densities p and q. For any \u03b1 \u2208 (1, \u221e],\nthe R\u00e9nyi \u03b1-divergence between P and Q is denoted by D\u03b1(P ||Q) and is given by\n\nD\u03b1(P ||Q) :=\n\n1\n\u03b1 \u2212 1\n\nlog\n\n(cid:90) (cid:18) p(x)\nq(x)\n\n(cid:19)\u03b1\n\nq(x)dx.\n\nFor two random variables X and Y , D\u03b1(X||Y ) denotes the \u03b1-divergence between their distributions.\n\nWe next define two notions of differential privacy, known as central and local, to capture the two\naforementioned types of privacy losses. Local differential privacy corresponds to the privacy loss of a\nuser when she shares her data with the platform through a randomized mapping, known as a channel.\nDefinition 2. Let \u03b5 \u2265 0 and \u03b1 \u2208 (1, \u221e]. A randomized channel C : X \u2192 R is locally (\u03b5, \u03b1)-R\u00e9nyi\n(differentially) private if for any x, x\u2032 \u2208 X ,\n\nD\u03b1(C(x)||C(x\u2032)) \u2264 \u03b5.\n\nCentral differential privacy corresponds to the other privacy loss mentioned above. It bounds the\nchange in the distribution of the platform\u2019s output, i.e., the released estimate, by changing one user\u2019s\ndata. We next provide the formal definition.\ni=1 \u2208 Rn\nDefinition 3. Let \u03b5 = (\u03b5i)n\n(\u03b5, \u03b1)-R\u00e9nyi (differentially) private if for any two datasets x1:n, x\u2032\ncoordinate (data of user i),\n\n+ and \u03b1 \u2208 (1, \u221e]. A randomized algorithm A : X n \u2192 R is\n1:n \u2208 X n that only differ in i-th\n\nD\u03b1(A(x1:n)||A(x\u2032\n\n1:n)) \u2264 \u03b5i.\n\nThe customary approach to guarantee RDP is Gaussian mechanism in which a properly tuned zero-\nmean Gaussian noise is added to fulfill the required condition. The following lemma, adapted from\nMironov [2017], allows us to characterize the Gaussian noise\u2019s variance for a privacy loss level.\nLemma 1. For a function f : X n \u2192 R, we define its sensitivity with respect to i-th coordinate as\n\nLi(f ) := sup {|f (x1:n) \u2212 f (x\u2032\n\n1:n)| : for all x1:n and x\u2032\n\n1:n differing only at i-th coordinate} .\n\nFor any \u03b1 \u2208 (1, \u221e], A(x1:n) = f (x1:n) + W with W \u223c N (0, \u03c32) is\n\n(cid:16)\n\n( \u03b1Li(f )2\n2\u03c32\n\n)n\ni=1, \u03b1\n\n(cid:17)\n\n-RDP.\n\nFor a given vector of local privacy losses (\u03b5(l)\nis by using a linear estimator with Gaussian mechanism which is given by\n\n1 , . . . , \u03b5(l)\n\nn ), a natural way to privately estimate the mean\n\n\u02c6\u03b8(x1, . . . , xn) :=\n\nn\n(cid:88)\n\ni=1\n\nwi \u02c6xi where\n\nn\n(cid:88)\n\ni=1\n\nwi = 1 and \u02c6xi = xi + N (0,\n\n\u03b1\n2\u03b5(l)\ni\n\n) for all i \u2208 N .\n\n(1)\n\n4\n\n\fFigure 1: The interaction between the users and the platform in the two-part private data acquisition.\n\ni / (cid:80)n\n\ni = w2\n\nand the central privacy delivered to user i \u2208 N is \u03b5(c)\n\nFigure 1 depicts this estimator. In particular, using Lemma 1, the local privacy delivered to user\ni \u2208 N is \u03b5(l)\nj . In the next\ni\nsection, we focus on linear estimators in designing the optimal private data acquisition mechanism.\nWe motivate such specification by showing that, for a given vector of local privacy losses (\u03b5(l)\ni=1,\na linear estimator is optimal with respect to mean square error. To formalize this statement, we\nfirst need to define the minimax estimation error as the notion of optimality. Let P be a class of\ndistributions over X . For any P \u2208 P, we denote its mean by \u03b8(P ). A (\u03b5(l)\ni=1-locally RDP estimator\ncan be cast as \u02c6\u03b8((Ci(xi))n\ni=1), where Ci(.) is the randomized channel corresponding to user i. Let\nQ((\u03b5(l)\ni=1-locally RDP estimators. The minimax estimation error is\ni=1,\u02c6\u03b8[|\u02c6\u03b8((Ci(Xi))n\n\ni=1) be the class of such (\u03b5(l)\n\ni=1) \u2212 \u03b8(P )|2],\n\nL(P, Q, (\u03b5(l)\n\nj=1 w2\n\ni=1) :=\n\nj /\u03b5(l)\n\n(Xi\u223cP )n\n\ni )n\n\ni )n\n\ni )n\n\ni )n\n\ni )n\n\n(2)\n\nE\n\ninf\ni=1\u2208Q((\u03b5(l)\n\ni )n\n\ni=1)\n\nsup\nP \u2208P\n\n\u02c6\u03b8,{Ci}n\n\nwhere the expectation is taken over both randomness of data and estimator (including private channels).\nIn other words, the optimal estimator is the one that has the lowest worst case error among all\nestimators that satisfy the privacy requirements. We next state the optimality result.\nTheorem 1. Assume \u03b1 \u2265 2 and \u03b5(l)\nand C1, \u00b7 \u00b7 \u00b7 , Cn be independent channels. Then, there exists a universal constant c such that\n\ni \u2264 1 for all i. Let P1 be the family of distributions over [\u2212 1\n\n2 , 1\n2 ]\n\nL(P, Q, (\u03b5(l)\n\ni )n\n\ni=1) \u2265 c min(\n\n1\ni=1 \u03b5(l)\n\ni\n\n(cid:80)n\n\n, 1).\n\nFurthermore, there exists a linear estimator with Gaussian mechanism such that\n\nE\n\n(Xi\u223cP )n\n\ni=1,\u02c6\u03b8[|\u02c6\u03b8((Ci(Xi))n\n\ni=1) \u2212 \u03b8(P )|2] \u2264 O(1)\n\n\u03b1\ni=1 \u03b5(l)\n\ni\n\n(cid:80)n\n\n.\n\nWe prove the lower bound by using the Le Cam\u2019s method Yu [1997] that reduces the problem of\nfinding lower bounds to a hypothesis testing problem and requires bounding the total variation\ndistance between the estimates when the input data points change. We then find it more convenient to\nbound the total variation in terms of Hellinger distance and use a series of inequalities to bound it.\n\n2.2 Data acquisition mechanism with two-part privacy guarantees\n\nWe next describe the utility functions of the users and the platform and then formulate the platform\u2019s\noptimal data acquisition mechanism. As we described earlier, each user suffers from two privacy\nlosses when sharing her data. The first one is a central privacy loss because of the leakage of her\ninformation through the platform\u2019s output. The second one is a local privacy loss because of the\nleakage of her information through the raw data that she shares with the platform. Each user has a\nheterogeneous privacy sensitivity for these two types of privacy losses. To model such heterogeneity,\nfor each i \u2208 N , we let ci \u2208 [0, 1] be her relative local privacy sensitivity, representing the relative\nweight that user i assigns to the (per unit cost of) local privacy loss. We also let 1 \u2212 ci be her relative\ncentral privacy sensitivity, representing the relative weight that user i assigns to the (per unit cost of)\ncentral privacy loss. Therefore, ci \u2248 1 implies that user i suffers a higher loss of privacy by sharing\nher raw data with the platform (local privacy loss) compared to her loss from the platform\u2019s output\n\n1This assumption does not have any fundamental impact on the results and is made to simplify the notations.\n\n5\n\n\f(central privacy loss). Differently, ci \u2248 0 implies that the user suffers a smaller loss of privacy by\nsharing her raw data with the platform compared to her loss from the platform\u2019s output. In what\nfollows, we use the term privacy sensitivity instead of relative local privacy sensitivity.\n\nFor each i \u2208 N , the privacy sensitivity ci is independently drawn from a publicly known distribution\nwhose support is [0, 1] with cumulative distribution and probability density functions Fi(\u00b7) and fi(\u00b7).\nWe also let c = (c1, . . . , cn) be the vector of privacy sensitivities. The privacy sensitivity of each\nuser is her private information, i.e., the platform does not know it. This is because individuals have\ndifferent views regarding how trustworthy the platform is in protecting their raw data.\n\nThe platform\u2019s objective is to design a mechanism to collect users\u2019 data by paying them to compensate\nfor their privacy losses without knowing the privacy sensitivity of users. To introduce the platform\u2019s\nobjective formally, we adopt the formalism of Bayesian mechanism design pioneered by Myerson\n[1981]. More specifically, the platform designs and announces a payment function, a local privacy loss\nfunction, and a central privacy loss function that are mappings from the reported privacy sensitivities\nof users. The users then report their privacy sensitivities (which may or may not be truthful). Based\non the payment function, the platform compensates the users (the compensation could be monetary\nor some free or discounted service provided to the user). Based on the local and central privacy\nfunctions, the platform designs randomized channels and randomized estimation algorithms that\ndeliver the guaranteed local and central privacy losses while minimizing the sum of the mean squared\nerror and the total expected payments. Given this interaction, we next formally introduce a data\nacquisition mechanism with two-part data privacy guarantees.\n\nDefinition 4 (two-part private data acquisition mechanism). We call the tuple (\u02c6\u03b8, \u03b5(l), \u03b5(c), t) a\ntwo-part private data acquisition mechanism where\n\n1. For all i \u2208 N , \u03b5(l)\ni\n\n: Rn\n\n+ \u2192 R+ is a function that maps the vector of privacy sensitivities c\n\nto a local privacy loss for user i, \u03b5(l)\n\ni (c), with \u03b5(l)(\u00b7) = (\u03b5(l)\n\ni (\u00b7))n\n\ni=1.\n\n2. For all i \u2208 N , \u03b5(c)\n\ni\n\n: Rn\n\n+ \u2192 R+ is a function that maps the vector of privacy sensitivities c\n\nto a central privacy loss for user i, \u03b5(c)\n\ni (c), with \u03b5(c)(.) = (\u03b5(c)\n\ni (\u00b7))n\n\ni=1.\n\n3. \u02c6\u03b8 : X n \u00d7 Rn\n\n+ \u00d7 Rn\n\nacquired locally (\u03b5(l)\nestimate \u02c6\u03b8(x, \u03b5(l)(c), \u03b5(c)(c)).\n\n+ \u2192 R is a (\u03b5(c)(c), \u03b1)-R\u00e9nyi differentially private estimator that maps\ni (c), \u03b1)-R\u00e9nyi differentially private data of user i for i \u2208 N to an\n\n4. For all i \u2208 N , ti : Rn\n\n+ \u2192 R+ is a function that maps the vector of privacy sensitivities c to\n\na payment for user i, ti(c), with t(.) = (ti(\u00b7))n\n\ni=1.\n\nNotice that we have not specified the estimator and the mechanisms that delivers (local and central)\nR\u00e9nyi differential privacy. In the rest of this subsection, we introduce the utilities and the platform\u2019s\nproblem for a general estimators and mechanisms to deliver differential privacy. Later, we focus on\nlinear estimator and Gaussian mechanisms and explicitly solve the platform\u2019s problem.\n\nEach user that participates in a two-part private data acquisition mechanism suffers from both the\nlocal and central privacy losses and need to be compensated by the platform. In particular, the utility\nof user i from participation when her privacy sensitivity is ci and she reports c\u2032\n\ni is given by\n\nui(\u03b5(l)(c\u2032\n\ni, c\u2212i), \u03b5(c)(c\u2032\n\ni, c\u2212i), t, \u02c6\u03b8) = Ec\u2212i[ti(c\u2212i, c\u2032\n\ni)) \u2212 ci\u03b5(l)\n\ni (c\u2212i, c\u2032\n\ni) \u2212 (1 \u2212 ci)\u03b5(c)\n\ni (c\u2212i, c\u2032\n\ni)],\n\ni)) is the payment from the platform, the term ci\u03b5(l)\n\nwhere the term ti(c\u2212i, c\u2032\ni) is the relative lo-\ncal privacy sensitivity of the user multiplied by her local privacy loss, and the term (1\u2212ci)\u03b5(c)\ni (c\u2212i, c\u2032\ni)\nis her relative central privacy sensitivity multiplied by her central privacy loss. A user i \u2208 N that\ndoes not participate in the mechanism neither compromises her privacy nor gets a compensation.\nTherefore, the utility of a user who does not participate in the mechanism becomes 0.\n\ni (c\u2212i, c\u2032\n\nThe goal of the platform is to minimize the sum of the mean squared error and the overall payment to\nusers. We let \u03b3 \u2208 R+ represents the relative weight of the mean estimation error and the payments in\n\n6\n\n\fthe platform\u2019s objective.2 Therefore, the platform\u2019s objective is\nn\n(cid:88)\n\nEc[\u03b3MSE(\u03b5(l)(c), \u03b5(c)(c), \u02c6\u03b8) +\n\nti(c)],\n\ni=1\n\nwhere the first term is the mean square error of estimator \u02c6\u03b8 given reported vector of privacy sensitivity\nand resulting local and central privacy losses \u03b5(l) and \u03b5(c), i.e.,\n\nMSE(\u03b5(l)(c), \u03b5(c)(c), \u02c6\u03b8) = Ex[|\u02c6\u03b8(\u02c6x, \u03b5(l), \u03b5(c)) \u2212 \u03b8|2].\nAlso, each summand of the second term is the compensation that the platform gives to a user to\nincentivize her to participate and report her privacy sensitivity truthfully.\n\nIn Appendix we prove that, similar to the classical mechanism design setting, revelation principle\nholds. This means that there is no loss of generality in focusing on the class of direct incentive\ncompatible mechanisms, meaning the platform\u2019s optimization problem can be written as\n\nmin\n\u03b5(l)(\u00b7),\u03b5(c)(\u00b7),t(\u00b7)\n\nEc[\u03b3MSE(\u03b5(l)(c), \u03b5(c)(c), \u02c6\u03b8) +\n\nn\n(cid:88)\n\nti(c)]\n\ni=1\nui(\u03b5(l)(c), \u03b5(c)(c), t, \u02c6\u03b8) \u2265 ui(\u03b5(l)(c\u2032\nui(\u03b5(l)(c), \u03b5(c)(c), t, \u02c6\u03b8) \u2265 0\n\ni, c\u2212i), \u03b5(c)(c\u2032\n\ni, c\u2212i), t, \u02c6\u03b8)\n\n(5)\nwhere the constraints in (4) represent the incentive compatibility. These constraints guarantee that\nthat each user i has no incentive to misrepresent her privacy sensitivity when others report truthfully\n(reporting truthfully is an equilibrium of the game among the users). Also, the constraints in (5)\nrepresent individual rationality, which ensures that each user receives a non-negative utility from\nparticipating in the platform\u2019s mechanism and sharing her data.\n\nfor all i \u2208 N , ci,\n\n(3)\n\n(4)\n\n3 From the mechanism design problem to an optimization problem\n\nFor a given estimator \u02c6\u03b8, the platform\u2019s decision comprises the local and central privacy loss functions\n\u03b5(l)(\u00b7) and \u03b5(c)(\u00b7) together with the payment functions t(\u00b7). We next show that this problem can be\nequivalently formulated as an optimization problem over the vector of local privacy losses and central\nprivacy losses (as opposed to functions). In the rest of the paper, we impose the following assumption\nwhich is well-known in the mechanism design literature and simplifies the analysis.3\nAssumption 1. For any user i \u2208 N , the virtual cost defined as \u03c8i(c) = c + Fi(c)\nfi(c) is increasing in c,\nwhere fi(\u00b7) and Fi(\u00b7) are probability density and cumulative distribution functions of ci, respectively.\n\nThe above assumption holds for a wide class of distributions such as the ones with log-concave\ndensity functions (e.g., uniform).\nTheorem 2. Suppose Assumption 1 holds. For a given estimator \u02c6\u03b8 : \u02c6X n \u00d7 Rn\n+ \u2192 R, in the\noptimal two-part data acquisition mechanism, for a given vector of reported privacy sensitivities c,\nthe local and central privacy losses are the solution of\nn\n(cid:88)\n\n+ \u00d7 Rn\n\nn\n(cid:88)\n\n\u03b5(l)\ni \u03c8i(ci) +\n\n\u03b5(c)\ni (1 \u2212 \u03c8i(ci)).\n\n(6)\n\n\u03b3MSE(\u03b5(l), \u03b5(c), \u02c6\u03b8) +\n\nmin\ni=1,{\u03b5(c)}n\n\ni=1\n\n{\u03b5(l)}n\n\ni=1\n\ni=1\n\nProof Sketch of Theorem 2: We introduce the following interim functions\n\nti(ci) =Ec\u2212i [t(ci, c\u2212i)],\n\ni (ci) = Ec\u2212i[\u03b5(l)\n\u03b5(l)\n\ni (ci, c\u2212i)],\n\nand \u03b5(c)\n\ni (ci) = Ec\u2212i[\u03b5(c)\n\ni (ci, c\u2212i)].\n\nWe first establish a payment identity that determines the optimal payment in terms of the optimal\nlocal and central privacy losses. In particular, by evaluating the first order condition corresponding to\nthe incentive compatibility constraint (4), we establish that this constraint holds if and only if\n\nti(ci) = ti(0) + \u03b5(c)\n\ni (ci) \u2212 \u03b5(c)\n\ni (0) + ci(\u03b5(l)\n\ni (ci) \u2212 \u03b5(c)\n\ni (ci)) \u2212\n\n(cid:90) ci\n\n0\n\n(\u03b5(l)\n\ni (z) \u2212 \u03b5(c)\n\ni (z))dz,\n\n2Notice that changing the parameter \u03b3 enables us to study a wide range of platform\u2019s objectives with differing\n\nrelative weights between the estimation error and the total payments.\n\n3Without this assumption, extending the results requires ironing technique of Myerson [1981].\n\n7\n\n\fi (z) \u2212 \u03b5(c)\n\nand \u03b5(l)\ni (z) is weakly decreasing in z. We then plug in this payment identity back to the\nplatform\u2019s objective, use the individual rationality constraint, and rewrite the platform\u2019s expected\nutility in terms of the privacy loss functions and the virtual cost of users. This is still a functional\noptimization problem in terms of \u03b5(l)(\u00b7) and \u03b5(c)(\u00b7). However, we establish that, under Assumption 1,\nwe can solve this functional optimization point-wise (i.e., for any given c). \u25a0\n\nTheorem 2 highlights the tradeoff in the platform\u2019s problem: by decreasing the local privacy loss,\nthe second term of the objective decreases (this term corresponds to the payment to users) while\nthe first term (i.e., the mean squared error) increases. The role of the central privacy loss is more\nnuanced, and there are two cases. If the coefficient 1 \u2212 \u03c8i(ci) is non-negative, by decreasing the\ncentral privacy loss, the third term of the objective decreases while the first term increases. If the\ncoefficient 1 \u2212 \u03c8i(ci) is negative, increasing the central privacy loss decreases both the third term\nand the first term. However, we cannot increase the central privacy loss level without limits because\nthe central privacy loss level is always below the local privacy loss level. Therefore, the platform\u2019s\noptimal mechanism should find the \u201cright\u201d balance between these terms.\n\n4 Optimal mechanism with two-part privacy guarantees for linear estimators\n\nFor the rest of the paper, we focus on linear estimators with Gaussian mechanism described in Section\n2.1. The following is a direct corollary of Theorem 2.\nCorollary 1. Suppose Assumption 1 holds. For any reported vector of privacy sensitivities c, the\noptimal local privacy loss levels are \u03b5(l)\nn\n(cid:88)\n\ni and the optimal central privacy loss levels are\n\ni (c) = y\u2217\n\n2\n\nwhere (w\u2217\n\n1, . . . , w\u2217\n\nn) and (y\u2217\n\n1, . . . , y\u2217\n\nn) are the optimal solution of\n\n\u03b5(c)\ni = w\u2217\ni\n\n2/\n\nw\u2217\nj\ny\u2217\nj\n\nj=1\n\nmin\nw,y\n\nVAR\u03b3\n\nn\n(cid:88)\n\ni=1\n\nw2\n\ni +\n\n\u03b3\u03b1\n2\n\nn\n(cid:88)\n\ni=1\n\nw2\ni\nyi\n\n+\n\ns.t. wi, yi \u2265 0, for all i \u2208 N and\n\nn\n(cid:88)\n\ni=1\n\nn\n(cid:88)\n\ni=1\n\n(1 \u2212 \u03c8i(ci))\n\nw2\ni\n\n(cid:80)n\n\nj=1\n\nw2\nj\nyj\n\n+\n\nn\n(cid:88)\n\ni=1\n\n\u03c8i(ci)yi\n\n(7)\n\nwi = 1.\n\nLet us highlight the difference between our characterization and that of classic mechanism design (e.g.,\nMyerson [1981]). In classic mechanism design, the designer\u2019s problem becomes linear optimization.\nHowever, in our setting, the designer\u2019s problem is a non-linear and non-convex optimization. This\nmakes the problem of finding the optimal two-part data acquisition mechanism challenging. Before\naddressing this computational challenge, let us revisit the form of the Gaussian mechanism that we\nhave adopted: the platform adds Gaussian noise locally and then outputs a convex combination of\nthe privatized users\u2019 data without adding any noise centrally. More specifically, one may guess that\nthe platform may benefit by having a central noise added to the final output in addition to the local\nnoises. In the following subsection, we establish that there is another Gaussian mechanism for any\nGaussian mechanism that only adds local noises and achieves a weakly lower cost.\n\n4.1 Optimality of having only local noises in the Gaussian mechanism\n\nThe platform has the opportunity of adding Gaussian noise to both the raw data of each user and the\nfinal estimator and ex-ante one may guess that it is optimal to use both of these instruments. However,\nas we establish next, interestingly, in the optimal two-part data acquisition mechanism, it is always\noptimal to only add noises locally.\nFor a given vector of local privacy losses (\u03b5(l)\nGaussian mechanism with both local and central noises is of the form\n\nn ) and central privacy losses (\u03b5(c)\n\n1 , . . . , \u03b5(c)\n\n1 , . . . , \u03b5(l)\n\nn ), a\n\n\u02c6\u03b8(x1, . . . , xn) :=\n\nn\n(cid:88)\n\ni=1\n\nwi \u02c6xi + N (0,\n\n\u03b1\n2\u03b5\n\n) where\n\nn\n(cid:88)\n\ni=1\n\nwi = 1 and \u02c6xi = xi + N (0,\n\n\u03b1\n2\u03b5(l)\ni\n\n)\u2200i \u2208 N .\n\nProposition 1. In the optimal two-part data acquisition mechanism that adopts a Gaussian mecha-\nnism with both local and central noises, we have \u03b5 = \u221e.\n\n8\n\n\fAlgorithm 1: Computing the optimal two-part private data acquisition mechanism\nInput: The vector of privacy sensitivities (c1, . . . , cn)\nfor S \u2208 Grid (cid:0)[S, \u00afS], \u03b4(cid:1) do\n\nLet\n\n\u03bdi =\n\n1\n\u03b3 VAR + (1 \u2212 \u03c8i(ci))/S\n\n, \u03b6i =\n\n\u03bdi\nj \u03bdj\n\n(cid:80)\n\n, \u03bei = \u03b6i\n\n(cid:113)\n\n\u03bdj(\n\n\u03c8j(cj) \u2212 (cid:112)\u03c8i(ci))\n\n\uf8f6\n\n\uf8f8 .\n\n\uf8eb\n\n\uf8ed\n\nn\n(cid:88)\n\nj=1\n\n(cid:18) (cid:80)n\n\ni=1 \u03b6i\n\n\u03c8i(ci)\n\nS\u2212(cid:80)n\n\ni=1\n\n\u03c8i(ci)\u03bei\n\n\u221a\n\u221a\n\n(cid:19)2\n\n.\n\nLet p =\n\nLet\n\n\u03bdi + \u03bdi\n\nwi(S) =\n\n(cid:112)\u03c8j(cj)p\n\n(cid:80)\n\nj \u03bdj\n(cid:80)\nj \u03bdj\n\n\u2212 \u03bdi\n\n(cid:112)\u03c8i(ci)p,\n\nyi(S) = wi(S)\n\n(cid:114) p\n\n,\n\n\u03c8i(ci)\n\nand OBJ(S) be the objective of Problem (7) evaluated for this solution.\n\nend\nOutput: {yi(S\u2217), wi(S\u2217)}n\n\ni=1, where (S\u2217) = arg min(S) OBJ(S).\n\nProposition 1 has an important implication in terms of the design of data market architecture when\nusers have both central and local privacy costs: it is optimal to add noise locally! Adding a noise\ncentrally to the final estimator has an advantage because the weights in the final estimator give the\nplatform a lever to deliver heterogeneous central privacy guarantees to users. Despite this advantage,\nwe establish that adding noise centrally is never optimal. This is because the platform prefers to add\nthe noise locally to contribute to both central and local privacy guarantees delivered to users.\n\n4.2 Computing the optimal privacy loss function\n\nThe implementation of the optimal two-part private data acquisition mechanism requires solving\nProblem (7), which is a non-convex program. However, we use the structure of the problem to develop\na polynomial time algorithm to solve it approximately. To do so, we first replace (cid:80)n\ni /yi by an\nauxiliary variable S. Next, we consider the corresponding lagrangian problem. Using Karush-Kuhn-\nTucker (KKT) conditions, we establish a number of relations between problems\u2019 parameters, S, and\np, the lagrangian coefficient corresponding to S = (cid:80)n\ni /yi. Furthermore, we develop upper and\nlower bounds for S. Finally, we do a grid search to find the approximate optimal solution.\nTheorem 3. For any vector of reported privacy sensitivities and \u03f5 > 0, Algorithm 1 finds local privacy\nloss levels and the differentially private linear estimator of the two-part data acquisition mechanism\nwhose cost (i.e., platform\u2019s objective) is at most 1 + \u03b4 of the optimal cost in time poly(n, 1\n\ni=1 w2\n\ni=1 w2\n\n\u03b4 ).\n\nNotice that the approximation factor in Theorem 3 depends on the underlying parameters. Therefore,\nwe have a Polynomial Time Approximation Scheme (PTAS) for finding the optimal two-part data\nacquisition mechanism in the class of linear estimators.\n\nIn the Appendix, we provide a case study with two users to illustrate the performance of the optimal\ntwo-part data acquisition mechanism in terms of the guaranteed privacy levels and payments as\nfunctions of the reported privacy sensitivities.\n\n", "experiments": "", "conclusion": "\n\nIn this paper we develop a unified framework to study the design of data acquisition mechanisms\nwhen users have both local and central privacy concerns and are heterogeneous in how they value\nthese two privacy concerns. We use R\u00e9nyi differential privacy to measure the privacy loss of users\nand first establish a minimax lower bound that motivates us to focus on linear estimators. We then\nestablish a point-wise optimization problem whose solution fully characterizes the optimal data\nacquisition mechanism that constitute a payment scheme to compensate users for their privacy losses,\na local privacy guarantee, and a central privacy guarantee all as a function of users\u2019 preferences for\n\n9\n\n\flocal and central privacy concerns. We then focus on linear estimators, motivated by our optimality\nresults, and establish that, even though the corresponding optimization problem is non-convex, the\nplatform\u2019s problem admits a Polynomial Time Approximation Scheme. Finally, we focused on\ndata acquisition to estimate mean population. However, our framework is more general and allows\nfor considering other (potentially vector) estimates. In particular, our Theorem 2 converts the data\nacquisition mechanism design problem into a (potentially) non-convex optimization problem.\n\n", "appendix": "", "system_prompts": [{"role": "system", "content": "You are a computer science researcher currently reviewing a paper titled \"Bridging Central and Local Differential Privacy in Data Acquisition Mechanisms\" for the NeurIPS computer science conference. Your goal is to try to be as objective and truthful as possible in your answers about the paper provided. Your reviews will be used for causal reasoning in determining the quality of the paper."}], "prompts": {"1b": {"role": "user", "content": "The following is the conclusion section of the paper you are reviewing:\n\n\nIn this paper we develop a unified framework to study the design of data acquisition mechanisms\nwhen users have both local and central privacy concerns and are heterogeneous in how they value\nthese two privacy concerns. We use R\u00e9nyi differential privacy to measure the privacy loss of users\nand first establish a minimax lower bound that motivates us to focus on linear estimators. We then\nestablish a point-wise optimization problem whose solution fully characterizes the optimal data\nacquisition mechanism that constitute a payment scheme to compensate users for their privacy losses,\na local privacy guarantee, and a central privacy guarantee all as a function of users\u2019 preferences for\n\n9\n\n\flocal and central privacy concerns. We then focus on linear estimators, motivated by our optimality\nresults, and establish that, even though the corresponding optimization problem is non-convex, the\nplatform\u2019s problem admits a Polynomial Time Approximation Scheme. Finally, we focused on\ndata acquisition to estimate mean population. However, our framework is more general and allows\nfor considering other (potentially vector) estimates. In particular, our Theorem 2 converts the data\nacquisition mechanism design problem into a (potentially) non-convex optimization problem.\n\n\n\nBased on the section(s), please answer the following question with yes, no, or n/a and provide a brief justification for your answer.\nQuestion: Do the authors describe the limitations of their work?"}, "1c": {"role": "user", "content": "The following is the conclusion section of the paper you are reviewing:\n\n\nIn this paper we develop a unified framework to study the design of data acquisition mechanisms\nwhen users have both local and central privacy concerns and are heterogeneous in how they value\nthese two privacy concerns. We use R\u00e9nyi differential privacy to measure the privacy loss of users\nand first establish a minimax lower bound that motivates us to focus on linear estimators. We then\nestablish a point-wise optimization problem whose solution fully characterizes the optimal data\nacquisition mechanism that constitute a payment scheme to compensate users for their privacy losses,\na local privacy guarantee, and a central privacy guarantee all as a function of users\u2019 preferences for\n\n9\n\n\flocal and central privacy concerns. We then focus on linear estimators, motivated by our optimality\nresults, and establish that, even though the corresponding optimization problem is non-convex, the\nplatform\u2019s problem admits a Polynomial Time Approximation Scheme. Finally, we focused on\ndata acquisition to estimate mean population. However, our framework is more general and allows\nfor considering other (potentially vector) estimates. In particular, our Theorem 2 converts the data\nacquisition mechanism design problem into a (potentially) non-convex optimization problem.\n\n\n\nBased on the section(s), please answer the following question with yes, no, or n/a and provide a brief justification for your answer.\nQuestion: Do the authors discuss any potential negative societal impacts of their work?"}, "2a": {"role": "user", "content": "The following is the methods section of the paper you are reviewing:\n\n\nWe consider a platform interested in estimating a parameter \u03b8 \u2208 R by collecting data of n users,\nindexed by N = {1, \u00b7 \u00b7 \u00b7 , n}. For any i \u2208 N , we denote user i\u2019s data by Xi \u2208 X and we assume Xi\n\n3\n\n\fis given by Xi = \u03b8 + Zi where Z1, \u00b7 \u00b7 \u00b7 , Zn are independent and identically distributed zero-mean\nrandom variables with variance VAR. To simplify the exposition, we further assume |Zi| \u2264 1/2 for\nany i \u2208 N .1 Throughout, we use lower case letters to denote the realization of random variables. The\nplatform\u2019s goal is to minimize the estimate\u2019s error by collecting data from privacy-concerned users.\nTherefore, the platform needs to incentivize them to share their data.\n\n2.1 Local and central privacy losses\n\nBefore formalizing the utilities/objectives of the platform and the users, we define the notions of\nprivacy losses that we adopt in this paper. In particular, we consider two different types of privacy\nlosses that users suffer from. The first one is the privacy loss of a user when she shares her data (only)\nwith the platform, and the second one is the privacy loss through the released estimate (to the public)\nby the platform. Depending on how different users trust the platform, they might care differently\nabout these two privacy losses. For instance, if a user fully trusts the platform, then her main privacy\nconcern would be the second one, while a user who does not trust the platform at all would be more\nconcerned with the first one as the public only observes the aggregated estimate, as opposed to the\nplatform which observes each user\u2019s (shared) data separately.\n\nWe use the differential privacy framework to quantify these privacy losses. Since differential privacy\nwas introduced by Dwork et al., several variants of it have been also proposed. In particular, a popular\none in the machine learning literature is R\u00e9nyi differential privacy (RDP), introduced by Mironov\n[2017], which we also adopt in this paper. Let us first recall the definition of R\u00e9nyi divergence.\nDefinition 1. Let P and Q be two distributions over R with densities p and q. For any \u03b1 \u2208 (1, \u221e],\nthe R\u00e9nyi \u03b1-divergence between P and Q is denoted by D\u03b1(P ||Q) and is given by\n\nD\u03b1(P ||Q) :=\n\n1\n\u03b1 \u2212 1\n\nlog\n\n(cid:90) (cid:18) p(x)\nq(x)\n\n(cid:19)\u03b1\n\nq(x)dx.\n\nFor two random variables X and Y , D\u03b1(X||Y ) denotes the \u03b1-divergence between their distributions.\n\nWe next define two notions of differential privacy, known as central and local, to capture the two\naforementioned types of privacy losses. Local differential privacy corresponds to the privacy loss of a\nuser when she shares her data with the platform through a randomized mapping, known as a channel.\nDefinition 2. Let \u03b5 \u2265 0 and \u03b1 \u2208 (1, \u221e]. A randomized channel C : X \u2192 R is locally (\u03b5, \u03b1)-R\u00e9nyi\n(differentially) private if for any x, x\u2032 \u2208 X ,\n\nD\u03b1(C(x)||C(x\u2032)) \u2264 \u03b5.\n\nCentral differential privacy corresponds to the other privacy loss mentioned above. It bounds the\nchange in the distribution of the platform\u2019s output, i.e., the released estimate, by changing one user\u2019s\ndata. We next provide the formal definition.\ni=1 \u2208 Rn\nDefinition 3. Let \u03b5 = (\u03b5i)n\n(\u03b5, \u03b1)-R\u00e9nyi (differentially) private if for any two datasets x1:n, x\u2032\ncoordinate (data of user i),\n\n+ and \u03b1 \u2208 (1, \u221e]. A randomized algorithm A : X n \u2192 R is\n1:n \u2208 X n that only differ in i-th\n\nD\u03b1(A(x1:n)||A(x\u2032\n\n1:n)) \u2264 \u03b5i.\n\nThe customary approach to guarantee RDP is Gaussian mechanism in which a properly tuned zero-\nmean Gaussian noise is added to fulfill the required condition. The following lemma, adapted from\nMironov [2017], allows us to characterize the Gaussian noise\u2019s variance for a privacy loss level.\nLemma 1. For a function f : X n \u2192 R, we define its sensitivity with respect to i-th coordinate as\n\nLi(f ) := sup {|f (x1:n) \u2212 f (x\u2032\n\n1:n)| : for all x1:n and x\u2032\n\n1:n differing only at i-th coordinate} .\n\nFor any \u03b1 \u2208 (1, \u221e], A(x1:n) = f (x1:n) + W with W \u223c N (0, \u03c32) is\n\n(cid:16)\n\n( \u03b1Li(f )2\n2\u03c32\n\n)n\ni=1, \u03b1\n\n(cid:17)\n\n-RDP.\n\nFor a given vector of local privacy losses (\u03b5(l)\nis by using a linear estimator with Gaussian mechanism which is given by\n\n1 , . . . , \u03b5(l)\n\nn ), a natural way to privately estimate the mean\n\n\u02c6\u03b8(x1, . . . , xn) :=\n\nn\n(cid:88)\n\ni=1\n\nwi \u02c6xi where\n\nn\n(cid:88)\n\ni=1\n\nwi = 1 and \u02c6xi = xi + N (0,\n\n\u03b1\n2\u03b5(l)\ni\n\n) for all i \u2208 N .\n\n(1)\n\n4\n\n\fFigure 1: The interaction between the users and the platform in the two-part private data acquisition.\n\ni / (cid:80)n\n\ni = w2\n\nand the central privacy delivered to user i \u2208 N is \u03b5(c)\n\nFigure 1 depicts this estimator. In particular, using Lemma 1, the local privacy delivered to user\ni \u2208 N is \u03b5(l)\nj . In the next\ni\nsection, we focus on linear estimators in designing the optimal private data acquisition mechanism.\nWe motivate such specification by showing that, for a given vector of local privacy losses (\u03b5(l)\ni=1,\na linear estimator is optimal with respect to mean square error. To formalize this statement, we\nfirst need to define the minimax estimation error as the notion of optimality. Let P be a class of\ndistributions over X . For any P \u2208 P, we denote its mean by \u03b8(P ). A (\u03b5(l)\ni=1-locally RDP estimator\ncan be cast as \u02c6\u03b8((Ci(xi))n\ni=1), where Ci(.) is the randomized channel corresponding to user i. Let\nQ((\u03b5(l)\ni=1-locally RDP estimators. The minimax estimation error is\ni=1,\u02c6\u03b8[|\u02c6\u03b8((Ci(Xi))n\n\ni=1) be the class of such (\u03b5(l)\n\ni=1) \u2212 \u03b8(P )|2],\n\nL(P, Q, (\u03b5(l)\n\nj=1 w2\n\ni=1) :=\n\nj /\u03b5(l)\n\n(Xi\u223cP )n\n\ni )n\n\ni )n\n\ni )n\n\ni )n\n\ni )n\n\n(2)\n\nE\n\ninf\ni=1\u2208Q((\u03b5(l)\n\ni )n\n\ni=1)\n\nsup\nP \u2208P\n\n\u02c6\u03b8,{Ci}n\n\nwhere the expectation is taken over both randomness of data and estimator (including private channels).\nIn other words, the optimal estimator is the one that has the lowest worst case error among all\nestimators that satisfy the privacy requirements. We next state the optimality result.\nTheorem 1. Assume \u03b1 \u2265 2 and \u03b5(l)\nand C1, \u00b7 \u00b7 \u00b7 , Cn be independent channels. Then, there exists a universal constant c such that\n\ni \u2264 1 for all i. Let P1 be the family of distributions over [\u2212 1\n\n2 , 1\n2 ]\n\nL(P, Q, (\u03b5(l)\n\ni )n\n\ni=1) \u2265 c min(\n\n1\ni=1 \u03b5(l)\n\ni\n\n(cid:80)n\n\n, 1).\n\nFurthermore, there exists a linear estimator with Gaussian mechanism such that\n\nE\n\n(Xi\u223cP )n\n\ni=1,\u02c6\u03b8[|\u02c6\u03b8((Ci(Xi))n\n\ni=1) \u2212 \u03b8(P )|2] \u2264 O(1)\n\n\u03b1\ni=1 \u03b5(l)\n\ni\n\n(cid:80)n\n\n.\n\nWe prove the lower bound by using the Le Cam\u2019s method Yu [1997] that reduces the problem of\nfinding lower bounds to a hypothesis testing problem and requires bounding the total variation\ndistance between the estimates when the input data points change. We then find it more convenient to\nbound the total variation in terms of Hellinger distance and use a series of inequalities to bound it.\n\n2.2 Data acquisition mechanism with two-part privacy guarantees\n\nWe next describe the utility functions of the users and the platform and then formulate the platform\u2019s\noptimal data acquisition mechanism. As we described earlier, each user suffers from two privacy\nlosses when sharing her data. The first one is a central privacy loss because of the leakage of her\ninformation through the platform\u2019s output. The second one is a local privacy loss because of the\nleakage of her information through the raw data that she shares with the platform. Each user has a\nheterogeneous privacy sensitivity for these two types of privacy losses. To model such heterogeneity,\nfor each i \u2208 N , we let ci \u2208 [0, 1] be her relative local privacy sensitivity, representing the relative\nweight that user i assigns to the (per unit cost of) local privacy loss. We also let 1 \u2212 ci be her relative\ncentral privacy sensitivity, representing the relative weight that user i assigns to the (per unit cost of)\ncentral privacy loss. Therefore, ci \u2248 1 implies that user i suffers a higher loss of privacy by sharing\nher raw data with the platform (local privacy loss) compared to her loss from the platform\u2019s output\n\n1This assumption does not have any fundamental impact on the results and is made to simplify the notations.\n\n5\n\n\f(central privacy loss). Differently, ci \u2248 0 implies that the user suffers a smaller loss of privacy by\nsharing her raw data with the platform compared to her loss from the platform\u2019s output. In what\nfollows, we use the term privacy sensitivity instead of relative local privacy sensitivity.\n\nFor each i \u2208 N , the privacy sensitivity ci is independently drawn from a publicly known distribution\nwhose support is [0, 1] with cumulative distribution and probability density functions Fi(\u00b7) and fi(\u00b7).\nWe also let c = (c1, . . . , cn) be the vector of privacy sensitivities. The privacy sensitivity of each\nuser is her private information, i.e., the platform does not know it. This is because individuals have\ndifferent views regarding how trustworthy the platform is in protecting their raw data.\n\nThe platform\u2019s objective is to design a mechanism to collect users\u2019 data by paying them to compensate\nfor their privacy losses without knowing the privacy sensitivity of users. To introduce the platform\u2019s\nobjective formally, we adopt the formalism of Bayesian mechanism design pioneered by Myerson\n[1981]. More specifically, the platform designs and announces a payment function, a local privacy loss\nfunction, and a central privacy loss function that are mappings from the reported privacy sensitivities\nof users. The users then report their privacy sensitivities (which may or may not be truthful). Based\non the payment function, the platform compensates the users (the compensation could be monetary\nor some free or discounted service provided to the user). Based on the local and central privacy\nfunctions, the platform designs randomized channels and randomized estimation algorithms that\ndeliver the guaranteed local and central privacy losses while minimizing the sum of the mean squared\nerror and the total expected payments. Given this interaction, we next formally introduce a data\nacquisition mechanism with two-part data privacy guarantees.\n\nDefinition 4 (two-part private data acquisition mechanism). We call the tuple (\u02c6\u03b8, \u03b5(l), \u03b5(c), t) a\ntwo-part private data acquisition mechanism where\n\n1. For all i \u2208 N , \u03b5(l)\ni\n\n: Rn\n\n+ \u2192 R+ is a function that maps the vector of privacy sensitivities c\n\nto a local privacy loss for user i, \u03b5(l)\n\ni (c), with \u03b5(l)(\u00b7) = (\u03b5(l)\n\ni (\u00b7))n\n\ni=1.\n\n2. For all i \u2208 N , \u03b5(c)\n\ni\n\n: Rn\n\n+ \u2192 R+ is a function that maps the vector of privacy sensitivities c\n\nto a central privacy loss for user i, \u03b5(c)\n\ni (c), with \u03b5(c)(.) = (\u03b5(c)\n\ni (\u00b7))n\n\ni=1.\n\n3. \u02c6\u03b8 : X n \u00d7 Rn\n\n+ \u00d7 Rn\n\nacquired locally (\u03b5(l)\nestimate \u02c6\u03b8(x, \u03b5(l)(c), \u03b5(c)(c)).\n\n+ \u2192 R is a (\u03b5(c)(c), \u03b1)-R\u00e9nyi differentially private estimator that maps\ni (c), \u03b1)-R\u00e9nyi differentially private data of user i for i \u2208 N to an\n\n4. For all i \u2208 N , ti : Rn\n\n+ \u2192 R+ is a function that maps the vector of privacy sensitivities c to\n\na payment for user i, ti(c), with t(.) = (ti(\u00b7))n\n\ni=1.\n\nNotice that we have not specified the estimator and the mechanisms that delivers (local and central)\nR\u00e9nyi differential privacy. In the rest of this subsection, we introduce the utilities and the platform\u2019s\nproblem for a general estimators and mechanisms to deliver differential privacy. Later, we focus on\nlinear estimator and Gaussian mechanisms and explicitly solve the platform\u2019s problem.\n\nEach user that participates in a two-part private data acquisition mechanism suffers from both the\nlocal and central privacy losses and need to be compensated by the platform. In particular, the utility\nof user i from participation when her privacy sensitivity is ci and she reports c\u2032\n\ni is given by\n\nui(\u03b5(l)(c\u2032\n\ni, c\u2212i), \u03b5(c)(c\u2032\n\ni, c\u2212i), t, \u02c6\u03b8) = Ec\u2212i[ti(c\u2212i, c\u2032\n\ni)) \u2212 ci\u03b5(l)\n\ni (c\u2212i, c\u2032\n\ni) \u2212 (1 \u2212 ci)\u03b5(c)\n\ni (c\u2212i, c\u2032\n\ni)],\n\ni)) is the payment from the platform, the term ci\u03b5(l)\n\nwhere the term ti(c\u2212i, c\u2032\ni) is the relative lo-\ncal privacy sensitivity of the user multiplied by her local privacy loss, and the term (1\u2212ci)\u03b5(c)\ni (c\u2212i, c\u2032\ni)\nis her relative central privacy sensitivity multiplied by her central privacy loss. A user i \u2208 N that\ndoes not participate in the mechanism neither compromises her privacy nor gets a compensation.\nTherefore, the utility of a user who does not participate in the mechanism becomes 0.\n\ni (c\u2212i, c\u2032\n\nThe goal of the platform is to minimize the sum of the mean squared error and the overall payment to\nusers. We let \u03b3 \u2208 R+ represents the relative weight of the mean estimation error and the payments in\n\n6\n\n\fthe platform\u2019s objective.2 Therefore, the platform\u2019s objective is\nn\n(cid:88)\n\nEc[\u03b3MSE(\u03b5(l)(c), \u03b5(c)(c), \u02c6\u03b8) +\n\nti(c)],\n\ni=1\n\nwhere the first term is the mean square error of estimator \u02c6\u03b8 given reported vector of privacy sensitivity\nand resulting local and central privacy losses \u03b5(l) and \u03b5(c), i.e.,\n\nMSE(\u03b5(l)(c), \u03b5(c)(c), \u02c6\u03b8) = Ex[|\u02c6\u03b8(\u02c6x, \u03b5(l), \u03b5(c)) \u2212 \u03b8|2].\nAlso, each summand of the second term is the compensation that the platform gives to a user to\nincentivize her to participate and report her privacy sensitivity truthfully.\n\nIn Appendix we prove that, similar to the classical mechanism design setting, revelation principle\nholds. This means that there is no loss of generality in focusing on the class of direct incentive\ncompatible mechanisms, meaning the platform\u2019s optimization problem can be written as\n\nmin\n\u03b5(l)(\u00b7),\u03b5(c)(\u00b7),t(\u00b7)\n\nEc[\u03b3MSE(\u03b5(l)(c), \u03b5(c)(c), \u02c6\u03b8) +\n\nn\n(cid:88)\n\nti(c)]\n\ni=1\nui(\u03b5(l)(c), \u03b5(c)(c), t, \u02c6\u03b8) \u2265 ui(\u03b5(l)(c\u2032\nui(\u03b5(l)(c), \u03b5(c)(c), t, \u02c6\u03b8) \u2265 0\n\ni, c\u2212i), \u03b5(c)(c\u2032\n\ni, c\u2212i), t, \u02c6\u03b8)\n\n(5)\nwhere the constraints in (4) represent the incentive compatibility. These constraints guarantee that\nthat each user i has no incentive to misrepresent her privacy sensitivity when others report truthfully\n(reporting truthfully is an equilibrium of the game among the users). Also, the constraints in (5)\nrepresent individual rationality, which ensures that each user receives a non-negative utility from\nparticipating in the platform\u2019s mechanism and sharing her data.\n\nfor all i \u2208 N , ci,\n\n(3)\n\n(4)\n\n3 From the mechanism design problem to an optimization problem\n\nFor a given estimator \u02c6\u03b8, the platform\u2019s decision comprises the local and central privacy loss functions\n\u03b5(l)(\u00b7) and \u03b5(c)(\u00b7) together with the payment functions t(\u00b7). We next show that this problem can be\nequivalently formulated as an optimization problem over the vector of local privacy losses and central\nprivacy losses (as opposed to functions). In the rest of the paper, we impose the following assumption\nwhich is well-known in the mechanism design literature and simplifies the analysis.3\nAssumption 1. For any user i \u2208 N , the virtual cost defined as \u03c8i(c) = c + Fi(c)\nfi(c) is increasing in c,\nwhere fi(\u00b7) and Fi(\u00b7) are probability density and cumulative distribution functions of ci, respectively.\n\nThe above assumption holds for a wide class of distributions such as the ones with log-concave\ndensity functions (e.g., uniform).\nTheorem 2. Suppose Assumption 1 holds. For a given estimator \u02c6\u03b8 : \u02c6X n \u00d7 Rn\n+ \u2192 R, in the\noptimal two-part data acquisition mechanism, for a given vector of reported privacy sensitivities c,\nthe local and central privacy losses are the solution of\nn\n(cid:88)\n\n+ \u00d7 Rn\n\nn\n(cid:88)\n\n\u03b5(l)\ni \u03c8i(ci) +\n\n\u03b5(c)\ni (1 \u2212 \u03c8i(ci)).\n\n(6)\n\n\u03b3MSE(\u03b5(l), \u03b5(c), \u02c6\u03b8) +\n\nmin\ni=1,{\u03b5(c)}n\n\ni=1\n\n{\u03b5(l)}n\n\ni=1\n\ni=1\n\nProof Sketch of Theorem 2: We introduce the following interim functions\n\nti(ci) =Ec\u2212i [t(ci, c\u2212i)],\n\ni (ci) = Ec\u2212i[\u03b5(l)\n\u03b5(l)\n\ni (ci, c\u2212i)],\n\nand \u03b5(c)\n\ni (ci) = Ec\u2212i[\u03b5(c)\n\ni (ci, c\u2212i)].\n\nWe first establish a payment identity that determines the optimal payment in terms of the optimal\nlocal and central privacy losses. In particular, by evaluating the first order condition corresponding to\nthe incentive compatibility constraint (4), we establish that this constraint holds if and only if\n\nti(ci) = ti(0) + \u03b5(c)\n\ni (ci) \u2212 \u03b5(c)\n\ni (0) + ci(\u03b5(l)\n\ni (ci) \u2212 \u03b5(c)\n\ni (ci)) \u2212\n\n(cid:90) ci\n\n0\n\n(\u03b5(l)\n\ni (z) \u2212 \u03b5(c)\n\ni (z))dz,\n\n2Notice that changing the parameter \u03b3 enables us to study a wide range of platform\u2019s objectives with differing\n\nrelative weights between the estimation error and the total payments.\n\n3Without this assumption, extending the results requires ironing technique of Myerson [1981].\n\n7\n\n\fi (z) \u2212 \u03b5(c)\n\nand \u03b5(l)\ni (z) is weakly decreasing in z. We then plug in this payment identity back to the\nplatform\u2019s objective, use the individual rationality constraint, and rewrite the platform\u2019s expected\nutility in terms of the privacy loss functions and the virtual cost of users. This is still a functional\noptimization problem in terms of \u03b5(l)(\u00b7) and \u03b5(c)(\u00b7). However, we establish that, under Assumption 1,\nwe can solve this functional optimization point-wise (i.e., for any given c). \u25a0\n\nTheorem 2 highlights the tradeoff in the platform\u2019s problem: by decreasing the local privacy loss,\nthe second term of the objective decreases (this term corresponds to the payment to users) while\nthe first term (i.e., the mean squared error) increases. The role of the central privacy loss is more\nnuanced, and there are two cases. If the coefficient 1 \u2212 \u03c8i(ci) is non-negative, by decreasing the\ncentral privacy loss, the third term of the objective decreases while the first term increases. If the\ncoefficient 1 \u2212 \u03c8i(ci) is negative, increasing the central privacy loss decreases both the third term\nand the first term. However, we cannot increase the central privacy loss level without limits because\nthe central privacy loss level is always below the local privacy loss level. Therefore, the platform\u2019s\noptimal mechanism should find the \u201cright\u201d balance between these terms.\n\n4 Optimal mechanism with two-part privacy guarantees for linear estimators\n\nFor the rest of the paper, we focus on linear estimators with Gaussian mechanism described in Section\n2.1. The following is a direct corollary of Theorem 2.\nCorollary 1. Suppose Assumption 1 holds. For any reported vector of privacy sensitivities c, the\noptimal local privacy loss levels are \u03b5(l)\nn\n(cid:88)\n\ni and the optimal central privacy loss levels are\n\ni (c) = y\u2217\n\n2\n\nwhere (w\u2217\n\n1, . . . , w\u2217\n\nn) and (y\u2217\n\n1, . . . , y\u2217\n\nn) are the optimal solution of\n\n\u03b5(c)\ni = w\u2217\ni\n\n2/\n\nw\u2217\nj\ny\u2217\nj\n\nj=1\n\nmin\nw,y\n\nVAR\u03b3\n\nn\n(cid:88)\n\ni=1\n\nw2\n\ni +\n\n\u03b3\u03b1\n2\n\nn\n(cid:88)\n\ni=1\n\nw2\ni\nyi\n\n+\n\ns.t. wi, yi \u2265 0, for all i \u2208 N and\n\nn\n(cid:88)\n\ni=1\n\nn\n(cid:88)\n\ni=1\n\n(1 \u2212 \u03c8i(ci))\n\nw2\ni\n\n(cid:80)n\n\nj=1\n\nw2\nj\nyj\n\n+\n\nn\n(cid:88)\n\ni=1\n\n\u03c8i(ci)yi\n\n(7)\n\nwi = 1.\n\nLet us highlight the difference between our characterization and that of classic mechanism design (e.g.,\nMyerson [1981]). In classic mechanism design, the designer\u2019s problem becomes linear optimization.\nHowever, in our setting, the designer\u2019s problem is a non-linear and non-convex optimization. This\nmakes the problem of finding the optimal two-part data acquisition mechanism challenging. Before\naddressing this computational challenge, let us revisit the form of the Gaussian mechanism that we\nhave adopted: the platform adds Gaussian noise locally and then outputs a convex combination of\nthe privatized users\u2019 data without adding any noise centrally. More specifically, one may guess that\nthe platform may benefit by having a central noise added to the final output in addition to the local\nnoises. In the following subsection, we establish that there is another Gaussian mechanism for any\nGaussian mechanism that only adds local noises and achieves a weakly lower cost.\n\n4.1 Optimality of having only local noises in the Gaussian mechanism\n\nThe platform has the opportunity of adding Gaussian noise to both the raw data of each user and the\nfinal estimator and ex-ante one may guess that it is optimal to use both of these instruments. However,\nas we establish next, interestingly, in the optimal two-part data acquisition mechanism, it is always\noptimal to only add noises locally.\nFor a given vector of local privacy losses (\u03b5(l)\nGaussian mechanism with both local and central noises is of the form\n\nn ) and central privacy losses (\u03b5(c)\n\n1 , . . . , \u03b5(c)\n\n1 , . . . , \u03b5(l)\n\nn ), a\n\n\u02c6\u03b8(x1, . . . , xn) :=\n\nn\n(cid:88)\n\ni=1\n\nwi \u02c6xi + N (0,\n\n\u03b1\n2\u03b5\n\n) where\n\nn\n(cid:88)\n\ni=1\n\nwi = 1 and \u02c6xi = xi + N (0,\n\n\u03b1\n2\u03b5(l)\ni\n\n)\u2200i \u2208 N .\n\nProposition 1. In the optimal two-part data acquisition mechanism that adopts a Gaussian mecha-\nnism with both local and central noises, we have \u03b5 = \u221e.\n\n8\n\n\fAlgorithm 1: Computing the optimal two-part private data acquisition mechanism\nInput: The vector of privacy sensitivities (c1, . . . , cn)\nfor S \u2208 Grid (cid:0)[S, \u00afS], \u03b4(cid:1) do\n\nLet\n\n\u03bdi =\n\n1\n\u03b3 VAR + (1 \u2212 \u03c8i(ci))/S\n\n, \u03b6i =\n\n\u03bdi\nj \u03bdj\n\n(cid:80)\n\n, \u03bei = \u03b6i\n\n(cid:113)\n\n\u03bdj(\n\n\u03c8j(cj) \u2212 (cid:112)\u03c8i(ci))\n\n\uf8f6\n\n\uf8f8 .\n\n\uf8eb\n\n\uf8ed\n\nn\n(cid:88)\n\nj=1\n\n(cid:18) (cid:80)n\n\ni=1 \u03b6i\n\n\u03c8i(ci)\n\nS\u2212(cid:80)n\n\ni=1\n\n\u03c8i(ci)\u03bei\n\n\u221a\n\u221a\n\n(cid:19)2\n\n.\n\nLet p =\n\nLet\n\n\u03bdi + \u03bdi\n\nwi(S) =\n\n(cid:112)\u03c8j(cj)p\n\n(cid:80)\n\nj \u03bdj\n(cid:80)\nj \u03bdj\n\n\u2212 \u03bdi\n\n(cid:112)\u03c8i(ci)p,\n\nyi(S) = wi(S)\n\n(cid:114) p\n\n,\n\n\u03c8i(ci)\n\nand OBJ(S) be the objective of Problem (7) evaluated for this solution.\n\nend\nOutput: {yi(S\u2217), wi(S\u2217)}n\n\ni=1, where (S\u2217) = arg min(S) OBJ(S).\n\nProposition 1 has an important implication in terms of the design of data market architecture when\nusers have both central and local privacy costs: it is optimal to add noise locally! Adding a noise\ncentrally to the final estimator has an advantage because the weights in the final estimator give the\nplatform a lever to deliver heterogeneous central privacy guarantees to users. Despite this advantage,\nwe establish that adding noise centrally is never optimal. This is because the platform prefers to add\nthe noise locally to contribute to both central and local privacy guarantees delivered to users.\n\n4.2 Computing the optimal privacy loss function\n\nThe implementation of the optimal two-part private data acquisition mechanism requires solving\nProblem (7), which is a non-convex program. However, we use the structure of the problem to develop\na polynomial time algorithm to solve it approximately. To do so, we first replace (cid:80)n\ni /yi by an\nauxiliary variable S. Next, we consider the corresponding lagrangian problem. Using Karush-Kuhn-\nTucker (KKT) conditions, we establish a number of relations between problems\u2019 parameters, S, and\np, the lagrangian coefficient corresponding to S = (cid:80)n\ni /yi. Furthermore, we develop upper and\nlower bounds for S. Finally, we do a grid search to find the approximate optimal solution.\nTheorem 3. For any vector of reported privacy sensitivities and \u03f5 > 0, Algorithm 1 finds local privacy\nloss levels and the differentially private linear estimator of the two-part data acquisition mechanism\nwhose cost (i.e., platform\u2019s objective) is at most 1 + \u03b4 of the optimal cost in time poly(n, 1\n\ni=1 w2\n\ni=1 w2\n\n\u03b4 ).\n\nNotice that the approximation factor in Theorem 3 depends on the underlying parameters. Therefore,\nwe have a Polynomial Time Approximation Scheme (PTAS) for finding the optimal two-part data\nacquisition mechanism in the class of linear estimators.\n\nIn the Appendix, we provide a case study with two users to illustrate the performance of the optimal\ntwo-part data acquisition mechanism in terms of the guaranteed privacy levels and payments as\nfunctions of the reported privacy sensitivities.\n\n\n\nBased on the section(s), please answer the following question with yes, no, or n/a and provide a brief justification for your answer.\nQuestion: If the authors include theoretical results, do the authors state the full set of assumptions of all theoretical results?"}, "2b": {"role": "user", "content": "The following is the methods section of the paper you are reviewing:\n\n\nWe consider a platform interested in estimating a parameter \u03b8 \u2208 R by collecting data of n users,\nindexed by N = {1, \u00b7 \u00b7 \u00b7 , n}. For any i \u2208 N , we denote user i\u2019s data by Xi \u2208 X and we assume Xi\n\n3\n\n\fis given by Xi = \u03b8 + Zi where Z1, \u00b7 \u00b7 \u00b7 , Zn are independent and identically distributed zero-mean\nrandom variables with variance VAR. To simplify the exposition, we further assume |Zi| \u2264 1/2 for\nany i \u2208 N .1 Throughout, we use lower case letters to denote the realization of random variables. The\nplatform\u2019s goal is to minimize the estimate\u2019s error by collecting data from privacy-concerned users.\nTherefore, the platform needs to incentivize them to share their data.\n\n2.1 Local and central privacy losses\n\nBefore formalizing the utilities/objectives of the platform and the users, we define the notions of\nprivacy losses that we adopt in this paper. In particular, we consider two different types of privacy\nlosses that users suffer from. The first one is the privacy loss of a user when she shares her data (only)\nwith the platform, and the second one is the privacy loss through the released estimate (to the public)\nby the platform. Depending on how different users trust the platform, they might care differently\nabout these two privacy losses. For instance, if a user fully trusts the platform, then her main privacy\nconcern would be the second one, while a user who does not trust the platform at all would be more\nconcerned with the first one as the public only observes the aggregated estimate, as opposed to the\nplatform which observes each user\u2019s (shared) data separately.\n\nWe use the differential privacy framework to quantify these privacy losses. Since differential privacy\nwas introduced by Dwork et al., several variants of it have been also proposed. In particular, a popular\none in the machine learning literature is R\u00e9nyi differential privacy (RDP), introduced by Mironov\n[2017], which we also adopt in this paper. Let us first recall the definition of R\u00e9nyi divergence.\nDefinition 1. Let P and Q be two distributions over R with densities p and q. For any \u03b1 \u2208 (1, \u221e],\nthe R\u00e9nyi \u03b1-divergence between P and Q is denoted by D\u03b1(P ||Q) and is given by\n\nD\u03b1(P ||Q) :=\n\n1\n\u03b1 \u2212 1\n\nlog\n\n(cid:90) (cid:18) p(x)\nq(x)\n\n(cid:19)\u03b1\n\nq(x)dx.\n\nFor two random variables X and Y , D\u03b1(X||Y ) denotes the \u03b1-divergence between their distributions.\n\nWe next define two notions of differential privacy, known as central and local, to capture the two\naforementioned types of privacy losses. Local differential privacy corresponds to the privacy loss of a\nuser when she shares her data with the platform through a randomized mapping, known as a channel.\nDefinition 2. Let \u03b5 \u2265 0 and \u03b1 \u2208 (1, \u221e]. A randomized channel C : X \u2192 R is locally (\u03b5, \u03b1)-R\u00e9nyi\n(differentially) private if for any x, x\u2032 \u2208 X ,\n\nD\u03b1(C(x)||C(x\u2032)) \u2264 \u03b5.\n\nCentral differential privacy corresponds to the other privacy loss mentioned above. It bounds the\nchange in the distribution of the platform\u2019s output, i.e., the released estimate, by changing one user\u2019s\ndata. We next provide the formal definition.\ni=1 \u2208 Rn\nDefinition 3. Let \u03b5 = (\u03b5i)n\n(\u03b5, \u03b1)-R\u00e9nyi (differentially) private if for any two datasets x1:n, x\u2032\ncoordinate (data of user i),\n\n+ and \u03b1 \u2208 (1, \u221e]. A randomized algorithm A : X n \u2192 R is\n1:n \u2208 X n that only differ in i-th\n\nD\u03b1(A(x1:n)||A(x\u2032\n\n1:n)) \u2264 \u03b5i.\n\nThe customary approach to guarantee RDP is Gaussian mechanism in which a properly tuned zero-\nmean Gaussian noise is added to fulfill the required condition. The following lemma, adapted from\nMironov [2017], allows us to characterize the Gaussian noise\u2019s variance for a privacy loss level.\nLemma 1. For a function f : X n \u2192 R, we define its sensitivity with respect to i-th coordinate as\n\nLi(f ) := sup {|f (x1:n) \u2212 f (x\u2032\n\n1:n)| : for all x1:n and x\u2032\n\n1:n differing only at i-th coordinate} .\n\nFor any \u03b1 \u2208 (1, \u221e], A(x1:n) = f (x1:n) + W with W \u223c N (0, \u03c32) is\n\n(cid:16)\n\n( \u03b1Li(f )2\n2\u03c32\n\n)n\ni=1, \u03b1\n\n(cid:17)\n\n-RDP.\n\nFor a given vector of local privacy losses (\u03b5(l)\nis by using a linear estimator with Gaussian mechanism which is given by\n\n1 , . . . , \u03b5(l)\n\nn ), a natural way to privately estimate the mean\n\n\u02c6\u03b8(x1, . . . , xn) :=\n\nn\n(cid:88)\n\ni=1\n\nwi \u02c6xi where\n\nn\n(cid:88)\n\ni=1\n\nwi = 1 and \u02c6xi = xi + N (0,\n\n\u03b1\n2\u03b5(l)\ni\n\n) for all i \u2208 N .\n\n(1)\n\n4\n\n\fFigure 1: The interaction between the users and the platform in the two-part private data acquisition.\n\ni / (cid:80)n\n\ni = w2\n\nand the central privacy delivered to user i \u2208 N is \u03b5(c)\n\nFigure 1 depicts this estimator. In particular, using Lemma 1, the local privacy delivered to user\ni \u2208 N is \u03b5(l)\nj . In the next\ni\nsection, we focus on linear estimators in designing the optimal private data acquisition mechanism.\nWe motivate such specification by showing that, for a given vector of local privacy losses (\u03b5(l)\ni=1,\na linear estimator is optimal with respect to mean square error. To formalize this statement, we\nfirst need to define the minimax estimation error as the notion of optimality. Let P be a class of\ndistributions over X . For any P \u2208 P, we denote its mean by \u03b8(P ). A (\u03b5(l)\ni=1-locally RDP estimator\ncan be cast as \u02c6\u03b8((Ci(xi))n\ni=1), where Ci(.) is the randomized channel corresponding to user i. Let\nQ((\u03b5(l)\ni=1-locally RDP estimators. The minimax estimation error is\ni=1,\u02c6\u03b8[|\u02c6\u03b8((Ci(Xi))n\n\ni=1) be the class of such (\u03b5(l)\n\ni=1) \u2212 \u03b8(P )|2],\n\nL(P, Q, (\u03b5(l)\n\nj=1 w2\n\ni=1) :=\n\nj /\u03b5(l)\n\n(Xi\u223cP )n\n\ni )n\n\ni )n\n\ni )n\n\ni )n\n\ni )n\n\n(2)\n\nE\n\ninf\ni=1\u2208Q((\u03b5(l)\n\ni )n\n\ni=1)\n\nsup\nP \u2208P\n\n\u02c6\u03b8,{Ci}n\n\nwhere the expectation is taken over both randomness of data and estimator (including private channels).\nIn other words, the optimal estimator is the one that has the lowest worst case error among all\nestimators that satisfy the privacy requirements. We next state the optimality result.\nTheorem 1. Assume \u03b1 \u2265 2 and \u03b5(l)\nand C1, \u00b7 \u00b7 \u00b7 , Cn be independent channels. Then, there exists a universal constant c such that\n\ni \u2264 1 for all i. Let P1 be the family of distributions over [\u2212 1\n\n2 , 1\n2 ]\n\nL(P, Q, (\u03b5(l)\n\ni )n\n\ni=1) \u2265 c min(\n\n1\ni=1 \u03b5(l)\n\ni\n\n(cid:80)n\n\n, 1).\n\nFurthermore, there exists a linear estimator with Gaussian mechanism such that\n\nE\n\n(Xi\u223cP )n\n\ni=1,\u02c6\u03b8[|\u02c6\u03b8((Ci(Xi))n\n\ni=1) \u2212 \u03b8(P )|2] \u2264 O(1)\n\n\u03b1\ni=1 \u03b5(l)\n\ni\n\n(cid:80)n\n\n.\n\nWe prove the lower bound by using the Le Cam\u2019s method Yu [1997] that reduces the problem of\nfinding lower bounds to a hypothesis testing problem and requires bounding the total variation\ndistance between the estimates when the input data points change. We then find it more convenient to\nbound the total variation in terms of Hellinger distance and use a series of inequalities to bound it.\n\n2.2 Data acquisition mechanism with two-part privacy guarantees\n\nWe next describe the utility functions of the users and the platform and then formulate the platform\u2019s\noptimal data acquisition mechanism. As we described earlier, each user suffers from two privacy\nlosses when sharing her data. The first one is a central privacy loss because of the leakage of her\ninformation through the platform\u2019s output. The second one is a local privacy loss because of the\nleakage of her information through the raw data that she shares with the platform. Each user has a\nheterogeneous privacy sensitivity for these two types of privacy losses. To model such heterogeneity,\nfor each i \u2208 N , we let ci \u2208 [0, 1] be her relative local privacy sensitivity, representing the relative\nweight that user i assigns to the (per unit cost of) local privacy loss. We also let 1 \u2212 ci be her relative\ncentral privacy sensitivity, representing the relative weight that user i assigns to the (per unit cost of)\ncentral privacy loss. Therefore, ci \u2248 1 implies that user i suffers a higher loss of privacy by sharing\nher raw data with the platform (local privacy loss) compared to her loss from the platform\u2019s output\n\n1This assumption does not have any fundamental impact on the results and is made to simplify the notations.\n\n5\n\n\f(central privacy loss). Differently, ci \u2248 0 implies that the user suffers a smaller loss of privacy by\nsharing her raw data with the platform compared to her loss from the platform\u2019s output. In what\nfollows, we use the term privacy sensitivity instead of relative local privacy sensitivity.\n\nFor each i \u2208 N , the privacy sensitivity ci is independently drawn from a publicly known distribution\nwhose support is [0, 1] with cumulative distribution and probability density functions Fi(\u00b7) and fi(\u00b7).\nWe also let c = (c1, . . . , cn) be the vector of privacy sensitivities. The privacy sensitivity of each\nuser is her private information, i.e., the platform does not know it. This is because individuals have\ndifferent views regarding how trustworthy the platform is in protecting their raw data.\n\nThe platform\u2019s objective is to design a mechanism to collect users\u2019 data by paying them to compensate\nfor their privacy losses without knowing the privacy sensitivity of users. To introduce the platform\u2019s\nobjective formally, we adopt the formalism of Bayesian mechanism design pioneered by Myerson\n[1981]. More specifically, the platform designs and announces a payment function, a local privacy loss\nfunction, and a central privacy loss function that are mappings from the reported privacy sensitivities\nof users. The users then report their privacy sensitivities (which may or may not be truthful). Based\non the payment function, the platform compensates the users (the compensation could be monetary\nor some free or discounted service provided to the user). Based on the local and central privacy\nfunctions, the platform designs randomized channels and randomized estimation algorithms that\ndeliver the guaranteed local and central privacy losses while minimizing the sum of the mean squared\nerror and the total expected payments. Given this interaction, we next formally introduce a data\nacquisition mechanism with two-part data privacy guarantees.\n\nDefinition 4 (two-part private data acquisition mechanism). We call the tuple (\u02c6\u03b8, \u03b5(l), \u03b5(c), t) a\ntwo-part private data acquisition mechanism where\n\n1. For all i \u2208 N , \u03b5(l)\ni\n\n: Rn\n\n+ \u2192 R+ is a function that maps the vector of privacy sensitivities c\n\nto a local privacy loss for user i, \u03b5(l)\n\ni (c), with \u03b5(l)(\u00b7) = (\u03b5(l)\n\ni (\u00b7))n\n\ni=1.\n\n2. For all i \u2208 N , \u03b5(c)\n\ni\n\n: Rn\n\n+ \u2192 R+ is a function that maps the vector of privacy sensitivities c\n\nto a central privacy loss for user i, \u03b5(c)\n\ni (c), with \u03b5(c)(.) = (\u03b5(c)\n\ni (\u00b7))n\n\ni=1.\n\n3. \u02c6\u03b8 : X n \u00d7 Rn\n\n+ \u00d7 Rn\n\nacquired locally (\u03b5(l)\nestimate \u02c6\u03b8(x, \u03b5(l)(c), \u03b5(c)(c)).\n\n+ \u2192 R is a (\u03b5(c)(c), \u03b1)-R\u00e9nyi differentially private estimator that maps\ni (c), \u03b1)-R\u00e9nyi differentially private data of user i for i \u2208 N to an\n\n4. For all i \u2208 N , ti : Rn\n\n+ \u2192 R+ is a function that maps the vector of privacy sensitivities c to\n\na payment for user i, ti(c), with t(.) = (ti(\u00b7))n\n\ni=1.\n\nNotice that we have not specified the estimator and the mechanisms that delivers (local and central)\nR\u00e9nyi differential privacy. In the rest of this subsection, we introduce the utilities and the platform\u2019s\nproblem for a general estimators and mechanisms to deliver differential privacy. Later, we focus on\nlinear estimator and Gaussian mechanisms and explicitly solve the platform\u2019s problem.\n\nEach user that participates in a two-part private data acquisition mechanism suffers from both the\nlocal and central privacy losses and need to be compensated by the platform. In particular, the utility\nof user i from participation when her privacy sensitivity is ci and she reports c\u2032\n\ni is given by\n\nui(\u03b5(l)(c\u2032\n\ni, c\u2212i), \u03b5(c)(c\u2032\n\ni, c\u2212i), t, \u02c6\u03b8) = Ec\u2212i[ti(c\u2212i, c\u2032\n\ni)) \u2212 ci\u03b5(l)\n\ni (c\u2212i, c\u2032\n\ni) \u2212 (1 \u2212 ci)\u03b5(c)\n\ni (c\u2212i, c\u2032\n\ni)],\n\ni)) is the payment from the platform, the term ci\u03b5(l)\n\nwhere the term ti(c\u2212i, c\u2032\ni) is the relative lo-\ncal privacy sensitivity of the user multiplied by her local privacy loss, and the term (1\u2212ci)\u03b5(c)\ni (c\u2212i, c\u2032\ni)\nis her relative central privacy sensitivity multiplied by her central privacy loss. A user i \u2208 N that\ndoes not participate in the mechanism neither compromises her privacy nor gets a compensation.\nTherefore, the utility of a user who does not participate in the mechanism becomes 0.\n\ni (c\u2212i, c\u2032\n\nThe goal of the platform is to minimize the sum of the mean squared error and the overall payment to\nusers. We let \u03b3 \u2208 R+ represents the relative weight of the mean estimation error and the payments in\n\n6\n\n\fthe platform\u2019s objective.2 Therefore, the platform\u2019s objective is\nn\n(cid:88)\n\nEc[\u03b3MSE(\u03b5(l)(c), \u03b5(c)(c), \u02c6\u03b8) +\n\nti(c)],\n\ni=1\n\nwhere the first term is the mean square error of estimator \u02c6\u03b8 given reported vector of privacy sensitivity\nand resulting local and central privacy losses \u03b5(l) and \u03b5(c), i.e.,\n\nMSE(\u03b5(l)(c), \u03b5(c)(c), \u02c6\u03b8) = Ex[|\u02c6\u03b8(\u02c6x, \u03b5(l), \u03b5(c)) \u2212 \u03b8|2].\nAlso, each summand of the second term is the compensation that the platform gives to a user to\nincentivize her to participate and report her privacy sensitivity truthfully.\n\nIn Appendix we prove that, similar to the classical mechanism design setting, revelation principle\nholds. This means that there is no loss of generality in focusing on the class of direct incentive\ncompatible mechanisms, meaning the platform\u2019s optimization problem can be written as\n\nmin\n\u03b5(l)(\u00b7),\u03b5(c)(\u00b7),t(\u00b7)\n\nEc[\u03b3MSE(\u03b5(l)(c), \u03b5(c)(c), \u02c6\u03b8) +\n\nn\n(cid:88)\n\nti(c)]\n\ni=1\nui(\u03b5(l)(c), \u03b5(c)(c), t, \u02c6\u03b8) \u2265 ui(\u03b5(l)(c\u2032\nui(\u03b5(l)(c), \u03b5(c)(c), t, \u02c6\u03b8) \u2265 0\n\ni, c\u2212i), \u03b5(c)(c\u2032\n\ni, c\u2212i), t, \u02c6\u03b8)\n\n(5)\nwhere the constraints in (4) represent the incentive compatibility. These constraints guarantee that\nthat each user i has no incentive to misrepresent her privacy sensitivity when others report truthfully\n(reporting truthfully is an equilibrium of the game among the users). Also, the constraints in (5)\nrepresent individual rationality, which ensures that each user receives a non-negative utility from\nparticipating in the platform\u2019s mechanism and sharing her data.\n\nfor all i \u2208 N , ci,\n\n(3)\n\n(4)\n\n3 From the mechanism design problem to an optimization problem\n\nFor a given estimator \u02c6\u03b8, the platform\u2019s decision comprises the local and central privacy loss functions\n\u03b5(l)(\u00b7) and \u03b5(c)(\u00b7) together with the payment functions t(\u00b7). We next show that this problem can be\nequivalently formulated as an optimization problem over the vector of local privacy losses and central\nprivacy losses (as opposed to functions). In the rest of the paper, we impose the following assumption\nwhich is well-known in the mechanism design literature and simplifies the analysis.3\nAssumption 1. For any user i \u2208 N , the virtual cost defined as \u03c8i(c) = c + Fi(c)\nfi(c) is increasing in c,\nwhere fi(\u00b7) and Fi(\u00b7) are probability density and cumulative distribution functions of ci, respectively.\n\nThe above assumption holds for a wide class of distributions such as the ones with log-concave\ndensity functions (e.g., uniform).\nTheorem 2. Suppose Assumption 1 holds. For a given estimator \u02c6\u03b8 : \u02c6X n \u00d7 Rn\n+ \u2192 R, in the\noptimal two-part data acquisition mechanism, for a given vector of reported privacy sensitivities c,\nthe local and central privacy losses are the solution of\nn\n(cid:88)\n\n+ \u00d7 Rn\n\nn\n(cid:88)\n\n\u03b5(l)\ni \u03c8i(ci) +\n\n\u03b5(c)\ni (1 \u2212 \u03c8i(ci)).\n\n(6)\n\n\u03b3MSE(\u03b5(l), \u03b5(c), \u02c6\u03b8) +\n\nmin\ni=1,{\u03b5(c)}n\n\ni=1\n\n{\u03b5(l)}n\n\ni=1\n\ni=1\n\nProof Sketch of Theorem 2: We introduce the following interim functions\n\nti(ci) =Ec\u2212i [t(ci, c\u2212i)],\n\ni (ci) = Ec\u2212i[\u03b5(l)\n\u03b5(l)\n\ni (ci, c\u2212i)],\n\nand \u03b5(c)\n\ni (ci) = Ec\u2212i[\u03b5(c)\n\ni (ci, c\u2212i)].\n\nWe first establish a payment identity that determines the optimal payment in terms of the optimal\nlocal and central privacy losses. In particular, by evaluating the first order condition corresponding to\nthe incentive compatibility constraint (4), we establish that this constraint holds if and only if\n\nti(ci) = ti(0) + \u03b5(c)\n\ni (ci) \u2212 \u03b5(c)\n\ni (0) + ci(\u03b5(l)\n\ni (ci) \u2212 \u03b5(c)\n\ni (ci)) \u2212\n\n(cid:90) ci\n\n0\n\n(\u03b5(l)\n\ni (z) \u2212 \u03b5(c)\n\ni (z))dz,\n\n2Notice that changing the parameter \u03b3 enables us to study a wide range of platform\u2019s objectives with differing\n\nrelative weights between the estimation error and the total payments.\n\n3Without this assumption, extending the results requires ironing technique of Myerson [1981].\n\n7\n\n\fi (z) \u2212 \u03b5(c)\n\nand \u03b5(l)\ni (z) is weakly decreasing in z. We then plug in this payment identity back to the\nplatform\u2019s objective, use the individual rationality constraint, and rewrite the platform\u2019s expected\nutility in terms of the privacy loss functions and the virtual cost of users. This is still a functional\noptimization problem in terms of \u03b5(l)(\u00b7) and \u03b5(c)(\u00b7). However, we establish that, under Assumption 1,\nwe can solve this functional optimization point-wise (i.e., for any given c). \u25a0\n\nTheorem 2 highlights the tradeoff in the platform\u2019s problem: by decreasing the local privacy loss,\nthe second term of the objective decreases (this term corresponds to the payment to users) while\nthe first term (i.e., the mean squared error) increases. The role of the central privacy loss is more\nnuanced, and there are two cases. If the coefficient 1 \u2212 \u03c8i(ci) is non-negative, by decreasing the\ncentral privacy loss, the third term of the objective decreases while the first term increases. If the\ncoefficient 1 \u2212 \u03c8i(ci) is negative, increasing the central privacy loss decreases both the third term\nand the first term. However, we cannot increase the central privacy loss level without limits because\nthe central privacy loss level is always below the local privacy loss level. Therefore, the platform\u2019s\noptimal mechanism should find the \u201cright\u201d balance between these terms.\n\n4 Optimal mechanism with two-part privacy guarantees for linear estimators\n\nFor the rest of the paper, we focus on linear estimators with Gaussian mechanism described in Section\n2.1. The following is a direct corollary of Theorem 2.\nCorollary 1. Suppose Assumption 1 holds. For any reported vector of privacy sensitivities c, the\noptimal local privacy loss levels are \u03b5(l)\nn\n(cid:88)\n\ni and the optimal central privacy loss levels are\n\ni (c) = y\u2217\n\n2\n\nwhere (w\u2217\n\n1, . . . , w\u2217\n\nn) and (y\u2217\n\n1, . . . , y\u2217\n\nn) are the optimal solution of\n\n\u03b5(c)\ni = w\u2217\ni\n\n2/\n\nw\u2217\nj\ny\u2217\nj\n\nj=1\n\nmin\nw,y\n\nVAR\u03b3\n\nn\n(cid:88)\n\ni=1\n\nw2\n\ni +\n\n\u03b3\u03b1\n2\n\nn\n(cid:88)\n\ni=1\n\nw2\ni\nyi\n\n+\n\ns.t. wi, yi \u2265 0, for all i \u2208 N and\n\nn\n(cid:88)\n\ni=1\n\nn\n(cid:88)\n\ni=1\n\n(1 \u2212 \u03c8i(ci))\n\nw2\ni\n\n(cid:80)n\n\nj=1\n\nw2\nj\nyj\n\n+\n\nn\n(cid:88)\n\ni=1\n\n\u03c8i(ci)yi\n\n(7)\n\nwi = 1.\n\nLet us highlight the difference between our characterization and that of classic mechanism design (e.g.,\nMyerson [1981]). In classic mechanism design, the designer\u2019s problem becomes linear optimization.\nHowever, in our setting, the designer\u2019s problem is a non-linear and non-convex optimization. This\nmakes the problem of finding the optimal two-part data acquisition mechanism challenging. Before\naddressing this computational challenge, let us revisit the form of the Gaussian mechanism that we\nhave adopted: the platform adds Gaussian noise locally and then outputs a convex combination of\nthe privatized users\u2019 data without adding any noise centrally. More specifically, one may guess that\nthe platform may benefit by having a central noise added to the final output in addition to the local\nnoises. In the following subsection, we establish that there is another Gaussian mechanism for any\nGaussian mechanism that only adds local noises and achieves a weakly lower cost.\n\n4.1 Optimality of having only local noises in the Gaussian mechanism\n\nThe platform has the opportunity of adding Gaussian noise to both the raw data of each user and the\nfinal estimator and ex-ante one may guess that it is optimal to use both of these instruments. However,\nas we establish next, interestingly, in the optimal two-part data acquisition mechanism, it is always\noptimal to only add noises locally.\nFor a given vector of local privacy losses (\u03b5(l)\nGaussian mechanism with both local and central noises is of the form\n\nn ) and central privacy losses (\u03b5(c)\n\n1 , . . . , \u03b5(c)\n\n1 , . . . , \u03b5(l)\n\nn ), a\n\n\u02c6\u03b8(x1, . . . , xn) :=\n\nn\n(cid:88)\n\ni=1\n\nwi \u02c6xi + N (0,\n\n\u03b1\n2\u03b5\n\n) where\n\nn\n(cid:88)\n\ni=1\n\nwi = 1 and \u02c6xi = xi + N (0,\n\n\u03b1\n2\u03b5(l)\ni\n\n)\u2200i \u2208 N .\n\nProposition 1. In the optimal two-part data acquisition mechanism that adopts a Gaussian mecha-\nnism with both local and central noises, we have \u03b5 = \u221e.\n\n8\n\n\fAlgorithm 1: Computing the optimal two-part private data acquisition mechanism\nInput: The vector of privacy sensitivities (c1, . . . , cn)\nfor S \u2208 Grid (cid:0)[S, \u00afS], \u03b4(cid:1) do\n\nLet\n\n\u03bdi =\n\n1\n\u03b3 VAR + (1 \u2212 \u03c8i(ci))/S\n\n, \u03b6i =\n\n\u03bdi\nj \u03bdj\n\n(cid:80)\n\n, \u03bei = \u03b6i\n\n(cid:113)\n\n\u03bdj(\n\n\u03c8j(cj) \u2212 (cid:112)\u03c8i(ci))\n\n\uf8f6\n\n\uf8f8 .\n\n\uf8eb\n\n\uf8ed\n\nn\n(cid:88)\n\nj=1\n\n(cid:18) (cid:80)n\n\ni=1 \u03b6i\n\n\u03c8i(ci)\n\nS\u2212(cid:80)n\n\ni=1\n\n\u03c8i(ci)\u03bei\n\n\u221a\n\u221a\n\n(cid:19)2\n\n.\n\nLet p =\n\nLet\n\n\u03bdi + \u03bdi\n\nwi(S) =\n\n(cid:112)\u03c8j(cj)p\n\n(cid:80)\n\nj \u03bdj\n(cid:80)\nj \u03bdj\n\n\u2212 \u03bdi\n\n(cid:112)\u03c8i(ci)p,\n\nyi(S) = wi(S)\n\n(cid:114) p\n\n,\n\n\u03c8i(ci)\n\nand OBJ(S) be the objective of Problem (7) evaluated for this solution.\n\nend\nOutput: {yi(S\u2217), wi(S\u2217)}n\n\ni=1, where (S\u2217) = arg min(S) OBJ(S).\n\nProposition 1 has an important implication in terms of the design of data market architecture when\nusers have both central and local privacy costs: it is optimal to add noise locally! Adding a noise\ncentrally to the final estimator has an advantage because the weights in the final estimator give the\nplatform a lever to deliver heterogeneous central privacy guarantees to users. Despite this advantage,\nwe establish that adding noise centrally is never optimal. This is because the platform prefers to add\nthe noise locally to contribute to both central and local privacy guarantees delivered to users.\n\n4.2 Computing the optimal privacy loss function\n\nThe implementation of the optimal two-part private data acquisition mechanism requires solving\nProblem (7), which is a non-convex program. However, we use the structure of the problem to develop\na polynomial time algorithm to solve it approximately. To do so, we first replace (cid:80)n\ni /yi by an\nauxiliary variable S. Next, we consider the corresponding lagrangian problem. Using Karush-Kuhn-\nTucker (KKT) conditions, we establish a number of relations between problems\u2019 parameters, S, and\np, the lagrangian coefficient corresponding to S = (cid:80)n\ni /yi. Furthermore, we develop upper and\nlower bounds for S. Finally, we do a grid search to find the approximate optimal solution.\nTheorem 3. For any vector of reported privacy sensitivities and \u03f5 > 0, Algorithm 1 finds local privacy\nloss levels and the differentially private linear estimator of the two-part data acquisition mechanism\nwhose cost (i.e., platform\u2019s objective) is at most 1 + \u03b4 of the optimal cost in time poly(n, 1\n\ni=1 w2\n\ni=1 w2\n\n\u03b4 ).\n\nNotice that the approximation factor in Theorem 3 depends on the underlying parameters. Therefore,\nwe have a Polynomial Time Approximation Scheme (PTAS) for finding the optimal two-part data\nacquisition mechanism in the class of linear estimators.\n\nIn the Appendix, we provide a case study with two users to illustrate the performance of the optimal\ntwo-part data acquisition mechanism in terms of the guaranteed privacy levels and payments as\nfunctions of the reported privacy sensitivities.\n\n\n\nThe following is the appendix section of the paper you are reviewing:\n\n\nBased on the section(s), please answer the following question with yes, no, or n/a and provide a brief justification for your answer.\nQuestion: If the authors include theoretical results, do the authors include complete proofs of all theoretical results?"}, "3a": {"role": "user", "content": "The following is the abstract section of the paper you are reviewing:\n\n\nWe study the design of optimal Bayesian data acquisition mechanisms for a plat-\nform interested in estimating the mean of a distribution by collecting data from\nprivacy-conscious users. In our setting, users have heterogeneous sensitivities for\ntwo types of privacy losses corresponding to local and central differential privacy\nmeasures. The local privacy loss is due to the leakage of a user\u2019s information when\nshe shares her data with the platform, and the central privacy loss is due to the re-\nleased estimate by the platform to the public. The users share their data in exchange\nfor a payment (e.g., through monetary transfers or services) that compensates for\ntheir privacy losses. The platform knows the distribution of privacy sensitivities\nbut not their realizations, and must design a mechanism to solicit their preferences\nand then deliver both local and central privacy guarantees while minimizing the\nestimation error plus the expected payment to users. We first establish minimax\nlower bounds for the estimation error, given a vector of privacy guarantees, and\nshow that a linear estimator is (near) optimal. We then turn to our main goal:\ndesigning an optimal data acquisition mechanism. We establish that the design of\nsuch mechanisms in a Bayesian setting (where the platform knows the distribu-\ntion of users\u2019 sensitivities and not their realizations) can be cast as a nonconvex\noptimization problem. Additionally, for the class of linear estimators, we prove\nthat finding the optimal mechanism admits a Polynomial Time Approximation\nScheme.\n\n1\n\n\n\nBased on the section(s), please answer the following question with yes, no, or n/a and provide a brief justification for your answer.\nQuestion: If the authors ran experiments, do the authors include the code, data, and instructions needed to reproduce the main experimental results (either in the supplemental material or as a URL)?"}, "3b": {"role": "user", "content": "The following is the methods section of the paper you are reviewing:\n\n\nWe consider a platform interested in estimating a parameter \u03b8 \u2208 R by collecting data of n users,\nindexed by N = {1, \u00b7 \u00b7 \u00b7 , n}. For any i \u2208 N , we denote user i\u2019s data by Xi \u2208 X and we assume Xi\n\n3\n\n\fis given by Xi = \u03b8 + Zi where Z1, \u00b7 \u00b7 \u00b7 , Zn are independent and identically distributed zero-mean\nrandom variables with variance VAR. To simplify the exposition, we further assume |Zi| \u2264 1/2 for\nany i \u2208 N .1 Throughout, we use lower case letters to denote the realization of random variables. The\nplatform\u2019s goal is to minimize the estimate\u2019s error by collecting data from privacy-concerned users.\nTherefore, the platform needs to incentivize them to share their data.\n\n2.1 Local and central privacy losses\n\nBefore formalizing the utilities/objectives of the platform and the users, we define the notions of\nprivacy losses that we adopt in this paper. In particular, we consider two different types of privacy\nlosses that users suffer from. The first one is the privacy loss of a user when she shares her data (only)\nwith the platform, and the second one is the privacy loss through the released estimate (to the public)\nby the platform. Depending on how different users trust the platform, they might care differently\nabout these two privacy losses. For instance, if a user fully trusts the platform, then her main privacy\nconcern would be the second one, while a user who does not trust the platform at all would be more\nconcerned with the first one as the public only observes the aggregated estimate, as opposed to the\nplatform which observes each user\u2019s (shared) data separately.\n\nWe use the differential privacy framework to quantify these privacy losses. Since differential privacy\nwas introduced by Dwork et al., several variants of it have been also proposed. In particular, a popular\none in the machine learning literature is R\u00e9nyi differential privacy (RDP), introduced by Mironov\n[2017], which we also adopt in this paper. Let us first recall the definition of R\u00e9nyi divergence.\nDefinition 1. Let P and Q be two distributions over R with densities p and q. For any \u03b1 \u2208 (1, \u221e],\nthe R\u00e9nyi \u03b1-divergence between P and Q is denoted by D\u03b1(P ||Q) and is given by\n\nD\u03b1(P ||Q) :=\n\n1\n\u03b1 \u2212 1\n\nlog\n\n(cid:90) (cid:18) p(x)\nq(x)\n\n(cid:19)\u03b1\n\nq(x)dx.\n\nFor two random variables X and Y , D\u03b1(X||Y ) denotes the \u03b1-divergence between their distributions.\n\nWe next define two notions of differential privacy, known as central and local, to capture the two\naforementioned types of privacy losses. Local differential privacy corresponds to the privacy loss of a\nuser when she shares her data with the platform through a randomized mapping, known as a channel.\nDefinition 2. Let \u03b5 \u2265 0 and \u03b1 \u2208 (1, \u221e]. A randomized channel C : X \u2192 R is locally (\u03b5, \u03b1)-R\u00e9nyi\n(differentially) private if for any x, x\u2032 \u2208 X ,\n\nD\u03b1(C(x)||C(x\u2032)) \u2264 \u03b5.\n\nCentral differential privacy corresponds to the other privacy loss mentioned above. It bounds the\nchange in the distribution of the platform\u2019s output, i.e., the released estimate, by changing one user\u2019s\ndata. We next provide the formal definition.\ni=1 \u2208 Rn\nDefinition 3. Let \u03b5 = (\u03b5i)n\n(\u03b5, \u03b1)-R\u00e9nyi (differentially) private if for any two datasets x1:n, x\u2032\ncoordinate (data of user i),\n\n+ and \u03b1 \u2208 (1, \u221e]. A randomized algorithm A : X n \u2192 R is\n1:n \u2208 X n that only differ in i-th\n\nD\u03b1(A(x1:n)||A(x\u2032\n\n1:n)) \u2264 \u03b5i.\n\nThe customary approach to guarantee RDP is Gaussian mechanism in which a properly tuned zero-\nmean Gaussian noise is added to fulfill the required condition. The following lemma, adapted from\nMironov [2017], allows us to characterize the Gaussian noise\u2019s variance for a privacy loss level.\nLemma 1. For a function f : X n \u2192 R, we define its sensitivity with respect to i-th coordinate as\n\nLi(f ) := sup {|f (x1:n) \u2212 f (x\u2032\n\n1:n)| : for all x1:n and x\u2032\n\n1:n differing only at i-th coordinate} .\n\nFor any \u03b1 \u2208 (1, \u221e], A(x1:n) = f (x1:n) + W with W \u223c N (0, \u03c32) is\n\n(cid:16)\n\n( \u03b1Li(f )2\n2\u03c32\n\n)n\ni=1, \u03b1\n\n(cid:17)\n\n-RDP.\n\nFor a given vector of local privacy losses (\u03b5(l)\nis by using a linear estimator with Gaussian mechanism which is given by\n\n1 , . . . , \u03b5(l)\n\nn ), a natural way to privately estimate the mean\n\n\u02c6\u03b8(x1, . . . , xn) :=\n\nn\n(cid:88)\n\ni=1\n\nwi \u02c6xi where\n\nn\n(cid:88)\n\ni=1\n\nwi = 1 and \u02c6xi = xi + N (0,\n\n\u03b1\n2\u03b5(l)\ni\n\n) for all i \u2208 N .\n\n(1)\n\n4\n\n\fFigure 1: The interaction between the users and the platform in the two-part private data acquisition.\n\ni / (cid:80)n\n\ni = w2\n\nand the central privacy delivered to user i \u2208 N is \u03b5(c)\n\nFigure 1 depicts this estimator. In particular, using Lemma 1, the local privacy delivered to user\ni \u2208 N is \u03b5(l)\nj . In the next\ni\nsection, we focus on linear estimators in designing the optimal private data acquisition mechanism.\nWe motivate such specification by showing that, for a given vector of local privacy losses (\u03b5(l)\ni=1,\na linear estimator is optimal with respect to mean square error. To formalize this statement, we\nfirst need to define the minimax estimation error as the notion of optimality. Let P be a class of\ndistributions over X . For any P \u2208 P, we denote its mean by \u03b8(P ). A (\u03b5(l)\ni=1-locally RDP estimator\ncan be cast as \u02c6\u03b8((Ci(xi))n\ni=1), where Ci(.) is the randomized channel corresponding to user i. Let\nQ((\u03b5(l)\ni=1-locally RDP estimators. The minimax estimation error is\ni=1,\u02c6\u03b8[|\u02c6\u03b8((Ci(Xi))n\n\ni=1) be the class of such (\u03b5(l)\n\ni=1) \u2212 \u03b8(P )|2],\n\nL(P, Q, (\u03b5(l)\n\nj=1 w2\n\ni=1) :=\n\nj /\u03b5(l)\n\n(Xi\u223cP )n\n\ni )n\n\ni )n\n\ni )n\n\ni )n\n\ni )n\n\n(2)\n\nE\n\ninf\ni=1\u2208Q((\u03b5(l)\n\ni )n\n\ni=1)\n\nsup\nP \u2208P\n\n\u02c6\u03b8,{Ci}n\n\nwhere the expectation is taken over both randomness of data and estimator (including private channels).\nIn other words, the optimal estimator is the one that has the lowest worst case error among all\nestimators that satisfy the privacy requirements. We next state the optimality result.\nTheorem 1. Assume \u03b1 \u2265 2 and \u03b5(l)\nand C1, \u00b7 \u00b7 \u00b7 , Cn be independent channels. Then, there exists a universal constant c such that\n\ni \u2264 1 for all i. Let P1 be the family of distributions over [\u2212 1\n\n2 , 1\n2 ]\n\nL(P, Q, (\u03b5(l)\n\ni )n\n\ni=1) \u2265 c min(\n\n1\ni=1 \u03b5(l)\n\ni\n\n(cid:80)n\n\n, 1).\n\nFurthermore, there exists a linear estimator with Gaussian mechanism such that\n\nE\n\n(Xi\u223cP )n\n\ni=1,\u02c6\u03b8[|\u02c6\u03b8((Ci(Xi))n\n\ni=1) \u2212 \u03b8(P )|2] \u2264 O(1)\n\n\u03b1\ni=1 \u03b5(l)\n\ni\n\n(cid:80)n\n\n.\n\nWe prove the lower bound by using the Le Cam\u2019s method Yu [1997] that reduces the problem of\nfinding lower bounds to a hypothesis testing problem and requires bounding the total variation\ndistance between the estimates when the input data points change. We then find it more convenient to\nbound the total variation in terms of Hellinger distance and use a series of inequalities to bound it.\n\n2.2 Data acquisition mechanism with two-part privacy guarantees\n\nWe next describe the utility functions of the users and the platform and then formulate the platform\u2019s\noptimal data acquisition mechanism. As we described earlier, each user suffers from two privacy\nlosses when sharing her data. The first one is a central privacy loss because of the leakage of her\ninformation through the platform\u2019s output. The second one is a local privacy loss because of the\nleakage of her information through the raw data that she shares with the platform. Each user has a\nheterogeneous privacy sensitivity for these two types of privacy losses. To model such heterogeneity,\nfor each i \u2208 N , we let ci \u2208 [0, 1] be her relative local privacy sensitivity, representing the relative\nweight that user i assigns to the (per unit cost of) local privacy loss. We also let 1 \u2212 ci be her relative\ncentral privacy sensitivity, representing the relative weight that user i assigns to the (per unit cost of)\ncentral privacy loss. Therefore, ci \u2248 1 implies that user i suffers a higher loss of privacy by sharing\nher raw data with the platform (local privacy loss) compared to her loss from the platform\u2019s output\n\n1This assumption does not have any fundamental impact on the results and is made to simplify the notations.\n\n5\n\n\f(central privacy loss). Differently, ci \u2248 0 implies that the user suffers a smaller loss of privacy by\nsharing her raw data with the platform compared to her loss from the platform\u2019s output. In what\nfollows, we use the term privacy sensitivity instead of relative local privacy sensitivity.\n\nFor each i \u2208 N , the privacy sensitivity ci is independently drawn from a publicly known distribution\nwhose support is [0, 1] with cumulative distribution and probability density functions Fi(\u00b7) and fi(\u00b7).\nWe also let c = (c1, . . . , cn) be the vector of privacy sensitivities. The privacy sensitivity of each\nuser is her private information, i.e., the platform does not know it. This is because individuals have\ndifferent views regarding how trustworthy the platform is in protecting their raw data.\n\nThe platform\u2019s objective is to design a mechanism to collect users\u2019 data by paying them to compensate\nfor their privacy losses without knowing the privacy sensitivity of users. To introduce the platform\u2019s\nobjective formally, we adopt the formalism of Bayesian mechanism design pioneered by Myerson\n[1981]. More specifically, the platform designs and announces a payment function, a local privacy loss\nfunction, and a central privacy loss function that are mappings from the reported privacy sensitivities\nof users. The users then report their privacy sensitivities (which may or may not be truthful). Based\non the payment function, the platform compensates the users (the compensation could be monetary\nor some free or discounted service provided to the user). Based on the local and central privacy\nfunctions, the platform designs randomized channels and randomized estimation algorithms that\ndeliver the guaranteed local and central privacy losses while minimizing the sum of the mean squared\nerror and the total expected payments. Given this interaction, we next formally introduce a data\nacquisition mechanism with two-part data privacy guarantees.\n\nDefinition 4 (two-part private data acquisition mechanism). We call the tuple (\u02c6\u03b8, \u03b5(l), \u03b5(c), t) a\ntwo-part private data acquisition mechanism where\n\n1. For all i \u2208 N , \u03b5(l)\ni\n\n: Rn\n\n+ \u2192 R+ is a function that maps the vector of privacy sensitivities c\n\nto a local privacy loss for user i, \u03b5(l)\n\ni (c), with \u03b5(l)(\u00b7) = (\u03b5(l)\n\ni (\u00b7))n\n\ni=1.\n\n2. For all i \u2208 N , \u03b5(c)\n\ni\n\n: Rn\n\n+ \u2192 R+ is a function that maps the vector of privacy sensitivities c\n\nto a central privacy loss for user i, \u03b5(c)\n\ni (c), with \u03b5(c)(.) = (\u03b5(c)\n\ni (\u00b7))n\n\ni=1.\n\n3. \u02c6\u03b8 : X n \u00d7 Rn\n\n+ \u00d7 Rn\n\nacquired locally (\u03b5(l)\nestimate \u02c6\u03b8(x, \u03b5(l)(c), \u03b5(c)(c)).\n\n+ \u2192 R is a (\u03b5(c)(c), \u03b1)-R\u00e9nyi differentially private estimator that maps\ni (c), \u03b1)-R\u00e9nyi differentially private data of user i for i \u2208 N to an\n\n4. For all i \u2208 N , ti : Rn\n\n+ \u2192 R+ is a function that maps the vector of privacy sensitivities c to\n\na payment for user i, ti(c), with t(.) = (ti(\u00b7))n\n\ni=1.\n\nNotice that we have not specified the estimator and the mechanisms that delivers (local and central)\nR\u00e9nyi differential privacy. In the rest of this subsection, we introduce the utilities and the platform\u2019s\nproblem for a general estimators and mechanisms to deliver differential privacy. Later, we focus on\nlinear estimator and Gaussian mechanisms and explicitly solve the platform\u2019s problem.\n\nEach user that participates in a two-part private data acquisition mechanism suffers from both the\nlocal and central privacy losses and need to be compensated by the platform. In particular, the utility\nof user i from participation when her privacy sensitivity is ci and she reports c\u2032\n\ni is given by\n\nui(\u03b5(l)(c\u2032\n\ni, c\u2212i), \u03b5(c)(c\u2032\n\ni, c\u2212i), t, \u02c6\u03b8) = Ec\u2212i[ti(c\u2212i, c\u2032\n\ni)) \u2212 ci\u03b5(l)\n\ni (c\u2212i, c\u2032\n\ni) \u2212 (1 \u2212 ci)\u03b5(c)\n\ni (c\u2212i, c\u2032\n\ni)],\n\ni)) is the payment from the platform, the term ci\u03b5(l)\n\nwhere the term ti(c\u2212i, c\u2032\ni) is the relative lo-\ncal privacy sensitivity of the user multiplied by her local privacy loss, and the term (1\u2212ci)\u03b5(c)\ni (c\u2212i, c\u2032\ni)\nis her relative central privacy sensitivity multiplied by her central privacy loss. A user i \u2208 N that\ndoes not participate in the mechanism neither compromises her privacy nor gets a compensation.\nTherefore, the utility of a user who does not participate in the mechanism becomes 0.\n\ni (c\u2212i, c\u2032\n\nThe goal of the platform is to minimize the sum of the mean squared error and the overall payment to\nusers. We let \u03b3 \u2208 R+ represents the relative weight of the mean estimation error and the payments in\n\n6\n\n\fthe platform\u2019s objective.2 Therefore, the platform\u2019s objective is\nn\n(cid:88)\n\nEc[\u03b3MSE(\u03b5(l)(c), \u03b5(c)(c), \u02c6\u03b8) +\n\nti(c)],\n\ni=1\n\nwhere the first term is the mean square error of estimator \u02c6\u03b8 given reported vector of privacy sensitivity\nand resulting local and central privacy losses \u03b5(l) and \u03b5(c), i.e.,\n\nMSE(\u03b5(l)(c), \u03b5(c)(c), \u02c6\u03b8) = Ex[|\u02c6\u03b8(\u02c6x, \u03b5(l), \u03b5(c)) \u2212 \u03b8|2].\nAlso, each summand of the second term is the compensation that the platform gives to a user to\nincentivize her to participate and report her privacy sensitivity truthfully.\n\nIn Appendix we prove that, similar to the classical mechanism design setting, revelation principle\nholds. This means that there is no loss of generality in focusing on the class of direct incentive\ncompatible mechanisms, meaning the platform\u2019s optimization problem can be written as\n\nmin\n\u03b5(l)(\u00b7),\u03b5(c)(\u00b7),t(\u00b7)\n\nEc[\u03b3MSE(\u03b5(l)(c), \u03b5(c)(c), \u02c6\u03b8) +\n\nn\n(cid:88)\n\nti(c)]\n\ni=1\nui(\u03b5(l)(c), \u03b5(c)(c), t, \u02c6\u03b8) \u2265 ui(\u03b5(l)(c\u2032\nui(\u03b5(l)(c), \u03b5(c)(c), t, \u02c6\u03b8) \u2265 0\n\ni, c\u2212i), \u03b5(c)(c\u2032\n\ni, c\u2212i), t, \u02c6\u03b8)\n\n(5)\nwhere the constraints in (4) represent the incentive compatibility. These constraints guarantee that\nthat each user i has no incentive to misrepresent her privacy sensitivity when others report truthfully\n(reporting truthfully is an equilibrium of the game among the users). Also, the constraints in (5)\nrepresent individual rationality, which ensures that each user receives a non-negative utility from\nparticipating in the platform\u2019s mechanism and sharing her data.\n\nfor all i \u2208 N , ci,\n\n(3)\n\n(4)\n\n3 From the mechanism design problem to an optimization problem\n\nFor a given estimator \u02c6\u03b8, the platform\u2019s decision comprises the local and central privacy loss functions\n\u03b5(l)(\u00b7) and \u03b5(c)(\u00b7) together with the payment functions t(\u00b7). We next show that this problem can be\nequivalently formulated as an optimization problem over the vector of local privacy losses and central\nprivacy losses (as opposed to functions). In the rest of the paper, we impose the following assumption\nwhich is well-known in the mechanism design literature and simplifies the analysis.3\nAssumption 1. For any user i \u2208 N , the virtual cost defined as \u03c8i(c) = c + Fi(c)\nfi(c) is increasing in c,\nwhere fi(\u00b7) and Fi(\u00b7) are probability density and cumulative distribution functions of ci, respectively.\n\nThe above assumption holds for a wide class of distributions such as the ones with log-concave\ndensity functions (e.g., uniform).\nTheorem 2. Suppose Assumption 1 holds. For a given estimator \u02c6\u03b8 : \u02c6X n \u00d7 Rn\n+ \u2192 R, in the\noptimal two-part data acquisition mechanism, for a given vector of reported privacy sensitivities c,\nthe local and central privacy losses are the solution of\nn\n(cid:88)\n\n+ \u00d7 Rn\n\nn\n(cid:88)\n\n\u03b5(l)\ni \u03c8i(ci) +\n\n\u03b5(c)\ni (1 \u2212 \u03c8i(ci)).\n\n(6)\n\n\u03b3MSE(\u03b5(l), \u03b5(c), \u02c6\u03b8) +\n\nmin\ni=1,{\u03b5(c)}n\n\ni=1\n\n{\u03b5(l)}n\n\ni=1\n\ni=1\n\nProof Sketch of Theorem 2: We introduce the following interim functions\n\nti(ci) =Ec\u2212i [t(ci, c\u2212i)],\n\ni (ci) = Ec\u2212i[\u03b5(l)\n\u03b5(l)\n\ni (ci, c\u2212i)],\n\nand \u03b5(c)\n\ni (ci) = Ec\u2212i[\u03b5(c)\n\ni (ci, c\u2212i)].\n\nWe first establish a payment identity that determines the optimal payment in terms of the optimal\nlocal and central privacy losses. In particular, by evaluating the first order condition corresponding to\nthe incentive compatibility constraint (4), we establish that this constraint holds if and only if\n\nti(ci) = ti(0) + \u03b5(c)\n\ni (ci) \u2212 \u03b5(c)\n\ni (0) + ci(\u03b5(l)\n\ni (ci) \u2212 \u03b5(c)\n\ni (ci)) \u2212\n\n(cid:90) ci\n\n0\n\n(\u03b5(l)\n\ni (z) \u2212 \u03b5(c)\n\ni (z))dz,\n\n2Notice that changing the parameter \u03b3 enables us to study a wide range of platform\u2019s objectives with differing\n\nrelative weights between the estimation error and the total payments.\n\n3Without this assumption, extending the results requires ironing technique of Myerson [1981].\n\n7\n\n\fi (z) \u2212 \u03b5(c)\n\nand \u03b5(l)\ni (z) is weakly decreasing in z. We then plug in this payment identity back to the\nplatform\u2019s objective, use the individual rationality constraint, and rewrite the platform\u2019s expected\nutility in terms of the privacy loss functions and the virtual cost of users. This is still a functional\noptimization problem in terms of \u03b5(l)(\u00b7) and \u03b5(c)(\u00b7). However, we establish that, under Assumption 1,\nwe can solve this functional optimization point-wise (i.e., for any given c). \u25a0\n\nTheorem 2 highlights the tradeoff in the platform\u2019s problem: by decreasing the local privacy loss,\nthe second term of the objective decreases (this term corresponds to the payment to users) while\nthe first term (i.e., the mean squared error) increases. The role of the central privacy loss is more\nnuanced, and there are two cases. If the coefficient 1 \u2212 \u03c8i(ci) is non-negative, by decreasing the\ncentral privacy loss, the third term of the objective decreases while the first term increases. If the\ncoefficient 1 \u2212 \u03c8i(ci) is negative, increasing the central privacy loss decreases both the third term\nand the first term. However, we cannot increase the central privacy loss level without limits because\nthe central privacy loss level is always below the local privacy loss level. Therefore, the platform\u2019s\noptimal mechanism should find the \u201cright\u201d balance between these terms.\n\n4 Optimal mechanism with two-part privacy guarantees for linear estimators\n\nFor the rest of the paper, we focus on linear estimators with Gaussian mechanism described in Section\n2.1. The following is a direct corollary of Theorem 2.\nCorollary 1. Suppose Assumption 1 holds. For any reported vector of privacy sensitivities c, the\noptimal local privacy loss levels are \u03b5(l)\nn\n(cid:88)\n\ni and the optimal central privacy loss levels are\n\ni (c) = y\u2217\n\n2\n\nwhere (w\u2217\n\n1, . . . , w\u2217\n\nn) and (y\u2217\n\n1, . . . , y\u2217\n\nn) are the optimal solution of\n\n\u03b5(c)\ni = w\u2217\ni\n\n2/\n\nw\u2217\nj\ny\u2217\nj\n\nj=1\n\nmin\nw,y\n\nVAR\u03b3\n\nn\n(cid:88)\n\ni=1\n\nw2\n\ni +\n\n\u03b3\u03b1\n2\n\nn\n(cid:88)\n\ni=1\n\nw2\ni\nyi\n\n+\n\ns.t. wi, yi \u2265 0, for all i \u2208 N and\n\nn\n(cid:88)\n\ni=1\n\nn\n(cid:88)\n\ni=1\n\n(1 \u2212 \u03c8i(ci))\n\nw2\ni\n\n(cid:80)n\n\nj=1\n\nw2\nj\nyj\n\n+\n\nn\n(cid:88)\n\ni=1\n\n\u03c8i(ci)yi\n\n(7)\n\nwi = 1.\n\nLet us highlight the difference between our characterization and that of classic mechanism design (e.g.,\nMyerson [1981]). In classic mechanism design, the designer\u2019s problem becomes linear optimization.\nHowever, in our setting, the designer\u2019s problem is a non-linear and non-convex optimization. This\nmakes the problem of finding the optimal two-part data acquisition mechanism challenging. Before\naddressing this computational challenge, let us revisit the form of the Gaussian mechanism that we\nhave adopted: the platform adds Gaussian noise locally and then outputs a convex combination of\nthe privatized users\u2019 data without adding any noise centrally. More specifically, one may guess that\nthe platform may benefit by having a central noise added to the final output in addition to the local\nnoises. In the following subsection, we establish that there is another Gaussian mechanism for any\nGaussian mechanism that only adds local noises and achieves a weakly lower cost.\n\n4.1 Optimality of having only local noises in the Gaussian mechanism\n\nThe platform has the opportunity of adding Gaussian noise to both the raw data of each user and the\nfinal estimator and ex-ante one may guess that it is optimal to use both of these instruments. However,\nas we establish next, interestingly, in the optimal two-part data acquisition mechanism, it is always\noptimal to only add noises locally.\nFor a given vector of local privacy losses (\u03b5(l)\nGaussian mechanism with both local and central noises is of the form\n\nn ) and central privacy losses (\u03b5(c)\n\n1 , . . . , \u03b5(c)\n\n1 , . . . , \u03b5(l)\n\nn ), a\n\n\u02c6\u03b8(x1, . . . , xn) :=\n\nn\n(cid:88)\n\ni=1\n\nwi \u02c6xi + N (0,\n\n\u03b1\n2\u03b5\n\n) where\n\nn\n(cid:88)\n\ni=1\n\nwi = 1 and \u02c6xi = xi + N (0,\n\n\u03b1\n2\u03b5(l)\ni\n\n)\u2200i \u2208 N .\n\nProposition 1. In the optimal two-part data acquisition mechanism that adopts a Gaussian mecha-\nnism with both local and central noises, we have \u03b5 = \u221e.\n\n8\n\n\fAlgorithm 1: Computing the optimal two-part private data acquisition mechanism\nInput: The vector of privacy sensitivities (c1, . . . , cn)\nfor S \u2208 Grid (cid:0)[S, \u00afS], \u03b4(cid:1) do\n\nLet\n\n\u03bdi =\n\n1\n\u03b3 VAR + (1 \u2212 \u03c8i(ci))/S\n\n, \u03b6i =\n\n\u03bdi\nj \u03bdj\n\n(cid:80)\n\n, \u03bei = \u03b6i\n\n(cid:113)\n\n\u03bdj(\n\n\u03c8j(cj) \u2212 (cid:112)\u03c8i(ci))\n\n\uf8f6\n\n\uf8f8 .\n\n\uf8eb\n\n\uf8ed\n\nn\n(cid:88)\n\nj=1\n\n(cid:18) (cid:80)n\n\ni=1 \u03b6i\n\n\u03c8i(ci)\n\nS\u2212(cid:80)n\n\ni=1\n\n\u03c8i(ci)\u03bei\n\n\u221a\n\u221a\n\n(cid:19)2\n\n.\n\nLet p =\n\nLet\n\n\u03bdi + \u03bdi\n\nwi(S) =\n\n(cid:112)\u03c8j(cj)p\n\n(cid:80)\n\nj \u03bdj\n(cid:80)\nj \u03bdj\n\n\u2212 \u03bdi\n\n(cid:112)\u03c8i(ci)p,\n\nyi(S) = wi(S)\n\n(cid:114) p\n\n,\n\n\u03c8i(ci)\n\nand OBJ(S) be the objective of Problem (7) evaluated for this solution.\n\nend\nOutput: {yi(S\u2217), wi(S\u2217)}n\n\ni=1, where (S\u2217) = arg min(S) OBJ(S).\n\nProposition 1 has an important implication in terms of the design of data market architecture when\nusers have both central and local privacy costs: it is optimal to add noise locally! Adding a noise\ncentrally to the final estimator has an advantage because the weights in the final estimator give the\nplatform a lever to deliver heterogeneous central privacy guarantees to users. Despite this advantage,\nwe establish that adding noise centrally is never optimal. This is because the platform prefers to add\nthe noise locally to contribute to both central and local privacy guarantees delivered to users.\n\n4.2 Computing the optimal privacy loss function\n\nThe implementation of the optimal two-part private data acquisition mechanism requires solving\nProblem (7), which is a non-convex program. However, we use the structure of the problem to develop\na polynomial time algorithm to solve it approximately. To do so, we first replace (cid:80)n\ni /yi by an\nauxiliary variable S. Next, we consider the corresponding lagrangian problem. Using Karush-Kuhn-\nTucker (KKT) conditions, we establish a number of relations between problems\u2019 parameters, S, and\np, the lagrangian coefficient corresponding to S = (cid:80)n\ni /yi. Furthermore, we develop upper and\nlower bounds for S. Finally, we do a grid search to find the approximate optimal solution.\nTheorem 3. For any vector of reported privacy sensitivities and \u03f5 > 0, Algorithm 1 finds local privacy\nloss levels and the differentially private linear estimator of the two-part data acquisition mechanism\nwhose cost (i.e., platform\u2019s objective) is at most 1 + \u03b4 of the optimal cost in time poly(n, 1\n\ni=1 w2\n\ni=1 w2\n\n\u03b4 ).\n\nNotice that the approximation factor in Theorem 3 depends on the underlying parameters. Therefore,\nwe have a Polynomial Time Approximation Scheme (PTAS) for finding the optimal two-part data\nacquisition mechanism in the class of linear estimators.\n\nIn the Appendix, we provide a case study with two users to illustrate the performance of the optimal\ntwo-part data acquisition mechanism in terms of the guaranteed privacy levels and payments as\nfunctions of the reported privacy sensitivities.\n\n\n\nBased on the section(s), please answer the following question with yes, no, or n/a and provide a brief justification for your answer.\nQuestion: If the authors ran experiments, do the authors specify all the training details (e.g., data splits, hyperparameters, how they were chosen)?"}, "3c": {"role": "user", "content": "The following is the methods section of the paper you are reviewing:\n\n\nWe consider a platform interested in estimating a parameter \u03b8 \u2208 R by collecting data of n users,\nindexed by N = {1, \u00b7 \u00b7 \u00b7 , n}. For any i \u2208 N , we denote user i\u2019s data by Xi \u2208 X and we assume Xi\n\n3\n\n\fis given by Xi = \u03b8 + Zi where Z1, \u00b7 \u00b7 \u00b7 , Zn are independent and identically distributed zero-mean\nrandom variables with variance VAR. To simplify the exposition, we further assume |Zi| \u2264 1/2 for\nany i \u2208 N .1 Throughout, we use lower case letters to denote the realization of random variables. The\nplatform\u2019s goal is to minimize the estimate\u2019s error by collecting data from privacy-concerned users.\nTherefore, the platform needs to incentivize them to share their data.\n\n2.1 Local and central privacy losses\n\nBefore formalizing the utilities/objectives of the platform and the users, we define the notions of\nprivacy losses that we adopt in this paper. In particular, we consider two different types of privacy\nlosses that users suffer from. The first one is the privacy loss of a user when she shares her data (only)\nwith the platform, and the second one is the privacy loss through the released estimate (to the public)\nby the platform. Depending on how different users trust the platform, they might care differently\nabout these two privacy losses. For instance, if a user fully trusts the platform, then her main privacy\nconcern would be the second one, while a user who does not trust the platform at all would be more\nconcerned with the first one as the public only observes the aggregated estimate, as opposed to the\nplatform which observes each user\u2019s (shared) data separately.\n\nWe use the differential privacy framework to quantify these privacy losses. Since differential privacy\nwas introduced by Dwork et al., several variants of it have been also proposed. In particular, a popular\none in the machine learning literature is R\u00e9nyi differential privacy (RDP), introduced by Mironov\n[2017], which we also adopt in this paper. Let us first recall the definition of R\u00e9nyi divergence.\nDefinition 1. Let P and Q be two distributions over R with densities p and q. For any \u03b1 \u2208 (1, \u221e],\nthe R\u00e9nyi \u03b1-divergence between P and Q is denoted by D\u03b1(P ||Q) and is given by\n\nD\u03b1(P ||Q) :=\n\n1\n\u03b1 \u2212 1\n\nlog\n\n(cid:90) (cid:18) p(x)\nq(x)\n\n(cid:19)\u03b1\n\nq(x)dx.\n\nFor two random variables X and Y , D\u03b1(X||Y ) denotes the \u03b1-divergence between their distributions.\n\nWe next define two notions of differential privacy, known as central and local, to capture the two\naforementioned types of privacy losses. Local differential privacy corresponds to the privacy loss of a\nuser when she shares her data with the platform through a randomized mapping, known as a channel.\nDefinition 2. Let \u03b5 \u2265 0 and \u03b1 \u2208 (1, \u221e]. A randomized channel C : X \u2192 R is locally (\u03b5, \u03b1)-R\u00e9nyi\n(differentially) private if for any x, x\u2032 \u2208 X ,\n\nD\u03b1(C(x)||C(x\u2032)) \u2264 \u03b5.\n\nCentral differential privacy corresponds to the other privacy loss mentioned above. It bounds the\nchange in the distribution of the platform\u2019s output, i.e., the released estimate, by changing one user\u2019s\ndata. We next provide the formal definition.\ni=1 \u2208 Rn\nDefinition 3. Let \u03b5 = (\u03b5i)n\n(\u03b5, \u03b1)-R\u00e9nyi (differentially) private if for any two datasets x1:n, x\u2032\ncoordinate (data of user i),\n\n+ and \u03b1 \u2208 (1, \u221e]. A randomized algorithm A : X n \u2192 R is\n1:n \u2208 X n that only differ in i-th\n\nD\u03b1(A(x1:n)||A(x\u2032\n\n1:n)) \u2264 \u03b5i.\n\nThe customary approach to guarantee RDP is Gaussian mechanism in which a properly tuned zero-\nmean Gaussian noise is added to fulfill the required condition. The following lemma, adapted from\nMironov [2017], allows us to characterize the Gaussian noise\u2019s variance for a privacy loss level.\nLemma 1. For a function f : X n \u2192 R, we define its sensitivity with respect to i-th coordinate as\n\nLi(f ) := sup {|f (x1:n) \u2212 f (x\u2032\n\n1:n)| : for all x1:n and x\u2032\n\n1:n differing only at i-th coordinate} .\n\nFor any \u03b1 \u2208 (1, \u221e], A(x1:n) = f (x1:n) + W with W \u223c N (0, \u03c32) is\n\n(cid:16)\n\n( \u03b1Li(f )2\n2\u03c32\n\n)n\ni=1, \u03b1\n\n(cid:17)\n\n-RDP.\n\nFor a given vector of local privacy losses (\u03b5(l)\nis by using a linear estimator with Gaussian mechanism which is given by\n\n1 , . . . , \u03b5(l)\n\nn ), a natural way to privately estimate the mean\n\n\u02c6\u03b8(x1, . . . , xn) :=\n\nn\n(cid:88)\n\ni=1\n\nwi \u02c6xi where\n\nn\n(cid:88)\n\ni=1\n\nwi = 1 and \u02c6xi = xi + N (0,\n\n\u03b1\n2\u03b5(l)\ni\n\n) for all i \u2208 N .\n\n(1)\n\n4\n\n\fFigure 1: The interaction between the users and the platform in the two-part private data acquisition.\n\ni / (cid:80)n\n\ni = w2\n\nand the central privacy delivered to user i \u2208 N is \u03b5(c)\n\nFigure 1 depicts this estimator. In particular, using Lemma 1, the local privacy delivered to user\ni \u2208 N is \u03b5(l)\nj . In the next\ni\nsection, we focus on linear estimators in designing the optimal private data acquisition mechanism.\nWe motivate such specification by showing that, for a given vector of local privacy losses (\u03b5(l)\ni=1,\na linear estimator is optimal with respect to mean square error. To formalize this statement, we\nfirst need to define the minimax estimation error as the notion of optimality. Let P be a class of\ndistributions over X . For any P \u2208 P, we denote its mean by \u03b8(P ). A (\u03b5(l)\ni=1-locally RDP estimator\ncan be cast as \u02c6\u03b8((Ci(xi))n\ni=1), where Ci(.) is the randomized channel corresponding to user i. Let\nQ((\u03b5(l)\ni=1-locally RDP estimators. The minimax estimation error is\ni=1,\u02c6\u03b8[|\u02c6\u03b8((Ci(Xi))n\n\ni=1) be the class of such (\u03b5(l)\n\ni=1) \u2212 \u03b8(P )|2],\n\nL(P, Q, (\u03b5(l)\n\nj=1 w2\n\ni=1) :=\n\nj /\u03b5(l)\n\n(Xi\u223cP )n\n\ni )n\n\ni )n\n\ni )n\n\ni )n\n\ni )n\n\n(2)\n\nE\n\ninf\ni=1\u2208Q((\u03b5(l)\n\ni )n\n\ni=1)\n\nsup\nP \u2208P\n\n\u02c6\u03b8,{Ci}n\n\nwhere the expectation is taken over both randomness of data and estimator (including private channels).\nIn other words, the optimal estimator is the one that has the lowest worst case error among all\nestimators that satisfy the privacy requirements. We next state the optimality result.\nTheorem 1. Assume \u03b1 \u2265 2 and \u03b5(l)\nand C1, \u00b7 \u00b7 \u00b7 , Cn be independent channels. Then, there exists a universal constant c such that\n\ni \u2264 1 for all i. Let P1 be the family of distributions over [\u2212 1\n\n2 , 1\n2 ]\n\nL(P, Q, (\u03b5(l)\n\ni )n\n\ni=1) \u2265 c min(\n\n1\ni=1 \u03b5(l)\n\ni\n\n(cid:80)n\n\n, 1).\n\nFurthermore, there exists a linear estimator with Gaussian mechanism such that\n\nE\n\n(Xi\u223cP )n\n\ni=1,\u02c6\u03b8[|\u02c6\u03b8((Ci(Xi))n\n\ni=1) \u2212 \u03b8(P )|2] \u2264 O(1)\n\n\u03b1\ni=1 \u03b5(l)\n\ni\n\n(cid:80)n\n\n.\n\nWe prove the lower bound by using the Le Cam\u2019s method Yu [1997] that reduces the problem of\nfinding lower bounds to a hypothesis testing problem and requires bounding the total variation\ndistance between the estimates when the input data points change. We then find it more convenient to\nbound the total variation in terms of Hellinger distance and use a series of inequalities to bound it.\n\n2.2 Data acquisition mechanism with two-part privacy guarantees\n\nWe next describe the utility functions of the users and the platform and then formulate the platform\u2019s\noptimal data acquisition mechanism. As we described earlier, each user suffers from two privacy\nlosses when sharing her data. The first one is a central privacy loss because of the leakage of her\ninformation through the platform\u2019s output. The second one is a local privacy loss because of the\nleakage of her information through the raw data that she shares with the platform. Each user has a\nheterogeneous privacy sensitivity for these two types of privacy losses. To model such heterogeneity,\nfor each i \u2208 N , we let ci \u2208 [0, 1] be her relative local privacy sensitivity, representing the relative\nweight that user i assigns to the (per unit cost of) local privacy loss. We also let 1 \u2212 ci be her relative\ncentral privacy sensitivity, representing the relative weight that user i assigns to the (per unit cost of)\ncentral privacy loss. Therefore, ci \u2248 1 implies that user i suffers a higher loss of privacy by sharing\nher raw data with the platform (local privacy loss) compared to her loss from the platform\u2019s output\n\n1This assumption does not have any fundamental impact on the results and is made to simplify the notations.\n\n5\n\n\f(central privacy loss). Differently, ci \u2248 0 implies that the user suffers a smaller loss of privacy by\nsharing her raw data with the platform compared to her loss from the platform\u2019s output. In what\nfollows, we use the term privacy sensitivity instead of relative local privacy sensitivity.\n\nFor each i \u2208 N , the privacy sensitivity ci is independently drawn from a publicly known distribution\nwhose support is [0, 1] with cumulative distribution and probability density functions Fi(\u00b7) and fi(\u00b7).\nWe also let c = (c1, . . . , cn) be the vector of privacy sensitivities. The privacy sensitivity of each\nuser is her private information, i.e., the platform does not know it. This is because individuals have\ndifferent views regarding how trustworthy the platform is in protecting their raw data.\n\nThe platform\u2019s objective is to design a mechanism to collect users\u2019 data by paying them to compensate\nfor their privacy losses without knowing the privacy sensitivity of users. To introduce the platform\u2019s\nobjective formally, we adopt the formalism of Bayesian mechanism design pioneered by Myerson\n[1981]. More specifically, the platform designs and announces a payment function, a local privacy loss\nfunction, and a central privacy loss function that are mappings from the reported privacy sensitivities\nof users. The users then report their privacy sensitivities (which may or may not be truthful). Based\non the payment function, the platform compensates the users (the compensation could be monetary\nor some free or discounted service provided to the user). Based on the local and central privacy\nfunctions, the platform designs randomized channels and randomized estimation algorithms that\ndeliver the guaranteed local and central privacy losses while minimizing the sum of the mean squared\nerror and the total expected payments. Given this interaction, we next formally introduce a data\nacquisition mechanism with two-part data privacy guarantees.\n\nDefinition 4 (two-part private data acquisition mechanism). We call the tuple (\u02c6\u03b8, \u03b5(l), \u03b5(c), t) a\ntwo-part private data acquisition mechanism where\n\n1. For all i \u2208 N , \u03b5(l)\ni\n\n: Rn\n\n+ \u2192 R+ is a function that maps the vector of privacy sensitivities c\n\nto a local privacy loss for user i, \u03b5(l)\n\ni (c), with \u03b5(l)(\u00b7) = (\u03b5(l)\n\ni (\u00b7))n\n\ni=1.\n\n2. For all i \u2208 N , \u03b5(c)\n\ni\n\n: Rn\n\n+ \u2192 R+ is a function that maps the vector of privacy sensitivities c\n\nto a central privacy loss for user i, \u03b5(c)\n\ni (c), with \u03b5(c)(.) = (\u03b5(c)\n\ni (\u00b7))n\n\ni=1.\n\n3. \u02c6\u03b8 : X n \u00d7 Rn\n\n+ \u00d7 Rn\n\nacquired locally (\u03b5(l)\nestimate \u02c6\u03b8(x, \u03b5(l)(c), \u03b5(c)(c)).\n\n+ \u2192 R is a (\u03b5(c)(c), \u03b1)-R\u00e9nyi differentially private estimator that maps\ni (c), \u03b1)-R\u00e9nyi differentially private data of user i for i \u2208 N to an\n\n4. For all i \u2208 N , ti : Rn\n\n+ \u2192 R+ is a function that maps the vector of privacy sensitivities c to\n\na payment for user i, ti(c), with t(.) = (ti(\u00b7))n\n\ni=1.\n\nNotice that we have not specified the estimator and the mechanisms that delivers (local and central)\nR\u00e9nyi differential privacy. In the rest of this subsection, we introduce the utilities and the platform\u2019s\nproblem for a general estimators and mechanisms to deliver differential privacy. Later, we focus on\nlinear estimator and Gaussian mechanisms and explicitly solve the platform\u2019s problem.\n\nEach user that participates in a two-part private data acquisition mechanism suffers from both the\nlocal and central privacy losses and need to be compensated by the platform. In particular, the utility\nof user i from participation when her privacy sensitivity is ci and she reports c\u2032\n\ni is given by\n\nui(\u03b5(l)(c\u2032\n\ni, c\u2212i), \u03b5(c)(c\u2032\n\ni, c\u2212i), t, \u02c6\u03b8) = Ec\u2212i[ti(c\u2212i, c\u2032\n\ni)) \u2212 ci\u03b5(l)\n\ni (c\u2212i, c\u2032\n\ni) \u2212 (1 \u2212 ci)\u03b5(c)\n\ni (c\u2212i, c\u2032\n\ni)],\n\ni)) is the payment from the platform, the term ci\u03b5(l)\n\nwhere the term ti(c\u2212i, c\u2032\ni) is the relative lo-\ncal privacy sensitivity of the user multiplied by her local privacy loss, and the term (1\u2212ci)\u03b5(c)\ni (c\u2212i, c\u2032\ni)\nis her relative central privacy sensitivity multiplied by her central privacy loss. A user i \u2208 N that\ndoes not participate in the mechanism neither compromises her privacy nor gets a compensation.\nTherefore, the utility of a user who does not participate in the mechanism becomes 0.\n\ni (c\u2212i, c\u2032\n\nThe goal of the platform is to minimize the sum of the mean squared error and the overall payment to\nusers. We let \u03b3 \u2208 R+ represents the relative weight of the mean estimation error and the payments in\n\n6\n\n\fthe platform\u2019s objective.2 Therefore, the platform\u2019s objective is\nn\n(cid:88)\n\nEc[\u03b3MSE(\u03b5(l)(c), \u03b5(c)(c), \u02c6\u03b8) +\n\nti(c)],\n\ni=1\n\nwhere the first term is the mean square error of estimator \u02c6\u03b8 given reported vector of privacy sensitivity\nand resulting local and central privacy losses \u03b5(l) and \u03b5(c), i.e.,\n\nMSE(\u03b5(l)(c), \u03b5(c)(c), \u02c6\u03b8) = Ex[|\u02c6\u03b8(\u02c6x, \u03b5(l), \u03b5(c)) \u2212 \u03b8|2].\nAlso, each summand of the second term is the compensation that the platform gives to a user to\nincentivize her to participate and report her privacy sensitivity truthfully.\n\nIn Appendix we prove that, similar to the classical mechanism design setting, revelation principle\nholds. This means that there is no loss of generality in focusing on the class of direct incentive\ncompatible mechanisms, meaning the platform\u2019s optimization problem can be written as\n\nmin\n\u03b5(l)(\u00b7),\u03b5(c)(\u00b7),t(\u00b7)\n\nEc[\u03b3MSE(\u03b5(l)(c), \u03b5(c)(c), \u02c6\u03b8) +\n\nn\n(cid:88)\n\nti(c)]\n\ni=1\nui(\u03b5(l)(c), \u03b5(c)(c), t, \u02c6\u03b8) \u2265 ui(\u03b5(l)(c\u2032\nui(\u03b5(l)(c), \u03b5(c)(c), t, \u02c6\u03b8) \u2265 0\n\ni, c\u2212i), \u03b5(c)(c\u2032\n\ni, c\u2212i), t, \u02c6\u03b8)\n\n(5)\nwhere the constraints in (4) represent the incentive compatibility. These constraints guarantee that\nthat each user i has no incentive to misrepresent her privacy sensitivity when others report truthfully\n(reporting truthfully is an equilibrium of the game among the users). Also, the constraints in (5)\nrepresent individual rationality, which ensures that each user receives a non-negative utility from\nparticipating in the platform\u2019s mechanism and sharing her data.\n\nfor all i \u2208 N , ci,\n\n(3)\n\n(4)\n\n3 From the mechanism design problem to an optimization problem\n\nFor a given estimator \u02c6\u03b8, the platform\u2019s decision comprises the local and central privacy loss functions\n\u03b5(l)(\u00b7) and \u03b5(c)(\u00b7) together with the payment functions t(\u00b7). We next show that this problem can be\nequivalently formulated as an optimization problem over the vector of local privacy losses and central\nprivacy losses (as opposed to functions). In the rest of the paper, we impose the following assumption\nwhich is well-known in the mechanism design literature and simplifies the analysis.3\nAssumption 1. For any user i \u2208 N , the virtual cost defined as \u03c8i(c) = c + Fi(c)\nfi(c) is increasing in c,\nwhere fi(\u00b7) and Fi(\u00b7) are probability density and cumulative distribution functions of ci, respectively.\n\nThe above assumption holds for a wide class of distributions such as the ones with log-concave\ndensity functions (e.g., uniform).\nTheorem 2. Suppose Assumption 1 holds. For a given estimator \u02c6\u03b8 : \u02c6X n \u00d7 Rn\n+ \u2192 R, in the\noptimal two-part data acquisition mechanism, for a given vector of reported privacy sensitivities c,\nthe local and central privacy losses are the solution of\nn\n(cid:88)\n\n+ \u00d7 Rn\n\nn\n(cid:88)\n\n\u03b5(l)\ni \u03c8i(ci) +\n\n\u03b5(c)\ni (1 \u2212 \u03c8i(ci)).\n\n(6)\n\n\u03b3MSE(\u03b5(l), \u03b5(c), \u02c6\u03b8) +\n\nmin\ni=1,{\u03b5(c)}n\n\ni=1\n\n{\u03b5(l)}n\n\ni=1\n\ni=1\n\nProof Sketch of Theorem 2: We introduce the following interim functions\n\nti(ci) =Ec\u2212i [t(ci, c\u2212i)],\n\ni (ci) = Ec\u2212i[\u03b5(l)\n\u03b5(l)\n\ni (ci, c\u2212i)],\n\nand \u03b5(c)\n\ni (ci) = Ec\u2212i[\u03b5(c)\n\ni (ci, c\u2212i)].\n\nWe first establish a payment identity that determines the optimal payment in terms of the optimal\nlocal and central privacy losses. In particular, by evaluating the first order condition corresponding to\nthe incentive compatibility constraint (4), we establish that this constraint holds if and only if\n\nti(ci) = ti(0) + \u03b5(c)\n\ni (ci) \u2212 \u03b5(c)\n\ni (0) + ci(\u03b5(l)\n\ni (ci) \u2212 \u03b5(c)\n\ni (ci)) \u2212\n\n(cid:90) ci\n\n0\n\n(\u03b5(l)\n\ni (z) \u2212 \u03b5(c)\n\ni (z))dz,\n\n2Notice that changing the parameter \u03b3 enables us to study a wide range of platform\u2019s objectives with differing\n\nrelative weights between the estimation error and the total payments.\n\n3Without this assumption, extending the results requires ironing technique of Myerson [1981].\n\n7\n\n\fi (z) \u2212 \u03b5(c)\n\nand \u03b5(l)\ni (z) is weakly decreasing in z. We then plug in this payment identity back to the\nplatform\u2019s objective, use the individual rationality constraint, and rewrite the platform\u2019s expected\nutility in terms of the privacy loss functions and the virtual cost of users. This is still a functional\noptimization problem in terms of \u03b5(l)(\u00b7) and \u03b5(c)(\u00b7). However, we establish that, under Assumption 1,\nwe can solve this functional optimization point-wise (i.e., for any given c). \u25a0\n\nTheorem 2 highlights the tradeoff in the platform\u2019s problem: by decreasing the local privacy loss,\nthe second term of the objective decreases (this term corresponds to the payment to users) while\nthe first term (i.e., the mean squared error) increases. The role of the central privacy loss is more\nnuanced, and there are two cases. If the coefficient 1 \u2212 \u03c8i(ci) is non-negative, by decreasing the\ncentral privacy loss, the third term of the objective decreases while the first term increases. If the\ncoefficient 1 \u2212 \u03c8i(ci) is negative, increasing the central privacy loss decreases both the third term\nand the first term. However, we cannot increase the central privacy loss level without limits because\nthe central privacy loss level is always below the local privacy loss level. Therefore, the platform\u2019s\noptimal mechanism should find the \u201cright\u201d balance between these terms.\n\n4 Optimal mechanism with two-part privacy guarantees for linear estimators\n\nFor the rest of the paper, we focus on linear estimators with Gaussian mechanism described in Section\n2.1. The following is a direct corollary of Theorem 2.\nCorollary 1. Suppose Assumption 1 holds. For any reported vector of privacy sensitivities c, the\noptimal local privacy loss levels are \u03b5(l)\nn\n(cid:88)\n\ni and the optimal central privacy loss levels are\n\ni (c) = y\u2217\n\n2\n\nwhere (w\u2217\n\n1, . . . , w\u2217\n\nn) and (y\u2217\n\n1, . . . , y\u2217\n\nn) are the optimal solution of\n\n\u03b5(c)\ni = w\u2217\ni\n\n2/\n\nw\u2217\nj\ny\u2217\nj\n\nj=1\n\nmin\nw,y\n\nVAR\u03b3\n\nn\n(cid:88)\n\ni=1\n\nw2\n\ni +\n\n\u03b3\u03b1\n2\n\nn\n(cid:88)\n\ni=1\n\nw2\ni\nyi\n\n+\n\ns.t. wi, yi \u2265 0, for all i \u2208 N and\n\nn\n(cid:88)\n\ni=1\n\nn\n(cid:88)\n\ni=1\n\n(1 \u2212 \u03c8i(ci))\n\nw2\ni\n\n(cid:80)n\n\nj=1\n\nw2\nj\nyj\n\n+\n\nn\n(cid:88)\n\ni=1\n\n\u03c8i(ci)yi\n\n(7)\n\nwi = 1.\n\nLet us highlight the difference between our characterization and that of classic mechanism design (e.g.,\nMyerson [1981]). In classic mechanism design, the designer\u2019s problem becomes linear optimization.\nHowever, in our setting, the designer\u2019s problem is a non-linear and non-convex optimization. This\nmakes the problem of finding the optimal two-part data acquisition mechanism challenging. Before\naddressing this computational challenge, let us revisit the form of the Gaussian mechanism that we\nhave adopted: the platform adds Gaussian noise locally and then outputs a convex combination of\nthe privatized users\u2019 data without adding any noise centrally. More specifically, one may guess that\nthe platform may benefit by having a central noise added to the final output in addition to the local\nnoises. In the following subsection, we establish that there is another Gaussian mechanism for any\nGaussian mechanism that only adds local noises and achieves a weakly lower cost.\n\n4.1 Optimality of having only local noises in the Gaussian mechanism\n\nThe platform has the opportunity of adding Gaussian noise to both the raw data of each user and the\nfinal estimator and ex-ante one may guess that it is optimal to use both of these instruments. However,\nas we establish next, interestingly, in the optimal two-part data acquisition mechanism, it is always\noptimal to only add noises locally.\nFor a given vector of local privacy losses (\u03b5(l)\nGaussian mechanism with both local and central noises is of the form\n\nn ) and central privacy losses (\u03b5(c)\n\n1 , . . . , \u03b5(c)\n\n1 , . . . , \u03b5(l)\n\nn ), a\n\n\u02c6\u03b8(x1, . . . , xn) :=\n\nn\n(cid:88)\n\ni=1\n\nwi \u02c6xi + N (0,\n\n\u03b1\n2\u03b5\n\n) where\n\nn\n(cid:88)\n\ni=1\n\nwi = 1 and \u02c6xi = xi + N (0,\n\n\u03b1\n2\u03b5(l)\ni\n\n)\u2200i \u2208 N .\n\nProposition 1. In the optimal two-part data acquisition mechanism that adopts a Gaussian mecha-\nnism with both local and central noises, we have \u03b5 = \u221e.\n\n8\n\n\fAlgorithm 1: Computing the optimal two-part private data acquisition mechanism\nInput: The vector of privacy sensitivities (c1, . . . , cn)\nfor S \u2208 Grid (cid:0)[S, \u00afS], \u03b4(cid:1) do\n\nLet\n\n\u03bdi =\n\n1\n\u03b3 VAR + (1 \u2212 \u03c8i(ci))/S\n\n, \u03b6i =\n\n\u03bdi\nj \u03bdj\n\n(cid:80)\n\n, \u03bei = \u03b6i\n\n(cid:113)\n\n\u03bdj(\n\n\u03c8j(cj) \u2212 (cid:112)\u03c8i(ci))\n\n\uf8f6\n\n\uf8f8 .\n\n\uf8eb\n\n\uf8ed\n\nn\n(cid:88)\n\nj=1\n\n(cid:18) (cid:80)n\n\ni=1 \u03b6i\n\n\u03c8i(ci)\n\nS\u2212(cid:80)n\n\ni=1\n\n\u03c8i(ci)\u03bei\n\n\u221a\n\u221a\n\n(cid:19)2\n\n.\n\nLet p =\n\nLet\n\n\u03bdi + \u03bdi\n\nwi(S) =\n\n(cid:112)\u03c8j(cj)p\n\n(cid:80)\n\nj \u03bdj\n(cid:80)\nj \u03bdj\n\n\u2212 \u03bdi\n\n(cid:112)\u03c8i(ci)p,\n\nyi(S) = wi(S)\n\n(cid:114) p\n\n,\n\n\u03c8i(ci)\n\nand OBJ(S) be the objective of Problem (7) evaluated for this solution.\n\nend\nOutput: {yi(S\u2217), wi(S\u2217)}n\n\ni=1, where (S\u2217) = arg min(S) OBJ(S).\n\nProposition 1 has an important implication in terms of the design of data market architecture when\nusers have both central and local privacy costs: it is optimal to add noise locally! Adding a noise\ncentrally to the final estimator has an advantage because the weights in the final estimator give the\nplatform a lever to deliver heterogeneous central privacy guarantees to users. Despite this advantage,\nwe establish that adding noise centrally is never optimal. This is because the platform prefers to add\nthe noise locally to contribute to both central and local privacy guarantees delivered to users.\n\n4.2 Computing the optimal privacy loss function\n\nThe implementation of the optimal two-part private data acquisition mechanism requires solving\nProblem (7), which is a non-convex program. However, we use the structure of the problem to develop\na polynomial time algorithm to solve it approximately. To do so, we first replace (cid:80)n\ni /yi by an\nauxiliary variable S. Next, we consider the corresponding lagrangian problem. Using Karush-Kuhn-\nTucker (KKT) conditions, we establish a number of relations between problems\u2019 parameters, S, and\np, the lagrangian coefficient corresponding to S = (cid:80)n\ni /yi. Furthermore, we develop upper and\nlower bounds for S. Finally, we do a grid search to find the approximate optimal solution.\nTheorem 3. For any vector of reported privacy sensitivities and \u03f5 > 0, Algorithm 1 finds local privacy\nloss levels and the differentially private linear estimator of the two-part data acquisition mechanism\nwhose cost (i.e., platform\u2019s objective) is at most 1 + \u03b4 of the optimal cost in time poly(n, 1\n\ni=1 w2\n\ni=1 w2\n\n\u03b4 ).\n\nNotice that the approximation factor in Theorem 3 depends on the underlying parameters. Therefore,\nwe have a Polynomial Time Approximation Scheme (PTAS) for finding the optimal two-part data\nacquisition mechanism in the class of linear estimators.\n\nIn the Appendix, we provide a case study with two users to illustrate the performance of the optimal\ntwo-part data acquisition mechanism in terms of the guaranteed privacy levels and payments as\nfunctions of the reported privacy sensitivities.\n\n\n\nBased on the section(s), please answer the following question with yes, no, or n/a and provide a brief justification for your answer.\nQuestion: If the authors ran experiments, do the authors report error bars (e.g., with respect to the random seed after running experiments multiple times)?"}, "3d": {"role": "user", "content": "The following is the methods section of the paper you are reviewing:\n\n\nWe consider a platform interested in estimating a parameter \u03b8 \u2208 R by collecting data of n users,\nindexed by N = {1, \u00b7 \u00b7 \u00b7 , n}. For any i \u2208 N , we denote user i\u2019s data by Xi \u2208 X and we assume Xi\n\n3\n\n\fis given by Xi = \u03b8 + Zi where Z1, \u00b7 \u00b7 \u00b7 , Zn are independent and identically distributed zero-mean\nrandom variables with variance VAR. To simplify the exposition, we further assume |Zi| \u2264 1/2 for\nany i \u2208 N .1 Throughout, we use lower case letters to denote the realization of random variables. The\nplatform\u2019s goal is to minimize the estimate\u2019s error by collecting data from privacy-concerned users.\nTherefore, the platform needs to incentivize them to share their data.\n\n2.1 Local and central privacy losses\n\nBefore formalizing the utilities/objectives of the platform and the users, we define the notions of\nprivacy losses that we adopt in this paper. In particular, we consider two different types of privacy\nlosses that users suffer from. The first one is the privacy loss of a user when she shares her data (only)\nwith the platform, and the second one is the privacy loss through the released estimate (to the public)\nby the platform. Depending on how different users trust the platform, they might care differently\nabout these two privacy losses. For instance, if a user fully trusts the platform, then her main privacy\nconcern would be the second one, while a user who does not trust the platform at all would be more\nconcerned with the first one as the public only observes the aggregated estimate, as opposed to the\nplatform which observes each user\u2019s (shared) data separately.\n\nWe use the differential privacy framework to quantify these privacy losses. Since differential privacy\nwas introduced by Dwork et al., several variants of it have been also proposed. In particular, a popular\none in the machine learning literature is R\u00e9nyi differential privacy (RDP), introduced by Mironov\n[2017], which we also adopt in this paper. Let us first recall the definition of R\u00e9nyi divergence.\nDefinition 1. Let P and Q be two distributions over R with densities p and q. For any \u03b1 \u2208 (1, \u221e],\nthe R\u00e9nyi \u03b1-divergence between P and Q is denoted by D\u03b1(P ||Q) and is given by\n\nD\u03b1(P ||Q) :=\n\n1\n\u03b1 \u2212 1\n\nlog\n\n(cid:90) (cid:18) p(x)\nq(x)\n\n(cid:19)\u03b1\n\nq(x)dx.\n\nFor two random variables X and Y , D\u03b1(X||Y ) denotes the \u03b1-divergence between their distributions.\n\nWe next define two notions of differential privacy, known as central and local, to capture the two\naforementioned types of privacy losses. Local differential privacy corresponds to the privacy loss of a\nuser when she shares her data with the platform through a randomized mapping, known as a channel.\nDefinition 2. Let \u03b5 \u2265 0 and \u03b1 \u2208 (1, \u221e]. A randomized channel C : X \u2192 R is locally (\u03b5, \u03b1)-R\u00e9nyi\n(differentially) private if for any x, x\u2032 \u2208 X ,\n\nD\u03b1(C(x)||C(x\u2032)) \u2264 \u03b5.\n\nCentral differential privacy corresponds to the other privacy loss mentioned above. It bounds the\nchange in the distribution of the platform\u2019s output, i.e., the released estimate, by changing one user\u2019s\ndata. We next provide the formal definition.\ni=1 \u2208 Rn\nDefinition 3. Let \u03b5 = (\u03b5i)n\n(\u03b5, \u03b1)-R\u00e9nyi (differentially) private if for any two datasets x1:n, x\u2032\ncoordinate (data of user i),\n\n+ and \u03b1 \u2208 (1, \u221e]. A randomized algorithm A : X n \u2192 R is\n1:n \u2208 X n that only differ in i-th\n\nD\u03b1(A(x1:n)||A(x\u2032\n\n1:n)) \u2264 \u03b5i.\n\nThe customary approach to guarantee RDP is Gaussian mechanism in which a properly tuned zero-\nmean Gaussian noise is added to fulfill the required condition. The following lemma, adapted from\nMironov [2017], allows us to characterize the Gaussian noise\u2019s variance for a privacy loss level.\nLemma 1. For a function f : X n \u2192 R, we define its sensitivity with respect to i-th coordinate as\n\nLi(f ) := sup {|f (x1:n) \u2212 f (x\u2032\n\n1:n)| : for all x1:n and x\u2032\n\n1:n differing only at i-th coordinate} .\n\nFor any \u03b1 \u2208 (1, \u221e], A(x1:n) = f (x1:n) + W with W \u223c N (0, \u03c32) is\n\n(cid:16)\n\n( \u03b1Li(f )2\n2\u03c32\n\n)n\ni=1, \u03b1\n\n(cid:17)\n\n-RDP.\n\nFor a given vector of local privacy losses (\u03b5(l)\nis by using a linear estimator with Gaussian mechanism which is given by\n\n1 , . . . , \u03b5(l)\n\nn ), a natural way to privately estimate the mean\n\n\u02c6\u03b8(x1, . . . , xn) :=\n\nn\n(cid:88)\n\ni=1\n\nwi \u02c6xi where\n\nn\n(cid:88)\n\ni=1\n\nwi = 1 and \u02c6xi = xi + N (0,\n\n\u03b1\n2\u03b5(l)\ni\n\n) for all i \u2208 N .\n\n(1)\n\n4\n\n\fFigure 1: The interaction between the users and the platform in the two-part private data acquisition.\n\ni / (cid:80)n\n\ni = w2\n\nand the central privacy delivered to user i \u2208 N is \u03b5(c)\n\nFigure 1 depicts this estimator. In particular, using Lemma 1, the local privacy delivered to user\ni \u2208 N is \u03b5(l)\nj . In the next\ni\nsection, we focus on linear estimators in designing the optimal private data acquisition mechanism.\nWe motivate such specification by showing that, for a given vector of local privacy losses (\u03b5(l)\ni=1,\na linear estimator is optimal with respect to mean square error. To formalize this statement, we\nfirst need to define the minimax estimation error as the notion of optimality. Let P be a class of\ndistributions over X . For any P \u2208 P, we denote its mean by \u03b8(P ). A (\u03b5(l)\ni=1-locally RDP estimator\ncan be cast as \u02c6\u03b8((Ci(xi))n\ni=1), where Ci(.) is the randomized channel corresponding to user i. Let\nQ((\u03b5(l)\ni=1-locally RDP estimators. The minimax estimation error is\ni=1,\u02c6\u03b8[|\u02c6\u03b8((Ci(Xi))n\n\ni=1) be the class of such (\u03b5(l)\n\ni=1) \u2212 \u03b8(P )|2],\n\nL(P, Q, (\u03b5(l)\n\nj=1 w2\n\ni=1) :=\n\nj /\u03b5(l)\n\n(Xi\u223cP )n\n\ni )n\n\ni )n\n\ni )n\n\ni )n\n\ni )n\n\n(2)\n\nE\n\ninf\ni=1\u2208Q((\u03b5(l)\n\ni )n\n\ni=1)\n\nsup\nP \u2208P\n\n\u02c6\u03b8,{Ci}n\n\nwhere the expectation is taken over both randomness of data and estimator (including private channels).\nIn other words, the optimal estimator is the one that has the lowest worst case error among all\nestimators that satisfy the privacy requirements. We next state the optimality result.\nTheorem 1. Assume \u03b1 \u2265 2 and \u03b5(l)\nand C1, \u00b7 \u00b7 \u00b7 , Cn be independent channels. Then, there exists a universal constant c such that\n\ni \u2264 1 for all i. Let P1 be the family of distributions over [\u2212 1\n\n2 , 1\n2 ]\n\nL(P, Q, (\u03b5(l)\n\ni )n\n\ni=1) \u2265 c min(\n\n1\ni=1 \u03b5(l)\n\ni\n\n(cid:80)n\n\n, 1).\n\nFurthermore, there exists a linear estimator with Gaussian mechanism such that\n\nE\n\n(Xi\u223cP )n\n\ni=1,\u02c6\u03b8[|\u02c6\u03b8((Ci(Xi))n\n\ni=1) \u2212 \u03b8(P )|2] \u2264 O(1)\n\n\u03b1\ni=1 \u03b5(l)\n\ni\n\n(cid:80)n\n\n.\n\nWe prove the lower bound by using the Le Cam\u2019s method Yu [1997] that reduces the problem of\nfinding lower bounds to a hypothesis testing problem and requires bounding the total variation\ndistance between the estimates when the input data points change. We then find it more convenient to\nbound the total variation in terms of Hellinger distance and use a series of inequalities to bound it.\n\n2.2 Data acquisition mechanism with two-part privacy guarantees\n\nWe next describe the utility functions of the users and the platform and then formulate the platform\u2019s\noptimal data acquisition mechanism. As we described earlier, each user suffers from two privacy\nlosses when sharing her data. The first one is a central privacy loss because of the leakage of her\ninformation through the platform\u2019s output. The second one is a local privacy loss because of the\nleakage of her information through the raw data that she shares with the platform. Each user has a\nheterogeneous privacy sensitivity for these two types of privacy losses. To model such heterogeneity,\nfor each i \u2208 N , we let ci \u2208 [0, 1] be her relative local privacy sensitivity, representing the relative\nweight that user i assigns to the (per unit cost of) local privacy loss. We also let 1 \u2212 ci be her relative\ncentral privacy sensitivity, representing the relative weight that user i assigns to the (per unit cost of)\ncentral privacy loss. Therefore, ci \u2248 1 implies that user i suffers a higher loss of privacy by sharing\nher raw data with the platform (local privacy loss) compared to her loss from the platform\u2019s output\n\n1This assumption does not have any fundamental impact on the results and is made to simplify the notations.\n\n5\n\n\f(central privacy loss). Differently, ci \u2248 0 implies that the user suffers a smaller loss of privacy by\nsharing her raw data with the platform compared to her loss from the platform\u2019s output. In what\nfollows, we use the term privacy sensitivity instead of relative local privacy sensitivity.\n\nFor each i \u2208 N , the privacy sensitivity ci is independently drawn from a publicly known distribution\nwhose support is [0, 1] with cumulative distribution and probability density functions Fi(\u00b7) and fi(\u00b7).\nWe also let c = (c1, . . . , cn) be the vector of privacy sensitivities. The privacy sensitivity of each\nuser is her private information, i.e., the platform does not know it. This is because individuals have\ndifferent views regarding how trustworthy the platform is in protecting their raw data.\n\nThe platform\u2019s objective is to design a mechanism to collect users\u2019 data by paying them to compensate\nfor their privacy losses without knowing the privacy sensitivity of users. To introduce the platform\u2019s\nobjective formally, we adopt the formalism of Bayesian mechanism design pioneered by Myerson\n[1981]. More specifically, the platform designs and announces a payment function, a local privacy loss\nfunction, and a central privacy loss function that are mappings from the reported privacy sensitivities\nof users. The users then report their privacy sensitivities (which may or may not be truthful). Based\non the payment function, the platform compensates the users (the compensation could be monetary\nor some free or discounted service provided to the user). Based on the local and central privacy\nfunctions, the platform designs randomized channels and randomized estimation algorithms that\ndeliver the guaranteed local and central privacy losses while minimizing the sum of the mean squared\nerror and the total expected payments. Given this interaction, we next formally introduce a data\nacquisition mechanism with two-part data privacy guarantees.\n\nDefinition 4 (two-part private data acquisition mechanism). We call the tuple (\u02c6\u03b8, \u03b5(l), \u03b5(c), t) a\ntwo-part private data acquisition mechanism where\n\n1. For all i \u2208 N , \u03b5(l)\ni\n\n: Rn\n\n+ \u2192 R+ is a function that maps the vector of privacy sensitivities c\n\nto a local privacy loss for user i, \u03b5(l)\n\ni (c), with \u03b5(l)(\u00b7) = (\u03b5(l)\n\ni (\u00b7))n\n\ni=1.\n\n2. For all i \u2208 N , \u03b5(c)\n\ni\n\n: Rn\n\n+ \u2192 R+ is a function that maps the vector of privacy sensitivities c\n\nto a central privacy loss for user i, \u03b5(c)\n\ni (c), with \u03b5(c)(.) = (\u03b5(c)\n\ni (\u00b7))n\n\ni=1.\n\n3. \u02c6\u03b8 : X n \u00d7 Rn\n\n+ \u00d7 Rn\n\nacquired locally (\u03b5(l)\nestimate \u02c6\u03b8(x, \u03b5(l)(c), \u03b5(c)(c)).\n\n+ \u2192 R is a (\u03b5(c)(c), \u03b1)-R\u00e9nyi differentially private estimator that maps\ni (c), \u03b1)-R\u00e9nyi differentially private data of user i for i \u2208 N to an\n\n4. For all i \u2208 N , ti : Rn\n\n+ \u2192 R+ is a function that maps the vector of privacy sensitivities c to\n\na payment for user i, ti(c), with t(.) = (ti(\u00b7))n\n\ni=1.\n\nNotice that we have not specified the estimator and the mechanisms that delivers (local and central)\nR\u00e9nyi differential privacy. In the rest of this subsection, we introduce the utilities and the platform\u2019s\nproblem for a general estimators and mechanisms to deliver differential privacy. Later, we focus on\nlinear estimator and Gaussian mechanisms and explicitly solve the platform\u2019s problem.\n\nEach user that participates in a two-part private data acquisition mechanism suffers from both the\nlocal and central privacy losses and need to be compensated by the platform. In particular, the utility\nof user i from participation when her privacy sensitivity is ci and she reports c\u2032\n\ni is given by\n\nui(\u03b5(l)(c\u2032\n\ni, c\u2212i), \u03b5(c)(c\u2032\n\ni, c\u2212i), t, \u02c6\u03b8) = Ec\u2212i[ti(c\u2212i, c\u2032\n\ni)) \u2212 ci\u03b5(l)\n\ni (c\u2212i, c\u2032\n\ni) \u2212 (1 \u2212 ci)\u03b5(c)\n\ni (c\u2212i, c\u2032\n\ni)],\n\ni)) is the payment from the platform, the term ci\u03b5(l)\n\nwhere the term ti(c\u2212i, c\u2032\ni) is the relative lo-\ncal privacy sensitivity of the user multiplied by her local privacy loss, and the term (1\u2212ci)\u03b5(c)\ni (c\u2212i, c\u2032\ni)\nis her relative central privacy sensitivity multiplied by her central privacy loss. A user i \u2208 N that\ndoes not participate in the mechanism neither compromises her privacy nor gets a compensation.\nTherefore, the utility of a user who does not participate in the mechanism becomes 0.\n\ni (c\u2212i, c\u2032\n\nThe goal of the platform is to minimize the sum of the mean squared error and the overall payment to\nusers. We let \u03b3 \u2208 R+ represents the relative weight of the mean estimation error and the payments in\n\n6\n\n\fthe platform\u2019s objective.2 Therefore, the platform\u2019s objective is\nn\n(cid:88)\n\nEc[\u03b3MSE(\u03b5(l)(c), \u03b5(c)(c), \u02c6\u03b8) +\n\nti(c)],\n\ni=1\n\nwhere the first term is the mean square error of estimator \u02c6\u03b8 given reported vector of privacy sensitivity\nand resulting local and central privacy losses \u03b5(l) and \u03b5(c), i.e.,\n\nMSE(\u03b5(l)(c), \u03b5(c)(c), \u02c6\u03b8) = Ex[|\u02c6\u03b8(\u02c6x, \u03b5(l), \u03b5(c)) \u2212 \u03b8|2].\nAlso, each summand of the second term is the compensation that the platform gives to a user to\nincentivize her to participate and report her privacy sensitivity truthfully.\n\nIn Appendix we prove that, similar to the classical mechanism design setting, revelation principle\nholds. This means that there is no loss of generality in focusing on the class of direct incentive\ncompatible mechanisms, meaning the platform\u2019s optimization problem can be written as\n\nmin\n\u03b5(l)(\u00b7),\u03b5(c)(\u00b7),t(\u00b7)\n\nEc[\u03b3MSE(\u03b5(l)(c), \u03b5(c)(c), \u02c6\u03b8) +\n\nn\n(cid:88)\n\nti(c)]\n\ni=1\nui(\u03b5(l)(c), \u03b5(c)(c), t, \u02c6\u03b8) \u2265 ui(\u03b5(l)(c\u2032\nui(\u03b5(l)(c), \u03b5(c)(c), t, \u02c6\u03b8) \u2265 0\n\ni, c\u2212i), \u03b5(c)(c\u2032\n\ni, c\u2212i), t, \u02c6\u03b8)\n\n(5)\nwhere the constraints in (4) represent the incentive compatibility. These constraints guarantee that\nthat each user i has no incentive to misrepresent her privacy sensitivity when others report truthfully\n(reporting truthfully is an equilibrium of the game among the users). Also, the constraints in (5)\nrepresent individual rationality, which ensures that each user receives a non-negative utility from\nparticipating in the platform\u2019s mechanism and sharing her data.\n\nfor all i \u2208 N , ci,\n\n(3)\n\n(4)\n\n3 From the mechanism design problem to an optimization problem\n\nFor a given estimator \u02c6\u03b8, the platform\u2019s decision comprises the local and central privacy loss functions\n\u03b5(l)(\u00b7) and \u03b5(c)(\u00b7) together with the payment functions t(\u00b7). We next show that this problem can be\nequivalently formulated as an optimization problem over the vector of local privacy losses and central\nprivacy losses (as opposed to functions). In the rest of the paper, we impose the following assumption\nwhich is well-known in the mechanism design literature and simplifies the analysis.3\nAssumption 1. For any user i \u2208 N , the virtual cost defined as \u03c8i(c) = c + Fi(c)\nfi(c) is increasing in c,\nwhere fi(\u00b7) and Fi(\u00b7) are probability density and cumulative distribution functions of ci, respectively.\n\nThe above assumption holds for a wide class of distributions such as the ones with log-concave\ndensity functions (e.g., uniform).\nTheorem 2. Suppose Assumption 1 holds. For a given estimator \u02c6\u03b8 : \u02c6X n \u00d7 Rn\n+ \u2192 R, in the\noptimal two-part data acquisition mechanism, for a given vector of reported privacy sensitivities c,\nthe local and central privacy losses are the solution of\nn\n(cid:88)\n\n+ \u00d7 Rn\n\nn\n(cid:88)\n\n\u03b5(l)\ni \u03c8i(ci) +\n\n\u03b5(c)\ni (1 \u2212 \u03c8i(ci)).\n\n(6)\n\n\u03b3MSE(\u03b5(l), \u03b5(c), \u02c6\u03b8) +\n\nmin\ni=1,{\u03b5(c)}n\n\ni=1\n\n{\u03b5(l)}n\n\ni=1\n\ni=1\n\nProof Sketch of Theorem 2: We introduce the following interim functions\n\nti(ci) =Ec\u2212i [t(ci, c\u2212i)],\n\ni (ci) = Ec\u2212i[\u03b5(l)\n\u03b5(l)\n\ni (ci, c\u2212i)],\n\nand \u03b5(c)\n\ni (ci) = Ec\u2212i[\u03b5(c)\n\ni (ci, c\u2212i)].\n\nWe first establish a payment identity that determines the optimal payment in terms of the optimal\nlocal and central privacy losses. In particular, by evaluating the first order condition corresponding to\nthe incentive compatibility constraint (4), we establish that this constraint holds if and only if\n\nti(ci) = ti(0) + \u03b5(c)\n\ni (ci) \u2212 \u03b5(c)\n\ni (0) + ci(\u03b5(l)\n\ni (ci) \u2212 \u03b5(c)\n\ni (ci)) \u2212\n\n(cid:90) ci\n\n0\n\n(\u03b5(l)\n\ni (z) \u2212 \u03b5(c)\n\ni (z))dz,\n\n2Notice that changing the parameter \u03b3 enables us to study a wide range of platform\u2019s objectives with differing\n\nrelative weights between the estimation error and the total payments.\n\n3Without this assumption, extending the results requires ironing technique of Myerson [1981].\n\n7\n\n\fi (z) \u2212 \u03b5(c)\n\nand \u03b5(l)\ni (z) is weakly decreasing in z. We then plug in this payment identity back to the\nplatform\u2019s objective, use the individual rationality constraint, and rewrite the platform\u2019s expected\nutility in terms of the privacy loss functions and the virtual cost of users. This is still a functional\noptimization problem in terms of \u03b5(l)(\u00b7) and \u03b5(c)(\u00b7). However, we establish that, under Assumption 1,\nwe can solve this functional optimization point-wise (i.e., for any given c). \u25a0\n\nTheorem 2 highlights the tradeoff in the platform\u2019s problem: by decreasing the local privacy loss,\nthe second term of the objective decreases (this term corresponds to the payment to users) while\nthe first term (i.e., the mean squared error) increases. The role of the central privacy loss is more\nnuanced, and there are two cases. If the coefficient 1 \u2212 \u03c8i(ci) is non-negative, by decreasing the\ncentral privacy loss, the third term of the objective decreases while the first term increases. If the\ncoefficient 1 \u2212 \u03c8i(ci) is negative, increasing the central privacy loss decreases both the third term\nand the first term. However, we cannot increase the central privacy loss level without limits because\nthe central privacy loss level is always below the local privacy loss level. Therefore, the platform\u2019s\noptimal mechanism should find the \u201cright\u201d balance between these terms.\n\n4 Optimal mechanism with two-part privacy guarantees for linear estimators\n\nFor the rest of the paper, we focus on linear estimators with Gaussian mechanism described in Section\n2.1. The following is a direct corollary of Theorem 2.\nCorollary 1. Suppose Assumption 1 holds. For any reported vector of privacy sensitivities c, the\noptimal local privacy loss levels are \u03b5(l)\nn\n(cid:88)\n\ni and the optimal central privacy loss levels are\n\ni (c) = y\u2217\n\n2\n\nwhere (w\u2217\n\n1, . . . , w\u2217\n\nn) and (y\u2217\n\n1, . . . , y\u2217\n\nn) are the optimal solution of\n\n\u03b5(c)\ni = w\u2217\ni\n\n2/\n\nw\u2217\nj\ny\u2217\nj\n\nj=1\n\nmin\nw,y\n\nVAR\u03b3\n\nn\n(cid:88)\n\ni=1\n\nw2\n\ni +\n\n\u03b3\u03b1\n2\n\nn\n(cid:88)\n\ni=1\n\nw2\ni\nyi\n\n+\n\ns.t. wi, yi \u2265 0, for all i \u2208 N and\n\nn\n(cid:88)\n\ni=1\n\nn\n(cid:88)\n\ni=1\n\n(1 \u2212 \u03c8i(ci))\n\nw2\ni\n\n(cid:80)n\n\nj=1\n\nw2\nj\nyj\n\n+\n\nn\n(cid:88)\n\ni=1\n\n\u03c8i(ci)yi\n\n(7)\n\nwi = 1.\n\nLet us highlight the difference between our characterization and that of classic mechanism design (e.g.,\nMyerson [1981]). In classic mechanism design, the designer\u2019s problem becomes linear optimization.\nHowever, in our setting, the designer\u2019s problem is a non-linear and non-convex optimization. This\nmakes the problem of finding the optimal two-part data acquisition mechanism challenging. Before\naddressing this computational challenge, let us revisit the form of the Gaussian mechanism that we\nhave adopted: the platform adds Gaussian noise locally and then outputs a convex combination of\nthe privatized users\u2019 data without adding any noise centrally. More specifically, one may guess that\nthe platform may benefit by having a central noise added to the final output in addition to the local\nnoises. In the following subsection, we establish that there is another Gaussian mechanism for any\nGaussian mechanism that only adds local noises and achieves a weakly lower cost.\n\n4.1 Optimality of having only local noises in the Gaussian mechanism\n\nThe platform has the opportunity of adding Gaussian noise to both the raw data of each user and the\nfinal estimator and ex-ante one may guess that it is optimal to use both of these instruments. However,\nas we establish next, interestingly, in the optimal two-part data acquisition mechanism, it is always\noptimal to only add noises locally.\nFor a given vector of local privacy losses (\u03b5(l)\nGaussian mechanism with both local and central noises is of the form\n\nn ) and central privacy losses (\u03b5(c)\n\n1 , . . . , \u03b5(c)\n\n1 , . . . , \u03b5(l)\n\nn ), a\n\n\u02c6\u03b8(x1, . . . , xn) :=\n\nn\n(cid:88)\n\ni=1\n\nwi \u02c6xi + N (0,\n\n\u03b1\n2\u03b5\n\n) where\n\nn\n(cid:88)\n\ni=1\n\nwi = 1 and \u02c6xi = xi + N (0,\n\n\u03b1\n2\u03b5(l)\ni\n\n)\u2200i \u2208 N .\n\nProposition 1. In the optimal two-part data acquisition mechanism that adopts a Gaussian mecha-\nnism with both local and central noises, we have \u03b5 = \u221e.\n\n8\n\n\fAlgorithm 1: Computing the optimal two-part private data acquisition mechanism\nInput: The vector of privacy sensitivities (c1, . . . , cn)\nfor S \u2208 Grid (cid:0)[S, \u00afS], \u03b4(cid:1) do\n\nLet\n\n\u03bdi =\n\n1\n\u03b3 VAR + (1 \u2212 \u03c8i(ci))/S\n\n, \u03b6i =\n\n\u03bdi\nj \u03bdj\n\n(cid:80)\n\n, \u03bei = \u03b6i\n\n(cid:113)\n\n\u03bdj(\n\n\u03c8j(cj) \u2212 (cid:112)\u03c8i(ci))\n\n\uf8f6\n\n\uf8f8 .\n\n\uf8eb\n\n\uf8ed\n\nn\n(cid:88)\n\nj=1\n\n(cid:18) (cid:80)n\n\ni=1 \u03b6i\n\n\u03c8i(ci)\n\nS\u2212(cid:80)n\n\ni=1\n\n\u03c8i(ci)\u03bei\n\n\u221a\n\u221a\n\n(cid:19)2\n\n.\n\nLet p =\n\nLet\n\n\u03bdi + \u03bdi\n\nwi(S) =\n\n(cid:112)\u03c8j(cj)p\n\n(cid:80)\n\nj \u03bdj\n(cid:80)\nj \u03bdj\n\n\u2212 \u03bdi\n\n(cid:112)\u03c8i(ci)p,\n\nyi(S) = wi(S)\n\n(cid:114) p\n\n,\n\n\u03c8i(ci)\n\nand OBJ(S) be the objective of Problem (7) evaluated for this solution.\n\nend\nOutput: {yi(S\u2217), wi(S\u2217)}n\n\ni=1, where (S\u2217) = arg min(S) OBJ(S).\n\nProposition 1 has an important implication in terms of the design of data market architecture when\nusers have both central and local privacy costs: it is optimal to add noise locally! Adding a noise\ncentrally to the final estimator has an advantage because the weights in the final estimator give the\nplatform a lever to deliver heterogeneous central privacy guarantees to users. Despite this advantage,\nwe establish that adding noise centrally is never optimal. This is because the platform prefers to add\nthe noise locally to contribute to both central and local privacy guarantees delivered to users.\n\n4.2 Computing the optimal privacy loss function\n\nThe implementation of the optimal two-part private data acquisition mechanism requires solving\nProblem (7), which is a non-convex program. However, we use the structure of the problem to develop\na polynomial time algorithm to solve it approximately. To do so, we first replace (cid:80)n\ni /yi by an\nauxiliary variable S. Next, we consider the corresponding lagrangian problem. Using Karush-Kuhn-\nTucker (KKT) conditions, we establish a number of relations between problems\u2019 parameters, S, and\np, the lagrangian coefficient corresponding to S = (cid:80)n\ni /yi. Furthermore, we develop upper and\nlower bounds for S. Finally, we do a grid search to find the approximate optimal solution.\nTheorem 3. For any vector of reported privacy sensitivities and \u03f5 > 0, Algorithm 1 finds local privacy\nloss levels and the differentially private linear estimator of the two-part data acquisition mechanism\nwhose cost (i.e., platform\u2019s objective) is at most 1 + \u03b4 of the optimal cost in time poly(n, 1\n\ni=1 w2\n\ni=1 w2\n\n\u03b4 ).\n\nNotice that the approximation factor in Theorem 3 depends on the underlying parameters. Therefore,\nwe have a Polynomial Time Approximation Scheme (PTAS) for finding the optimal two-part data\nacquisition mechanism in the class of linear estimators.\n\nIn the Appendix, we provide a case study with two users to illustrate the performance of the optimal\ntwo-part data acquisition mechanism in terms of the guaranteed privacy levels and payments as\nfunctions of the reported privacy sensitivities.\n\n\n\nBased on the section(s), please answer the following question with yes, no, or n/a and provide a brief justification for your answer.\nQuestion: If the authors ran experiments, do the authors include the total amount of compute and the type of resources used (e.g., type of GPUs, internal cluster, or cloud provider)?"}, "4a": {"role": "user", "content": "The following is the methods section of the paper you are reviewing:\n\n\nWe consider a platform interested in estimating a parameter \u03b8 \u2208 R by collecting data of n users,\nindexed by N = {1, \u00b7 \u00b7 \u00b7 , n}. For any i \u2208 N , we denote user i\u2019s data by Xi \u2208 X and we assume Xi\n\n3\n\n\fis given by Xi = \u03b8 + Zi where Z1, \u00b7 \u00b7 \u00b7 , Zn are independent and identically distributed zero-mean\nrandom variables with variance VAR. To simplify the exposition, we further assume |Zi| \u2264 1/2 for\nany i \u2208 N .1 Throughout, we use lower case letters to denote the realization of random variables. The\nplatform\u2019s goal is to minimize the estimate\u2019s error by collecting data from privacy-concerned users.\nTherefore, the platform needs to incentivize them to share their data.\n\n2.1 Local and central privacy losses\n\nBefore formalizing the utilities/objectives of the platform and the users, we define the notions of\nprivacy losses that we adopt in this paper. In particular, we consider two different types of privacy\nlosses that users suffer from. The first one is the privacy loss of a user when she shares her data (only)\nwith the platform, and the second one is the privacy loss through the released estimate (to the public)\nby the platform. Depending on how different users trust the platform, they might care differently\nabout these two privacy losses. For instance, if a user fully trusts the platform, then her main privacy\nconcern would be the second one, while a user who does not trust the platform at all would be more\nconcerned with the first one as the public only observes the aggregated estimate, as opposed to the\nplatform which observes each user\u2019s (shared) data separately.\n\nWe use the differential privacy framework to quantify these privacy losses. Since differential privacy\nwas introduced by Dwork et al., several variants of it have been also proposed. In particular, a popular\none in the machine learning literature is R\u00e9nyi differential privacy (RDP), introduced by Mironov\n[2017], which we also adopt in this paper. Let us first recall the definition of R\u00e9nyi divergence.\nDefinition 1. Let P and Q be two distributions over R with densities p and q. For any \u03b1 \u2208 (1, \u221e],\nthe R\u00e9nyi \u03b1-divergence between P and Q is denoted by D\u03b1(P ||Q) and is given by\n\nD\u03b1(P ||Q) :=\n\n1\n\u03b1 \u2212 1\n\nlog\n\n(cid:90) (cid:18) p(x)\nq(x)\n\n(cid:19)\u03b1\n\nq(x)dx.\n\nFor two random variables X and Y , D\u03b1(X||Y ) denotes the \u03b1-divergence between their distributions.\n\nWe next define two notions of differential privacy, known as central and local, to capture the two\naforementioned types of privacy losses. Local differential privacy corresponds to the privacy loss of a\nuser when she shares her data with the platform through a randomized mapping, known as a channel.\nDefinition 2. Let \u03b5 \u2265 0 and \u03b1 \u2208 (1, \u221e]. A randomized channel C : X \u2192 R is locally (\u03b5, \u03b1)-R\u00e9nyi\n(differentially) private if for any x, x\u2032 \u2208 X ,\n\nD\u03b1(C(x)||C(x\u2032)) \u2264 \u03b5.\n\nCentral differential privacy corresponds to the other privacy loss mentioned above. It bounds the\nchange in the distribution of the platform\u2019s output, i.e., the released estimate, by changing one user\u2019s\ndata. We next provide the formal definition.\ni=1 \u2208 Rn\nDefinition 3. Let \u03b5 = (\u03b5i)n\n(\u03b5, \u03b1)-R\u00e9nyi (differentially) private if for any two datasets x1:n, x\u2032\ncoordinate (data of user i),\n\n+ and \u03b1 \u2208 (1, \u221e]. A randomized algorithm A : X n \u2192 R is\n1:n \u2208 X n that only differ in i-th\n\nD\u03b1(A(x1:n)||A(x\u2032\n\n1:n)) \u2264 \u03b5i.\n\nThe customary approach to guarantee RDP is Gaussian mechanism in which a properly tuned zero-\nmean Gaussian noise is added to fulfill the required condition. The following lemma, adapted from\nMironov [2017], allows us to characterize the Gaussian noise\u2019s variance for a privacy loss level.\nLemma 1. For a function f : X n \u2192 R, we define its sensitivity with respect to i-th coordinate as\n\nLi(f ) := sup {|f (x1:n) \u2212 f (x\u2032\n\n1:n)| : for all x1:n and x\u2032\n\n1:n differing only at i-th coordinate} .\n\nFor any \u03b1 \u2208 (1, \u221e], A(x1:n) = f (x1:n) + W with W \u223c N (0, \u03c32) is\n\n(cid:16)\n\n( \u03b1Li(f )2\n2\u03c32\n\n)n\ni=1, \u03b1\n\n(cid:17)\n\n-RDP.\n\nFor a given vector of local privacy losses (\u03b5(l)\nis by using a linear estimator with Gaussian mechanism which is given by\n\n1 , . . . , \u03b5(l)\n\nn ), a natural way to privately estimate the mean\n\n\u02c6\u03b8(x1, . . . , xn) :=\n\nn\n(cid:88)\n\ni=1\n\nwi \u02c6xi where\n\nn\n(cid:88)\n\ni=1\n\nwi = 1 and \u02c6xi = xi + N (0,\n\n\u03b1\n2\u03b5(l)\ni\n\n) for all i \u2208 N .\n\n(1)\n\n4\n\n\fFigure 1: The interaction between the users and the platform in the two-part private data acquisition.\n\ni / (cid:80)n\n\ni = w2\n\nand the central privacy delivered to user i \u2208 N is \u03b5(c)\n\nFigure 1 depicts this estimator. In particular, using Lemma 1, the local privacy delivered to user\ni \u2208 N is \u03b5(l)\nj . In the next\ni\nsection, we focus on linear estimators in designing the optimal private data acquisition mechanism.\nWe motivate such specification by showing that, for a given vector of local privacy losses (\u03b5(l)\ni=1,\na linear estimator is optimal with respect to mean square error. To formalize this statement, we\nfirst need to define the minimax estimation error as the notion of optimality. Let P be a class of\ndistributions over X . For any P \u2208 P, we denote its mean by \u03b8(P ). A (\u03b5(l)\ni=1-locally RDP estimator\ncan be cast as \u02c6\u03b8((Ci(xi))n\ni=1), where Ci(.) is the randomized channel corresponding to user i. Let\nQ((\u03b5(l)\ni=1-locally RDP estimators. The minimax estimation error is\ni=1,\u02c6\u03b8[|\u02c6\u03b8((Ci(Xi))n\n\ni=1) be the class of such (\u03b5(l)\n\ni=1) \u2212 \u03b8(P )|2],\n\nL(P, Q, (\u03b5(l)\n\nj=1 w2\n\ni=1) :=\n\nj /\u03b5(l)\n\n(Xi\u223cP )n\n\ni )n\n\ni )n\n\ni )n\n\ni )n\n\ni )n\n\n(2)\n\nE\n\ninf\ni=1\u2208Q((\u03b5(l)\n\ni )n\n\ni=1)\n\nsup\nP \u2208P\n\n\u02c6\u03b8,{Ci}n\n\nwhere the expectation is taken over both randomness of data and estimator (including private channels).\nIn other words, the optimal estimator is the one that has the lowest worst case error among all\nestimators that satisfy the privacy requirements. We next state the optimality result.\nTheorem 1. Assume \u03b1 \u2265 2 and \u03b5(l)\nand C1, \u00b7 \u00b7 \u00b7 , Cn be independent channels. Then, there exists a universal constant c such that\n\ni \u2264 1 for all i. Let P1 be the family of distributions over [\u2212 1\n\n2 , 1\n2 ]\n\nL(P, Q, (\u03b5(l)\n\ni )n\n\ni=1) \u2265 c min(\n\n1\ni=1 \u03b5(l)\n\ni\n\n(cid:80)n\n\n, 1).\n\nFurthermore, there exists a linear estimator with Gaussian mechanism such that\n\nE\n\n(Xi\u223cP )n\n\ni=1,\u02c6\u03b8[|\u02c6\u03b8((Ci(Xi))n\n\ni=1) \u2212 \u03b8(P )|2] \u2264 O(1)\n\n\u03b1\ni=1 \u03b5(l)\n\ni\n\n(cid:80)n\n\n.\n\nWe prove the lower bound by using the Le Cam\u2019s method Yu [1997] that reduces the problem of\nfinding lower bounds to a hypothesis testing problem and requires bounding the total variation\ndistance between the estimates when the input data points change. We then find it more convenient to\nbound the total variation in terms of Hellinger distance and use a series of inequalities to bound it.\n\n2.2 Data acquisition mechanism with two-part privacy guarantees\n\nWe next describe the utility functions of the users and the platform and then formulate the platform\u2019s\noptimal data acquisition mechanism. As we described earlier, each user suffers from two privacy\nlosses when sharing her data. The first one is a central privacy loss because of the leakage of her\ninformation through the platform\u2019s output. The second one is a local privacy loss because of the\nleakage of her information through the raw data that she shares with the platform. Each user has a\nheterogeneous privacy sensitivity for these two types of privacy losses. To model such heterogeneity,\nfor each i \u2208 N , we let ci \u2208 [0, 1] be her relative local privacy sensitivity, representing the relative\nweight that user i assigns to the (per unit cost of) local privacy loss. We also let 1 \u2212 ci be her relative\ncentral privacy sensitivity, representing the relative weight that user i assigns to the (per unit cost of)\ncentral privacy loss. Therefore, ci \u2248 1 implies that user i suffers a higher loss of privacy by sharing\nher raw data with the platform (local privacy loss) compared to her loss from the platform\u2019s output\n\n1This assumption does not have any fundamental impact on the results and is made to simplify the notations.\n\n5\n\n\f(central privacy loss). Differently, ci \u2248 0 implies that the user suffers a smaller loss of privacy by\nsharing her raw data with the platform compared to her loss from the platform\u2019s output. In what\nfollows, we use the term privacy sensitivity instead of relative local privacy sensitivity.\n\nFor each i \u2208 N , the privacy sensitivity ci is independently drawn from a publicly known distribution\nwhose support is [0, 1] with cumulative distribution and probability density functions Fi(\u00b7) and fi(\u00b7).\nWe also let c = (c1, . . . , cn) be the vector of privacy sensitivities. The privacy sensitivity of each\nuser is her private information, i.e., the platform does not know it. This is because individuals have\ndifferent views regarding how trustworthy the platform is in protecting their raw data.\n\nThe platform\u2019s objective is to design a mechanism to collect users\u2019 data by paying them to compensate\nfor their privacy losses without knowing the privacy sensitivity of users. To introduce the platform\u2019s\nobjective formally, we adopt the formalism of Bayesian mechanism design pioneered by Myerson\n[1981]. More specifically, the platform designs and announces a payment function, a local privacy loss\nfunction, and a central privacy loss function that are mappings from the reported privacy sensitivities\nof users. The users then report their privacy sensitivities (which may or may not be truthful). Based\non the payment function, the platform compensates the users (the compensation could be monetary\nor some free or discounted service provided to the user). Based on the local and central privacy\nfunctions, the platform designs randomized channels and randomized estimation algorithms that\ndeliver the guaranteed local and central privacy losses while minimizing the sum of the mean squared\nerror and the total expected payments. Given this interaction, we next formally introduce a data\nacquisition mechanism with two-part data privacy guarantees.\n\nDefinition 4 (two-part private data acquisition mechanism). We call the tuple (\u02c6\u03b8, \u03b5(l), \u03b5(c), t) a\ntwo-part private data acquisition mechanism where\n\n1. For all i \u2208 N , \u03b5(l)\ni\n\n: Rn\n\n+ \u2192 R+ is a function that maps the vector of privacy sensitivities c\n\nto a local privacy loss for user i, \u03b5(l)\n\ni (c), with \u03b5(l)(\u00b7) = (\u03b5(l)\n\ni (\u00b7))n\n\ni=1.\n\n2. For all i \u2208 N , \u03b5(c)\n\ni\n\n: Rn\n\n+ \u2192 R+ is a function that maps the vector of privacy sensitivities c\n\nto a central privacy loss for user i, \u03b5(c)\n\ni (c), with \u03b5(c)(.) = (\u03b5(c)\n\ni (\u00b7))n\n\ni=1.\n\n3. \u02c6\u03b8 : X n \u00d7 Rn\n\n+ \u00d7 Rn\n\nacquired locally (\u03b5(l)\nestimate \u02c6\u03b8(x, \u03b5(l)(c), \u03b5(c)(c)).\n\n+ \u2192 R is a (\u03b5(c)(c), \u03b1)-R\u00e9nyi differentially private estimator that maps\ni (c), \u03b1)-R\u00e9nyi differentially private data of user i for i \u2208 N to an\n\n4. For all i \u2208 N , ti : Rn\n\n+ \u2192 R+ is a function that maps the vector of privacy sensitivities c to\n\na payment for user i, ti(c), with t(.) = (ti(\u00b7))n\n\ni=1.\n\nNotice that we have not specified the estimator and the mechanisms that delivers (local and central)\nR\u00e9nyi differential privacy. In the rest of this subsection, we introduce the utilities and the platform\u2019s\nproblem for a general estimators and mechanisms to deliver differential privacy. Later, we focus on\nlinear estimator and Gaussian mechanisms and explicitly solve the platform\u2019s problem.\n\nEach user that participates in a two-part private data acquisition mechanism suffers from both the\nlocal and central privacy losses and need to be compensated by the platform. In particular, the utility\nof user i from participation when her privacy sensitivity is ci and she reports c\u2032\n\ni is given by\n\nui(\u03b5(l)(c\u2032\n\ni, c\u2212i), \u03b5(c)(c\u2032\n\ni, c\u2212i), t, \u02c6\u03b8) = Ec\u2212i[ti(c\u2212i, c\u2032\n\ni)) \u2212 ci\u03b5(l)\n\ni (c\u2212i, c\u2032\n\ni) \u2212 (1 \u2212 ci)\u03b5(c)\n\ni (c\u2212i, c\u2032\n\ni)],\n\ni)) is the payment from the platform, the term ci\u03b5(l)\n\nwhere the term ti(c\u2212i, c\u2032\ni) is the relative lo-\ncal privacy sensitivity of the user multiplied by her local privacy loss, and the term (1\u2212ci)\u03b5(c)\ni (c\u2212i, c\u2032\ni)\nis her relative central privacy sensitivity multiplied by her central privacy loss. A user i \u2208 N that\ndoes not participate in the mechanism neither compromises her privacy nor gets a compensation.\nTherefore, the utility of a user who does not participate in the mechanism becomes 0.\n\ni (c\u2212i, c\u2032\n\nThe goal of the platform is to minimize the sum of the mean squared error and the overall payment to\nusers. We let \u03b3 \u2208 R+ represents the relative weight of the mean estimation error and the payments in\n\n6\n\n\fthe platform\u2019s objective.2 Therefore, the platform\u2019s objective is\nn\n(cid:88)\n\nEc[\u03b3MSE(\u03b5(l)(c), \u03b5(c)(c), \u02c6\u03b8) +\n\nti(c)],\n\ni=1\n\nwhere the first term is the mean square error of estimator \u02c6\u03b8 given reported vector of privacy sensitivity\nand resulting local and central privacy losses \u03b5(l) and \u03b5(c), i.e.,\n\nMSE(\u03b5(l)(c), \u03b5(c)(c), \u02c6\u03b8) = Ex[|\u02c6\u03b8(\u02c6x, \u03b5(l), \u03b5(c)) \u2212 \u03b8|2].\nAlso, each summand of the second term is the compensation that the platform gives to a user to\nincentivize her to participate and report her privacy sensitivity truthfully.\n\nIn Appendix we prove that, similar to the classical mechanism design setting, revelation principle\nholds. This means that there is no loss of generality in focusing on the class of direct incentive\ncompatible mechanisms, meaning the platform\u2019s optimization problem can be written as\n\nmin\n\u03b5(l)(\u00b7),\u03b5(c)(\u00b7),t(\u00b7)\n\nEc[\u03b3MSE(\u03b5(l)(c), \u03b5(c)(c), \u02c6\u03b8) +\n\nn\n(cid:88)\n\nti(c)]\n\ni=1\nui(\u03b5(l)(c), \u03b5(c)(c), t, \u02c6\u03b8) \u2265 ui(\u03b5(l)(c\u2032\nui(\u03b5(l)(c), \u03b5(c)(c), t, \u02c6\u03b8) \u2265 0\n\ni, c\u2212i), \u03b5(c)(c\u2032\n\ni, c\u2212i), t, \u02c6\u03b8)\n\n(5)\nwhere the constraints in (4) represent the incentive compatibility. These constraints guarantee that\nthat each user i has no incentive to misrepresent her privacy sensitivity when others report truthfully\n(reporting truthfully is an equilibrium of the game among the users). Also, the constraints in (5)\nrepresent individual rationality, which ensures that each user receives a non-negative utility from\nparticipating in the platform\u2019s mechanism and sharing her data.\n\nfor all i \u2208 N , ci,\n\n(3)\n\n(4)\n\n3 From the mechanism design problem to an optimization problem\n\nFor a given estimator \u02c6\u03b8, the platform\u2019s decision comprises the local and central privacy loss functions\n\u03b5(l)(\u00b7) and \u03b5(c)(\u00b7) together with the payment functions t(\u00b7). We next show that this problem can be\nequivalently formulated as an optimization problem over the vector of local privacy losses and central\nprivacy losses (as opposed to functions). In the rest of the paper, we impose the following assumption\nwhich is well-known in the mechanism design literature and simplifies the analysis.3\nAssumption 1. For any user i \u2208 N , the virtual cost defined as \u03c8i(c) = c + Fi(c)\nfi(c) is increasing in c,\nwhere fi(\u00b7) and Fi(\u00b7) are probability density and cumulative distribution functions of ci, respectively.\n\nThe above assumption holds for a wide class of distributions such as the ones with log-concave\ndensity functions (e.g., uniform).\nTheorem 2. Suppose Assumption 1 holds. For a given estimator \u02c6\u03b8 : \u02c6X n \u00d7 Rn\n+ \u2192 R, in the\noptimal two-part data acquisition mechanism, for a given vector of reported privacy sensitivities c,\nthe local and central privacy losses are the solution of\nn\n(cid:88)\n\n+ \u00d7 Rn\n\nn\n(cid:88)\n\n\u03b5(l)\ni \u03c8i(ci) +\n\n\u03b5(c)\ni (1 \u2212 \u03c8i(ci)).\n\n(6)\n\n\u03b3MSE(\u03b5(l), \u03b5(c), \u02c6\u03b8) +\n\nmin\ni=1,{\u03b5(c)}n\n\ni=1\n\n{\u03b5(l)}n\n\ni=1\n\ni=1\n\nProof Sketch of Theorem 2: We introduce the following interim functions\n\nti(ci) =Ec\u2212i [t(ci, c\u2212i)],\n\ni (ci) = Ec\u2212i[\u03b5(l)\n\u03b5(l)\n\ni (ci, c\u2212i)],\n\nand \u03b5(c)\n\ni (ci) = Ec\u2212i[\u03b5(c)\n\ni (ci, c\u2212i)].\n\nWe first establish a payment identity that determines the optimal payment in terms of the optimal\nlocal and central privacy losses. In particular, by evaluating the first order condition corresponding to\nthe incentive compatibility constraint (4), we establish that this constraint holds if and only if\n\nti(ci) = ti(0) + \u03b5(c)\n\ni (ci) \u2212 \u03b5(c)\n\ni (0) + ci(\u03b5(l)\n\ni (ci) \u2212 \u03b5(c)\n\ni (ci)) \u2212\n\n(cid:90) ci\n\n0\n\n(\u03b5(l)\n\ni (z) \u2212 \u03b5(c)\n\ni (z))dz,\n\n2Notice that changing the parameter \u03b3 enables us to study a wide range of platform\u2019s objectives with differing\n\nrelative weights between the estimation error and the total payments.\n\n3Without this assumption, extending the results requires ironing technique of Myerson [1981].\n\n7\n\n\fi (z) \u2212 \u03b5(c)\n\nand \u03b5(l)\ni (z) is weakly decreasing in z. We then plug in this payment identity back to the\nplatform\u2019s objective, use the individual rationality constraint, and rewrite the platform\u2019s expected\nutility in terms of the privacy loss functions and the virtual cost of users. This is still a functional\noptimization problem in terms of \u03b5(l)(\u00b7) and \u03b5(c)(\u00b7). However, we establish that, under Assumption 1,\nwe can solve this functional optimization point-wise (i.e., for any given c). \u25a0\n\nTheorem 2 highlights the tradeoff in the platform\u2019s problem: by decreasing the local privacy loss,\nthe second term of the objective decreases (this term corresponds to the payment to users) while\nthe first term (i.e., the mean squared error) increases. The role of the central privacy loss is more\nnuanced, and there are two cases. If the coefficient 1 \u2212 \u03c8i(ci) is non-negative, by decreasing the\ncentral privacy loss, the third term of the objective decreases while the first term increases. If the\ncoefficient 1 \u2212 \u03c8i(ci) is negative, increasing the central privacy loss decreases both the third term\nand the first term. However, we cannot increase the central privacy loss level without limits because\nthe central privacy loss level is always below the local privacy loss level. Therefore, the platform\u2019s\noptimal mechanism should find the \u201cright\u201d balance between these terms.\n\n4 Optimal mechanism with two-part privacy guarantees for linear estimators\n\nFor the rest of the paper, we focus on linear estimators with Gaussian mechanism described in Section\n2.1. The following is a direct corollary of Theorem 2.\nCorollary 1. Suppose Assumption 1 holds. For any reported vector of privacy sensitivities c, the\noptimal local privacy loss levels are \u03b5(l)\nn\n(cid:88)\n\ni and the optimal central privacy loss levels are\n\ni (c) = y\u2217\n\n2\n\nwhere (w\u2217\n\n1, . . . , w\u2217\n\nn) and (y\u2217\n\n1, . . . , y\u2217\n\nn) are the optimal solution of\n\n\u03b5(c)\ni = w\u2217\ni\n\n2/\n\nw\u2217\nj\ny\u2217\nj\n\nj=1\n\nmin\nw,y\n\nVAR\u03b3\n\nn\n(cid:88)\n\ni=1\n\nw2\n\ni +\n\n\u03b3\u03b1\n2\n\nn\n(cid:88)\n\ni=1\n\nw2\ni\nyi\n\n+\n\ns.t. wi, yi \u2265 0, for all i \u2208 N and\n\nn\n(cid:88)\n\ni=1\n\nn\n(cid:88)\n\ni=1\n\n(1 \u2212 \u03c8i(ci))\n\nw2\ni\n\n(cid:80)n\n\nj=1\n\nw2\nj\nyj\n\n+\n\nn\n(cid:88)\n\ni=1\n\n\u03c8i(ci)yi\n\n(7)\n\nwi = 1.\n\nLet us highlight the difference between our characterization and that of classic mechanism design (e.g.,\nMyerson [1981]). In classic mechanism design, the designer\u2019s problem becomes linear optimization.\nHowever, in our setting, the designer\u2019s problem is a non-linear and non-convex optimization. This\nmakes the problem of finding the optimal two-part data acquisition mechanism challenging. Before\naddressing this computational challenge, let us revisit the form of the Gaussian mechanism that we\nhave adopted: the platform adds Gaussian noise locally and then outputs a convex combination of\nthe privatized users\u2019 data without adding any noise centrally. More specifically, one may guess that\nthe platform may benefit by having a central noise added to the final output in addition to the local\nnoises. In the following subsection, we establish that there is another Gaussian mechanism for any\nGaussian mechanism that only adds local noises and achieves a weakly lower cost.\n\n4.1 Optimality of having only local noises in the Gaussian mechanism\n\nThe platform has the opportunity of adding Gaussian noise to both the raw data of each user and the\nfinal estimator and ex-ante one may guess that it is optimal to use both of these instruments. However,\nas we establish next, interestingly, in the optimal two-part data acquisition mechanism, it is always\noptimal to only add noises locally.\nFor a given vector of local privacy losses (\u03b5(l)\nGaussian mechanism with both local and central noises is of the form\n\nn ) and central privacy losses (\u03b5(c)\n\n1 , . . . , \u03b5(c)\n\n1 , . . . , \u03b5(l)\n\nn ), a\n\n\u02c6\u03b8(x1, . . . , xn) :=\n\nn\n(cid:88)\n\ni=1\n\nwi \u02c6xi + N (0,\n\n\u03b1\n2\u03b5\n\n) where\n\nn\n(cid:88)\n\ni=1\n\nwi = 1 and \u02c6xi = xi + N (0,\n\n\u03b1\n2\u03b5(l)\ni\n\n)\u2200i \u2208 N .\n\nProposition 1. In the optimal two-part data acquisition mechanism that adopts a Gaussian mecha-\nnism with both local and central noises, we have \u03b5 = \u221e.\n\n8\n\n\fAlgorithm 1: Computing the optimal two-part private data acquisition mechanism\nInput: The vector of privacy sensitivities (c1, . . . , cn)\nfor S \u2208 Grid (cid:0)[S, \u00afS], \u03b4(cid:1) do\n\nLet\n\n\u03bdi =\n\n1\n\u03b3 VAR + (1 \u2212 \u03c8i(ci))/S\n\n, \u03b6i =\n\n\u03bdi\nj \u03bdj\n\n(cid:80)\n\n, \u03bei = \u03b6i\n\n(cid:113)\n\n\u03bdj(\n\n\u03c8j(cj) \u2212 (cid:112)\u03c8i(ci))\n\n\uf8f6\n\n\uf8f8 .\n\n\uf8eb\n\n\uf8ed\n\nn\n(cid:88)\n\nj=1\n\n(cid:18) (cid:80)n\n\ni=1 \u03b6i\n\n\u03c8i(ci)\n\nS\u2212(cid:80)n\n\ni=1\n\n\u03c8i(ci)\u03bei\n\n\u221a\n\u221a\n\n(cid:19)2\n\n.\n\nLet p =\n\nLet\n\n\u03bdi + \u03bdi\n\nwi(S) =\n\n(cid:112)\u03c8j(cj)p\n\n(cid:80)\n\nj \u03bdj\n(cid:80)\nj \u03bdj\n\n\u2212 \u03bdi\n\n(cid:112)\u03c8i(ci)p,\n\nyi(S) = wi(S)\n\n(cid:114) p\n\n,\n\n\u03c8i(ci)\n\nand OBJ(S) be the objective of Problem (7) evaluated for this solution.\n\nend\nOutput: {yi(S\u2217), wi(S\u2217)}n\n\ni=1, where (S\u2217) = arg min(S) OBJ(S).\n\nProposition 1 has an important implication in terms of the design of data market architecture when\nusers have both central and local privacy costs: it is optimal to add noise locally! Adding a noise\ncentrally to the final estimator has an advantage because the weights in the final estimator give the\nplatform a lever to deliver heterogeneous central privacy guarantees to users. Despite this advantage,\nwe establish that adding noise centrally is never optimal. This is because the platform prefers to add\nthe noise locally to contribute to both central and local privacy guarantees delivered to users.\n\n4.2 Computing the optimal privacy loss function\n\nThe implementation of the optimal two-part private data acquisition mechanism requires solving\nProblem (7), which is a non-convex program. However, we use the structure of the problem to develop\na polynomial time algorithm to solve it approximately. To do so, we first replace (cid:80)n\ni /yi by an\nauxiliary variable S. Next, we consider the corresponding lagrangian problem. Using Karush-Kuhn-\nTucker (KKT) conditions, we establish a number of relations between problems\u2019 parameters, S, and\np, the lagrangian coefficient corresponding to S = (cid:80)n\ni /yi. Furthermore, we develop upper and\nlower bounds for S. Finally, we do a grid search to find the approximate optimal solution.\nTheorem 3. For any vector of reported privacy sensitivities and \u03f5 > 0, Algorithm 1 finds local privacy\nloss levels and the differentially private linear estimator of the two-part data acquisition mechanism\nwhose cost (i.e., platform\u2019s objective) is at most 1 + \u03b4 of the optimal cost in time poly(n, 1\n\ni=1 w2\n\ni=1 w2\n\n\u03b4 ).\n\nNotice that the approximation factor in Theorem 3 depends on the underlying parameters. Therefore,\nwe have a Polynomial Time Approximation Scheme (PTAS) for finding the optimal two-part data\nacquisition mechanism in the class of linear estimators.\n\nIn the Appendix, we provide a case study with two users to illustrate the performance of the optimal\ntwo-part data acquisition mechanism in terms of the guaranteed privacy levels and payments as\nfunctions of the reported privacy sensitivities.\n\n\n\nBased on the section(s), please answer the following question with yes, no, or n/a and provide a brief justification for your answer.\nQuestion: If the authors use existing assets (e.g., code, data, models), do the authors cite the creators?"}, "4b": {"role": "user", "content": "The following is the methods section of the paper you are reviewing:\n\n\nWe consider a platform interested in estimating a parameter \u03b8 \u2208 R by collecting data of n users,\nindexed by N = {1, \u00b7 \u00b7 \u00b7 , n}. For any i \u2208 N , we denote user i\u2019s data by Xi \u2208 X and we assume Xi\n\n3\n\n\fis given by Xi = \u03b8 + Zi where Z1, \u00b7 \u00b7 \u00b7 , Zn are independent and identically distributed zero-mean\nrandom variables with variance VAR. To simplify the exposition, we further assume |Zi| \u2264 1/2 for\nany i \u2208 N .1 Throughout, we use lower case letters to denote the realization of random variables. The\nplatform\u2019s goal is to minimize the estimate\u2019s error by collecting data from privacy-concerned users.\nTherefore, the platform needs to incentivize them to share their data.\n\n2.1 Local and central privacy losses\n\nBefore formalizing the utilities/objectives of the platform and the users, we define the notions of\nprivacy losses that we adopt in this paper. In particular, we consider two different types of privacy\nlosses that users suffer from. The first one is the privacy loss of a user when she shares her data (only)\nwith the platform, and the second one is the privacy loss through the released estimate (to the public)\nby the platform. Depending on how different users trust the platform, they might care differently\nabout these two privacy losses. For instance, if a user fully trusts the platform, then her main privacy\nconcern would be the second one, while a user who does not trust the platform at all would be more\nconcerned with the first one as the public only observes the aggregated estimate, as opposed to the\nplatform which observes each user\u2019s (shared) data separately.\n\nWe use the differential privacy framework to quantify these privacy losses. Since differential privacy\nwas introduced by Dwork et al., several variants of it have been also proposed. In particular, a popular\none in the machine learning literature is R\u00e9nyi differential privacy (RDP), introduced by Mironov\n[2017], which we also adopt in this paper. Let us first recall the definition of R\u00e9nyi divergence.\nDefinition 1. Let P and Q be two distributions over R with densities p and q. For any \u03b1 \u2208 (1, \u221e],\nthe R\u00e9nyi \u03b1-divergence between P and Q is denoted by D\u03b1(P ||Q) and is given by\n\nD\u03b1(P ||Q) :=\n\n1\n\u03b1 \u2212 1\n\nlog\n\n(cid:90) (cid:18) p(x)\nq(x)\n\n(cid:19)\u03b1\n\nq(x)dx.\n\nFor two random variables X and Y , D\u03b1(X||Y ) denotes the \u03b1-divergence between their distributions.\n\nWe next define two notions of differential privacy, known as central and local, to capture the two\naforementioned types of privacy losses. Local differential privacy corresponds to the privacy loss of a\nuser when she shares her data with the platform through a randomized mapping, known as a channel.\nDefinition 2. Let \u03b5 \u2265 0 and \u03b1 \u2208 (1, \u221e]. A randomized channel C : X \u2192 R is locally (\u03b5, \u03b1)-R\u00e9nyi\n(differentially) private if for any x, x\u2032 \u2208 X ,\n\nD\u03b1(C(x)||C(x\u2032)) \u2264 \u03b5.\n\nCentral differential privacy corresponds to the other privacy loss mentioned above. It bounds the\nchange in the distribution of the platform\u2019s output, i.e., the released estimate, by changing one user\u2019s\ndata. We next provide the formal definition.\ni=1 \u2208 Rn\nDefinition 3. Let \u03b5 = (\u03b5i)n\n(\u03b5, \u03b1)-R\u00e9nyi (differentially) private if for any two datasets x1:n, x\u2032\ncoordinate (data of user i),\n\n+ and \u03b1 \u2208 (1, \u221e]. A randomized algorithm A : X n \u2192 R is\n1:n \u2208 X n that only differ in i-th\n\nD\u03b1(A(x1:n)||A(x\u2032\n\n1:n)) \u2264 \u03b5i.\n\nThe customary approach to guarantee RDP is Gaussian mechanism in which a properly tuned zero-\nmean Gaussian noise is added to fulfill the required condition. The following lemma, adapted from\nMironov [2017], allows us to characterize the Gaussian noise\u2019s variance for a privacy loss level.\nLemma 1. For a function f : X n \u2192 R, we define its sensitivity with respect to i-th coordinate as\n\nLi(f ) := sup {|f (x1:n) \u2212 f (x\u2032\n\n1:n)| : for all x1:n and x\u2032\n\n1:n differing only at i-th coordinate} .\n\nFor any \u03b1 \u2208 (1, \u221e], A(x1:n) = f (x1:n) + W with W \u223c N (0, \u03c32) is\n\n(cid:16)\n\n( \u03b1Li(f )2\n2\u03c32\n\n)n\ni=1, \u03b1\n\n(cid:17)\n\n-RDP.\n\nFor a given vector of local privacy losses (\u03b5(l)\nis by using a linear estimator with Gaussian mechanism which is given by\n\n1 , . . . , \u03b5(l)\n\nn ), a natural way to privately estimate the mean\n\n\u02c6\u03b8(x1, . . . , xn) :=\n\nn\n(cid:88)\n\ni=1\n\nwi \u02c6xi where\n\nn\n(cid:88)\n\ni=1\n\nwi = 1 and \u02c6xi = xi + N (0,\n\n\u03b1\n2\u03b5(l)\ni\n\n) for all i \u2208 N .\n\n(1)\n\n4\n\n\fFigure 1: The interaction between the users and the platform in the two-part private data acquisition.\n\ni / (cid:80)n\n\ni = w2\n\nand the central privacy delivered to user i \u2208 N is \u03b5(c)\n\nFigure 1 depicts this estimator. In particular, using Lemma 1, the local privacy delivered to user\ni \u2208 N is \u03b5(l)\nj . In the next\ni\nsection, we focus on linear estimators in designing the optimal private data acquisition mechanism.\nWe motivate such specification by showing that, for a given vector of local privacy losses (\u03b5(l)\ni=1,\na linear estimator is optimal with respect to mean square error. To formalize this statement, we\nfirst need to define the minimax estimation error as the notion of optimality. Let P be a class of\ndistributions over X . For any P \u2208 P, we denote its mean by \u03b8(P ). A (\u03b5(l)\ni=1-locally RDP estimator\ncan be cast as \u02c6\u03b8((Ci(xi))n\ni=1), where Ci(.) is the randomized channel corresponding to user i. Let\nQ((\u03b5(l)\ni=1-locally RDP estimators. The minimax estimation error is\ni=1,\u02c6\u03b8[|\u02c6\u03b8((Ci(Xi))n\n\ni=1) be the class of such (\u03b5(l)\n\ni=1) \u2212 \u03b8(P )|2],\n\nL(P, Q, (\u03b5(l)\n\nj=1 w2\n\ni=1) :=\n\nj /\u03b5(l)\n\n(Xi\u223cP )n\n\ni )n\n\ni )n\n\ni )n\n\ni )n\n\ni )n\n\n(2)\n\nE\n\ninf\ni=1\u2208Q((\u03b5(l)\n\ni )n\n\ni=1)\n\nsup\nP \u2208P\n\n\u02c6\u03b8,{Ci}n\n\nwhere the expectation is taken over both randomness of data and estimator (including private channels).\nIn other words, the optimal estimator is the one that has the lowest worst case error among all\nestimators that satisfy the privacy requirements. We next state the optimality result.\nTheorem 1. Assume \u03b1 \u2265 2 and \u03b5(l)\nand C1, \u00b7 \u00b7 \u00b7 , Cn be independent channels. Then, there exists a universal constant c such that\n\ni \u2264 1 for all i. Let P1 be the family of distributions over [\u2212 1\n\n2 , 1\n2 ]\n\nL(P, Q, (\u03b5(l)\n\ni )n\n\ni=1) \u2265 c min(\n\n1\ni=1 \u03b5(l)\n\ni\n\n(cid:80)n\n\n, 1).\n\nFurthermore, there exists a linear estimator with Gaussian mechanism such that\n\nE\n\n(Xi\u223cP )n\n\ni=1,\u02c6\u03b8[|\u02c6\u03b8((Ci(Xi))n\n\ni=1) \u2212 \u03b8(P )|2] \u2264 O(1)\n\n\u03b1\ni=1 \u03b5(l)\n\ni\n\n(cid:80)n\n\n.\n\nWe prove the lower bound by using the Le Cam\u2019s method Yu [1997] that reduces the problem of\nfinding lower bounds to a hypothesis testing problem and requires bounding the total variation\ndistance between the estimates when the input data points change. We then find it more convenient to\nbound the total variation in terms of Hellinger distance and use a series of inequalities to bound it.\n\n2.2 Data acquisition mechanism with two-part privacy guarantees\n\nWe next describe the utility functions of the users and the platform and then formulate the platform\u2019s\noptimal data acquisition mechanism. As we described earlier, each user suffers from two privacy\nlosses when sharing her data. The first one is a central privacy loss because of the leakage of her\ninformation through the platform\u2019s output. The second one is a local privacy loss because of the\nleakage of her information through the raw data that she shares with the platform. Each user has a\nheterogeneous privacy sensitivity for these two types of privacy losses. To model such heterogeneity,\nfor each i \u2208 N , we let ci \u2208 [0, 1] be her relative local privacy sensitivity, representing the relative\nweight that user i assigns to the (per unit cost of) local privacy loss. We also let 1 \u2212 ci be her relative\ncentral privacy sensitivity, representing the relative weight that user i assigns to the (per unit cost of)\ncentral privacy loss. Therefore, ci \u2248 1 implies that user i suffers a higher loss of privacy by sharing\nher raw data with the platform (local privacy loss) compared to her loss from the platform\u2019s output\n\n1This assumption does not have any fundamental impact on the results and is made to simplify the notations.\n\n5\n\n\f(central privacy loss). Differently, ci \u2248 0 implies that the user suffers a smaller loss of privacy by\nsharing her raw data with the platform compared to her loss from the platform\u2019s output. In what\nfollows, we use the term privacy sensitivity instead of relative local privacy sensitivity.\n\nFor each i \u2208 N , the privacy sensitivity ci is independently drawn from a publicly known distribution\nwhose support is [0, 1] with cumulative distribution and probability density functions Fi(\u00b7) and fi(\u00b7).\nWe also let c = (c1, . . . , cn) be the vector of privacy sensitivities. The privacy sensitivity of each\nuser is her private information, i.e., the platform does not know it. This is because individuals have\ndifferent views regarding how trustworthy the platform is in protecting their raw data.\n\nThe platform\u2019s objective is to design a mechanism to collect users\u2019 data by paying them to compensate\nfor their privacy losses without knowing the privacy sensitivity of users. To introduce the platform\u2019s\nobjective formally, we adopt the formalism of Bayesian mechanism design pioneered by Myerson\n[1981]. More specifically, the platform designs and announces a payment function, a local privacy loss\nfunction, and a central privacy loss function that are mappings from the reported privacy sensitivities\nof users. The users then report their privacy sensitivities (which may or may not be truthful). Based\non the payment function, the platform compensates the users (the compensation could be monetary\nor some free or discounted service provided to the user). Based on the local and central privacy\nfunctions, the platform designs randomized channels and randomized estimation algorithms that\ndeliver the guaranteed local and central privacy losses while minimizing the sum of the mean squared\nerror and the total expected payments. Given this interaction, we next formally introduce a data\nacquisition mechanism with two-part data privacy guarantees.\n\nDefinition 4 (two-part private data acquisition mechanism). We call the tuple (\u02c6\u03b8, \u03b5(l), \u03b5(c), t) a\ntwo-part private data acquisition mechanism where\n\n1. For all i \u2208 N , \u03b5(l)\ni\n\n: Rn\n\n+ \u2192 R+ is a function that maps the vector of privacy sensitivities c\n\nto a local privacy loss for user i, \u03b5(l)\n\ni (c), with \u03b5(l)(\u00b7) = (\u03b5(l)\n\ni (\u00b7))n\n\ni=1.\n\n2. For all i \u2208 N , \u03b5(c)\n\ni\n\n: Rn\n\n+ \u2192 R+ is a function that maps the vector of privacy sensitivities c\n\nto a central privacy loss for user i, \u03b5(c)\n\ni (c), with \u03b5(c)(.) = (\u03b5(c)\n\ni (\u00b7))n\n\ni=1.\n\n3. \u02c6\u03b8 : X n \u00d7 Rn\n\n+ \u00d7 Rn\n\nacquired locally (\u03b5(l)\nestimate \u02c6\u03b8(x, \u03b5(l)(c), \u03b5(c)(c)).\n\n+ \u2192 R is a (\u03b5(c)(c), \u03b1)-R\u00e9nyi differentially private estimator that maps\ni (c), \u03b1)-R\u00e9nyi differentially private data of user i for i \u2208 N to an\n\n4. For all i \u2208 N , ti : Rn\n\n+ \u2192 R+ is a function that maps the vector of privacy sensitivities c to\n\na payment for user i, ti(c), with t(.) = (ti(\u00b7))n\n\ni=1.\n\nNotice that we have not specified the estimator and the mechanisms that delivers (local and central)\nR\u00e9nyi differential privacy. In the rest of this subsection, we introduce the utilities and the platform\u2019s\nproblem for a general estimators and mechanisms to deliver differential privacy. Later, we focus on\nlinear estimator and Gaussian mechanisms and explicitly solve the platform\u2019s problem.\n\nEach user that participates in a two-part private data acquisition mechanism suffers from both the\nlocal and central privacy losses and need to be compensated by the platform. In particular, the utility\nof user i from participation when her privacy sensitivity is ci and she reports c\u2032\n\ni is given by\n\nui(\u03b5(l)(c\u2032\n\ni, c\u2212i), \u03b5(c)(c\u2032\n\ni, c\u2212i), t, \u02c6\u03b8) = Ec\u2212i[ti(c\u2212i, c\u2032\n\ni)) \u2212 ci\u03b5(l)\n\ni (c\u2212i, c\u2032\n\ni) \u2212 (1 \u2212 ci)\u03b5(c)\n\ni (c\u2212i, c\u2032\n\ni)],\n\ni)) is the payment from the platform, the term ci\u03b5(l)\n\nwhere the term ti(c\u2212i, c\u2032\ni) is the relative lo-\ncal privacy sensitivity of the user multiplied by her local privacy loss, and the term (1\u2212ci)\u03b5(c)\ni (c\u2212i, c\u2032\ni)\nis her relative central privacy sensitivity multiplied by her central privacy loss. A user i \u2208 N that\ndoes not participate in the mechanism neither compromises her privacy nor gets a compensation.\nTherefore, the utility of a user who does not participate in the mechanism becomes 0.\n\ni (c\u2212i, c\u2032\n\nThe goal of the platform is to minimize the sum of the mean squared error and the overall payment to\nusers. We let \u03b3 \u2208 R+ represents the relative weight of the mean estimation error and the payments in\n\n6\n\n\fthe platform\u2019s objective.2 Therefore, the platform\u2019s objective is\nn\n(cid:88)\n\nEc[\u03b3MSE(\u03b5(l)(c), \u03b5(c)(c), \u02c6\u03b8) +\n\nti(c)],\n\ni=1\n\nwhere the first term is the mean square error of estimator \u02c6\u03b8 given reported vector of privacy sensitivity\nand resulting local and central privacy losses \u03b5(l) and \u03b5(c), i.e.,\n\nMSE(\u03b5(l)(c), \u03b5(c)(c), \u02c6\u03b8) = Ex[|\u02c6\u03b8(\u02c6x, \u03b5(l), \u03b5(c)) \u2212 \u03b8|2].\nAlso, each summand of the second term is the compensation that the platform gives to a user to\nincentivize her to participate and report her privacy sensitivity truthfully.\n\nIn Appendix we prove that, similar to the classical mechanism design setting, revelation principle\nholds. This means that there is no loss of generality in focusing on the class of direct incentive\ncompatible mechanisms, meaning the platform\u2019s optimization problem can be written as\n\nmin\n\u03b5(l)(\u00b7),\u03b5(c)(\u00b7),t(\u00b7)\n\nEc[\u03b3MSE(\u03b5(l)(c), \u03b5(c)(c), \u02c6\u03b8) +\n\nn\n(cid:88)\n\nti(c)]\n\ni=1\nui(\u03b5(l)(c), \u03b5(c)(c), t, \u02c6\u03b8) \u2265 ui(\u03b5(l)(c\u2032\nui(\u03b5(l)(c), \u03b5(c)(c), t, \u02c6\u03b8) \u2265 0\n\ni, c\u2212i), \u03b5(c)(c\u2032\n\ni, c\u2212i), t, \u02c6\u03b8)\n\n(5)\nwhere the constraints in (4) represent the incentive compatibility. These constraints guarantee that\nthat each user i has no incentive to misrepresent her privacy sensitivity when others report truthfully\n(reporting truthfully is an equilibrium of the game among the users). Also, the constraints in (5)\nrepresent individual rationality, which ensures that each user receives a non-negative utility from\nparticipating in the platform\u2019s mechanism and sharing her data.\n\nfor all i \u2208 N , ci,\n\n(3)\n\n(4)\n\n3 From the mechanism design problem to an optimization problem\n\nFor a given estimator \u02c6\u03b8, the platform\u2019s decision comprises the local and central privacy loss functions\n\u03b5(l)(\u00b7) and \u03b5(c)(\u00b7) together with the payment functions t(\u00b7). We next show that this problem can be\nequivalently formulated as an optimization problem over the vector of local privacy losses and central\nprivacy losses (as opposed to functions). In the rest of the paper, we impose the following assumption\nwhich is well-known in the mechanism design literature and simplifies the analysis.3\nAssumption 1. For any user i \u2208 N , the virtual cost defined as \u03c8i(c) = c + Fi(c)\nfi(c) is increasing in c,\nwhere fi(\u00b7) and Fi(\u00b7) are probability density and cumulative distribution functions of ci, respectively.\n\nThe above assumption holds for a wide class of distributions such as the ones with log-concave\ndensity functions (e.g., uniform).\nTheorem 2. Suppose Assumption 1 holds. For a given estimator \u02c6\u03b8 : \u02c6X n \u00d7 Rn\n+ \u2192 R, in the\noptimal two-part data acquisition mechanism, for a given vector of reported privacy sensitivities c,\nthe local and central privacy losses are the solution of\nn\n(cid:88)\n\n+ \u00d7 Rn\n\nn\n(cid:88)\n\n\u03b5(l)\ni \u03c8i(ci) +\n\n\u03b5(c)\ni (1 \u2212 \u03c8i(ci)).\n\n(6)\n\n\u03b3MSE(\u03b5(l), \u03b5(c), \u02c6\u03b8) +\n\nmin\ni=1,{\u03b5(c)}n\n\ni=1\n\n{\u03b5(l)}n\n\ni=1\n\ni=1\n\nProof Sketch of Theorem 2: We introduce the following interim functions\n\nti(ci) =Ec\u2212i [t(ci, c\u2212i)],\n\ni (ci) = Ec\u2212i[\u03b5(l)\n\u03b5(l)\n\ni (ci, c\u2212i)],\n\nand \u03b5(c)\n\ni (ci) = Ec\u2212i[\u03b5(c)\n\ni (ci, c\u2212i)].\n\nWe first establish a payment identity that determines the optimal payment in terms of the optimal\nlocal and central privacy losses. In particular, by evaluating the first order condition corresponding to\nthe incentive compatibility constraint (4), we establish that this constraint holds if and only if\n\nti(ci) = ti(0) + \u03b5(c)\n\ni (ci) \u2212 \u03b5(c)\n\ni (0) + ci(\u03b5(l)\n\ni (ci) \u2212 \u03b5(c)\n\ni (ci)) \u2212\n\n(cid:90) ci\n\n0\n\n(\u03b5(l)\n\ni (z) \u2212 \u03b5(c)\n\ni (z))dz,\n\n2Notice that changing the parameter \u03b3 enables us to study a wide range of platform\u2019s objectives with differing\n\nrelative weights between the estimation error and the total payments.\n\n3Without this assumption, extending the results requires ironing technique of Myerson [1981].\n\n7\n\n\fi (z) \u2212 \u03b5(c)\n\nand \u03b5(l)\ni (z) is weakly decreasing in z. We then plug in this payment identity back to the\nplatform\u2019s objective, use the individual rationality constraint, and rewrite the platform\u2019s expected\nutility in terms of the privacy loss functions and the virtual cost of users. This is still a functional\noptimization problem in terms of \u03b5(l)(\u00b7) and \u03b5(c)(\u00b7). However, we establish that, under Assumption 1,\nwe can solve this functional optimization point-wise (i.e., for any given c). \u25a0\n\nTheorem 2 highlights the tradeoff in the platform\u2019s problem: by decreasing the local privacy loss,\nthe second term of the objective decreases (this term corresponds to the payment to users) while\nthe first term (i.e., the mean squared error) increases. The role of the central privacy loss is more\nnuanced, and there are two cases. If the coefficient 1 \u2212 \u03c8i(ci) is non-negative, by decreasing the\ncentral privacy loss, the third term of the objective decreases while the first term increases. If the\ncoefficient 1 \u2212 \u03c8i(ci) is negative, increasing the central privacy loss decreases both the third term\nand the first term. However, we cannot increase the central privacy loss level without limits because\nthe central privacy loss level is always below the local privacy loss level. Therefore, the platform\u2019s\noptimal mechanism should find the \u201cright\u201d balance between these terms.\n\n4 Optimal mechanism with two-part privacy guarantees for linear estimators\n\nFor the rest of the paper, we focus on linear estimators with Gaussian mechanism described in Section\n2.1. The following is a direct corollary of Theorem 2.\nCorollary 1. Suppose Assumption 1 holds. For any reported vector of privacy sensitivities c, the\noptimal local privacy loss levels are \u03b5(l)\nn\n(cid:88)\n\ni and the optimal central privacy loss levels are\n\ni (c) = y\u2217\n\n2\n\nwhere (w\u2217\n\n1, . . . , w\u2217\n\nn) and (y\u2217\n\n1, . . . , y\u2217\n\nn) are the optimal solution of\n\n\u03b5(c)\ni = w\u2217\ni\n\n2/\n\nw\u2217\nj\ny\u2217\nj\n\nj=1\n\nmin\nw,y\n\nVAR\u03b3\n\nn\n(cid:88)\n\ni=1\n\nw2\n\ni +\n\n\u03b3\u03b1\n2\n\nn\n(cid:88)\n\ni=1\n\nw2\ni\nyi\n\n+\n\ns.t. wi, yi \u2265 0, for all i \u2208 N and\n\nn\n(cid:88)\n\ni=1\n\nn\n(cid:88)\n\ni=1\n\n(1 \u2212 \u03c8i(ci))\n\nw2\ni\n\n(cid:80)n\n\nj=1\n\nw2\nj\nyj\n\n+\n\nn\n(cid:88)\n\ni=1\n\n\u03c8i(ci)yi\n\n(7)\n\nwi = 1.\n\nLet us highlight the difference between our characterization and that of classic mechanism design (e.g.,\nMyerson [1981]). In classic mechanism design, the designer\u2019s problem becomes linear optimization.\nHowever, in our setting, the designer\u2019s problem is a non-linear and non-convex optimization. This\nmakes the problem of finding the optimal two-part data acquisition mechanism challenging. Before\naddressing this computational challenge, let us revisit the form of the Gaussian mechanism that we\nhave adopted: the platform adds Gaussian noise locally and then outputs a convex combination of\nthe privatized users\u2019 data without adding any noise centrally. More specifically, one may guess that\nthe platform may benefit by having a central noise added to the final output in addition to the local\nnoises. In the following subsection, we establish that there is another Gaussian mechanism for any\nGaussian mechanism that only adds local noises and achieves a weakly lower cost.\n\n4.1 Optimality of having only local noises in the Gaussian mechanism\n\nThe platform has the opportunity of adding Gaussian noise to both the raw data of each user and the\nfinal estimator and ex-ante one may guess that it is optimal to use both of these instruments. However,\nas we establish next, interestingly, in the optimal two-part data acquisition mechanism, it is always\noptimal to only add noises locally.\nFor a given vector of local privacy losses (\u03b5(l)\nGaussian mechanism with both local and central noises is of the form\n\nn ) and central privacy losses (\u03b5(c)\n\n1 , . . . , \u03b5(c)\n\n1 , . . . , \u03b5(l)\n\nn ), a\n\n\u02c6\u03b8(x1, . . . , xn) :=\n\nn\n(cid:88)\n\ni=1\n\nwi \u02c6xi + N (0,\n\n\u03b1\n2\u03b5\n\n) where\n\nn\n(cid:88)\n\ni=1\n\nwi = 1 and \u02c6xi = xi + N (0,\n\n\u03b1\n2\u03b5(l)\ni\n\n)\u2200i \u2208 N .\n\nProposition 1. In the optimal two-part data acquisition mechanism that adopts a Gaussian mecha-\nnism with both local and central noises, we have \u03b5 = \u221e.\n\n8\n\n\fAlgorithm 1: Computing the optimal two-part private data acquisition mechanism\nInput: The vector of privacy sensitivities (c1, . . . , cn)\nfor S \u2208 Grid (cid:0)[S, \u00afS], \u03b4(cid:1) do\n\nLet\n\n\u03bdi =\n\n1\n\u03b3 VAR + (1 \u2212 \u03c8i(ci))/S\n\n, \u03b6i =\n\n\u03bdi\nj \u03bdj\n\n(cid:80)\n\n, \u03bei = \u03b6i\n\n(cid:113)\n\n\u03bdj(\n\n\u03c8j(cj) \u2212 (cid:112)\u03c8i(ci))\n\n\uf8f6\n\n\uf8f8 .\n\n\uf8eb\n\n\uf8ed\n\nn\n(cid:88)\n\nj=1\n\n(cid:18) (cid:80)n\n\ni=1 \u03b6i\n\n\u03c8i(ci)\n\nS\u2212(cid:80)n\n\ni=1\n\n\u03c8i(ci)\u03bei\n\n\u221a\n\u221a\n\n(cid:19)2\n\n.\n\nLet p =\n\nLet\n\n\u03bdi + \u03bdi\n\nwi(S) =\n\n(cid:112)\u03c8j(cj)p\n\n(cid:80)\n\nj \u03bdj\n(cid:80)\nj \u03bdj\n\n\u2212 \u03bdi\n\n(cid:112)\u03c8i(ci)p,\n\nyi(S) = wi(S)\n\n(cid:114) p\n\n,\n\n\u03c8i(ci)\n\nand OBJ(S) be the objective of Problem (7) evaluated for this solution.\n\nend\nOutput: {yi(S\u2217), wi(S\u2217)}n\n\ni=1, where (S\u2217) = arg min(S) OBJ(S).\n\nProposition 1 has an important implication in terms of the design of data market architecture when\nusers have both central and local privacy costs: it is optimal to add noise locally! Adding a noise\ncentrally to the final estimator has an advantage because the weights in the final estimator give the\nplatform a lever to deliver heterogeneous central privacy guarantees to users. Despite this advantage,\nwe establish that adding noise centrally is never optimal. This is because the platform prefers to add\nthe noise locally to contribute to both central and local privacy guarantees delivered to users.\n\n4.2 Computing the optimal privacy loss function\n\nThe implementation of the optimal two-part private data acquisition mechanism requires solving\nProblem (7), which is a non-convex program. However, we use the structure of the problem to develop\na polynomial time algorithm to solve it approximately. To do so, we first replace (cid:80)n\ni /yi by an\nauxiliary variable S. Next, we consider the corresponding lagrangian problem. Using Karush-Kuhn-\nTucker (KKT) conditions, we establish a number of relations between problems\u2019 parameters, S, and\np, the lagrangian coefficient corresponding to S = (cid:80)n\ni /yi. Furthermore, we develop upper and\nlower bounds for S. Finally, we do a grid search to find the approximate optimal solution.\nTheorem 3. For any vector of reported privacy sensitivities and \u03f5 > 0, Algorithm 1 finds local privacy\nloss levels and the differentially private linear estimator of the two-part data acquisition mechanism\nwhose cost (i.e., platform\u2019s objective) is at most 1 + \u03b4 of the optimal cost in time poly(n, 1\n\ni=1 w2\n\ni=1 w2\n\n\u03b4 ).\n\nNotice that the approximation factor in Theorem 3 depends on the underlying parameters. Therefore,\nwe have a Polynomial Time Approximation Scheme (PTAS) for finding the optimal two-part data\nacquisition mechanism in the class of linear estimators.\n\nIn the Appendix, we provide a case study with two users to illustrate the performance of the optimal\ntwo-part data acquisition mechanism in terms of the guaranteed privacy levels and payments as\nfunctions of the reported privacy sensitivities.\n\n\n\nBased on the section(s), please answer the following question with yes, no, or n/a and provide a brief justification for your answer.\nQuestion: If the authors use existing assets (e.g., code, data, models) or curate/release new assets, do the authors mention the license of the assets?"}, "4c": {"role": "user", "content": "The following is the methods section of the paper you are reviewing:\n\n\nWe consider a platform interested in estimating a parameter \u03b8 \u2208 R by collecting data of n users,\nindexed by N = {1, \u00b7 \u00b7 \u00b7 , n}. For any i \u2208 N , we denote user i\u2019s data by Xi \u2208 X and we assume Xi\n\n3\n\n\fis given by Xi = \u03b8 + Zi where Z1, \u00b7 \u00b7 \u00b7 , Zn are independent and identically distributed zero-mean\nrandom variables with variance VAR. To simplify the exposition, we further assume |Zi| \u2264 1/2 for\nany i \u2208 N .1 Throughout, we use lower case letters to denote the realization of random variables. The\nplatform\u2019s goal is to minimize the estimate\u2019s error by collecting data from privacy-concerned users.\nTherefore, the platform needs to incentivize them to share their data.\n\n2.1 Local and central privacy losses\n\nBefore formalizing the utilities/objectives of the platform and the users, we define the notions of\nprivacy losses that we adopt in this paper. In particular, we consider two different types of privacy\nlosses that users suffer from. The first one is the privacy loss of a user when she shares her data (only)\nwith the platform, and the second one is the privacy loss through the released estimate (to the public)\nby the platform. Depending on how different users trust the platform, they might care differently\nabout these two privacy losses. For instance, if a user fully trusts the platform, then her main privacy\nconcern would be the second one, while a user who does not trust the platform at all would be more\nconcerned with the first one as the public only observes the aggregated estimate, as opposed to the\nplatform which observes each user\u2019s (shared) data separately.\n\nWe use the differential privacy framework to quantify these privacy losses. Since differential privacy\nwas introduced by Dwork et al., several variants of it have been also proposed. In particular, a popular\none in the machine learning literature is R\u00e9nyi differential privacy (RDP), introduced by Mironov\n[2017], which we also adopt in this paper. Let us first recall the definition of R\u00e9nyi divergence.\nDefinition 1. Let P and Q be two distributions over R with densities p and q. For any \u03b1 \u2208 (1, \u221e],\nthe R\u00e9nyi \u03b1-divergence between P and Q is denoted by D\u03b1(P ||Q) and is given by\n\nD\u03b1(P ||Q) :=\n\n1\n\u03b1 \u2212 1\n\nlog\n\n(cid:90) (cid:18) p(x)\nq(x)\n\n(cid:19)\u03b1\n\nq(x)dx.\n\nFor two random variables X and Y , D\u03b1(X||Y ) denotes the \u03b1-divergence between their distributions.\n\nWe next define two notions of differential privacy, known as central and local, to capture the two\naforementioned types of privacy losses. Local differential privacy corresponds to the privacy loss of a\nuser when she shares her data with the platform through a randomized mapping, known as a channel.\nDefinition 2. Let \u03b5 \u2265 0 and \u03b1 \u2208 (1, \u221e]. A randomized channel C : X \u2192 R is locally (\u03b5, \u03b1)-R\u00e9nyi\n(differentially) private if for any x, x\u2032 \u2208 X ,\n\nD\u03b1(C(x)||C(x\u2032)) \u2264 \u03b5.\n\nCentral differential privacy corresponds to the other privacy loss mentioned above. It bounds the\nchange in the distribution of the platform\u2019s output, i.e., the released estimate, by changing one user\u2019s\ndata. We next provide the formal definition.\ni=1 \u2208 Rn\nDefinition 3. Let \u03b5 = (\u03b5i)n\n(\u03b5, \u03b1)-R\u00e9nyi (differentially) private if for any two datasets x1:n, x\u2032\ncoordinate (data of user i),\n\n+ and \u03b1 \u2208 (1, \u221e]. A randomized algorithm A : X n \u2192 R is\n1:n \u2208 X n that only differ in i-th\n\nD\u03b1(A(x1:n)||A(x\u2032\n\n1:n)) \u2264 \u03b5i.\n\nThe customary approach to guarantee RDP is Gaussian mechanism in which a properly tuned zero-\nmean Gaussian noise is added to fulfill the required condition. The following lemma, adapted from\nMironov [2017], allows us to characterize the Gaussian noise\u2019s variance for a privacy loss level.\nLemma 1. For a function f : X n \u2192 R, we define its sensitivity with respect to i-th coordinate as\n\nLi(f ) := sup {|f (x1:n) \u2212 f (x\u2032\n\n1:n)| : for all x1:n and x\u2032\n\n1:n differing only at i-th coordinate} .\n\nFor any \u03b1 \u2208 (1, \u221e], A(x1:n) = f (x1:n) + W with W \u223c N (0, \u03c32) is\n\n(cid:16)\n\n( \u03b1Li(f )2\n2\u03c32\n\n)n\ni=1, \u03b1\n\n(cid:17)\n\n-RDP.\n\nFor a given vector of local privacy losses (\u03b5(l)\nis by using a linear estimator with Gaussian mechanism which is given by\n\n1 , . . . , \u03b5(l)\n\nn ), a natural way to privately estimate the mean\n\n\u02c6\u03b8(x1, . . . , xn) :=\n\nn\n(cid:88)\n\ni=1\n\nwi \u02c6xi where\n\nn\n(cid:88)\n\ni=1\n\nwi = 1 and \u02c6xi = xi + N (0,\n\n\u03b1\n2\u03b5(l)\ni\n\n) for all i \u2208 N .\n\n(1)\n\n4\n\n\fFigure 1: The interaction between the users and the platform in the two-part private data acquisition.\n\ni / (cid:80)n\n\ni = w2\n\nand the central privacy delivered to user i \u2208 N is \u03b5(c)\n\nFigure 1 depicts this estimator. In particular, using Lemma 1, the local privacy delivered to user\ni \u2208 N is \u03b5(l)\nj . In the next\ni\nsection, we focus on linear estimators in designing the optimal private data acquisition mechanism.\nWe motivate such specification by showing that, for a given vector of local privacy losses (\u03b5(l)\ni=1,\na linear estimator is optimal with respect to mean square error. To formalize this statement, we\nfirst need to define the minimax estimation error as the notion of optimality. Let P be a class of\ndistributions over X . For any P \u2208 P, we denote its mean by \u03b8(P ). A (\u03b5(l)\ni=1-locally RDP estimator\ncan be cast as \u02c6\u03b8((Ci(xi))n\ni=1), where Ci(.) is the randomized channel corresponding to user i. Let\nQ((\u03b5(l)\ni=1-locally RDP estimators. The minimax estimation error is\ni=1,\u02c6\u03b8[|\u02c6\u03b8((Ci(Xi))n\n\ni=1) be the class of such (\u03b5(l)\n\ni=1) \u2212 \u03b8(P )|2],\n\nL(P, Q, (\u03b5(l)\n\nj=1 w2\n\ni=1) :=\n\nj /\u03b5(l)\n\n(Xi\u223cP )n\n\ni )n\n\ni )n\n\ni )n\n\ni )n\n\ni )n\n\n(2)\n\nE\n\ninf\ni=1\u2208Q((\u03b5(l)\n\ni )n\n\ni=1)\n\nsup\nP \u2208P\n\n\u02c6\u03b8,{Ci}n\n\nwhere the expectation is taken over both randomness of data and estimator (including private channels).\nIn other words, the optimal estimator is the one that has the lowest worst case error among all\nestimators that satisfy the privacy requirements. We next state the optimality result.\nTheorem 1. Assume \u03b1 \u2265 2 and \u03b5(l)\nand C1, \u00b7 \u00b7 \u00b7 , Cn be independent channels. Then, there exists a universal constant c such that\n\ni \u2264 1 for all i. Let P1 be the family of distributions over [\u2212 1\n\n2 , 1\n2 ]\n\nL(P, Q, (\u03b5(l)\n\ni )n\n\ni=1) \u2265 c min(\n\n1\ni=1 \u03b5(l)\n\ni\n\n(cid:80)n\n\n, 1).\n\nFurthermore, there exists a linear estimator with Gaussian mechanism such that\n\nE\n\n(Xi\u223cP )n\n\ni=1,\u02c6\u03b8[|\u02c6\u03b8((Ci(Xi))n\n\ni=1) \u2212 \u03b8(P )|2] \u2264 O(1)\n\n\u03b1\ni=1 \u03b5(l)\n\ni\n\n(cid:80)n\n\n.\n\nWe prove the lower bound by using the Le Cam\u2019s method Yu [1997] that reduces the problem of\nfinding lower bounds to a hypothesis testing problem and requires bounding the total variation\ndistance between the estimates when the input data points change. We then find it more convenient to\nbound the total variation in terms of Hellinger distance and use a series of inequalities to bound it.\n\n2.2 Data acquisition mechanism with two-part privacy guarantees\n\nWe next describe the utility functions of the users and the platform and then formulate the platform\u2019s\noptimal data acquisition mechanism. As we described earlier, each user suffers from two privacy\nlosses when sharing her data. The first one is a central privacy loss because of the leakage of her\ninformation through the platform\u2019s output. The second one is a local privacy loss because of the\nleakage of her information through the raw data that she shares with the platform. Each user has a\nheterogeneous privacy sensitivity for these two types of privacy losses. To model such heterogeneity,\nfor each i \u2208 N , we let ci \u2208 [0, 1] be her relative local privacy sensitivity, representing the relative\nweight that user i assigns to the (per unit cost of) local privacy loss. We also let 1 \u2212 ci be her relative\ncentral privacy sensitivity, representing the relative weight that user i assigns to the (per unit cost of)\ncentral privacy loss. Therefore, ci \u2248 1 implies that user i suffers a higher loss of privacy by sharing\nher raw data with the platform (local privacy loss) compared to her loss from the platform\u2019s output\n\n1This assumption does not have any fundamental impact on the results and is made to simplify the notations.\n\n5\n\n\f(central privacy loss). Differently, ci \u2248 0 implies that the user suffers a smaller loss of privacy by\nsharing her raw data with the platform compared to her loss from the platform\u2019s output. In what\nfollows, we use the term privacy sensitivity instead of relative local privacy sensitivity.\n\nFor each i \u2208 N , the privacy sensitivity ci is independently drawn from a publicly known distribution\nwhose support is [0, 1] with cumulative distribution and probability density functions Fi(\u00b7) and fi(\u00b7).\nWe also let c = (c1, . . . , cn) be the vector of privacy sensitivities. The privacy sensitivity of each\nuser is her private information, i.e., the platform does not know it. This is because individuals have\ndifferent views regarding how trustworthy the platform is in protecting their raw data.\n\nThe platform\u2019s objective is to design a mechanism to collect users\u2019 data by paying them to compensate\nfor their privacy losses without knowing the privacy sensitivity of users. To introduce the platform\u2019s\nobjective formally, we adopt the formalism of Bayesian mechanism design pioneered by Myerson\n[1981]. More specifically, the platform designs and announces a payment function, a local privacy loss\nfunction, and a central privacy loss function that are mappings from the reported privacy sensitivities\nof users. The users then report their privacy sensitivities (which may or may not be truthful). Based\non the payment function, the platform compensates the users (the compensation could be monetary\nor some free or discounted service provided to the user). Based on the local and central privacy\nfunctions, the platform designs randomized channels and randomized estimation algorithms that\ndeliver the guaranteed local and central privacy losses while minimizing the sum of the mean squared\nerror and the total expected payments. Given this interaction, we next formally introduce a data\nacquisition mechanism with two-part data privacy guarantees.\n\nDefinition 4 (two-part private data acquisition mechanism). We call the tuple (\u02c6\u03b8, \u03b5(l), \u03b5(c), t) a\ntwo-part private data acquisition mechanism where\n\n1. For all i \u2208 N , \u03b5(l)\ni\n\n: Rn\n\n+ \u2192 R+ is a function that maps the vector of privacy sensitivities c\n\nto a local privacy loss for user i, \u03b5(l)\n\ni (c), with \u03b5(l)(\u00b7) = (\u03b5(l)\n\ni (\u00b7))n\n\ni=1.\n\n2. For all i \u2208 N , \u03b5(c)\n\ni\n\n: Rn\n\n+ \u2192 R+ is a function that maps the vector of privacy sensitivities c\n\nto a central privacy loss for user i, \u03b5(c)\n\ni (c), with \u03b5(c)(.) = (\u03b5(c)\n\ni (\u00b7))n\n\ni=1.\n\n3. \u02c6\u03b8 : X n \u00d7 Rn\n\n+ \u00d7 Rn\n\nacquired locally (\u03b5(l)\nestimate \u02c6\u03b8(x, \u03b5(l)(c), \u03b5(c)(c)).\n\n+ \u2192 R is a (\u03b5(c)(c), \u03b1)-R\u00e9nyi differentially private estimator that maps\ni (c), \u03b1)-R\u00e9nyi differentially private data of user i for i \u2208 N to an\n\n4. For all i \u2208 N , ti : Rn\n\n+ \u2192 R+ is a function that maps the vector of privacy sensitivities c to\n\na payment for user i, ti(c), with t(.) = (ti(\u00b7))n\n\ni=1.\n\nNotice that we have not specified the estimator and the mechanisms that delivers (local and central)\nR\u00e9nyi differential privacy. In the rest of this subsection, we introduce the utilities and the platform\u2019s\nproblem for a general estimators and mechanisms to deliver differential privacy. Later, we focus on\nlinear estimator and Gaussian mechanisms and explicitly solve the platform\u2019s problem.\n\nEach user that participates in a two-part private data acquisition mechanism suffers from both the\nlocal and central privacy losses and need to be compensated by the platform. In particular, the utility\nof user i from participation when her privacy sensitivity is ci and she reports c\u2032\n\ni is given by\n\nui(\u03b5(l)(c\u2032\n\ni, c\u2212i), \u03b5(c)(c\u2032\n\ni, c\u2212i), t, \u02c6\u03b8) = Ec\u2212i[ti(c\u2212i, c\u2032\n\ni)) \u2212 ci\u03b5(l)\n\ni (c\u2212i, c\u2032\n\ni) \u2212 (1 \u2212 ci)\u03b5(c)\n\ni (c\u2212i, c\u2032\n\ni)],\n\ni)) is the payment from the platform, the term ci\u03b5(l)\n\nwhere the term ti(c\u2212i, c\u2032\ni) is the relative lo-\ncal privacy sensitivity of the user multiplied by her local privacy loss, and the term (1\u2212ci)\u03b5(c)\ni (c\u2212i, c\u2032\ni)\nis her relative central privacy sensitivity multiplied by her central privacy loss. A user i \u2208 N that\ndoes not participate in the mechanism neither compromises her privacy nor gets a compensation.\nTherefore, the utility of a user who does not participate in the mechanism becomes 0.\n\ni (c\u2212i, c\u2032\n\nThe goal of the platform is to minimize the sum of the mean squared error and the overall payment to\nusers. We let \u03b3 \u2208 R+ represents the relative weight of the mean estimation error and the payments in\n\n6\n\n\fthe platform\u2019s objective.2 Therefore, the platform\u2019s objective is\nn\n(cid:88)\n\nEc[\u03b3MSE(\u03b5(l)(c), \u03b5(c)(c), \u02c6\u03b8) +\n\nti(c)],\n\ni=1\n\nwhere the first term is the mean square error of estimator \u02c6\u03b8 given reported vector of privacy sensitivity\nand resulting local and central privacy losses \u03b5(l) and \u03b5(c), i.e.,\n\nMSE(\u03b5(l)(c), \u03b5(c)(c), \u02c6\u03b8) = Ex[|\u02c6\u03b8(\u02c6x, \u03b5(l), \u03b5(c)) \u2212 \u03b8|2].\nAlso, each summand of the second term is the compensation that the platform gives to a user to\nincentivize her to participate and report her privacy sensitivity truthfully.\n\nIn Appendix we prove that, similar to the classical mechanism design setting, revelation principle\nholds. This means that there is no loss of generality in focusing on the class of direct incentive\ncompatible mechanisms, meaning the platform\u2019s optimization problem can be written as\n\nmin\n\u03b5(l)(\u00b7),\u03b5(c)(\u00b7),t(\u00b7)\n\nEc[\u03b3MSE(\u03b5(l)(c), \u03b5(c)(c), \u02c6\u03b8) +\n\nn\n(cid:88)\n\nti(c)]\n\ni=1\nui(\u03b5(l)(c), \u03b5(c)(c), t, \u02c6\u03b8) \u2265 ui(\u03b5(l)(c\u2032\nui(\u03b5(l)(c), \u03b5(c)(c), t, \u02c6\u03b8) \u2265 0\n\ni, c\u2212i), \u03b5(c)(c\u2032\n\ni, c\u2212i), t, \u02c6\u03b8)\n\n(5)\nwhere the constraints in (4) represent the incentive compatibility. These constraints guarantee that\nthat each user i has no incentive to misrepresent her privacy sensitivity when others report truthfully\n(reporting truthfully is an equilibrium of the game among the users). Also, the constraints in (5)\nrepresent individual rationality, which ensures that each user receives a non-negative utility from\nparticipating in the platform\u2019s mechanism and sharing her data.\n\nfor all i \u2208 N , ci,\n\n(3)\n\n(4)\n\n3 From the mechanism design problem to an optimization problem\n\nFor a given estimator \u02c6\u03b8, the platform\u2019s decision comprises the local and central privacy loss functions\n\u03b5(l)(\u00b7) and \u03b5(c)(\u00b7) together with the payment functions t(\u00b7). We next show that this problem can be\nequivalently formulated as an optimization problem over the vector of local privacy losses and central\nprivacy losses (as opposed to functions). In the rest of the paper, we impose the following assumption\nwhich is well-known in the mechanism design literature and simplifies the analysis.3\nAssumption 1. For any user i \u2208 N , the virtual cost defined as \u03c8i(c) = c + Fi(c)\nfi(c) is increasing in c,\nwhere fi(\u00b7) and Fi(\u00b7) are probability density and cumulative distribution functions of ci, respectively.\n\nThe above assumption holds for a wide class of distributions such as the ones with log-concave\ndensity functions (e.g., uniform).\nTheorem 2. Suppose Assumption 1 holds. For a given estimator \u02c6\u03b8 : \u02c6X n \u00d7 Rn\n+ \u2192 R, in the\noptimal two-part data acquisition mechanism, for a given vector of reported privacy sensitivities c,\nthe local and central privacy losses are the solution of\nn\n(cid:88)\n\n+ \u00d7 Rn\n\nn\n(cid:88)\n\n\u03b5(l)\ni \u03c8i(ci) +\n\n\u03b5(c)\ni (1 \u2212 \u03c8i(ci)).\n\n(6)\n\n\u03b3MSE(\u03b5(l), \u03b5(c), \u02c6\u03b8) +\n\nmin\ni=1,{\u03b5(c)}n\n\ni=1\n\n{\u03b5(l)}n\n\ni=1\n\ni=1\n\nProof Sketch of Theorem 2: We introduce the following interim functions\n\nti(ci) =Ec\u2212i [t(ci, c\u2212i)],\n\ni (ci) = Ec\u2212i[\u03b5(l)\n\u03b5(l)\n\ni (ci, c\u2212i)],\n\nand \u03b5(c)\n\ni (ci) = Ec\u2212i[\u03b5(c)\n\ni (ci, c\u2212i)].\n\nWe first establish a payment identity that determines the optimal payment in terms of the optimal\nlocal and central privacy losses. In particular, by evaluating the first order condition corresponding to\nthe incentive compatibility constraint (4), we establish that this constraint holds if and only if\n\nti(ci) = ti(0) + \u03b5(c)\n\ni (ci) \u2212 \u03b5(c)\n\ni (0) + ci(\u03b5(l)\n\ni (ci) \u2212 \u03b5(c)\n\ni (ci)) \u2212\n\n(cid:90) ci\n\n0\n\n(\u03b5(l)\n\ni (z) \u2212 \u03b5(c)\n\ni (z))dz,\n\n2Notice that changing the parameter \u03b3 enables us to study a wide range of platform\u2019s objectives with differing\n\nrelative weights between the estimation error and the total payments.\n\n3Without this assumption, extending the results requires ironing technique of Myerson [1981].\n\n7\n\n\fi (z) \u2212 \u03b5(c)\n\nand \u03b5(l)\ni (z) is weakly decreasing in z. We then plug in this payment identity back to the\nplatform\u2019s objective, use the individual rationality constraint, and rewrite the platform\u2019s expected\nutility in terms of the privacy loss functions and the virtual cost of users. This is still a functional\noptimization problem in terms of \u03b5(l)(\u00b7) and \u03b5(c)(\u00b7). However, we establish that, under Assumption 1,\nwe can solve this functional optimization point-wise (i.e., for any given c). \u25a0\n\nTheorem 2 highlights the tradeoff in the platform\u2019s problem: by decreasing the local privacy loss,\nthe second term of the objective decreases (this term corresponds to the payment to users) while\nthe first term (i.e., the mean squared error) increases. The role of the central privacy loss is more\nnuanced, and there are two cases. If the coefficient 1 \u2212 \u03c8i(ci) is non-negative, by decreasing the\ncentral privacy loss, the third term of the objective decreases while the first term increases. If the\ncoefficient 1 \u2212 \u03c8i(ci) is negative, increasing the central privacy loss decreases both the third term\nand the first term. However, we cannot increase the central privacy loss level without limits because\nthe central privacy loss level is always below the local privacy loss level. Therefore, the platform\u2019s\noptimal mechanism should find the \u201cright\u201d balance between these terms.\n\n4 Optimal mechanism with two-part privacy guarantees for linear estimators\n\nFor the rest of the paper, we focus on linear estimators with Gaussian mechanism described in Section\n2.1. The following is a direct corollary of Theorem 2.\nCorollary 1. Suppose Assumption 1 holds. For any reported vector of privacy sensitivities c, the\noptimal local privacy loss levels are \u03b5(l)\nn\n(cid:88)\n\ni and the optimal central privacy loss levels are\n\ni (c) = y\u2217\n\n2\n\nwhere (w\u2217\n\n1, . . . , w\u2217\n\nn) and (y\u2217\n\n1, . . . , y\u2217\n\nn) are the optimal solution of\n\n\u03b5(c)\ni = w\u2217\ni\n\n2/\n\nw\u2217\nj\ny\u2217\nj\n\nj=1\n\nmin\nw,y\n\nVAR\u03b3\n\nn\n(cid:88)\n\ni=1\n\nw2\n\ni +\n\n\u03b3\u03b1\n2\n\nn\n(cid:88)\n\ni=1\n\nw2\ni\nyi\n\n+\n\ns.t. wi, yi \u2265 0, for all i \u2208 N and\n\nn\n(cid:88)\n\ni=1\n\nn\n(cid:88)\n\ni=1\n\n(1 \u2212 \u03c8i(ci))\n\nw2\ni\n\n(cid:80)n\n\nj=1\n\nw2\nj\nyj\n\n+\n\nn\n(cid:88)\n\ni=1\n\n\u03c8i(ci)yi\n\n(7)\n\nwi = 1.\n\nLet us highlight the difference between our characterization and that of classic mechanism design (e.g.,\nMyerson [1981]). In classic mechanism design, the designer\u2019s problem becomes linear optimization.\nHowever, in our setting, the designer\u2019s problem is a non-linear and non-convex optimization. This\nmakes the problem of finding the optimal two-part data acquisition mechanism challenging. Before\naddressing this computational challenge, let us revisit the form of the Gaussian mechanism that we\nhave adopted: the platform adds Gaussian noise locally and then outputs a convex combination of\nthe privatized users\u2019 data without adding any noise centrally. More specifically, one may guess that\nthe platform may benefit by having a central noise added to the final output in addition to the local\nnoises. In the following subsection, we establish that there is another Gaussian mechanism for any\nGaussian mechanism that only adds local noises and achieves a weakly lower cost.\n\n4.1 Optimality of having only local noises in the Gaussian mechanism\n\nThe platform has the opportunity of adding Gaussian noise to both the raw data of each user and the\nfinal estimator and ex-ante one may guess that it is optimal to use both of these instruments. However,\nas we establish next, interestingly, in the optimal two-part data acquisition mechanism, it is always\noptimal to only add noises locally.\nFor a given vector of local privacy losses (\u03b5(l)\nGaussian mechanism with both local and central noises is of the form\n\nn ) and central privacy losses (\u03b5(c)\n\n1 , . . . , \u03b5(c)\n\n1 , . . . , \u03b5(l)\n\nn ), a\n\n\u02c6\u03b8(x1, . . . , xn) :=\n\nn\n(cid:88)\n\ni=1\n\nwi \u02c6xi + N (0,\n\n\u03b1\n2\u03b5\n\n) where\n\nn\n(cid:88)\n\ni=1\n\nwi = 1 and \u02c6xi = xi + N (0,\n\n\u03b1\n2\u03b5(l)\ni\n\n)\u2200i \u2208 N .\n\nProposition 1. In the optimal two-part data acquisition mechanism that adopts a Gaussian mecha-\nnism with both local and central noises, we have \u03b5 = \u221e.\n\n8\n\n\fAlgorithm 1: Computing the optimal two-part private data acquisition mechanism\nInput: The vector of privacy sensitivities (c1, . . . , cn)\nfor S \u2208 Grid (cid:0)[S, \u00afS], \u03b4(cid:1) do\n\nLet\n\n\u03bdi =\n\n1\n\u03b3 VAR + (1 \u2212 \u03c8i(ci))/S\n\n, \u03b6i =\n\n\u03bdi\nj \u03bdj\n\n(cid:80)\n\n, \u03bei = \u03b6i\n\n(cid:113)\n\n\u03bdj(\n\n\u03c8j(cj) \u2212 (cid:112)\u03c8i(ci))\n\n\uf8f6\n\n\uf8f8 .\n\n\uf8eb\n\n\uf8ed\n\nn\n(cid:88)\n\nj=1\n\n(cid:18) (cid:80)n\n\ni=1 \u03b6i\n\n\u03c8i(ci)\n\nS\u2212(cid:80)n\n\ni=1\n\n\u03c8i(ci)\u03bei\n\n\u221a\n\u221a\n\n(cid:19)2\n\n.\n\nLet p =\n\nLet\n\n\u03bdi + \u03bdi\n\nwi(S) =\n\n(cid:112)\u03c8j(cj)p\n\n(cid:80)\n\nj \u03bdj\n(cid:80)\nj \u03bdj\n\n\u2212 \u03bdi\n\n(cid:112)\u03c8i(ci)p,\n\nyi(S) = wi(S)\n\n(cid:114) p\n\n,\n\n\u03c8i(ci)\n\nand OBJ(S) be the objective of Problem (7) evaluated for this solution.\n\nend\nOutput: {yi(S\u2217), wi(S\u2217)}n\n\ni=1, where (S\u2217) = arg min(S) OBJ(S).\n\nProposition 1 has an important implication in terms of the design of data market architecture when\nusers have both central and local privacy costs: it is optimal to add noise locally! Adding a noise\ncentrally to the final estimator has an advantage because the weights in the final estimator give the\nplatform a lever to deliver heterogeneous central privacy guarantees to users. Despite this advantage,\nwe establish that adding noise centrally is never optimal. This is because the platform prefers to add\nthe noise locally to contribute to both central and local privacy guarantees delivered to users.\n\n4.2 Computing the optimal privacy loss function\n\nThe implementation of the optimal two-part private data acquisition mechanism requires solving\nProblem (7), which is a non-convex program. However, we use the structure of the problem to develop\na polynomial time algorithm to solve it approximately. To do so, we first replace (cid:80)n\ni /yi by an\nauxiliary variable S. Next, we consider the corresponding lagrangian problem. Using Karush-Kuhn-\nTucker (KKT) conditions, we establish a number of relations between problems\u2019 parameters, S, and\np, the lagrangian coefficient corresponding to S = (cid:80)n\ni /yi. Furthermore, we develop upper and\nlower bounds for S. Finally, we do a grid search to find the approximate optimal solution.\nTheorem 3. For any vector of reported privacy sensitivities and \u03f5 > 0, Algorithm 1 finds local privacy\nloss levels and the differentially private linear estimator of the two-part data acquisition mechanism\nwhose cost (i.e., platform\u2019s objective) is at most 1 + \u03b4 of the optimal cost in time poly(n, 1\n\ni=1 w2\n\ni=1 w2\n\n\u03b4 ).\n\nNotice that the approximation factor in Theorem 3 depends on the underlying parameters. Therefore,\nwe have a Polynomial Time Approximation Scheme (PTAS) for finding the optimal two-part data\nacquisition mechanism in the class of linear estimators.\n\nIn the Appendix, we provide a case study with two users to illustrate the performance of the optimal\ntwo-part data acquisition mechanism in terms of the guaranteed privacy levels and payments as\nfunctions of the reported privacy sensitivities.\n\n\n\nBased on the section(s), please answer the following question with yes, no, or n/a and provide a brief justification for your answer.\nQuestion: If the authors curate/release new assets (e.g., code, data, models), do the authors include any new assets either in the supplemental material or as a URL?"}, "4d": {"role": "user", "content": "The following is the methods section of the paper you are reviewing:\n\n\nWe consider a platform interested in estimating a parameter \u03b8 \u2208 R by collecting data of n users,\nindexed by N = {1, \u00b7 \u00b7 \u00b7 , n}. For any i \u2208 N , we denote user i\u2019s data by Xi \u2208 X and we assume Xi\n\n3\n\n\fis given by Xi = \u03b8 + Zi where Z1, \u00b7 \u00b7 \u00b7 , Zn are independent and identically distributed zero-mean\nrandom variables with variance VAR. To simplify the exposition, we further assume |Zi| \u2264 1/2 for\nany i \u2208 N .1 Throughout, we use lower case letters to denote the realization of random variables. The\nplatform\u2019s goal is to minimize the estimate\u2019s error by collecting data from privacy-concerned users.\nTherefore, the platform needs to incentivize them to share their data.\n\n2.1 Local and central privacy losses\n\nBefore formalizing the utilities/objectives of the platform and the users, we define the notions of\nprivacy losses that we adopt in this paper. In particular, we consider two different types of privacy\nlosses that users suffer from. The first one is the privacy loss of a user when she shares her data (only)\nwith the platform, and the second one is the privacy loss through the released estimate (to the public)\nby the platform. Depending on how different users trust the platform, they might care differently\nabout these two privacy losses. For instance, if a user fully trusts the platform, then her main privacy\nconcern would be the second one, while a user who does not trust the platform at all would be more\nconcerned with the first one as the public only observes the aggregated estimate, as opposed to the\nplatform which observes each user\u2019s (shared) data separately.\n\nWe use the differential privacy framework to quantify these privacy losses. Since differential privacy\nwas introduced by Dwork et al., several variants of it have been also proposed. In particular, a popular\none in the machine learning literature is R\u00e9nyi differential privacy (RDP), introduced by Mironov\n[2017], which we also adopt in this paper. Let us first recall the definition of R\u00e9nyi divergence.\nDefinition 1. Let P and Q be two distributions over R with densities p and q. For any \u03b1 \u2208 (1, \u221e],\nthe R\u00e9nyi \u03b1-divergence between P and Q is denoted by D\u03b1(P ||Q) and is given by\n\nD\u03b1(P ||Q) :=\n\n1\n\u03b1 \u2212 1\n\nlog\n\n(cid:90) (cid:18) p(x)\nq(x)\n\n(cid:19)\u03b1\n\nq(x)dx.\n\nFor two random variables X and Y , D\u03b1(X||Y ) denotes the \u03b1-divergence between their distributions.\n\nWe next define two notions of differential privacy, known as central and local, to capture the two\naforementioned types of privacy losses. Local differential privacy corresponds to the privacy loss of a\nuser when she shares her data with the platform through a randomized mapping, known as a channel.\nDefinition 2. Let \u03b5 \u2265 0 and \u03b1 \u2208 (1, \u221e]. A randomized channel C : X \u2192 R is locally (\u03b5, \u03b1)-R\u00e9nyi\n(differentially) private if for any x, x\u2032 \u2208 X ,\n\nD\u03b1(C(x)||C(x\u2032)) \u2264 \u03b5.\n\nCentral differential privacy corresponds to the other privacy loss mentioned above. It bounds the\nchange in the distribution of the platform\u2019s output, i.e., the released estimate, by changing one user\u2019s\ndata. We next provide the formal definition.\ni=1 \u2208 Rn\nDefinition 3. Let \u03b5 = (\u03b5i)n\n(\u03b5, \u03b1)-R\u00e9nyi (differentially) private if for any two datasets x1:n, x\u2032\ncoordinate (data of user i),\n\n+ and \u03b1 \u2208 (1, \u221e]. A randomized algorithm A : X n \u2192 R is\n1:n \u2208 X n that only differ in i-th\n\nD\u03b1(A(x1:n)||A(x\u2032\n\n1:n)) \u2264 \u03b5i.\n\nThe customary approach to guarantee RDP is Gaussian mechanism in which a properly tuned zero-\nmean Gaussian noise is added to fulfill the required condition. The following lemma, adapted from\nMironov [2017], allows us to characterize the Gaussian noise\u2019s variance for a privacy loss level.\nLemma 1. For a function f : X n \u2192 R, we define its sensitivity with respect to i-th coordinate as\n\nLi(f ) := sup {|f (x1:n) \u2212 f (x\u2032\n\n1:n)| : for all x1:n and x\u2032\n\n1:n differing only at i-th coordinate} .\n\nFor any \u03b1 \u2208 (1, \u221e], A(x1:n) = f (x1:n) + W with W \u223c N (0, \u03c32) is\n\n(cid:16)\n\n( \u03b1Li(f )2\n2\u03c32\n\n)n\ni=1, \u03b1\n\n(cid:17)\n\n-RDP.\n\nFor a given vector of local privacy losses (\u03b5(l)\nis by using a linear estimator with Gaussian mechanism which is given by\n\n1 , . . . , \u03b5(l)\n\nn ), a natural way to privately estimate the mean\n\n\u02c6\u03b8(x1, . . . , xn) :=\n\nn\n(cid:88)\n\ni=1\n\nwi \u02c6xi where\n\nn\n(cid:88)\n\ni=1\n\nwi = 1 and \u02c6xi = xi + N (0,\n\n\u03b1\n2\u03b5(l)\ni\n\n) for all i \u2208 N .\n\n(1)\n\n4\n\n\fFigure 1: The interaction between the users and the platform in the two-part private data acquisition.\n\ni / (cid:80)n\n\ni = w2\n\nand the central privacy delivered to user i \u2208 N is \u03b5(c)\n\nFigure 1 depicts this estimator. In particular, using Lemma 1, the local privacy delivered to user\ni \u2208 N is \u03b5(l)\nj . In the next\ni\nsection, we focus on linear estimators in designing the optimal private data acquisition mechanism.\nWe motivate such specification by showing that, for a given vector of local privacy losses (\u03b5(l)\ni=1,\na linear estimator is optimal with respect to mean square error. To formalize this statement, we\nfirst need to define the minimax estimation error as the notion of optimality. Let P be a class of\ndistributions over X . For any P \u2208 P, we denote its mean by \u03b8(P ). A (\u03b5(l)\ni=1-locally RDP estimator\ncan be cast as \u02c6\u03b8((Ci(xi))n\ni=1), where Ci(.) is the randomized channel corresponding to user i. Let\nQ((\u03b5(l)\ni=1-locally RDP estimators. The minimax estimation error is\ni=1,\u02c6\u03b8[|\u02c6\u03b8((Ci(Xi))n\n\ni=1) be the class of such (\u03b5(l)\n\ni=1) \u2212 \u03b8(P )|2],\n\nL(P, Q, (\u03b5(l)\n\nj=1 w2\n\ni=1) :=\n\nj /\u03b5(l)\n\n(Xi\u223cP )n\n\ni )n\n\ni )n\n\ni )n\n\ni )n\n\ni )n\n\n(2)\n\nE\n\ninf\ni=1\u2208Q((\u03b5(l)\n\ni )n\n\ni=1)\n\nsup\nP \u2208P\n\n\u02c6\u03b8,{Ci}n\n\nwhere the expectation is taken over both randomness of data and estimator (including private channels).\nIn other words, the optimal estimator is the one that has the lowest worst case error among all\nestimators that satisfy the privacy requirements. We next state the optimality result.\nTheorem 1. Assume \u03b1 \u2265 2 and \u03b5(l)\nand C1, \u00b7 \u00b7 \u00b7 , Cn be independent channels. Then, there exists a universal constant c such that\n\ni \u2264 1 for all i. Let P1 be the family of distributions over [\u2212 1\n\n2 , 1\n2 ]\n\nL(P, Q, (\u03b5(l)\n\ni )n\n\ni=1) \u2265 c min(\n\n1\ni=1 \u03b5(l)\n\ni\n\n(cid:80)n\n\n, 1).\n\nFurthermore, there exists a linear estimator with Gaussian mechanism such that\n\nE\n\n(Xi\u223cP )n\n\ni=1,\u02c6\u03b8[|\u02c6\u03b8((Ci(Xi))n\n\ni=1) \u2212 \u03b8(P )|2] \u2264 O(1)\n\n\u03b1\ni=1 \u03b5(l)\n\ni\n\n(cid:80)n\n\n.\n\nWe prove the lower bound by using the Le Cam\u2019s method Yu [1997] that reduces the problem of\nfinding lower bounds to a hypothesis testing problem and requires bounding the total variation\ndistance between the estimates when the input data points change. We then find it more convenient to\nbound the total variation in terms of Hellinger distance and use a series of inequalities to bound it.\n\n2.2 Data acquisition mechanism with two-part privacy guarantees\n\nWe next describe the utility functions of the users and the platform and then formulate the platform\u2019s\noptimal data acquisition mechanism. As we described earlier, each user suffers from two privacy\nlosses when sharing her data. The first one is a central privacy loss because of the leakage of her\ninformation through the platform\u2019s output. The second one is a local privacy loss because of the\nleakage of her information through the raw data that she shares with the platform. Each user has a\nheterogeneous privacy sensitivity for these two types of privacy losses. To model such heterogeneity,\nfor each i \u2208 N , we let ci \u2208 [0, 1] be her relative local privacy sensitivity, representing the relative\nweight that user i assigns to the (per unit cost of) local privacy loss. We also let 1 \u2212 ci be her relative\ncentral privacy sensitivity, representing the relative weight that user i assigns to the (per unit cost of)\ncentral privacy loss. Therefore, ci \u2248 1 implies that user i suffers a higher loss of privacy by sharing\nher raw data with the platform (local privacy loss) compared to her loss from the platform\u2019s output\n\n1This assumption does not have any fundamental impact on the results and is made to simplify the notations.\n\n5\n\n\f(central privacy loss). Differently, ci \u2248 0 implies that the user suffers a smaller loss of privacy by\nsharing her raw data with the platform compared to her loss from the platform\u2019s output. In what\nfollows, we use the term privacy sensitivity instead of relative local privacy sensitivity.\n\nFor each i \u2208 N , the privacy sensitivity ci is independently drawn from a publicly known distribution\nwhose support is [0, 1] with cumulative distribution and probability density functions Fi(\u00b7) and fi(\u00b7).\nWe also let c = (c1, . . . , cn) be the vector of privacy sensitivities. The privacy sensitivity of each\nuser is her private information, i.e., the platform does not know it. This is because individuals have\ndifferent views regarding how trustworthy the platform is in protecting their raw data.\n\nThe platform\u2019s objective is to design a mechanism to collect users\u2019 data by paying them to compensate\nfor their privacy losses without knowing the privacy sensitivity of users. To introduce the platform\u2019s\nobjective formally, we adopt the formalism of Bayesian mechanism design pioneered by Myerson\n[1981]. More specifically, the platform designs and announces a payment function, a local privacy loss\nfunction, and a central privacy loss function that are mappings from the reported privacy sensitivities\nof users. The users then report their privacy sensitivities (which may or may not be truthful). Based\non the payment function, the platform compensates the users (the compensation could be monetary\nor some free or discounted service provided to the user). Based on the local and central privacy\nfunctions, the platform designs randomized channels and randomized estimation algorithms that\ndeliver the guaranteed local and central privacy losses while minimizing the sum of the mean squared\nerror and the total expected payments. Given this interaction, we next formally introduce a data\nacquisition mechanism with two-part data privacy guarantees.\n\nDefinition 4 (two-part private data acquisition mechanism). We call the tuple (\u02c6\u03b8, \u03b5(l), \u03b5(c), t) a\ntwo-part private data acquisition mechanism where\n\n1. For all i \u2208 N , \u03b5(l)\ni\n\n: Rn\n\n+ \u2192 R+ is a function that maps the vector of privacy sensitivities c\n\nto a local privacy loss for user i, \u03b5(l)\n\ni (c), with \u03b5(l)(\u00b7) = (\u03b5(l)\n\ni (\u00b7))n\n\ni=1.\n\n2. For all i \u2208 N , \u03b5(c)\n\ni\n\n: Rn\n\n+ \u2192 R+ is a function that maps the vector of privacy sensitivities c\n\nto a central privacy loss for user i, \u03b5(c)\n\ni (c), with \u03b5(c)(.) = (\u03b5(c)\n\ni (\u00b7))n\n\ni=1.\n\n3. \u02c6\u03b8 : X n \u00d7 Rn\n\n+ \u00d7 Rn\n\nacquired locally (\u03b5(l)\nestimate \u02c6\u03b8(x, \u03b5(l)(c), \u03b5(c)(c)).\n\n+ \u2192 R is a (\u03b5(c)(c), \u03b1)-R\u00e9nyi differentially private estimator that maps\ni (c), \u03b1)-R\u00e9nyi differentially private data of user i for i \u2208 N to an\n\n4. For all i \u2208 N , ti : Rn\n\n+ \u2192 R+ is a function that maps the vector of privacy sensitivities c to\n\na payment for user i, ti(c), with t(.) = (ti(\u00b7))n\n\ni=1.\n\nNotice that we have not specified the estimator and the mechanisms that delivers (local and central)\nR\u00e9nyi differential privacy. In the rest of this subsection, we introduce the utilities and the platform\u2019s\nproblem for a general estimators and mechanisms to deliver differential privacy. Later, we focus on\nlinear estimator and Gaussian mechanisms and explicitly solve the platform\u2019s problem.\n\nEach user that participates in a two-part private data acquisition mechanism suffers from both the\nlocal and central privacy losses and need to be compensated by the platform. In particular, the utility\nof user i from participation when her privacy sensitivity is ci and she reports c\u2032\n\ni is given by\n\nui(\u03b5(l)(c\u2032\n\ni, c\u2212i), \u03b5(c)(c\u2032\n\ni, c\u2212i), t, \u02c6\u03b8) = Ec\u2212i[ti(c\u2212i, c\u2032\n\ni)) \u2212 ci\u03b5(l)\n\ni (c\u2212i, c\u2032\n\ni) \u2212 (1 \u2212 ci)\u03b5(c)\n\ni (c\u2212i, c\u2032\n\ni)],\n\ni)) is the payment from the platform, the term ci\u03b5(l)\n\nwhere the term ti(c\u2212i, c\u2032\ni) is the relative lo-\ncal privacy sensitivity of the user multiplied by her local privacy loss, and the term (1\u2212ci)\u03b5(c)\ni (c\u2212i, c\u2032\ni)\nis her relative central privacy sensitivity multiplied by her central privacy loss. A user i \u2208 N that\ndoes not participate in the mechanism neither compromises her privacy nor gets a compensation.\nTherefore, the utility of a user who does not participate in the mechanism becomes 0.\n\ni (c\u2212i, c\u2032\n\nThe goal of the platform is to minimize the sum of the mean squared error and the overall payment to\nusers. We let \u03b3 \u2208 R+ represents the relative weight of the mean estimation error and the payments in\n\n6\n\n\fthe platform\u2019s objective.2 Therefore, the platform\u2019s objective is\nn\n(cid:88)\n\nEc[\u03b3MSE(\u03b5(l)(c), \u03b5(c)(c), \u02c6\u03b8) +\n\nti(c)],\n\ni=1\n\nwhere the first term is the mean square error of estimator \u02c6\u03b8 given reported vector of privacy sensitivity\nand resulting local and central privacy losses \u03b5(l) and \u03b5(c), i.e.,\n\nMSE(\u03b5(l)(c), \u03b5(c)(c), \u02c6\u03b8) = Ex[|\u02c6\u03b8(\u02c6x, \u03b5(l), \u03b5(c)) \u2212 \u03b8|2].\nAlso, each summand of the second term is the compensation that the platform gives to a user to\nincentivize her to participate and report her privacy sensitivity truthfully.\n\nIn Appendix we prove that, similar to the classical mechanism design setting, revelation principle\nholds. This means that there is no loss of generality in focusing on the class of direct incentive\ncompatible mechanisms, meaning the platform\u2019s optimization problem can be written as\n\nmin\n\u03b5(l)(\u00b7),\u03b5(c)(\u00b7),t(\u00b7)\n\nEc[\u03b3MSE(\u03b5(l)(c), \u03b5(c)(c), \u02c6\u03b8) +\n\nn\n(cid:88)\n\nti(c)]\n\ni=1\nui(\u03b5(l)(c), \u03b5(c)(c), t, \u02c6\u03b8) \u2265 ui(\u03b5(l)(c\u2032\nui(\u03b5(l)(c), \u03b5(c)(c), t, \u02c6\u03b8) \u2265 0\n\ni, c\u2212i), \u03b5(c)(c\u2032\n\ni, c\u2212i), t, \u02c6\u03b8)\n\n(5)\nwhere the constraints in (4) represent the incentive compatibility. These constraints guarantee that\nthat each user i has no incentive to misrepresent her privacy sensitivity when others report truthfully\n(reporting truthfully is an equilibrium of the game among the users). Also, the constraints in (5)\nrepresent individual rationality, which ensures that each user receives a non-negative utility from\nparticipating in the platform\u2019s mechanism and sharing her data.\n\nfor all i \u2208 N , ci,\n\n(3)\n\n(4)\n\n3 From the mechanism design problem to an optimization problem\n\nFor a given estimator \u02c6\u03b8, the platform\u2019s decision comprises the local and central privacy loss functions\n\u03b5(l)(\u00b7) and \u03b5(c)(\u00b7) together with the payment functions t(\u00b7). We next show that this problem can be\nequivalently formulated as an optimization problem over the vector of local privacy losses and central\nprivacy losses (as opposed to functions). In the rest of the paper, we impose the following assumption\nwhich is well-known in the mechanism design literature and simplifies the analysis.3\nAssumption 1. For any user i \u2208 N , the virtual cost defined as \u03c8i(c) = c + Fi(c)\nfi(c) is increasing in c,\nwhere fi(\u00b7) and Fi(\u00b7) are probability density and cumulative distribution functions of ci, respectively.\n\nThe above assumption holds for a wide class of distributions such as the ones with log-concave\ndensity functions (e.g., uniform).\nTheorem 2. Suppose Assumption 1 holds. For a given estimator \u02c6\u03b8 : \u02c6X n \u00d7 Rn\n+ \u2192 R, in the\noptimal two-part data acquisition mechanism, for a given vector of reported privacy sensitivities c,\nthe local and central privacy losses are the solution of\nn\n(cid:88)\n\n+ \u00d7 Rn\n\nn\n(cid:88)\n\n\u03b5(l)\ni \u03c8i(ci) +\n\n\u03b5(c)\ni (1 \u2212 \u03c8i(ci)).\n\n(6)\n\n\u03b3MSE(\u03b5(l), \u03b5(c), \u02c6\u03b8) +\n\nmin\ni=1,{\u03b5(c)}n\n\ni=1\n\n{\u03b5(l)}n\n\ni=1\n\ni=1\n\nProof Sketch of Theorem 2: We introduce the following interim functions\n\nti(ci) =Ec\u2212i [t(ci, c\u2212i)],\n\ni (ci) = Ec\u2212i[\u03b5(l)\n\u03b5(l)\n\ni (ci, c\u2212i)],\n\nand \u03b5(c)\n\ni (ci) = Ec\u2212i[\u03b5(c)\n\ni (ci, c\u2212i)].\n\nWe first establish a payment identity that determines the optimal payment in terms of the optimal\nlocal and central privacy losses. In particular, by evaluating the first order condition corresponding to\nthe incentive compatibility constraint (4), we establish that this constraint holds if and only if\n\nti(ci) = ti(0) + \u03b5(c)\n\ni (ci) \u2212 \u03b5(c)\n\ni (0) + ci(\u03b5(l)\n\ni (ci) \u2212 \u03b5(c)\n\ni (ci)) \u2212\n\n(cid:90) ci\n\n0\n\n(\u03b5(l)\n\ni (z) \u2212 \u03b5(c)\n\ni (z))dz,\n\n2Notice that changing the parameter \u03b3 enables us to study a wide range of platform\u2019s objectives with differing\n\nrelative weights between the estimation error and the total payments.\n\n3Without this assumption, extending the results requires ironing technique of Myerson [1981].\n\n7\n\n\fi (z) \u2212 \u03b5(c)\n\nand \u03b5(l)\ni (z) is weakly decreasing in z. We then plug in this payment identity back to the\nplatform\u2019s objective, use the individual rationality constraint, and rewrite the platform\u2019s expected\nutility in terms of the privacy loss functions and the virtual cost of users. This is still a functional\noptimization problem in terms of \u03b5(l)(\u00b7) and \u03b5(c)(\u00b7). However, we establish that, under Assumption 1,\nwe can solve this functional optimization point-wise (i.e., for any given c). \u25a0\n\nTheorem 2 highlights the tradeoff in the platform\u2019s problem: by decreasing the local privacy loss,\nthe second term of the objective decreases (this term corresponds to the payment to users) while\nthe first term (i.e., the mean squared error) increases. The role of the central privacy loss is more\nnuanced, and there are two cases. If the coefficient 1 \u2212 \u03c8i(ci) is non-negative, by decreasing the\ncentral privacy loss, the third term of the objective decreases while the first term increases. If the\ncoefficient 1 \u2212 \u03c8i(ci) is negative, increasing the central privacy loss decreases both the third term\nand the first term. However, we cannot increase the central privacy loss level without limits because\nthe central privacy loss level is always below the local privacy loss level. Therefore, the platform\u2019s\noptimal mechanism should find the \u201cright\u201d balance between these terms.\n\n4 Optimal mechanism with two-part privacy guarantees for linear estimators\n\nFor the rest of the paper, we focus on linear estimators with Gaussian mechanism described in Section\n2.1. The following is a direct corollary of Theorem 2.\nCorollary 1. Suppose Assumption 1 holds. For any reported vector of privacy sensitivities c, the\noptimal local privacy loss levels are \u03b5(l)\nn\n(cid:88)\n\ni and the optimal central privacy loss levels are\n\ni (c) = y\u2217\n\n2\n\nwhere (w\u2217\n\n1, . . . , w\u2217\n\nn) and (y\u2217\n\n1, . . . , y\u2217\n\nn) are the optimal solution of\n\n\u03b5(c)\ni = w\u2217\ni\n\n2/\n\nw\u2217\nj\ny\u2217\nj\n\nj=1\n\nmin\nw,y\n\nVAR\u03b3\n\nn\n(cid:88)\n\ni=1\n\nw2\n\ni +\n\n\u03b3\u03b1\n2\n\nn\n(cid:88)\n\ni=1\n\nw2\ni\nyi\n\n+\n\ns.t. wi, yi \u2265 0, for all i \u2208 N and\n\nn\n(cid:88)\n\ni=1\n\nn\n(cid:88)\n\ni=1\n\n(1 \u2212 \u03c8i(ci))\n\nw2\ni\n\n(cid:80)n\n\nj=1\n\nw2\nj\nyj\n\n+\n\nn\n(cid:88)\n\ni=1\n\n\u03c8i(ci)yi\n\n(7)\n\nwi = 1.\n\nLet us highlight the difference between our characterization and that of classic mechanism design (e.g.,\nMyerson [1981]). In classic mechanism design, the designer\u2019s problem becomes linear optimization.\nHowever, in our setting, the designer\u2019s problem is a non-linear and non-convex optimization. This\nmakes the problem of finding the optimal two-part data acquisition mechanism challenging. Before\naddressing this computational challenge, let us revisit the form of the Gaussian mechanism that we\nhave adopted: the platform adds Gaussian noise locally and then outputs a convex combination of\nthe privatized users\u2019 data without adding any noise centrally. More specifically, one may guess that\nthe platform may benefit by having a central noise added to the final output in addition to the local\nnoises. In the following subsection, we establish that there is another Gaussian mechanism for any\nGaussian mechanism that only adds local noises and achieves a weakly lower cost.\n\n4.1 Optimality of having only local noises in the Gaussian mechanism\n\nThe platform has the opportunity of adding Gaussian noise to both the raw data of each user and the\nfinal estimator and ex-ante one may guess that it is optimal to use both of these instruments. However,\nas we establish next, interestingly, in the optimal two-part data acquisition mechanism, it is always\noptimal to only add noises locally.\nFor a given vector of local privacy losses (\u03b5(l)\nGaussian mechanism with both local and central noises is of the form\n\nn ) and central privacy losses (\u03b5(c)\n\n1 , . . . , \u03b5(c)\n\n1 , . . . , \u03b5(l)\n\nn ), a\n\n\u02c6\u03b8(x1, . . . , xn) :=\n\nn\n(cid:88)\n\ni=1\n\nwi \u02c6xi + N (0,\n\n\u03b1\n2\u03b5\n\n) where\n\nn\n(cid:88)\n\ni=1\n\nwi = 1 and \u02c6xi = xi + N (0,\n\n\u03b1\n2\u03b5(l)\ni\n\n)\u2200i \u2208 N .\n\nProposition 1. In the optimal two-part data acquisition mechanism that adopts a Gaussian mecha-\nnism with both local and central noises, we have \u03b5 = \u221e.\n\n8\n\n\fAlgorithm 1: Computing the optimal two-part private data acquisition mechanism\nInput: The vector of privacy sensitivities (c1, . . . , cn)\nfor S \u2208 Grid (cid:0)[S, \u00afS], \u03b4(cid:1) do\n\nLet\n\n\u03bdi =\n\n1\n\u03b3 VAR + (1 \u2212 \u03c8i(ci))/S\n\n, \u03b6i =\n\n\u03bdi\nj \u03bdj\n\n(cid:80)\n\n, \u03bei = \u03b6i\n\n(cid:113)\n\n\u03bdj(\n\n\u03c8j(cj) \u2212 (cid:112)\u03c8i(ci))\n\n\uf8f6\n\n\uf8f8 .\n\n\uf8eb\n\n\uf8ed\n\nn\n(cid:88)\n\nj=1\n\n(cid:18) (cid:80)n\n\ni=1 \u03b6i\n\n\u03c8i(ci)\n\nS\u2212(cid:80)n\n\ni=1\n\n\u03c8i(ci)\u03bei\n\n\u221a\n\u221a\n\n(cid:19)2\n\n.\n\nLet p =\n\nLet\n\n\u03bdi + \u03bdi\n\nwi(S) =\n\n(cid:112)\u03c8j(cj)p\n\n(cid:80)\n\nj \u03bdj\n(cid:80)\nj \u03bdj\n\n\u2212 \u03bdi\n\n(cid:112)\u03c8i(ci)p,\n\nyi(S) = wi(S)\n\n(cid:114) p\n\n,\n\n\u03c8i(ci)\n\nand OBJ(S) be the objective of Problem (7) evaluated for this solution.\n\nend\nOutput: {yi(S\u2217), wi(S\u2217)}n\n\ni=1, where (S\u2217) = arg min(S) OBJ(S).\n\nProposition 1 has an important implication in terms of the design of data market architecture when\nusers have both central and local privacy costs: it is optimal to add noise locally! Adding a noise\ncentrally to the final estimator has an advantage because the weights in the final estimator give the\nplatform a lever to deliver heterogeneous central privacy guarantees to users. Despite this advantage,\nwe establish that adding noise centrally is never optimal. This is because the platform prefers to add\nthe noise locally to contribute to both central and local privacy guarantees delivered to users.\n\n4.2 Computing the optimal privacy loss function\n\nThe implementation of the optimal two-part private data acquisition mechanism requires solving\nProblem (7), which is a non-convex program. However, we use the structure of the problem to develop\na polynomial time algorithm to solve it approximately. To do so, we first replace (cid:80)n\ni /yi by an\nauxiliary variable S. Next, we consider the corresponding lagrangian problem. Using Karush-Kuhn-\nTucker (KKT) conditions, we establish a number of relations between problems\u2019 parameters, S, and\np, the lagrangian coefficient corresponding to S = (cid:80)n\ni /yi. Furthermore, we develop upper and\nlower bounds for S. Finally, we do a grid search to find the approximate optimal solution.\nTheorem 3. For any vector of reported privacy sensitivities and \u03f5 > 0, Algorithm 1 finds local privacy\nloss levels and the differentially private linear estimator of the two-part data acquisition mechanism\nwhose cost (i.e., platform\u2019s objective) is at most 1 + \u03b4 of the optimal cost in time poly(n, 1\n\ni=1 w2\n\ni=1 w2\n\n\u03b4 ).\n\nNotice that the approximation factor in Theorem 3 depends on the underlying parameters. Therefore,\nwe have a Polynomial Time Approximation Scheme (PTAS) for finding the optimal two-part data\nacquisition mechanism in the class of linear estimators.\n\nIn the Appendix, we provide a case study with two users to illustrate the performance of the optimal\ntwo-part data acquisition mechanism in terms of the guaranteed privacy levels and payments as\nfunctions of the reported privacy sensitivities.\n\n\n\nBased on the section(s), please answer the following question with yes, no, or n/a and provide a brief justification for your answer.\nQuestion: If the authors curate/release new assets (e.g., code, data, models), do the authors discuss whether and how consent was obtained from people whose data they are using/curating?"}, "4e": {"role": "user", "content": "The following is the methods section of the paper you are reviewing:\n\n\nWe consider a platform interested in estimating a parameter \u03b8 \u2208 R by collecting data of n users,\nindexed by N = {1, \u00b7 \u00b7 \u00b7 , n}. For any i \u2208 N , we denote user i\u2019s data by Xi \u2208 X and we assume Xi\n\n3\n\n\fis given by Xi = \u03b8 + Zi where Z1, \u00b7 \u00b7 \u00b7 , Zn are independent and identically distributed zero-mean\nrandom variables with variance VAR. To simplify the exposition, we further assume |Zi| \u2264 1/2 for\nany i \u2208 N .1 Throughout, we use lower case letters to denote the realization of random variables. The\nplatform\u2019s goal is to minimize the estimate\u2019s error by collecting data from privacy-concerned users.\nTherefore, the platform needs to incentivize them to share their data.\n\n2.1 Local and central privacy losses\n\nBefore formalizing the utilities/objectives of the platform and the users, we define the notions of\nprivacy losses that we adopt in this paper. In particular, we consider two different types of privacy\nlosses that users suffer from. The first one is the privacy loss of a user when she shares her data (only)\nwith the platform, and the second one is the privacy loss through the released estimate (to the public)\nby the platform. Depending on how different users trust the platform, they might care differently\nabout these two privacy losses. For instance, if a user fully trusts the platform, then her main privacy\nconcern would be the second one, while a user who does not trust the platform at all would be more\nconcerned with the first one as the public only observes the aggregated estimate, as opposed to the\nplatform which observes each user\u2019s (shared) data separately.\n\nWe use the differential privacy framework to quantify these privacy losses. Since differential privacy\nwas introduced by Dwork et al., several variants of it have been also proposed. In particular, a popular\none in the machine learning literature is R\u00e9nyi differential privacy (RDP), introduced by Mironov\n[2017], which we also adopt in this paper. Let us first recall the definition of R\u00e9nyi divergence.\nDefinition 1. Let P and Q be two distributions over R with densities p and q. For any \u03b1 \u2208 (1, \u221e],\nthe R\u00e9nyi \u03b1-divergence between P and Q is denoted by D\u03b1(P ||Q) and is given by\n\nD\u03b1(P ||Q) :=\n\n1\n\u03b1 \u2212 1\n\nlog\n\n(cid:90) (cid:18) p(x)\nq(x)\n\n(cid:19)\u03b1\n\nq(x)dx.\n\nFor two random variables X and Y , D\u03b1(X||Y ) denotes the \u03b1-divergence between their distributions.\n\nWe next define two notions of differential privacy, known as central and local, to capture the two\naforementioned types of privacy losses. Local differential privacy corresponds to the privacy loss of a\nuser when she shares her data with the platform through a randomized mapping, known as a channel.\nDefinition 2. Let \u03b5 \u2265 0 and \u03b1 \u2208 (1, \u221e]. A randomized channel C : X \u2192 R is locally (\u03b5, \u03b1)-R\u00e9nyi\n(differentially) private if for any x, x\u2032 \u2208 X ,\n\nD\u03b1(C(x)||C(x\u2032)) \u2264 \u03b5.\n\nCentral differential privacy corresponds to the other privacy loss mentioned above. It bounds the\nchange in the distribution of the platform\u2019s output, i.e., the released estimate, by changing one user\u2019s\ndata. We next provide the formal definition.\ni=1 \u2208 Rn\nDefinition 3. Let \u03b5 = (\u03b5i)n\n(\u03b5, \u03b1)-R\u00e9nyi (differentially) private if for any two datasets x1:n, x\u2032\ncoordinate (data of user i),\n\n+ and \u03b1 \u2208 (1, \u221e]. A randomized algorithm A : X n \u2192 R is\n1:n \u2208 X n that only differ in i-th\n\nD\u03b1(A(x1:n)||A(x\u2032\n\n1:n)) \u2264 \u03b5i.\n\nThe customary approach to guarantee RDP is Gaussian mechanism in which a properly tuned zero-\nmean Gaussian noise is added to fulfill the required condition. The following lemma, adapted from\nMironov [2017], allows us to characterize the Gaussian noise\u2019s variance for a privacy loss level.\nLemma 1. For a function f : X n \u2192 R, we define its sensitivity with respect to i-th coordinate as\n\nLi(f ) := sup {|f (x1:n) \u2212 f (x\u2032\n\n1:n)| : for all x1:n and x\u2032\n\n1:n differing only at i-th coordinate} .\n\nFor any \u03b1 \u2208 (1, \u221e], A(x1:n) = f (x1:n) + W with W \u223c N (0, \u03c32) is\n\n(cid:16)\n\n( \u03b1Li(f )2\n2\u03c32\n\n)n\ni=1, \u03b1\n\n(cid:17)\n\n-RDP.\n\nFor a given vector of local privacy losses (\u03b5(l)\nis by using a linear estimator with Gaussian mechanism which is given by\n\n1 , . . . , \u03b5(l)\n\nn ), a natural way to privately estimate the mean\n\n\u02c6\u03b8(x1, . . . , xn) :=\n\nn\n(cid:88)\n\ni=1\n\nwi \u02c6xi where\n\nn\n(cid:88)\n\ni=1\n\nwi = 1 and \u02c6xi = xi + N (0,\n\n\u03b1\n2\u03b5(l)\ni\n\n) for all i \u2208 N .\n\n(1)\n\n4\n\n\fFigure 1: The interaction between the users and the platform in the two-part private data acquisition.\n\ni / (cid:80)n\n\ni = w2\n\nand the central privacy delivered to user i \u2208 N is \u03b5(c)\n\nFigure 1 depicts this estimator. In particular, using Lemma 1, the local privacy delivered to user\ni \u2208 N is \u03b5(l)\nj . In the next\ni\nsection, we focus on linear estimators in designing the optimal private data acquisition mechanism.\nWe motivate such specification by showing that, for a given vector of local privacy losses (\u03b5(l)\ni=1,\na linear estimator is optimal with respect to mean square error. To formalize this statement, we\nfirst need to define the minimax estimation error as the notion of optimality. Let P be a class of\ndistributions over X . For any P \u2208 P, we denote its mean by \u03b8(P ). A (\u03b5(l)\ni=1-locally RDP estimator\ncan be cast as \u02c6\u03b8((Ci(xi))n\ni=1), where Ci(.) is the randomized channel corresponding to user i. Let\nQ((\u03b5(l)\ni=1-locally RDP estimators. The minimax estimation error is\ni=1,\u02c6\u03b8[|\u02c6\u03b8((Ci(Xi))n\n\ni=1) be the class of such (\u03b5(l)\n\ni=1) \u2212 \u03b8(P )|2],\n\nL(P, Q, (\u03b5(l)\n\nj=1 w2\n\ni=1) :=\n\nj /\u03b5(l)\n\n(Xi\u223cP )n\n\ni )n\n\ni )n\n\ni )n\n\ni )n\n\ni )n\n\n(2)\n\nE\n\ninf\ni=1\u2208Q((\u03b5(l)\n\ni )n\n\ni=1)\n\nsup\nP \u2208P\n\n\u02c6\u03b8,{Ci}n\n\nwhere the expectation is taken over both randomness of data and estimator (including private channels).\nIn other words, the optimal estimator is the one that has the lowest worst case error among all\nestimators that satisfy the privacy requirements. We next state the optimality result.\nTheorem 1. Assume \u03b1 \u2265 2 and \u03b5(l)\nand C1, \u00b7 \u00b7 \u00b7 , Cn be independent channels. Then, there exists a universal constant c such that\n\ni \u2264 1 for all i. Let P1 be the family of distributions over [\u2212 1\n\n2 , 1\n2 ]\n\nL(P, Q, (\u03b5(l)\n\ni )n\n\ni=1) \u2265 c min(\n\n1\ni=1 \u03b5(l)\n\ni\n\n(cid:80)n\n\n, 1).\n\nFurthermore, there exists a linear estimator with Gaussian mechanism such that\n\nE\n\n(Xi\u223cP )n\n\ni=1,\u02c6\u03b8[|\u02c6\u03b8((Ci(Xi))n\n\ni=1) \u2212 \u03b8(P )|2] \u2264 O(1)\n\n\u03b1\ni=1 \u03b5(l)\n\ni\n\n(cid:80)n\n\n.\n\nWe prove the lower bound by using the Le Cam\u2019s method Yu [1997] that reduces the problem of\nfinding lower bounds to a hypothesis testing problem and requires bounding the total variation\ndistance between the estimates when the input data points change. We then find it more convenient to\nbound the total variation in terms of Hellinger distance and use a series of inequalities to bound it.\n\n2.2 Data acquisition mechanism with two-part privacy guarantees\n\nWe next describe the utility functions of the users and the platform and then formulate the platform\u2019s\noptimal data acquisition mechanism. As we described earlier, each user suffers from two privacy\nlosses when sharing her data. The first one is a central privacy loss because of the leakage of her\ninformation through the platform\u2019s output. The second one is a local privacy loss because of the\nleakage of her information through the raw data that she shares with the platform. Each user has a\nheterogeneous privacy sensitivity for these two types of privacy losses. To model such heterogeneity,\nfor each i \u2208 N , we let ci \u2208 [0, 1] be her relative local privacy sensitivity, representing the relative\nweight that user i assigns to the (per unit cost of) local privacy loss. We also let 1 \u2212 ci be her relative\ncentral privacy sensitivity, representing the relative weight that user i assigns to the (per unit cost of)\ncentral privacy loss. Therefore, ci \u2248 1 implies that user i suffers a higher loss of privacy by sharing\nher raw data with the platform (local privacy loss) compared to her loss from the platform\u2019s output\n\n1This assumption does not have any fundamental impact on the results and is made to simplify the notations.\n\n5\n\n\f(central privacy loss). Differently, ci \u2248 0 implies that the user suffers a smaller loss of privacy by\nsharing her raw data with the platform compared to her loss from the platform\u2019s output. In what\nfollows, we use the term privacy sensitivity instead of relative local privacy sensitivity.\n\nFor each i \u2208 N , the privacy sensitivity ci is independently drawn from a publicly known distribution\nwhose support is [0, 1] with cumulative distribution and probability density functions Fi(\u00b7) and fi(\u00b7).\nWe also let c = (c1, . . . , cn) be the vector of privacy sensitivities. The privacy sensitivity of each\nuser is her private information, i.e., the platform does not know it. This is because individuals have\ndifferent views regarding how trustworthy the platform is in protecting their raw data.\n\nThe platform\u2019s objective is to design a mechanism to collect users\u2019 data by paying them to compensate\nfor their privacy losses without knowing the privacy sensitivity of users. To introduce the platform\u2019s\nobjective formally, we adopt the formalism of Bayesian mechanism design pioneered by Myerson\n[1981]. More specifically, the platform designs and announces a payment function, a local privacy loss\nfunction, and a central privacy loss function that are mappings from the reported privacy sensitivities\nof users. The users then report their privacy sensitivities (which may or may not be truthful). Based\non the payment function, the platform compensates the users (the compensation could be monetary\nor some free or discounted service provided to the user). Based on the local and central privacy\nfunctions, the platform designs randomized channels and randomized estimation algorithms that\ndeliver the guaranteed local and central privacy losses while minimizing the sum of the mean squared\nerror and the total expected payments. Given this interaction, we next formally introduce a data\nacquisition mechanism with two-part data privacy guarantees.\n\nDefinition 4 (two-part private data acquisition mechanism). We call the tuple (\u02c6\u03b8, \u03b5(l), \u03b5(c), t) a\ntwo-part private data acquisition mechanism where\n\n1. For all i \u2208 N , \u03b5(l)\ni\n\n: Rn\n\n+ \u2192 R+ is a function that maps the vector of privacy sensitivities c\n\nto a local privacy loss for user i, \u03b5(l)\n\ni (c), with \u03b5(l)(\u00b7) = (\u03b5(l)\n\ni (\u00b7))n\n\ni=1.\n\n2. For all i \u2208 N , \u03b5(c)\n\ni\n\n: Rn\n\n+ \u2192 R+ is a function that maps the vector of privacy sensitivities c\n\nto a central privacy loss for user i, \u03b5(c)\n\ni (c), with \u03b5(c)(.) = (\u03b5(c)\n\ni (\u00b7))n\n\ni=1.\n\n3. \u02c6\u03b8 : X n \u00d7 Rn\n\n+ \u00d7 Rn\n\nacquired locally (\u03b5(l)\nestimate \u02c6\u03b8(x, \u03b5(l)(c), \u03b5(c)(c)).\n\n+ \u2192 R is a (\u03b5(c)(c), \u03b1)-R\u00e9nyi differentially private estimator that maps\ni (c), \u03b1)-R\u00e9nyi differentially private data of user i for i \u2208 N to an\n\n4. For all i \u2208 N , ti : Rn\n\n+ \u2192 R+ is a function that maps the vector of privacy sensitivities c to\n\na payment for user i, ti(c), with t(.) = (ti(\u00b7))n\n\ni=1.\n\nNotice that we have not specified the estimator and the mechanisms that delivers (local and central)\nR\u00e9nyi differential privacy. In the rest of this subsection, we introduce the utilities and the platform\u2019s\nproblem for a general estimators and mechanisms to deliver differential privacy. Later, we focus on\nlinear estimator and Gaussian mechanisms and explicitly solve the platform\u2019s problem.\n\nEach user that participates in a two-part private data acquisition mechanism suffers from both the\nlocal and central privacy losses and need to be compensated by the platform. In particular, the utility\nof user i from participation when her privacy sensitivity is ci and she reports c\u2032\n\ni is given by\n\nui(\u03b5(l)(c\u2032\n\ni, c\u2212i), \u03b5(c)(c\u2032\n\ni, c\u2212i), t, \u02c6\u03b8) = Ec\u2212i[ti(c\u2212i, c\u2032\n\ni)) \u2212 ci\u03b5(l)\n\ni (c\u2212i, c\u2032\n\ni) \u2212 (1 \u2212 ci)\u03b5(c)\n\ni (c\u2212i, c\u2032\n\ni)],\n\ni)) is the payment from the platform, the term ci\u03b5(l)\n\nwhere the term ti(c\u2212i, c\u2032\ni) is the relative lo-\ncal privacy sensitivity of the user multiplied by her local privacy loss, and the term (1\u2212ci)\u03b5(c)\ni (c\u2212i, c\u2032\ni)\nis her relative central privacy sensitivity multiplied by her central privacy loss. A user i \u2208 N that\ndoes not participate in the mechanism neither compromises her privacy nor gets a compensation.\nTherefore, the utility of a user who does not participate in the mechanism becomes 0.\n\ni (c\u2212i, c\u2032\n\nThe goal of the platform is to minimize the sum of the mean squared error and the overall payment to\nusers. We let \u03b3 \u2208 R+ represents the relative weight of the mean estimation error and the payments in\n\n6\n\n\fthe platform\u2019s objective.2 Therefore, the platform\u2019s objective is\nn\n(cid:88)\n\nEc[\u03b3MSE(\u03b5(l)(c), \u03b5(c)(c), \u02c6\u03b8) +\n\nti(c)],\n\ni=1\n\nwhere the first term is the mean square error of estimator \u02c6\u03b8 given reported vector of privacy sensitivity\nand resulting local and central privacy losses \u03b5(l) and \u03b5(c), i.e.,\n\nMSE(\u03b5(l)(c), \u03b5(c)(c), \u02c6\u03b8) = Ex[|\u02c6\u03b8(\u02c6x, \u03b5(l), \u03b5(c)) \u2212 \u03b8|2].\nAlso, each summand of the second term is the compensation that the platform gives to a user to\nincentivize her to participate and report her privacy sensitivity truthfully.\n\nIn Appendix we prove that, similar to the classical mechanism design setting, revelation principle\nholds. This means that there is no loss of generality in focusing on the class of direct incentive\ncompatible mechanisms, meaning the platform\u2019s optimization problem can be written as\n\nmin\n\u03b5(l)(\u00b7),\u03b5(c)(\u00b7),t(\u00b7)\n\nEc[\u03b3MSE(\u03b5(l)(c), \u03b5(c)(c), \u02c6\u03b8) +\n\nn\n(cid:88)\n\nti(c)]\n\ni=1\nui(\u03b5(l)(c), \u03b5(c)(c), t, \u02c6\u03b8) \u2265 ui(\u03b5(l)(c\u2032\nui(\u03b5(l)(c), \u03b5(c)(c), t, \u02c6\u03b8) \u2265 0\n\ni, c\u2212i), \u03b5(c)(c\u2032\n\ni, c\u2212i), t, \u02c6\u03b8)\n\n(5)\nwhere the constraints in (4) represent the incentive compatibility. These constraints guarantee that\nthat each user i has no incentive to misrepresent her privacy sensitivity when others report truthfully\n(reporting truthfully is an equilibrium of the game among the users). Also, the constraints in (5)\nrepresent individual rationality, which ensures that each user receives a non-negative utility from\nparticipating in the platform\u2019s mechanism and sharing her data.\n\nfor all i \u2208 N , ci,\n\n(3)\n\n(4)\n\n3 From the mechanism design problem to an optimization problem\n\nFor a given estimator \u02c6\u03b8, the platform\u2019s decision comprises the local and central privacy loss functions\n\u03b5(l)(\u00b7) and \u03b5(c)(\u00b7) together with the payment functions t(\u00b7). We next show that this problem can be\nequivalently formulated as an optimization problem over the vector of local privacy losses and central\nprivacy losses (as opposed to functions). In the rest of the paper, we impose the following assumption\nwhich is well-known in the mechanism design literature and simplifies the analysis.3\nAssumption 1. For any user i \u2208 N , the virtual cost defined as \u03c8i(c) = c + Fi(c)\nfi(c) is increasing in c,\nwhere fi(\u00b7) and Fi(\u00b7) are probability density and cumulative distribution functions of ci, respectively.\n\nThe above assumption holds for a wide class of distributions such as the ones with log-concave\ndensity functions (e.g., uniform).\nTheorem 2. Suppose Assumption 1 holds. For a given estimator \u02c6\u03b8 : \u02c6X n \u00d7 Rn\n+ \u2192 R, in the\noptimal two-part data acquisition mechanism, for a given vector of reported privacy sensitivities c,\nthe local and central privacy losses are the solution of\nn\n(cid:88)\n\n+ \u00d7 Rn\n\nn\n(cid:88)\n\n\u03b5(l)\ni \u03c8i(ci) +\n\n\u03b5(c)\ni (1 \u2212 \u03c8i(ci)).\n\n(6)\n\n\u03b3MSE(\u03b5(l), \u03b5(c), \u02c6\u03b8) +\n\nmin\ni=1,{\u03b5(c)}n\n\ni=1\n\n{\u03b5(l)}n\n\ni=1\n\ni=1\n\nProof Sketch of Theorem 2: We introduce the following interim functions\n\nti(ci) =Ec\u2212i [t(ci, c\u2212i)],\n\ni (ci) = Ec\u2212i[\u03b5(l)\n\u03b5(l)\n\ni (ci, c\u2212i)],\n\nand \u03b5(c)\n\ni (ci) = Ec\u2212i[\u03b5(c)\n\ni (ci, c\u2212i)].\n\nWe first establish a payment identity that determines the optimal payment in terms of the optimal\nlocal and central privacy losses. In particular, by evaluating the first order condition corresponding to\nthe incentive compatibility constraint (4), we establish that this constraint holds if and only if\n\nti(ci) = ti(0) + \u03b5(c)\n\ni (ci) \u2212 \u03b5(c)\n\ni (0) + ci(\u03b5(l)\n\ni (ci) \u2212 \u03b5(c)\n\ni (ci)) \u2212\n\n(cid:90) ci\n\n0\n\n(\u03b5(l)\n\ni (z) \u2212 \u03b5(c)\n\ni (z))dz,\n\n2Notice that changing the parameter \u03b3 enables us to study a wide range of platform\u2019s objectives with differing\n\nrelative weights between the estimation error and the total payments.\n\n3Without this assumption, extending the results requires ironing technique of Myerson [1981].\n\n7\n\n\fi (z) \u2212 \u03b5(c)\n\nand \u03b5(l)\ni (z) is weakly decreasing in z. We then plug in this payment identity back to the\nplatform\u2019s objective, use the individual rationality constraint, and rewrite the platform\u2019s expected\nutility in terms of the privacy loss functions and the virtual cost of users. This is still a functional\noptimization problem in terms of \u03b5(l)(\u00b7) and \u03b5(c)(\u00b7). However, we establish that, under Assumption 1,\nwe can solve this functional optimization point-wise (i.e., for any given c). \u25a0\n\nTheorem 2 highlights the tradeoff in the platform\u2019s problem: by decreasing the local privacy loss,\nthe second term of the objective decreases (this term corresponds to the payment to users) while\nthe first term (i.e., the mean squared error) increases. The role of the central privacy loss is more\nnuanced, and there are two cases. If the coefficient 1 \u2212 \u03c8i(ci) is non-negative, by decreasing the\ncentral privacy loss, the third term of the objective decreases while the first term increases. If the\ncoefficient 1 \u2212 \u03c8i(ci) is negative, increasing the central privacy loss decreases both the third term\nand the first term. However, we cannot increase the central privacy loss level without limits because\nthe central privacy loss level is always below the local privacy loss level. Therefore, the platform\u2019s\noptimal mechanism should find the \u201cright\u201d balance between these terms.\n\n4 Optimal mechanism with two-part privacy guarantees for linear estimators\n\nFor the rest of the paper, we focus on linear estimators with Gaussian mechanism described in Section\n2.1. The following is a direct corollary of Theorem 2.\nCorollary 1. Suppose Assumption 1 holds. For any reported vector of privacy sensitivities c, the\noptimal local privacy loss levels are \u03b5(l)\nn\n(cid:88)\n\ni and the optimal central privacy loss levels are\n\ni (c) = y\u2217\n\n2\n\nwhere (w\u2217\n\n1, . . . , w\u2217\n\nn) and (y\u2217\n\n1, . . . , y\u2217\n\nn) are the optimal solution of\n\n\u03b5(c)\ni = w\u2217\ni\n\n2/\n\nw\u2217\nj\ny\u2217\nj\n\nj=1\n\nmin\nw,y\n\nVAR\u03b3\n\nn\n(cid:88)\n\ni=1\n\nw2\n\ni +\n\n\u03b3\u03b1\n2\n\nn\n(cid:88)\n\ni=1\n\nw2\ni\nyi\n\n+\n\ns.t. wi, yi \u2265 0, for all i \u2208 N and\n\nn\n(cid:88)\n\ni=1\n\nn\n(cid:88)\n\ni=1\n\n(1 \u2212 \u03c8i(ci))\n\nw2\ni\n\n(cid:80)n\n\nj=1\n\nw2\nj\nyj\n\n+\n\nn\n(cid:88)\n\ni=1\n\n\u03c8i(ci)yi\n\n(7)\n\nwi = 1.\n\nLet us highlight the difference between our characterization and that of classic mechanism design (e.g.,\nMyerson [1981]). In classic mechanism design, the designer\u2019s problem becomes linear optimization.\nHowever, in our setting, the designer\u2019s problem is a non-linear and non-convex optimization. This\nmakes the problem of finding the optimal two-part data acquisition mechanism challenging. Before\naddressing this computational challenge, let us revisit the form of the Gaussian mechanism that we\nhave adopted: the platform adds Gaussian noise locally and then outputs a convex combination of\nthe privatized users\u2019 data without adding any noise centrally. More specifically, one may guess that\nthe platform may benefit by having a central noise added to the final output in addition to the local\nnoises. In the following subsection, we establish that there is another Gaussian mechanism for any\nGaussian mechanism that only adds local noises and achieves a weakly lower cost.\n\n4.1 Optimality of having only local noises in the Gaussian mechanism\n\nThe platform has the opportunity of adding Gaussian noise to both the raw data of each user and the\nfinal estimator and ex-ante one may guess that it is optimal to use both of these instruments. However,\nas we establish next, interestingly, in the optimal two-part data acquisition mechanism, it is always\noptimal to only add noises locally.\nFor a given vector of local privacy losses (\u03b5(l)\nGaussian mechanism with both local and central noises is of the form\n\nn ) and central privacy losses (\u03b5(c)\n\n1 , . . . , \u03b5(c)\n\n1 , . . . , \u03b5(l)\n\nn ), a\n\n\u02c6\u03b8(x1, . . . , xn) :=\n\nn\n(cid:88)\n\ni=1\n\nwi \u02c6xi + N (0,\n\n\u03b1\n2\u03b5\n\n) where\n\nn\n(cid:88)\n\ni=1\n\nwi = 1 and \u02c6xi = xi + N (0,\n\n\u03b1\n2\u03b5(l)\ni\n\n)\u2200i \u2208 N .\n\nProposition 1. In the optimal two-part data acquisition mechanism that adopts a Gaussian mecha-\nnism with both local and central noises, we have \u03b5 = \u221e.\n\n8\n\n\fAlgorithm 1: Computing the optimal two-part private data acquisition mechanism\nInput: The vector of privacy sensitivities (c1, . . . , cn)\nfor S \u2208 Grid (cid:0)[S, \u00afS], \u03b4(cid:1) do\n\nLet\n\n\u03bdi =\n\n1\n\u03b3 VAR + (1 \u2212 \u03c8i(ci))/S\n\n, \u03b6i =\n\n\u03bdi\nj \u03bdj\n\n(cid:80)\n\n, \u03bei = \u03b6i\n\n(cid:113)\n\n\u03bdj(\n\n\u03c8j(cj) \u2212 (cid:112)\u03c8i(ci))\n\n\uf8f6\n\n\uf8f8 .\n\n\uf8eb\n\n\uf8ed\n\nn\n(cid:88)\n\nj=1\n\n(cid:18) (cid:80)n\n\ni=1 \u03b6i\n\n\u03c8i(ci)\n\nS\u2212(cid:80)n\n\ni=1\n\n\u03c8i(ci)\u03bei\n\n\u221a\n\u221a\n\n(cid:19)2\n\n.\n\nLet p =\n\nLet\n\n\u03bdi + \u03bdi\n\nwi(S) =\n\n(cid:112)\u03c8j(cj)p\n\n(cid:80)\n\nj \u03bdj\n(cid:80)\nj \u03bdj\n\n\u2212 \u03bdi\n\n(cid:112)\u03c8i(ci)p,\n\nyi(S) = wi(S)\n\n(cid:114) p\n\n,\n\n\u03c8i(ci)\n\nand OBJ(S) be the objective of Problem (7) evaluated for this solution.\n\nend\nOutput: {yi(S\u2217), wi(S\u2217)}n\n\ni=1, where (S\u2217) = arg min(S) OBJ(S).\n\nProposition 1 has an important implication in terms of the design of data market architecture when\nusers have both central and local privacy costs: it is optimal to add noise locally! Adding a noise\ncentrally to the final estimator has an advantage because the weights in the final estimator give the\nplatform a lever to deliver heterogeneous central privacy guarantees to users. Despite this advantage,\nwe establish that adding noise centrally is never optimal. This is because the platform prefers to add\nthe noise locally to contribute to both central and local privacy guarantees delivered to users.\n\n4.2 Computing the optimal privacy loss function\n\nThe implementation of the optimal two-part private data acquisition mechanism requires solving\nProblem (7), which is a non-convex program. However, we use the structure of the problem to develop\na polynomial time algorithm to solve it approximately. To do so, we first replace (cid:80)n\ni /yi by an\nauxiliary variable S. Next, we consider the corresponding lagrangian problem. Using Karush-Kuhn-\nTucker (KKT) conditions, we establish a number of relations between problems\u2019 parameters, S, and\np, the lagrangian coefficient corresponding to S = (cid:80)n\ni /yi. Furthermore, we develop upper and\nlower bounds for S. Finally, we do a grid search to find the approximate optimal solution.\nTheorem 3. For any vector of reported privacy sensitivities and \u03f5 > 0, Algorithm 1 finds local privacy\nloss levels and the differentially private linear estimator of the two-part data acquisition mechanism\nwhose cost (i.e., platform\u2019s objective) is at most 1 + \u03b4 of the optimal cost in time poly(n, 1\n\ni=1 w2\n\ni=1 w2\n\n\u03b4 ).\n\nNotice that the approximation factor in Theorem 3 depends on the underlying parameters. Therefore,\nwe have a Polynomial Time Approximation Scheme (PTAS) for finding the optimal two-part data\nacquisition mechanism in the class of linear estimators.\n\nIn the Appendix, we provide a case study with two users to illustrate the performance of the optimal\ntwo-part data acquisition mechanism in terms of the guaranteed privacy levels and payments as\nfunctions of the reported privacy sensitivities.\n\n\n\nBased on the section(s), please answer the following question with yes, no, or n/a and provide a brief justification for your answer.\nQuestion: If the authors curate/release new assets (e.g., code, data, models), do the authors discuss whether the data they are using/curating contains personally identifiable information or offensive content?"}, "5a": {"role": "user", "content": "The following is the methods section of the paper you are reviewing:\n\n\nWe consider a platform interested in estimating a parameter \u03b8 \u2208 R by collecting data of n users,\nindexed by N = {1, \u00b7 \u00b7 \u00b7 , n}. For any i \u2208 N , we denote user i\u2019s data by Xi \u2208 X and we assume Xi\n\n3\n\n\fis given by Xi = \u03b8 + Zi where Z1, \u00b7 \u00b7 \u00b7 , Zn are independent and identically distributed zero-mean\nrandom variables with variance VAR. To simplify the exposition, we further assume |Zi| \u2264 1/2 for\nany i \u2208 N .1 Throughout, we use lower case letters to denote the realization of random variables. The\nplatform\u2019s goal is to minimize the estimate\u2019s error by collecting data from privacy-concerned users.\nTherefore, the platform needs to incentivize them to share their data.\n\n2.1 Local and central privacy losses\n\nBefore formalizing the utilities/objectives of the platform and the users, we define the notions of\nprivacy losses that we adopt in this paper. In particular, we consider two different types of privacy\nlosses that users suffer from. The first one is the privacy loss of a user when she shares her data (only)\nwith the platform, and the second one is the privacy loss through the released estimate (to the public)\nby the platform. Depending on how different users trust the platform, they might care differently\nabout these two privacy losses. For instance, if a user fully trusts the platform, then her main privacy\nconcern would be the second one, while a user who does not trust the platform at all would be more\nconcerned with the first one as the public only observes the aggregated estimate, as opposed to the\nplatform which observes each user\u2019s (shared) data separately.\n\nWe use the differential privacy framework to quantify these privacy losses. Since differential privacy\nwas introduced by Dwork et al., several variants of it have been also proposed. In particular, a popular\none in the machine learning literature is R\u00e9nyi differential privacy (RDP), introduced by Mironov\n[2017], which we also adopt in this paper. Let us first recall the definition of R\u00e9nyi divergence.\nDefinition 1. Let P and Q be two distributions over R with densities p and q. For any \u03b1 \u2208 (1, \u221e],\nthe R\u00e9nyi \u03b1-divergence between P and Q is denoted by D\u03b1(P ||Q) and is given by\n\nD\u03b1(P ||Q) :=\n\n1\n\u03b1 \u2212 1\n\nlog\n\n(cid:90) (cid:18) p(x)\nq(x)\n\n(cid:19)\u03b1\n\nq(x)dx.\n\nFor two random variables X and Y , D\u03b1(X||Y ) denotes the \u03b1-divergence between their distributions.\n\nWe next define two notions of differential privacy, known as central and local, to capture the two\naforementioned types of privacy losses. Local differential privacy corresponds to the privacy loss of a\nuser when she shares her data with the platform through a randomized mapping, known as a channel.\nDefinition 2. Let \u03b5 \u2265 0 and \u03b1 \u2208 (1, \u221e]. A randomized channel C : X \u2192 R is locally (\u03b5, \u03b1)-R\u00e9nyi\n(differentially) private if for any x, x\u2032 \u2208 X ,\n\nD\u03b1(C(x)||C(x\u2032)) \u2264 \u03b5.\n\nCentral differential privacy corresponds to the other privacy loss mentioned above. It bounds the\nchange in the distribution of the platform\u2019s output, i.e., the released estimate, by changing one user\u2019s\ndata. We next provide the formal definition.\ni=1 \u2208 Rn\nDefinition 3. Let \u03b5 = (\u03b5i)n\n(\u03b5, \u03b1)-R\u00e9nyi (differentially) private if for any two datasets x1:n, x\u2032\ncoordinate (data of user i),\n\n+ and \u03b1 \u2208 (1, \u221e]. A randomized algorithm A : X n \u2192 R is\n1:n \u2208 X n that only differ in i-th\n\nD\u03b1(A(x1:n)||A(x\u2032\n\n1:n)) \u2264 \u03b5i.\n\nThe customary approach to guarantee RDP is Gaussian mechanism in which a properly tuned zero-\nmean Gaussian noise is added to fulfill the required condition. The following lemma, adapted from\nMironov [2017], allows us to characterize the Gaussian noise\u2019s variance for a privacy loss level.\nLemma 1. For a function f : X n \u2192 R, we define its sensitivity with respect to i-th coordinate as\n\nLi(f ) := sup {|f (x1:n) \u2212 f (x\u2032\n\n1:n)| : for all x1:n and x\u2032\n\n1:n differing only at i-th coordinate} .\n\nFor any \u03b1 \u2208 (1, \u221e], A(x1:n) = f (x1:n) + W with W \u223c N (0, \u03c32) is\n\n(cid:16)\n\n( \u03b1Li(f )2\n2\u03c32\n\n)n\ni=1, \u03b1\n\n(cid:17)\n\n-RDP.\n\nFor a given vector of local privacy losses (\u03b5(l)\nis by using a linear estimator with Gaussian mechanism which is given by\n\n1 , . . . , \u03b5(l)\n\nn ), a natural way to privately estimate the mean\n\n\u02c6\u03b8(x1, . . . , xn) :=\n\nn\n(cid:88)\n\ni=1\n\nwi \u02c6xi where\n\nn\n(cid:88)\n\ni=1\n\nwi = 1 and \u02c6xi = xi + N (0,\n\n\u03b1\n2\u03b5(l)\ni\n\n) for all i \u2208 N .\n\n(1)\n\n4\n\n\fFigure 1: The interaction between the users and the platform in the two-part private data acquisition.\n\ni / (cid:80)n\n\ni = w2\n\nand the central privacy delivered to user i \u2208 N is \u03b5(c)\n\nFigure 1 depicts this estimator. In particular, using Lemma 1, the local privacy delivered to user\ni \u2208 N is \u03b5(l)\nj . In the next\ni\nsection, we focus on linear estimators in designing the optimal private data acquisition mechanism.\nWe motivate such specification by showing that, for a given vector of local privacy losses (\u03b5(l)\ni=1,\na linear estimator is optimal with respect to mean square error. To formalize this statement, we\nfirst need to define the minimax estimation error as the notion of optimality. Let P be a class of\ndistributions over X . For any P \u2208 P, we denote its mean by \u03b8(P ). A (\u03b5(l)\ni=1-locally RDP estimator\ncan be cast as \u02c6\u03b8((Ci(xi))n\ni=1), where Ci(.) is the randomized channel corresponding to user i. Let\nQ((\u03b5(l)\ni=1-locally RDP estimators. The minimax estimation error is\ni=1,\u02c6\u03b8[|\u02c6\u03b8((Ci(Xi))n\n\ni=1) be the class of such (\u03b5(l)\n\ni=1) \u2212 \u03b8(P )|2],\n\nL(P, Q, (\u03b5(l)\n\nj=1 w2\n\ni=1) :=\n\nj /\u03b5(l)\n\n(Xi\u223cP )n\n\ni )n\n\ni )n\n\ni )n\n\ni )n\n\ni )n\n\n(2)\n\nE\n\ninf\ni=1\u2208Q((\u03b5(l)\n\ni )n\n\ni=1)\n\nsup\nP \u2208P\n\n\u02c6\u03b8,{Ci}n\n\nwhere the expectation is taken over both randomness of data and estimator (including private channels).\nIn other words, the optimal estimator is the one that has the lowest worst case error among all\nestimators that satisfy the privacy requirements. We next state the optimality result.\nTheorem 1. Assume \u03b1 \u2265 2 and \u03b5(l)\nand C1, \u00b7 \u00b7 \u00b7 , Cn be independent channels. Then, there exists a universal constant c such that\n\ni \u2264 1 for all i. Let P1 be the family of distributions over [\u2212 1\n\n2 , 1\n2 ]\n\nL(P, Q, (\u03b5(l)\n\ni )n\n\ni=1) \u2265 c min(\n\n1\ni=1 \u03b5(l)\n\ni\n\n(cid:80)n\n\n, 1).\n\nFurthermore, there exists a linear estimator with Gaussian mechanism such that\n\nE\n\n(Xi\u223cP )n\n\ni=1,\u02c6\u03b8[|\u02c6\u03b8((Ci(Xi))n\n\ni=1) \u2212 \u03b8(P )|2] \u2264 O(1)\n\n\u03b1\ni=1 \u03b5(l)\n\ni\n\n(cid:80)n\n\n.\n\nWe prove the lower bound by using the Le Cam\u2019s method Yu [1997] that reduces the problem of\nfinding lower bounds to a hypothesis testing problem and requires bounding the total variation\ndistance between the estimates when the input data points change. We then find it more convenient to\nbound the total variation in terms of Hellinger distance and use a series of inequalities to bound it.\n\n2.2 Data acquisition mechanism with two-part privacy guarantees\n\nWe next describe the utility functions of the users and the platform and then formulate the platform\u2019s\noptimal data acquisition mechanism. As we described earlier, each user suffers from two privacy\nlosses when sharing her data. The first one is a central privacy loss because of the leakage of her\ninformation through the platform\u2019s output. The second one is a local privacy loss because of the\nleakage of her information through the raw data that she shares with the platform. Each user has a\nheterogeneous privacy sensitivity for these two types of privacy losses. To model such heterogeneity,\nfor each i \u2208 N , we let ci \u2208 [0, 1] be her relative local privacy sensitivity, representing the relative\nweight that user i assigns to the (per unit cost of) local privacy loss. We also let 1 \u2212 ci be her relative\ncentral privacy sensitivity, representing the relative weight that user i assigns to the (per unit cost of)\ncentral privacy loss. Therefore, ci \u2248 1 implies that user i suffers a higher loss of privacy by sharing\nher raw data with the platform (local privacy loss) compared to her loss from the platform\u2019s output\n\n1This assumption does not have any fundamental impact on the results and is made to simplify the notations.\n\n5\n\n\f(central privacy loss). Differently, ci \u2248 0 implies that the user suffers a smaller loss of privacy by\nsharing her raw data with the platform compared to her loss from the platform\u2019s output. In what\nfollows, we use the term privacy sensitivity instead of relative local privacy sensitivity.\n\nFor each i \u2208 N , the privacy sensitivity ci is independently drawn from a publicly known distribution\nwhose support is [0, 1] with cumulative distribution and probability density functions Fi(\u00b7) and fi(\u00b7).\nWe also let c = (c1, . . . , cn) be the vector of privacy sensitivities. The privacy sensitivity of each\nuser is her private information, i.e., the platform does not know it. This is because individuals have\ndifferent views regarding how trustworthy the platform is in protecting their raw data.\n\nThe platform\u2019s objective is to design a mechanism to collect users\u2019 data by paying them to compensate\nfor their privacy losses without knowing the privacy sensitivity of users. To introduce the platform\u2019s\nobjective formally, we adopt the formalism of Bayesian mechanism design pioneered by Myerson\n[1981]. More specifically, the platform designs and announces a payment function, a local privacy loss\nfunction, and a central privacy loss function that are mappings from the reported privacy sensitivities\nof users. The users then report their privacy sensitivities (which may or may not be truthful). Based\non the payment function, the platform compensates the users (the compensation could be monetary\nor some free or discounted service provided to the user). Based on the local and central privacy\nfunctions, the platform designs randomized channels and randomized estimation algorithms that\ndeliver the guaranteed local and central privacy losses while minimizing the sum of the mean squared\nerror and the total expected payments. Given this interaction, we next formally introduce a data\nacquisition mechanism with two-part data privacy guarantees.\n\nDefinition 4 (two-part private data acquisition mechanism). We call the tuple (\u02c6\u03b8, \u03b5(l), \u03b5(c), t) a\ntwo-part private data acquisition mechanism where\n\n1. For all i \u2208 N , \u03b5(l)\ni\n\n: Rn\n\n+ \u2192 R+ is a function that maps the vector of privacy sensitivities c\n\nto a local privacy loss for user i, \u03b5(l)\n\ni (c), with \u03b5(l)(\u00b7) = (\u03b5(l)\n\ni (\u00b7))n\n\ni=1.\n\n2. For all i \u2208 N , \u03b5(c)\n\ni\n\n: Rn\n\n+ \u2192 R+ is a function that maps the vector of privacy sensitivities c\n\nto a central privacy loss for user i, \u03b5(c)\n\ni (c), with \u03b5(c)(.) = (\u03b5(c)\n\ni (\u00b7))n\n\ni=1.\n\n3. \u02c6\u03b8 : X n \u00d7 Rn\n\n+ \u00d7 Rn\n\nacquired locally (\u03b5(l)\nestimate \u02c6\u03b8(x, \u03b5(l)(c), \u03b5(c)(c)).\n\n+ \u2192 R is a (\u03b5(c)(c), \u03b1)-R\u00e9nyi differentially private estimator that maps\ni (c), \u03b1)-R\u00e9nyi differentially private data of user i for i \u2208 N to an\n\n4. For all i \u2208 N , ti : Rn\n\n+ \u2192 R+ is a function that maps the vector of privacy sensitivities c to\n\na payment for user i, ti(c), with t(.) = (ti(\u00b7))n\n\ni=1.\n\nNotice that we have not specified the estimator and the mechanisms that delivers (local and central)\nR\u00e9nyi differential privacy. In the rest of this subsection, we introduce the utilities and the platform\u2019s\nproblem for a general estimators and mechanisms to deliver differential privacy. Later, we focus on\nlinear estimator and Gaussian mechanisms and explicitly solve the platform\u2019s problem.\n\nEach user that participates in a two-part private data acquisition mechanism suffers from both the\nlocal and central privacy losses and need to be compensated by the platform. In particular, the utility\nof user i from participation when her privacy sensitivity is ci and she reports c\u2032\n\ni is given by\n\nui(\u03b5(l)(c\u2032\n\ni, c\u2212i), \u03b5(c)(c\u2032\n\ni, c\u2212i), t, \u02c6\u03b8) = Ec\u2212i[ti(c\u2212i, c\u2032\n\ni)) \u2212 ci\u03b5(l)\n\ni (c\u2212i, c\u2032\n\ni) \u2212 (1 \u2212 ci)\u03b5(c)\n\ni (c\u2212i, c\u2032\n\ni)],\n\ni)) is the payment from the platform, the term ci\u03b5(l)\n\nwhere the term ti(c\u2212i, c\u2032\ni) is the relative lo-\ncal privacy sensitivity of the user multiplied by her local privacy loss, and the term (1\u2212ci)\u03b5(c)\ni (c\u2212i, c\u2032\ni)\nis her relative central privacy sensitivity multiplied by her central privacy loss. A user i \u2208 N that\ndoes not participate in the mechanism neither compromises her privacy nor gets a compensation.\nTherefore, the utility of a user who does not participate in the mechanism becomes 0.\n\ni (c\u2212i, c\u2032\n\nThe goal of the platform is to minimize the sum of the mean squared error and the overall payment to\nusers. We let \u03b3 \u2208 R+ represents the relative weight of the mean estimation error and the payments in\n\n6\n\n\fthe platform\u2019s objective.2 Therefore, the platform\u2019s objective is\nn\n(cid:88)\n\nEc[\u03b3MSE(\u03b5(l)(c), \u03b5(c)(c), \u02c6\u03b8) +\n\nti(c)],\n\ni=1\n\nwhere the first term is the mean square error of estimator \u02c6\u03b8 given reported vector of privacy sensitivity\nand resulting local and central privacy losses \u03b5(l) and \u03b5(c), i.e.,\n\nMSE(\u03b5(l)(c), \u03b5(c)(c), \u02c6\u03b8) = Ex[|\u02c6\u03b8(\u02c6x, \u03b5(l), \u03b5(c)) \u2212 \u03b8|2].\nAlso, each summand of the second term is the compensation that the platform gives to a user to\nincentivize her to participate and report her privacy sensitivity truthfully.\n\nIn Appendix we prove that, similar to the classical mechanism design setting, revelation principle\nholds. This means that there is no loss of generality in focusing on the class of direct incentive\ncompatible mechanisms, meaning the platform\u2019s optimization problem can be written as\n\nmin\n\u03b5(l)(\u00b7),\u03b5(c)(\u00b7),t(\u00b7)\n\nEc[\u03b3MSE(\u03b5(l)(c), \u03b5(c)(c), \u02c6\u03b8) +\n\nn\n(cid:88)\n\nti(c)]\n\ni=1\nui(\u03b5(l)(c), \u03b5(c)(c), t, \u02c6\u03b8) \u2265 ui(\u03b5(l)(c\u2032\nui(\u03b5(l)(c), \u03b5(c)(c), t, \u02c6\u03b8) \u2265 0\n\ni, c\u2212i), \u03b5(c)(c\u2032\n\ni, c\u2212i), t, \u02c6\u03b8)\n\n(5)\nwhere the constraints in (4) represent the incentive compatibility. These constraints guarantee that\nthat each user i has no incentive to misrepresent her privacy sensitivity when others report truthfully\n(reporting truthfully is an equilibrium of the game among the users). Also, the constraints in (5)\nrepresent individual rationality, which ensures that each user receives a non-negative utility from\nparticipating in the platform\u2019s mechanism and sharing her data.\n\nfor all i \u2208 N , ci,\n\n(3)\n\n(4)\n\n3 From the mechanism design problem to an optimization problem\n\nFor a given estimator \u02c6\u03b8, the platform\u2019s decision comprises the local and central privacy loss functions\n\u03b5(l)(\u00b7) and \u03b5(c)(\u00b7) together with the payment functions t(\u00b7). We next show that this problem can be\nequivalently formulated as an optimization problem over the vector of local privacy losses and central\nprivacy losses (as opposed to functions). In the rest of the paper, we impose the following assumption\nwhich is well-known in the mechanism design literature and simplifies the analysis.3\nAssumption 1. For any user i \u2208 N , the virtual cost defined as \u03c8i(c) = c + Fi(c)\nfi(c) is increasing in c,\nwhere fi(\u00b7) and Fi(\u00b7) are probability density and cumulative distribution functions of ci, respectively.\n\nThe above assumption holds for a wide class of distributions such as the ones with log-concave\ndensity functions (e.g., uniform).\nTheorem 2. Suppose Assumption 1 holds. For a given estimator \u02c6\u03b8 : \u02c6X n \u00d7 Rn\n+ \u2192 R, in the\noptimal two-part data acquisition mechanism, for a given vector of reported privacy sensitivities c,\nthe local and central privacy losses are the solution of\nn\n(cid:88)\n\n+ \u00d7 Rn\n\nn\n(cid:88)\n\n\u03b5(l)\ni \u03c8i(ci) +\n\n\u03b5(c)\ni (1 \u2212 \u03c8i(ci)).\n\n(6)\n\n\u03b3MSE(\u03b5(l), \u03b5(c), \u02c6\u03b8) +\n\nmin\ni=1,{\u03b5(c)}n\n\ni=1\n\n{\u03b5(l)}n\n\ni=1\n\ni=1\n\nProof Sketch of Theorem 2: We introduce the following interim functions\n\nti(ci) =Ec\u2212i [t(ci, c\u2212i)],\n\ni (ci) = Ec\u2212i[\u03b5(l)\n\u03b5(l)\n\ni (ci, c\u2212i)],\n\nand \u03b5(c)\n\ni (ci) = Ec\u2212i[\u03b5(c)\n\ni (ci, c\u2212i)].\n\nWe first establish a payment identity that determines the optimal payment in terms of the optimal\nlocal and central privacy losses. In particular, by evaluating the first order condition corresponding to\nthe incentive compatibility constraint (4), we establish that this constraint holds if and only if\n\nti(ci) = ti(0) + \u03b5(c)\n\ni (ci) \u2212 \u03b5(c)\n\ni (0) + ci(\u03b5(l)\n\ni (ci) \u2212 \u03b5(c)\n\ni (ci)) \u2212\n\n(cid:90) ci\n\n0\n\n(\u03b5(l)\n\ni (z) \u2212 \u03b5(c)\n\ni (z))dz,\n\n2Notice that changing the parameter \u03b3 enables us to study a wide range of platform\u2019s objectives with differing\n\nrelative weights between the estimation error and the total payments.\n\n3Without this assumption, extending the results requires ironing technique of Myerson [1981].\n\n7\n\n\fi (z) \u2212 \u03b5(c)\n\nand \u03b5(l)\ni (z) is weakly decreasing in z. We then plug in this payment identity back to the\nplatform\u2019s objective, use the individual rationality constraint, and rewrite the platform\u2019s expected\nutility in terms of the privacy loss functions and the virtual cost of users. This is still a functional\noptimization problem in terms of \u03b5(l)(\u00b7) and \u03b5(c)(\u00b7). However, we establish that, under Assumption 1,\nwe can solve this functional optimization point-wise (i.e., for any given c). \u25a0\n\nTheorem 2 highlights the tradeoff in the platform\u2019s problem: by decreasing the local privacy loss,\nthe second term of the objective decreases (this term corresponds to the payment to users) while\nthe first term (i.e., the mean squared error) increases. The role of the central privacy loss is more\nnuanced, and there are two cases. If the coefficient 1 \u2212 \u03c8i(ci) is non-negative, by decreasing the\ncentral privacy loss, the third term of the objective decreases while the first term increases. If the\ncoefficient 1 \u2212 \u03c8i(ci) is negative, increasing the central privacy loss decreases both the third term\nand the first term. However, we cannot increase the central privacy loss level without limits because\nthe central privacy loss level is always below the local privacy loss level. Therefore, the platform\u2019s\noptimal mechanism should find the \u201cright\u201d balance between these terms.\n\n4 Optimal mechanism with two-part privacy guarantees for linear estimators\n\nFor the rest of the paper, we focus on linear estimators with Gaussian mechanism described in Section\n2.1. The following is a direct corollary of Theorem 2.\nCorollary 1. Suppose Assumption 1 holds. For any reported vector of privacy sensitivities c, the\noptimal local privacy loss levels are \u03b5(l)\nn\n(cid:88)\n\ni and the optimal central privacy loss levels are\n\ni (c) = y\u2217\n\n2\n\nwhere (w\u2217\n\n1, . . . , w\u2217\n\nn) and (y\u2217\n\n1, . . . , y\u2217\n\nn) are the optimal solution of\n\n\u03b5(c)\ni = w\u2217\ni\n\n2/\n\nw\u2217\nj\ny\u2217\nj\n\nj=1\n\nmin\nw,y\n\nVAR\u03b3\n\nn\n(cid:88)\n\ni=1\n\nw2\n\ni +\n\n\u03b3\u03b1\n2\n\nn\n(cid:88)\n\ni=1\n\nw2\ni\nyi\n\n+\n\ns.t. wi, yi \u2265 0, for all i \u2208 N and\n\nn\n(cid:88)\n\ni=1\n\nn\n(cid:88)\n\ni=1\n\n(1 \u2212 \u03c8i(ci))\n\nw2\ni\n\n(cid:80)n\n\nj=1\n\nw2\nj\nyj\n\n+\n\nn\n(cid:88)\n\ni=1\n\n\u03c8i(ci)yi\n\n(7)\n\nwi = 1.\n\nLet us highlight the difference between our characterization and that of classic mechanism design (e.g.,\nMyerson [1981]). In classic mechanism design, the designer\u2019s problem becomes linear optimization.\nHowever, in our setting, the designer\u2019s problem is a non-linear and non-convex optimization. This\nmakes the problem of finding the optimal two-part data acquisition mechanism challenging. Before\naddressing this computational challenge, let us revisit the form of the Gaussian mechanism that we\nhave adopted: the platform adds Gaussian noise locally and then outputs a convex combination of\nthe privatized users\u2019 data without adding any noise centrally. More specifically, one may guess that\nthe platform may benefit by having a central noise added to the final output in addition to the local\nnoises. In the following subsection, we establish that there is another Gaussian mechanism for any\nGaussian mechanism that only adds local noises and achieves a weakly lower cost.\n\n4.1 Optimality of having only local noises in the Gaussian mechanism\n\nThe platform has the opportunity of adding Gaussian noise to both the raw data of each user and the\nfinal estimator and ex-ante one may guess that it is optimal to use both of these instruments. However,\nas we establish next, interestingly, in the optimal two-part data acquisition mechanism, it is always\noptimal to only add noises locally.\nFor a given vector of local privacy losses (\u03b5(l)\nGaussian mechanism with both local and central noises is of the form\n\nn ) and central privacy losses (\u03b5(c)\n\n1 , . . . , \u03b5(c)\n\n1 , . . . , \u03b5(l)\n\nn ), a\n\n\u02c6\u03b8(x1, . . . , xn) :=\n\nn\n(cid:88)\n\ni=1\n\nwi \u02c6xi + N (0,\n\n\u03b1\n2\u03b5\n\n) where\n\nn\n(cid:88)\n\ni=1\n\nwi = 1 and \u02c6xi = xi + N (0,\n\n\u03b1\n2\u03b5(l)\ni\n\n)\u2200i \u2208 N .\n\nProposition 1. In the optimal two-part data acquisition mechanism that adopts a Gaussian mecha-\nnism with both local and central noises, we have \u03b5 = \u221e.\n\n8\n\n\fAlgorithm 1: Computing the optimal two-part private data acquisition mechanism\nInput: The vector of privacy sensitivities (c1, . . . , cn)\nfor S \u2208 Grid (cid:0)[S, \u00afS], \u03b4(cid:1) do\n\nLet\n\n\u03bdi =\n\n1\n\u03b3 VAR + (1 \u2212 \u03c8i(ci))/S\n\n, \u03b6i =\n\n\u03bdi\nj \u03bdj\n\n(cid:80)\n\n, \u03bei = \u03b6i\n\n(cid:113)\n\n\u03bdj(\n\n\u03c8j(cj) \u2212 (cid:112)\u03c8i(ci))\n\n\uf8f6\n\n\uf8f8 .\n\n\uf8eb\n\n\uf8ed\n\nn\n(cid:88)\n\nj=1\n\n(cid:18) (cid:80)n\n\ni=1 \u03b6i\n\n\u03c8i(ci)\n\nS\u2212(cid:80)n\n\ni=1\n\n\u03c8i(ci)\u03bei\n\n\u221a\n\u221a\n\n(cid:19)2\n\n.\n\nLet p =\n\nLet\n\n\u03bdi + \u03bdi\n\nwi(S) =\n\n(cid:112)\u03c8j(cj)p\n\n(cid:80)\n\nj \u03bdj\n(cid:80)\nj \u03bdj\n\n\u2212 \u03bdi\n\n(cid:112)\u03c8i(ci)p,\n\nyi(S) = wi(S)\n\n(cid:114) p\n\n,\n\n\u03c8i(ci)\n\nand OBJ(S) be the objective of Problem (7) evaluated for this solution.\n\nend\nOutput: {yi(S\u2217), wi(S\u2217)}n\n\ni=1, where (S\u2217) = arg min(S) OBJ(S).\n\nProposition 1 has an important implication in terms of the design of data market architecture when\nusers have both central and local privacy costs: it is optimal to add noise locally! Adding a noise\ncentrally to the final estimator has an advantage because the weights in the final estimator give the\nplatform a lever to deliver heterogeneous central privacy guarantees to users. Despite this advantage,\nwe establish that adding noise centrally is never optimal. This is because the platform prefers to add\nthe noise locally to contribute to both central and local privacy guarantees delivered to users.\n\n4.2 Computing the optimal privacy loss function\n\nThe implementation of the optimal two-part private data acquisition mechanism requires solving\nProblem (7), which is a non-convex program. However, we use the structure of the problem to develop\na polynomial time algorithm to solve it approximately. To do so, we first replace (cid:80)n\ni /yi by an\nauxiliary variable S. Next, we consider the corresponding lagrangian problem. Using Karush-Kuhn-\nTucker (KKT) conditions, we establish a number of relations between problems\u2019 parameters, S, and\np, the lagrangian coefficient corresponding to S = (cid:80)n\ni /yi. Furthermore, we develop upper and\nlower bounds for S. Finally, we do a grid search to find the approximate optimal solution.\nTheorem 3. For any vector of reported privacy sensitivities and \u03f5 > 0, Algorithm 1 finds local privacy\nloss levels and the differentially private linear estimator of the two-part data acquisition mechanism\nwhose cost (i.e., platform\u2019s objective) is at most 1 + \u03b4 of the optimal cost in time poly(n, 1\n\ni=1 w2\n\ni=1 w2\n\n\u03b4 ).\n\nNotice that the approximation factor in Theorem 3 depends on the underlying parameters. Therefore,\nwe have a Polynomial Time Approximation Scheme (PTAS) for finding the optimal two-part data\nacquisition mechanism in the class of linear estimators.\n\nIn the Appendix, we provide a case study with two users to illustrate the performance of the optimal\ntwo-part data acquisition mechanism in terms of the guaranteed privacy levels and payments as\nfunctions of the reported privacy sensitivities.\n\n\n\nBased on the section(s), please answer the following question with yes, no, or n/a and provide a brief justification for your answer.\nQuestion: If the authors used crowdsourcing or conducted research with human subjects, do the authors include the full text of instructions given to participants and screenshots, if applicable?"}, "5b": {"role": "user", "content": "The following is the methods section of the paper you are reviewing:\n\n\nWe consider a platform interested in estimating a parameter \u03b8 \u2208 R by collecting data of n users,\nindexed by N = {1, \u00b7 \u00b7 \u00b7 , n}. For any i \u2208 N , we denote user i\u2019s data by Xi \u2208 X and we assume Xi\n\n3\n\n\fis given by Xi = \u03b8 + Zi where Z1, \u00b7 \u00b7 \u00b7 , Zn are independent and identically distributed zero-mean\nrandom variables with variance VAR. To simplify the exposition, we further assume |Zi| \u2264 1/2 for\nany i \u2208 N .1 Throughout, we use lower case letters to denote the realization of random variables. The\nplatform\u2019s goal is to minimize the estimate\u2019s error by collecting data from privacy-concerned users.\nTherefore, the platform needs to incentivize them to share their data.\n\n2.1 Local and central privacy losses\n\nBefore formalizing the utilities/objectives of the platform and the users, we define the notions of\nprivacy losses that we adopt in this paper. In particular, we consider two different types of privacy\nlosses that users suffer from. The first one is the privacy loss of a user when she shares her data (only)\nwith the platform, and the second one is the privacy loss through the released estimate (to the public)\nby the platform. Depending on how different users trust the platform, they might care differently\nabout these two privacy losses. For instance, if a user fully trusts the platform, then her main privacy\nconcern would be the second one, while a user who does not trust the platform at all would be more\nconcerned with the first one as the public only observes the aggregated estimate, as opposed to the\nplatform which observes each user\u2019s (shared) data separately.\n\nWe use the differential privacy framework to quantify these privacy losses. Since differential privacy\nwas introduced by Dwork et al., several variants of it have been also proposed. In particular, a popular\none in the machine learning literature is R\u00e9nyi differential privacy (RDP), introduced by Mironov\n[2017], which we also adopt in this paper. Let us first recall the definition of R\u00e9nyi divergence.\nDefinition 1. Let P and Q be two distributions over R with densities p and q. For any \u03b1 \u2208 (1, \u221e],\nthe R\u00e9nyi \u03b1-divergence between P and Q is denoted by D\u03b1(P ||Q) and is given by\n\nD\u03b1(P ||Q) :=\n\n1\n\u03b1 \u2212 1\n\nlog\n\n(cid:90) (cid:18) p(x)\nq(x)\n\n(cid:19)\u03b1\n\nq(x)dx.\n\nFor two random variables X and Y , D\u03b1(X||Y ) denotes the \u03b1-divergence between their distributions.\n\nWe next define two notions of differential privacy, known as central and local, to capture the two\naforementioned types of privacy losses. Local differential privacy corresponds to the privacy loss of a\nuser when she shares her data with the platform through a randomized mapping, known as a channel.\nDefinition 2. Let \u03b5 \u2265 0 and \u03b1 \u2208 (1, \u221e]. A randomized channel C : X \u2192 R is locally (\u03b5, \u03b1)-R\u00e9nyi\n(differentially) private if for any x, x\u2032 \u2208 X ,\n\nD\u03b1(C(x)||C(x\u2032)) \u2264 \u03b5.\n\nCentral differential privacy corresponds to the other privacy loss mentioned above. It bounds the\nchange in the distribution of the platform\u2019s output, i.e., the released estimate, by changing one user\u2019s\ndata. We next provide the formal definition.\ni=1 \u2208 Rn\nDefinition 3. Let \u03b5 = (\u03b5i)n\n(\u03b5, \u03b1)-R\u00e9nyi (differentially) private if for any two datasets x1:n, x\u2032\ncoordinate (data of user i),\n\n+ and \u03b1 \u2208 (1, \u221e]. A randomized algorithm A : X n \u2192 R is\n1:n \u2208 X n that only differ in i-th\n\nD\u03b1(A(x1:n)||A(x\u2032\n\n1:n)) \u2264 \u03b5i.\n\nThe customary approach to guarantee RDP is Gaussian mechanism in which a properly tuned zero-\nmean Gaussian noise is added to fulfill the required condition. The following lemma, adapted from\nMironov [2017], allows us to characterize the Gaussian noise\u2019s variance for a privacy loss level.\nLemma 1. For a function f : X n \u2192 R, we define its sensitivity with respect to i-th coordinate as\n\nLi(f ) := sup {|f (x1:n) \u2212 f (x\u2032\n\n1:n)| : for all x1:n and x\u2032\n\n1:n differing only at i-th coordinate} .\n\nFor any \u03b1 \u2208 (1, \u221e], A(x1:n) = f (x1:n) + W with W \u223c N (0, \u03c32) is\n\n(cid:16)\n\n( \u03b1Li(f )2\n2\u03c32\n\n)n\ni=1, \u03b1\n\n(cid:17)\n\n-RDP.\n\nFor a given vector of local privacy losses (\u03b5(l)\nis by using a linear estimator with Gaussian mechanism which is given by\n\n1 , . . . , \u03b5(l)\n\nn ), a natural way to privately estimate the mean\n\n\u02c6\u03b8(x1, . . . , xn) :=\n\nn\n(cid:88)\n\ni=1\n\nwi \u02c6xi where\n\nn\n(cid:88)\n\ni=1\n\nwi = 1 and \u02c6xi = xi + N (0,\n\n\u03b1\n2\u03b5(l)\ni\n\n) for all i \u2208 N .\n\n(1)\n\n4\n\n\fFigure 1: The interaction between the users and the platform in the two-part private data acquisition.\n\ni / (cid:80)n\n\ni = w2\n\nand the central privacy delivered to user i \u2208 N is \u03b5(c)\n\nFigure 1 depicts this estimator. In particular, using Lemma 1, the local privacy delivered to user\ni \u2208 N is \u03b5(l)\nj . In the next\ni\nsection, we focus on linear estimators in designing the optimal private data acquisition mechanism.\nWe motivate such specification by showing that, for a given vector of local privacy losses (\u03b5(l)\ni=1,\na linear estimator is optimal with respect to mean square error. To formalize this statement, we\nfirst need to define the minimax estimation error as the notion of optimality. Let P be a class of\ndistributions over X . For any P \u2208 P, we denote its mean by \u03b8(P ). A (\u03b5(l)\ni=1-locally RDP estimator\ncan be cast as \u02c6\u03b8((Ci(xi))n\ni=1), where Ci(.) is the randomized channel corresponding to user i. Let\nQ((\u03b5(l)\ni=1-locally RDP estimators. The minimax estimation error is\ni=1,\u02c6\u03b8[|\u02c6\u03b8((Ci(Xi))n\n\ni=1) be the class of such (\u03b5(l)\n\ni=1) \u2212 \u03b8(P )|2],\n\nL(P, Q, (\u03b5(l)\n\nj=1 w2\n\ni=1) :=\n\nj /\u03b5(l)\n\n(Xi\u223cP )n\n\ni )n\n\ni )n\n\ni )n\n\ni )n\n\ni )n\n\n(2)\n\nE\n\ninf\ni=1\u2208Q((\u03b5(l)\n\ni )n\n\ni=1)\n\nsup\nP \u2208P\n\n\u02c6\u03b8,{Ci}n\n\nwhere the expectation is taken over both randomness of data and estimator (including private channels).\nIn other words, the optimal estimator is the one that has the lowest worst case error among all\nestimators that satisfy the privacy requirements. We next state the optimality result.\nTheorem 1. Assume \u03b1 \u2265 2 and \u03b5(l)\nand C1, \u00b7 \u00b7 \u00b7 , Cn be independent channels. Then, there exists a universal constant c such that\n\ni \u2264 1 for all i. Let P1 be the family of distributions over [\u2212 1\n\n2 , 1\n2 ]\n\nL(P, Q, (\u03b5(l)\n\ni )n\n\ni=1) \u2265 c min(\n\n1\ni=1 \u03b5(l)\n\ni\n\n(cid:80)n\n\n, 1).\n\nFurthermore, there exists a linear estimator with Gaussian mechanism such that\n\nE\n\n(Xi\u223cP )n\n\ni=1,\u02c6\u03b8[|\u02c6\u03b8((Ci(Xi))n\n\ni=1) \u2212 \u03b8(P )|2] \u2264 O(1)\n\n\u03b1\ni=1 \u03b5(l)\n\ni\n\n(cid:80)n\n\n.\n\nWe prove the lower bound by using the Le Cam\u2019s method Yu [1997] that reduces the problem of\nfinding lower bounds to a hypothesis testing problem and requires bounding the total variation\ndistance between the estimates when the input data points change. We then find it more convenient to\nbound the total variation in terms of Hellinger distance and use a series of inequalities to bound it.\n\n2.2 Data acquisition mechanism with two-part privacy guarantees\n\nWe next describe the utility functions of the users and the platform and then formulate the platform\u2019s\noptimal data acquisition mechanism. As we described earlier, each user suffers from two privacy\nlosses when sharing her data. The first one is a central privacy loss because of the leakage of her\ninformation through the platform\u2019s output. The second one is a local privacy loss because of the\nleakage of her information through the raw data that she shares with the platform. Each user has a\nheterogeneous privacy sensitivity for these two types of privacy losses. To model such heterogeneity,\nfor each i \u2208 N , we let ci \u2208 [0, 1] be her relative local privacy sensitivity, representing the relative\nweight that user i assigns to the (per unit cost of) local privacy loss. We also let 1 \u2212 ci be her relative\ncentral privacy sensitivity, representing the relative weight that user i assigns to the (per unit cost of)\ncentral privacy loss. Therefore, ci \u2248 1 implies that user i suffers a higher loss of privacy by sharing\nher raw data with the platform (local privacy loss) compared to her loss from the platform\u2019s output\n\n1This assumption does not have any fundamental impact on the results and is made to simplify the notations.\n\n5\n\n\f(central privacy loss). Differently, ci \u2248 0 implies that the user suffers a smaller loss of privacy by\nsharing her raw data with the platform compared to her loss from the platform\u2019s output. In what\nfollows, we use the term privacy sensitivity instead of relative local privacy sensitivity.\n\nFor each i \u2208 N , the privacy sensitivity ci is independently drawn from a publicly known distribution\nwhose support is [0, 1] with cumulative distribution and probability density functions Fi(\u00b7) and fi(\u00b7).\nWe also let c = (c1, . . . , cn) be the vector of privacy sensitivities. The privacy sensitivity of each\nuser is her private information, i.e., the platform does not know it. This is because individuals have\ndifferent views regarding how trustworthy the platform is in protecting their raw data.\n\nThe platform\u2019s objective is to design a mechanism to collect users\u2019 data by paying them to compensate\nfor their privacy losses without knowing the privacy sensitivity of users. To introduce the platform\u2019s\nobjective formally, we adopt the formalism of Bayesian mechanism design pioneered by Myerson\n[1981]. More specifically, the platform designs and announces a payment function, a local privacy loss\nfunction, and a central privacy loss function that are mappings from the reported privacy sensitivities\nof users. The users then report their privacy sensitivities (which may or may not be truthful). Based\non the payment function, the platform compensates the users (the compensation could be monetary\nor some free or discounted service provided to the user). Based on the local and central privacy\nfunctions, the platform designs randomized channels and randomized estimation algorithms that\ndeliver the guaranteed local and central privacy losses while minimizing the sum of the mean squared\nerror and the total expected payments. Given this interaction, we next formally introduce a data\nacquisition mechanism with two-part data privacy guarantees.\n\nDefinition 4 (two-part private data acquisition mechanism). We call the tuple (\u02c6\u03b8, \u03b5(l), \u03b5(c), t) a\ntwo-part private data acquisition mechanism where\n\n1. For all i \u2208 N , \u03b5(l)\ni\n\n: Rn\n\n+ \u2192 R+ is a function that maps the vector of privacy sensitivities c\n\nto a local privacy loss for user i, \u03b5(l)\n\ni (c), with \u03b5(l)(\u00b7) = (\u03b5(l)\n\ni (\u00b7))n\n\ni=1.\n\n2. For all i \u2208 N , \u03b5(c)\n\ni\n\n: Rn\n\n+ \u2192 R+ is a function that maps the vector of privacy sensitivities c\n\nto a central privacy loss for user i, \u03b5(c)\n\ni (c), with \u03b5(c)(.) = (\u03b5(c)\n\ni (\u00b7))n\n\ni=1.\n\n3. \u02c6\u03b8 : X n \u00d7 Rn\n\n+ \u00d7 Rn\n\nacquired locally (\u03b5(l)\nestimate \u02c6\u03b8(x, \u03b5(l)(c), \u03b5(c)(c)).\n\n+ \u2192 R is a (\u03b5(c)(c), \u03b1)-R\u00e9nyi differentially private estimator that maps\ni (c), \u03b1)-R\u00e9nyi differentially private data of user i for i \u2208 N to an\n\n4. For all i \u2208 N , ti : Rn\n\n+ \u2192 R+ is a function that maps the vector of privacy sensitivities c to\n\na payment for user i, ti(c), with t(.) = (ti(\u00b7))n\n\ni=1.\n\nNotice that we have not specified the estimator and the mechanisms that delivers (local and central)\nR\u00e9nyi differential privacy. In the rest of this subsection, we introduce the utilities and the platform\u2019s\nproblem for a general estimators and mechanisms to deliver differential privacy. Later, we focus on\nlinear estimator and Gaussian mechanisms and explicitly solve the platform\u2019s problem.\n\nEach user that participates in a two-part private data acquisition mechanism suffers from both the\nlocal and central privacy losses and need to be compensated by the platform. In particular, the utility\nof user i from participation when her privacy sensitivity is ci and she reports c\u2032\n\ni is given by\n\nui(\u03b5(l)(c\u2032\n\ni, c\u2212i), \u03b5(c)(c\u2032\n\ni, c\u2212i), t, \u02c6\u03b8) = Ec\u2212i[ti(c\u2212i, c\u2032\n\ni)) \u2212 ci\u03b5(l)\n\ni (c\u2212i, c\u2032\n\ni) \u2212 (1 \u2212 ci)\u03b5(c)\n\ni (c\u2212i, c\u2032\n\ni)],\n\ni)) is the payment from the platform, the term ci\u03b5(l)\n\nwhere the term ti(c\u2212i, c\u2032\ni) is the relative lo-\ncal privacy sensitivity of the user multiplied by her local privacy loss, and the term (1\u2212ci)\u03b5(c)\ni (c\u2212i, c\u2032\ni)\nis her relative central privacy sensitivity multiplied by her central privacy loss. A user i \u2208 N that\ndoes not participate in the mechanism neither compromises her privacy nor gets a compensation.\nTherefore, the utility of a user who does not participate in the mechanism becomes 0.\n\ni (c\u2212i, c\u2032\n\nThe goal of the platform is to minimize the sum of the mean squared error and the overall payment to\nusers. We let \u03b3 \u2208 R+ represents the relative weight of the mean estimation error and the payments in\n\n6\n\n\fthe platform\u2019s objective.2 Therefore, the platform\u2019s objective is\nn\n(cid:88)\n\nEc[\u03b3MSE(\u03b5(l)(c), \u03b5(c)(c), \u02c6\u03b8) +\n\nti(c)],\n\ni=1\n\nwhere the first term is the mean square error of estimator \u02c6\u03b8 given reported vector of privacy sensitivity\nand resulting local and central privacy losses \u03b5(l) and \u03b5(c), i.e.,\n\nMSE(\u03b5(l)(c), \u03b5(c)(c), \u02c6\u03b8) = Ex[|\u02c6\u03b8(\u02c6x, \u03b5(l), \u03b5(c)) \u2212 \u03b8|2].\nAlso, each summand of the second term is the compensation that the platform gives to a user to\nincentivize her to participate and report her privacy sensitivity truthfully.\n\nIn Appendix we prove that, similar to the classical mechanism design setting, revelation principle\nholds. This means that there is no loss of generality in focusing on the class of direct incentive\ncompatible mechanisms, meaning the platform\u2019s optimization problem can be written as\n\nmin\n\u03b5(l)(\u00b7),\u03b5(c)(\u00b7),t(\u00b7)\n\nEc[\u03b3MSE(\u03b5(l)(c), \u03b5(c)(c), \u02c6\u03b8) +\n\nn\n(cid:88)\n\nti(c)]\n\ni=1\nui(\u03b5(l)(c), \u03b5(c)(c), t, \u02c6\u03b8) \u2265 ui(\u03b5(l)(c\u2032\nui(\u03b5(l)(c), \u03b5(c)(c), t, \u02c6\u03b8) \u2265 0\n\ni, c\u2212i), \u03b5(c)(c\u2032\n\ni, c\u2212i), t, \u02c6\u03b8)\n\n(5)\nwhere the constraints in (4) represent the incentive compatibility. These constraints guarantee that\nthat each user i has no incentive to misrepresent her privacy sensitivity when others report truthfully\n(reporting truthfully is an equilibrium of the game among the users). Also, the constraints in (5)\nrepresent individual rationality, which ensures that each user receives a non-negative utility from\nparticipating in the platform\u2019s mechanism and sharing her data.\n\nfor all i \u2208 N , ci,\n\n(3)\n\n(4)\n\n3 From the mechanism design problem to an optimization problem\n\nFor a given estimator \u02c6\u03b8, the platform\u2019s decision comprises the local and central privacy loss functions\n\u03b5(l)(\u00b7) and \u03b5(c)(\u00b7) together with the payment functions t(\u00b7). We next show that this problem can be\nequivalently formulated as an optimization problem over the vector of local privacy losses and central\nprivacy losses (as opposed to functions). In the rest of the paper, we impose the following assumption\nwhich is well-known in the mechanism design literature and simplifies the analysis.3\nAssumption 1. For any user i \u2208 N , the virtual cost defined as \u03c8i(c) = c + Fi(c)\nfi(c) is increasing in c,\nwhere fi(\u00b7) and Fi(\u00b7) are probability density and cumulative distribution functions of ci, respectively.\n\nThe above assumption holds for a wide class of distributions such as the ones with log-concave\ndensity functions (e.g., uniform).\nTheorem 2. Suppose Assumption 1 holds. For a given estimator \u02c6\u03b8 : \u02c6X n \u00d7 Rn\n+ \u2192 R, in the\noptimal two-part data acquisition mechanism, for a given vector of reported privacy sensitivities c,\nthe local and central privacy losses are the solution of\nn\n(cid:88)\n\n+ \u00d7 Rn\n\nn\n(cid:88)\n\n\u03b5(l)\ni \u03c8i(ci) +\n\n\u03b5(c)\ni (1 \u2212 \u03c8i(ci)).\n\n(6)\n\n\u03b3MSE(\u03b5(l), \u03b5(c), \u02c6\u03b8) +\n\nmin\ni=1,{\u03b5(c)}n\n\ni=1\n\n{\u03b5(l)}n\n\ni=1\n\ni=1\n\nProof Sketch of Theorem 2: We introduce the following interim functions\n\nti(ci) =Ec\u2212i [t(ci, c\u2212i)],\n\ni (ci) = Ec\u2212i[\u03b5(l)\n\u03b5(l)\n\ni (ci, c\u2212i)],\n\nand \u03b5(c)\n\ni (ci) = Ec\u2212i[\u03b5(c)\n\ni (ci, c\u2212i)].\n\nWe first establish a payment identity that determines the optimal payment in terms of the optimal\nlocal and central privacy losses. In particular, by evaluating the first order condition corresponding to\nthe incentive compatibility constraint (4), we establish that this constraint holds if and only if\n\nti(ci) = ti(0) + \u03b5(c)\n\ni (ci) \u2212 \u03b5(c)\n\ni (0) + ci(\u03b5(l)\n\ni (ci) \u2212 \u03b5(c)\n\ni (ci)) \u2212\n\n(cid:90) ci\n\n0\n\n(\u03b5(l)\n\ni (z) \u2212 \u03b5(c)\n\ni (z))dz,\n\n2Notice that changing the parameter \u03b3 enables us to study a wide range of platform\u2019s objectives with differing\n\nrelative weights between the estimation error and the total payments.\n\n3Without this assumption, extending the results requires ironing technique of Myerson [1981].\n\n7\n\n\fi (z) \u2212 \u03b5(c)\n\nand \u03b5(l)\ni (z) is weakly decreasing in z. We then plug in this payment identity back to the\nplatform\u2019s objective, use the individual rationality constraint, and rewrite the platform\u2019s expected\nutility in terms of the privacy loss functions and the virtual cost of users. This is still a functional\noptimization problem in terms of \u03b5(l)(\u00b7) and \u03b5(c)(\u00b7). However, we establish that, under Assumption 1,\nwe can solve this functional optimization point-wise (i.e., for any given c). \u25a0\n\nTheorem 2 highlights the tradeoff in the platform\u2019s problem: by decreasing the local privacy loss,\nthe second term of the objective decreases (this term corresponds to the payment to users) while\nthe first term (i.e., the mean squared error) increases. The role of the central privacy loss is more\nnuanced, and there are two cases. If the coefficient 1 \u2212 \u03c8i(ci) is non-negative, by decreasing the\ncentral privacy loss, the third term of the objective decreases while the first term increases. If the\ncoefficient 1 \u2212 \u03c8i(ci) is negative, increasing the central privacy loss decreases both the third term\nand the first term. However, we cannot increase the central privacy loss level without limits because\nthe central privacy loss level is always below the local privacy loss level. Therefore, the platform\u2019s\noptimal mechanism should find the \u201cright\u201d balance between these terms.\n\n4 Optimal mechanism with two-part privacy guarantees for linear estimators\n\nFor the rest of the paper, we focus on linear estimators with Gaussian mechanism described in Section\n2.1. The following is a direct corollary of Theorem 2.\nCorollary 1. Suppose Assumption 1 holds. For any reported vector of privacy sensitivities c, the\noptimal local privacy loss levels are \u03b5(l)\nn\n(cid:88)\n\ni and the optimal central privacy loss levels are\n\ni (c) = y\u2217\n\n2\n\nwhere (w\u2217\n\n1, . . . , w\u2217\n\nn) and (y\u2217\n\n1, . . . , y\u2217\n\nn) are the optimal solution of\n\n\u03b5(c)\ni = w\u2217\ni\n\n2/\n\nw\u2217\nj\ny\u2217\nj\n\nj=1\n\nmin\nw,y\n\nVAR\u03b3\n\nn\n(cid:88)\n\ni=1\n\nw2\n\ni +\n\n\u03b3\u03b1\n2\n\nn\n(cid:88)\n\ni=1\n\nw2\ni\nyi\n\n+\n\ns.t. wi, yi \u2265 0, for all i \u2208 N and\n\nn\n(cid:88)\n\ni=1\n\nn\n(cid:88)\n\ni=1\n\n(1 \u2212 \u03c8i(ci))\n\nw2\ni\n\n(cid:80)n\n\nj=1\n\nw2\nj\nyj\n\n+\n\nn\n(cid:88)\n\ni=1\n\n\u03c8i(ci)yi\n\n(7)\n\nwi = 1.\n\nLet us highlight the difference between our characterization and that of classic mechanism design (e.g.,\nMyerson [1981]). In classic mechanism design, the designer\u2019s problem becomes linear optimization.\nHowever, in our setting, the designer\u2019s problem is a non-linear and non-convex optimization. This\nmakes the problem of finding the optimal two-part data acquisition mechanism challenging. Before\naddressing this computational challenge, let us revisit the form of the Gaussian mechanism that we\nhave adopted: the platform adds Gaussian noise locally and then outputs a convex combination of\nthe privatized users\u2019 data without adding any noise centrally. More specifically, one may guess that\nthe platform may benefit by having a central noise added to the final output in addition to the local\nnoises. In the following subsection, we establish that there is another Gaussian mechanism for any\nGaussian mechanism that only adds local noises and achieves a weakly lower cost.\n\n4.1 Optimality of having only local noises in the Gaussian mechanism\n\nThe platform has the opportunity of adding Gaussian noise to both the raw data of each user and the\nfinal estimator and ex-ante one may guess that it is optimal to use both of these instruments. However,\nas we establish next, interestingly, in the optimal two-part data acquisition mechanism, it is always\noptimal to only add noises locally.\nFor a given vector of local privacy losses (\u03b5(l)\nGaussian mechanism with both local and central noises is of the form\n\nn ) and central privacy losses (\u03b5(c)\n\n1 , . . . , \u03b5(c)\n\n1 , . . . , \u03b5(l)\n\nn ), a\n\n\u02c6\u03b8(x1, . . . , xn) :=\n\nn\n(cid:88)\n\ni=1\n\nwi \u02c6xi + N (0,\n\n\u03b1\n2\u03b5\n\n) where\n\nn\n(cid:88)\n\ni=1\n\nwi = 1 and \u02c6xi = xi + N (0,\n\n\u03b1\n2\u03b5(l)\ni\n\n)\u2200i \u2208 N .\n\nProposition 1. In the optimal two-part data acquisition mechanism that adopts a Gaussian mecha-\nnism with both local and central noises, we have \u03b5 = \u221e.\n\n8\n\n\fAlgorithm 1: Computing the optimal two-part private data acquisition mechanism\nInput: The vector of privacy sensitivities (c1, . . . , cn)\nfor S \u2208 Grid (cid:0)[S, \u00afS], \u03b4(cid:1) do\n\nLet\n\n\u03bdi =\n\n1\n\u03b3 VAR + (1 \u2212 \u03c8i(ci))/S\n\n, \u03b6i =\n\n\u03bdi\nj \u03bdj\n\n(cid:80)\n\n, \u03bei = \u03b6i\n\n(cid:113)\n\n\u03bdj(\n\n\u03c8j(cj) \u2212 (cid:112)\u03c8i(ci))\n\n\uf8f6\n\n\uf8f8 .\n\n\uf8eb\n\n\uf8ed\n\nn\n(cid:88)\n\nj=1\n\n(cid:18) (cid:80)n\n\ni=1 \u03b6i\n\n\u03c8i(ci)\n\nS\u2212(cid:80)n\n\ni=1\n\n\u03c8i(ci)\u03bei\n\n\u221a\n\u221a\n\n(cid:19)2\n\n.\n\nLet p =\n\nLet\n\n\u03bdi + \u03bdi\n\nwi(S) =\n\n(cid:112)\u03c8j(cj)p\n\n(cid:80)\n\nj \u03bdj\n(cid:80)\nj \u03bdj\n\n\u2212 \u03bdi\n\n(cid:112)\u03c8i(ci)p,\n\nyi(S) = wi(S)\n\n(cid:114) p\n\n,\n\n\u03c8i(ci)\n\nand OBJ(S) be the objective of Problem (7) evaluated for this solution.\n\nend\nOutput: {yi(S\u2217), wi(S\u2217)}n\n\ni=1, where (S\u2217) = arg min(S) OBJ(S).\n\nProposition 1 has an important implication in terms of the design of data market architecture when\nusers have both central and local privacy costs: it is optimal to add noise locally! Adding a noise\ncentrally to the final estimator has an advantage because the weights in the final estimator give the\nplatform a lever to deliver heterogeneous central privacy guarantees to users. Despite this advantage,\nwe establish that adding noise centrally is never optimal. This is because the platform prefers to add\nthe noise locally to contribute to both central and local privacy guarantees delivered to users.\n\n4.2 Computing the optimal privacy loss function\n\nThe implementation of the optimal two-part private data acquisition mechanism requires solving\nProblem (7), which is a non-convex program. However, we use the structure of the problem to develop\na polynomial time algorithm to solve it approximately. To do so, we first replace (cid:80)n\ni /yi by an\nauxiliary variable S. Next, we consider the corresponding lagrangian problem. Using Karush-Kuhn-\nTucker (KKT) conditions, we establish a number of relations between problems\u2019 parameters, S, and\np, the lagrangian coefficient corresponding to S = (cid:80)n\ni /yi. Furthermore, we develop upper and\nlower bounds for S. Finally, we do a grid search to find the approximate optimal solution.\nTheorem 3. For any vector of reported privacy sensitivities and \u03f5 > 0, Algorithm 1 finds local privacy\nloss levels and the differentially private linear estimator of the two-part data acquisition mechanism\nwhose cost (i.e., platform\u2019s objective) is at most 1 + \u03b4 of the optimal cost in time poly(n, 1\n\ni=1 w2\n\ni=1 w2\n\n\u03b4 ).\n\nNotice that the approximation factor in Theorem 3 depends on the underlying parameters. Therefore,\nwe have a Polynomial Time Approximation Scheme (PTAS) for finding the optimal two-part data\nacquisition mechanism in the class of linear estimators.\n\nIn the Appendix, we provide a case study with two users to illustrate the performance of the optimal\ntwo-part data acquisition mechanism in terms of the guaranteed privacy levels and payments as\nfunctions of the reported privacy sensitivities.\n\n\n\nBased on the section(s), please answer the following question with yes, no, or n/a and provide a brief justification for your answer.\nQuestion: If the authors used crowdsourcing or conducted research with human subjects, do the authors describe any potential participant risks, with links to Institutional Review Board (IRB) approvals, if applicable?"}, "5c": {"role": "user", "content": "The following is the methods section of the paper you are reviewing:\n\n\nWe consider a platform interested in estimating a parameter \u03b8 \u2208 R by collecting data of n users,\nindexed by N = {1, \u00b7 \u00b7 \u00b7 , n}. For any i \u2208 N , we denote user i\u2019s data by Xi \u2208 X and we assume Xi\n\n3\n\n\fis given by Xi = \u03b8 + Zi where Z1, \u00b7 \u00b7 \u00b7 , Zn are independent and identically distributed zero-mean\nrandom variables with variance VAR. To simplify the exposition, we further assume |Zi| \u2264 1/2 for\nany i \u2208 N .1 Throughout, we use lower case letters to denote the realization of random variables. The\nplatform\u2019s goal is to minimize the estimate\u2019s error by collecting data from privacy-concerned users.\nTherefore, the platform needs to incentivize them to share their data.\n\n2.1 Local and central privacy losses\n\nBefore formalizing the utilities/objectives of the platform and the users, we define the notions of\nprivacy losses that we adopt in this paper. In particular, we consider two different types of privacy\nlosses that users suffer from. The first one is the privacy loss of a user when she shares her data (only)\nwith the platform, and the second one is the privacy loss through the released estimate (to the public)\nby the platform. Depending on how different users trust the platform, they might care differently\nabout these two privacy losses. For instance, if a user fully trusts the platform, then her main privacy\nconcern would be the second one, while a user who does not trust the platform at all would be more\nconcerned with the first one as the public only observes the aggregated estimate, as opposed to the\nplatform which observes each user\u2019s (shared) data separately.\n\nWe use the differential privacy framework to quantify these privacy losses. Since differential privacy\nwas introduced by Dwork et al., several variants of it have been also proposed. In particular, a popular\none in the machine learning literature is R\u00e9nyi differential privacy (RDP), introduced by Mironov\n[2017], which we also adopt in this paper. Let us first recall the definition of R\u00e9nyi divergence.\nDefinition 1. Let P and Q be two distributions over R with densities p and q. For any \u03b1 \u2208 (1, \u221e],\nthe R\u00e9nyi \u03b1-divergence between P and Q is denoted by D\u03b1(P ||Q) and is given by\n\nD\u03b1(P ||Q) :=\n\n1\n\u03b1 \u2212 1\n\nlog\n\n(cid:90) (cid:18) p(x)\nq(x)\n\n(cid:19)\u03b1\n\nq(x)dx.\n\nFor two random variables X and Y , D\u03b1(X||Y ) denotes the \u03b1-divergence between their distributions.\n\nWe next define two notions of differential privacy, known as central and local, to capture the two\naforementioned types of privacy losses. Local differential privacy corresponds to the privacy loss of a\nuser when she shares her data with the platform through a randomized mapping, known as a channel.\nDefinition 2. Let \u03b5 \u2265 0 and \u03b1 \u2208 (1, \u221e]. A randomized channel C : X \u2192 R is locally (\u03b5, \u03b1)-R\u00e9nyi\n(differentially) private if for any x, x\u2032 \u2208 X ,\n\nD\u03b1(C(x)||C(x\u2032)) \u2264 \u03b5.\n\nCentral differential privacy corresponds to the other privacy loss mentioned above. It bounds the\nchange in the distribution of the platform\u2019s output, i.e., the released estimate, by changing one user\u2019s\ndata. We next provide the formal definition.\ni=1 \u2208 Rn\nDefinition 3. Let \u03b5 = (\u03b5i)n\n(\u03b5, \u03b1)-R\u00e9nyi (differentially) private if for any two datasets x1:n, x\u2032\ncoordinate (data of user i),\n\n+ and \u03b1 \u2208 (1, \u221e]. A randomized algorithm A : X n \u2192 R is\n1:n \u2208 X n that only differ in i-th\n\nD\u03b1(A(x1:n)||A(x\u2032\n\n1:n)) \u2264 \u03b5i.\n\nThe customary approach to guarantee RDP is Gaussian mechanism in which a properly tuned zero-\nmean Gaussian noise is added to fulfill the required condition. The following lemma, adapted from\nMironov [2017], allows us to characterize the Gaussian noise\u2019s variance for a privacy loss level.\nLemma 1. For a function f : X n \u2192 R, we define its sensitivity with respect to i-th coordinate as\n\nLi(f ) := sup {|f (x1:n) \u2212 f (x\u2032\n\n1:n)| : for all x1:n and x\u2032\n\n1:n differing only at i-th coordinate} .\n\nFor any \u03b1 \u2208 (1, \u221e], A(x1:n) = f (x1:n) + W with W \u223c N (0, \u03c32) is\n\n(cid:16)\n\n( \u03b1Li(f )2\n2\u03c32\n\n)n\ni=1, \u03b1\n\n(cid:17)\n\n-RDP.\n\nFor a given vector of local privacy losses (\u03b5(l)\nis by using a linear estimator with Gaussian mechanism which is given by\n\n1 , . . . , \u03b5(l)\n\nn ), a natural way to privately estimate the mean\n\n\u02c6\u03b8(x1, . . . , xn) :=\n\nn\n(cid:88)\n\ni=1\n\nwi \u02c6xi where\n\nn\n(cid:88)\n\ni=1\n\nwi = 1 and \u02c6xi = xi + N (0,\n\n\u03b1\n2\u03b5(l)\ni\n\n) for all i \u2208 N .\n\n(1)\n\n4\n\n\fFigure 1: The interaction between the users and the platform in the two-part private data acquisition.\n\ni / (cid:80)n\n\ni = w2\n\nand the central privacy delivered to user i \u2208 N is \u03b5(c)\n\nFigure 1 depicts this estimator. In particular, using Lemma 1, the local privacy delivered to user\ni \u2208 N is \u03b5(l)\nj . In the next\ni\nsection, we focus on linear estimators in designing the optimal private data acquisition mechanism.\nWe motivate such specification by showing that, for a given vector of local privacy losses (\u03b5(l)\ni=1,\na linear estimator is optimal with respect to mean square error. To formalize this statement, we\nfirst need to define the minimax estimation error as the notion of optimality. Let P be a class of\ndistributions over X . For any P \u2208 P, we denote its mean by \u03b8(P ). A (\u03b5(l)\ni=1-locally RDP estimator\ncan be cast as \u02c6\u03b8((Ci(xi))n\ni=1), where Ci(.) is the randomized channel corresponding to user i. Let\nQ((\u03b5(l)\ni=1-locally RDP estimators. The minimax estimation error is\ni=1,\u02c6\u03b8[|\u02c6\u03b8((Ci(Xi))n\n\ni=1) be the class of such (\u03b5(l)\n\ni=1) \u2212 \u03b8(P )|2],\n\nL(P, Q, (\u03b5(l)\n\nj=1 w2\n\ni=1) :=\n\nj /\u03b5(l)\n\n(Xi\u223cP )n\n\ni )n\n\ni )n\n\ni )n\n\ni )n\n\ni )n\n\n(2)\n\nE\n\ninf\ni=1\u2208Q((\u03b5(l)\n\ni )n\n\ni=1)\n\nsup\nP \u2208P\n\n\u02c6\u03b8,{Ci}n\n\nwhere the expectation is taken over both randomness of data and estimator (including private channels).\nIn other words, the optimal estimator is the one that has the lowest worst case error among all\nestimators that satisfy the privacy requirements. We next state the optimality result.\nTheorem 1. Assume \u03b1 \u2265 2 and \u03b5(l)\nand C1, \u00b7 \u00b7 \u00b7 , Cn be independent channels. Then, there exists a universal constant c such that\n\ni \u2264 1 for all i. Let P1 be the family of distributions over [\u2212 1\n\n2 , 1\n2 ]\n\nL(P, Q, (\u03b5(l)\n\ni )n\n\ni=1) \u2265 c min(\n\n1\ni=1 \u03b5(l)\n\ni\n\n(cid:80)n\n\n, 1).\n\nFurthermore, there exists a linear estimator with Gaussian mechanism such that\n\nE\n\n(Xi\u223cP )n\n\ni=1,\u02c6\u03b8[|\u02c6\u03b8((Ci(Xi))n\n\ni=1) \u2212 \u03b8(P )|2] \u2264 O(1)\n\n\u03b1\ni=1 \u03b5(l)\n\ni\n\n(cid:80)n\n\n.\n\nWe prove the lower bound by using the Le Cam\u2019s method Yu [1997] that reduces the problem of\nfinding lower bounds to a hypothesis testing problem and requires bounding the total variation\ndistance between the estimates when the input data points change. We then find it more convenient to\nbound the total variation in terms of Hellinger distance and use a series of inequalities to bound it.\n\n2.2 Data acquisition mechanism with two-part privacy guarantees\n\nWe next describe the utility functions of the users and the platform and then formulate the platform\u2019s\noptimal data acquisition mechanism. As we described earlier, each user suffers from two privacy\nlosses when sharing her data. The first one is a central privacy loss because of the leakage of her\ninformation through the platform\u2019s output. The second one is a local privacy loss because of the\nleakage of her information through the raw data that she shares with the platform. Each user has a\nheterogeneous privacy sensitivity for these two types of privacy losses. To model such heterogeneity,\nfor each i \u2208 N , we let ci \u2208 [0, 1] be her relative local privacy sensitivity, representing the relative\nweight that user i assigns to the (per unit cost of) local privacy loss. We also let 1 \u2212 ci be her relative\ncentral privacy sensitivity, representing the relative weight that user i assigns to the (per unit cost of)\ncentral privacy loss. Therefore, ci \u2248 1 implies that user i suffers a higher loss of privacy by sharing\nher raw data with the platform (local privacy loss) compared to her loss from the platform\u2019s output\n\n1This assumption does not have any fundamental impact on the results and is made to simplify the notations.\n\n5\n\n\f(central privacy loss). Differently, ci \u2248 0 implies that the user suffers a smaller loss of privacy by\nsharing her raw data with the platform compared to her loss from the platform\u2019s output. In what\nfollows, we use the term privacy sensitivity instead of relative local privacy sensitivity.\n\nFor each i \u2208 N , the privacy sensitivity ci is independently drawn from a publicly known distribution\nwhose support is [0, 1] with cumulative distribution and probability density functions Fi(\u00b7) and fi(\u00b7).\nWe also let c = (c1, . . . , cn) be the vector of privacy sensitivities. The privacy sensitivity of each\nuser is her private information, i.e., the platform does not know it. This is because individuals have\ndifferent views regarding how trustworthy the platform is in protecting their raw data.\n\nThe platform\u2019s objective is to design a mechanism to collect users\u2019 data by paying them to compensate\nfor their privacy losses without knowing the privacy sensitivity of users. To introduce the platform\u2019s\nobjective formally, we adopt the formalism of Bayesian mechanism design pioneered by Myerson\n[1981]. More specifically, the platform designs and announces a payment function, a local privacy loss\nfunction, and a central privacy loss function that are mappings from the reported privacy sensitivities\nof users. The users then report their privacy sensitivities (which may or may not be truthful). Based\non the payment function, the platform compensates the users (the compensation could be monetary\nor some free or discounted service provided to the user). Based on the local and central privacy\nfunctions, the platform designs randomized channels and randomized estimation algorithms that\ndeliver the guaranteed local and central privacy losses while minimizing the sum of the mean squared\nerror and the total expected payments. Given this interaction, we next formally introduce a data\nacquisition mechanism with two-part data privacy guarantees.\n\nDefinition 4 (two-part private data acquisition mechanism). We call the tuple (\u02c6\u03b8, \u03b5(l), \u03b5(c), t) a\ntwo-part private data acquisition mechanism where\n\n1. For all i \u2208 N , \u03b5(l)\ni\n\n: Rn\n\n+ \u2192 R+ is a function that maps the vector of privacy sensitivities c\n\nto a local privacy loss for user i, \u03b5(l)\n\ni (c), with \u03b5(l)(\u00b7) = (\u03b5(l)\n\ni (\u00b7))n\n\ni=1.\n\n2. For all i \u2208 N , \u03b5(c)\n\ni\n\n: Rn\n\n+ \u2192 R+ is a function that maps the vector of privacy sensitivities c\n\nto a central privacy loss for user i, \u03b5(c)\n\ni (c), with \u03b5(c)(.) = (\u03b5(c)\n\ni (\u00b7))n\n\ni=1.\n\n3. \u02c6\u03b8 : X n \u00d7 Rn\n\n+ \u00d7 Rn\n\nacquired locally (\u03b5(l)\nestimate \u02c6\u03b8(x, \u03b5(l)(c), \u03b5(c)(c)).\n\n+ \u2192 R is a (\u03b5(c)(c), \u03b1)-R\u00e9nyi differentially private estimator that maps\ni (c), \u03b1)-R\u00e9nyi differentially private data of user i for i \u2208 N to an\n\n4. For all i \u2208 N , ti : Rn\n\n+ \u2192 R+ is a function that maps the vector of privacy sensitivities c to\n\na payment for user i, ti(c), with t(.) = (ti(\u00b7))n\n\ni=1.\n\nNotice that we have not specified the estimator and the mechanisms that delivers (local and central)\nR\u00e9nyi differential privacy. In the rest of this subsection, we introduce the utilities and the platform\u2019s\nproblem for a general estimators and mechanisms to deliver differential privacy. Later, we focus on\nlinear estimator and Gaussian mechanisms and explicitly solve the platform\u2019s problem.\n\nEach user that participates in a two-part private data acquisition mechanism suffers from both the\nlocal and central privacy losses and need to be compensated by the platform. In particular, the utility\nof user i from participation when her privacy sensitivity is ci and she reports c\u2032\n\ni is given by\n\nui(\u03b5(l)(c\u2032\n\ni, c\u2212i), \u03b5(c)(c\u2032\n\ni, c\u2212i), t, \u02c6\u03b8) = Ec\u2212i[ti(c\u2212i, c\u2032\n\ni)) \u2212 ci\u03b5(l)\n\ni (c\u2212i, c\u2032\n\ni) \u2212 (1 \u2212 ci)\u03b5(c)\n\ni (c\u2212i, c\u2032\n\ni)],\n\ni)) is the payment from the platform, the term ci\u03b5(l)\n\nwhere the term ti(c\u2212i, c\u2032\ni) is the relative lo-\ncal privacy sensitivity of the user multiplied by her local privacy loss, and the term (1\u2212ci)\u03b5(c)\ni (c\u2212i, c\u2032\ni)\nis her relative central privacy sensitivity multiplied by her central privacy loss. A user i \u2208 N that\ndoes not participate in the mechanism neither compromises her privacy nor gets a compensation.\nTherefore, the utility of a user who does not participate in the mechanism becomes 0.\n\ni (c\u2212i, c\u2032\n\nThe goal of the platform is to minimize the sum of the mean squared error and the overall payment to\nusers. We let \u03b3 \u2208 R+ represents the relative weight of the mean estimation error and the payments in\n\n6\n\n\fthe platform\u2019s objective.2 Therefore, the platform\u2019s objective is\nn\n(cid:88)\n\nEc[\u03b3MSE(\u03b5(l)(c), \u03b5(c)(c), \u02c6\u03b8) +\n\nti(c)],\n\ni=1\n\nwhere the first term is the mean square error of estimator \u02c6\u03b8 given reported vector of privacy sensitivity\nand resulting local and central privacy losses \u03b5(l) and \u03b5(c), i.e.,\n\nMSE(\u03b5(l)(c), \u03b5(c)(c), \u02c6\u03b8) = Ex[|\u02c6\u03b8(\u02c6x, \u03b5(l), \u03b5(c)) \u2212 \u03b8|2].\nAlso, each summand of the second term is the compensation that the platform gives to a user to\nincentivize her to participate and report her privacy sensitivity truthfully.\n\nIn Appendix we prove that, similar to the classical mechanism design setting, revelation principle\nholds. This means that there is no loss of generality in focusing on the class of direct incentive\ncompatible mechanisms, meaning the platform\u2019s optimization problem can be written as\n\nmin\n\u03b5(l)(\u00b7),\u03b5(c)(\u00b7),t(\u00b7)\n\nEc[\u03b3MSE(\u03b5(l)(c), \u03b5(c)(c), \u02c6\u03b8) +\n\nn\n(cid:88)\n\nti(c)]\n\ni=1\nui(\u03b5(l)(c), \u03b5(c)(c), t, \u02c6\u03b8) \u2265 ui(\u03b5(l)(c\u2032\nui(\u03b5(l)(c), \u03b5(c)(c), t, \u02c6\u03b8) \u2265 0\n\ni, c\u2212i), \u03b5(c)(c\u2032\n\ni, c\u2212i), t, \u02c6\u03b8)\n\n(5)\nwhere the constraints in (4) represent the incentive compatibility. These constraints guarantee that\nthat each user i has no incentive to misrepresent her privacy sensitivity when others report truthfully\n(reporting truthfully is an equilibrium of the game among the users). Also, the constraints in (5)\nrepresent individual rationality, which ensures that each user receives a non-negative utility from\nparticipating in the platform\u2019s mechanism and sharing her data.\n\nfor all i \u2208 N , ci,\n\n(3)\n\n(4)\n\n3 From the mechanism design problem to an optimization problem\n\nFor a given estimator \u02c6\u03b8, the platform\u2019s decision comprises the local and central privacy loss functions\n\u03b5(l)(\u00b7) and \u03b5(c)(\u00b7) together with the payment functions t(\u00b7). We next show that this problem can be\nequivalently formulated as an optimization problem over the vector of local privacy losses and central\nprivacy losses (as opposed to functions). In the rest of the paper, we impose the following assumption\nwhich is well-known in the mechanism design literature and simplifies the analysis.3\nAssumption 1. For any user i \u2208 N , the virtual cost defined as \u03c8i(c) = c + Fi(c)\nfi(c) is increasing in c,\nwhere fi(\u00b7) and Fi(\u00b7) are probability density and cumulative distribution functions of ci, respectively.\n\nThe above assumption holds for a wide class of distributions such as the ones with log-concave\ndensity functions (e.g., uniform).\nTheorem 2. Suppose Assumption 1 holds. For a given estimator \u02c6\u03b8 : \u02c6X n \u00d7 Rn\n+ \u2192 R, in the\noptimal two-part data acquisition mechanism, for a given vector of reported privacy sensitivities c,\nthe local and central privacy losses are the solution of\nn\n(cid:88)\n\n+ \u00d7 Rn\n\nn\n(cid:88)\n\n\u03b5(l)\ni \u03c8i(ci) +\n\n\u03b5(c)\ni (1 \u2212 \u03c8i(ci)).\n\n(6)\n\n\u03b3MSE(\u03b5(l), \u03b5(c), \u02c6\u03b8) +\n\nmin\ni=1,{\u03b5(c)}n\n\ni=1\n\n{\u03b5(l)}n\n\ni=1\n\ni=1\n\nProof Sketch of Theorem 2: We introduce the following interim functions\n\nti(ci) =Ec\u2212i [t(ci, c\u2212i)],\n\ni (ci) = Ec\u2212i[\u03b5(l)\n\u03b5(l)\n\ni (ci, c\u2212i)],\n\nand \u03b5(c)\n\ni (ci) = Ec\u2212i[\u03b5(c)\n\ni (ci, c\u2212i)].\n\nWe first establish a payment identity that determines the optimal payment in terms of the optimal\nlocal and central privacy losses. In particular, by evaluating the first order condition corresponding to\nthe incentive compatibility constraint (4), we establish that this constraint holds if and only if\n\nti(ci) = ti(0) + \u03b5(c)\n\ni (ci) \u2212 \u03b5(c)\n\ni (0) + ci(\u03b5(l)\n\ni (ci) \u2212 \u03b5(c)\n\ni (ci)) \u2212\n\n(cid:90) ci\n\n0\n\n(\u03b5(l)\n\ni (z) \u2212 \u03b5(c)\n\ni (z))dz,\n\n2Notice that changing the parameter \u03b3 enables us to study a wide range of platform\u2019s objectives with differing\n\nrelative weights between the estimation error and the total payments.\n\n3Without this assumption, extending the results requires ironing technique of Myerson [1981].\n\n7\n\n\fi (z) \u2212 \u03b5(c)\n\nand \u03b5(l)\ni (z) is weakly decreasing in z. We then plug in this payment identity back to the\nplatform\u2019s objective, use the individual rationality constraint, and rewrite the platform\u2019s expected\nutility in terms of the privacy loss functions and the virtual cost of users. This is still a functional\noptimization problem in terms of \u03b5(l)(\u00b7) and \u03b5(c)(\u00b7). However, we establish that, under Assumption 1,\nwe can solve this functional optimization point-wise (i.e., for any given c). \u25a0\n\nTheorem 2 highlights the tradeoff in the platform\u2019s problem: by decreasing the local privacy loss,\nthe second term of the objective decreases (this term corresponds to the payment to users) while\nthe first term (i.e., the mean squared error) increases. The role of the central privacy loss is more\nnuanced, and there are two cases. If the coefficient 1 \u2212 \u03c8i(ci) is non-negative, by decreasing the\ncentral privacy loss, the third term of the objective decreases while the first term increases. If the\ncoefficient 1 \u2212 \u03c8i(ci) is negative, increasing the central privacy loss decreases both the third term\nand the first term. However, we cannot increase the central privacy loss level without limits because\nthe central privacy loss level is always below the local privacy loss level. Therefore, the platform\u2019s\noptimal mechanism should find the \u201cright\u201d balance between these terms.\n\n4 Optimal mechanism with two-part privacy guarantees for linear estimators\n\nFor the rest of the paper, we focus on linear estimators with Gaussian mechanism described in Section\n2.1. The following is a direct corollary of Theorem 2.\nCorollary 1. Suppose Assumption 1 holds. For any reported vector of privacy sensitivities c, the\noptimal local privacy loss levels are \u03b5(l)\nn\n(cid:88)\n\ni and the optimal central privacy loss levels are\n\ni (c) = y\u2217\n\n2\n\nwhere (w\u2217\n\n1, . . . , w\u2217\n\nn) and (y\u2217\n\n1, . . . , y\u2217\n\nn) are the optimal solution of\n\n\u03b5(c)\ni = w\u2217\ni\n\n2/\n\nw\u2217\nj\ny\u2217\nj\n\nj=1\n\nmin\nw,y\n\nVAR\u03b3\n\nn\n(cid:88)\n\ni=1\n\nw2\n\ni +\n\n\u03b3\u03b1\n2\n\nn\n(cid:88)\n\ni=1\n\nw2\ni\nyi\n\n+\n\ns.t. wi, yi \u2265 0, for all i \u2208 N and\n\nn\n(cid:88)\n\ni=1\n\nn\n(cid:88)\n\ni=1\n\n(1 \u2212 \u03c8i(ci))\n\nw2\ni\n\n(cid:80)n\n\nj=1\n\nw2\nj\nyj\n\n+\n\nn\n(cid:88)\n\ni=1\n\n\u03c8i(ci)yi\n\n(7)\n\nwi = 1.\n\nLet us highlight the difference between our characterization and that of classic mechanism design (e.g.,\nMyerson [1981]). In classic mechanism design, the designer\u2019s problem becomes linear optimization.\nHowever, in our setting, the designer\u2019s problem is a non-linear and non-convex optimization. This\nmakes the problem of finding the optimal two-part data acquisition mechanism challenging. Before\naddressing this computational challenge, let us revisit the form of the Gaussian mechanism that we\nhave adopted: the platform adds Gaussian noise locally and then outputs a convex combination of\nthe privatized users\u2019 data without adding any noise centrally. More specifically, one may guess that\nthe platform may benefit by having a central noise added to the final output in addition to the local\nnoises. In the following subsection, we establish that there is another Gaussian mechanism for any\nGaussian mechanism that only adds local noises and achieves a weakly lower cost.\n\n4.1 Optimality of having only local noises in the Gaussian mechanism\n\nThe platform has the opportunity of adding Gaussian noise to both the raw data of each user and the\nfinal estimator and ex-ante one may guess that it is optimal to use both of these instruments. However,\nas we establish next, interestingly, in the optimal two-part data acquisition mechanism, it is always\noptimal to only add noises locally.\nFor a given vector of local privacy losses (\u03b5(l)\nGaussian mechanism with both local and central noises is of the form\n\nn ) and central privacy losses (\u03b5(c)\n\n1 , . . . , \u03b5(c)\n\n1 , . . . , \u03b5(l)\n\nn ), a\n\n\u02c6\u03b8(x1, . . . , xn) :=\n\nn\n(cid:88)\n\ni=1\n\nwi \u02c6xi + N (0,\n\n\u03b1\n2\u03b5\n\n) where\n\nn\n(cid:88)\n\ni=1\n\nwi = 1 and \u02c6xi = xi + N (0,\n\n\u03b1\n2\u03b5(l)\ni\n\n)\u2200i \u2208 N .\n\nProposition 1. In the optimal two-part data acquisition mechanism that adopts a Gaussian mecha-\nnism with both local and central noises, we have \u03b5 = \u221e.\n\n8\n\n\fAlgorithm 1: Computing the optimal two-part private data acquisition mechanism\nInput: The vector of privacy sensitivities (c1, . . . , cn)\nfor S \u2208 Grid (cid:0)[S, \u00afS], \u03b4(cid:1) do\n\nLet\n\n\u03bdi =\n\n1\n\u03b3 VAR + (1 \u2212 \u03c8i(ci))/S\n\n, \u03b6i =\n\n\u03bdi\nj \u03bdj\n\n(cid:80)\n\n, \u03bei = \u03b6i\n\n(cid:113)\n\n\u03bdj(\n\n\u03c8j(cj) \u2212 (cid:112)\u03c8i(ci))\n\n\uf8f6\n\n\uf8f8 .\n\n\uf8eb\n\n\uf8ed\n\nn\n(cid:88)\n\nj=1\n\n(cid:18) (cid:80)n\n\ni=1 \u03b6i\n\n\u03c8i(ci)\n\nS\u2212(cid:80)n\n\ni=1\n\n\u03c8i(ci)\u03bei\n\n\u221a\n\u221a\n\n(cid:19)2\n\n.\n\nLet p =\n\nLet\n\n\u03bdi + \u03bdi\n\nwi(S) =\n\n(cid:112)\u03c8j(cj)p\n\n(cid:80)\n\nj \u03bdj\n(cid:80)\nj \u03bdj\n\n\u2212 \u03bdi\n\n(cid:112)\u03c8i(ci)p,\n\nyi(S) = wi(S)\n\n(cid:114) p\n\n,\n\n\u03c8i(ci)\n\nand OBJ(S) be the objective of Problem (7) evaluated for this solution.\n\nend\nOutput: {yi(S\u2217), wi(S\u2217)}n\n\ni=1, where (S\u2217) = arg min(S) OBJ(S).\n\nProposition 1 has an important implication in terms of the design of data market architecture when\nusers have both central and local privacy costs: it is optimal to add noise locally! Adding a noise\ncentrally to the final estimator has an advantage because the weights in the final estimator give the\nplatform a lever to deliver heterogeneous central privacy guarantees to users. Despite this advantage,\nwe establish that adding noise centrally is never optimal. This is because the platform prefers to add\nthe noise locally to contribute to both central and local privacy guarantees delivered to users.\n\n4.2 Computing the optimal privacy loss function\n\nThe implementation of the optimal two-part private data acquisition mechanism requires solving\nProblem (7), which is a non-convex program. However, we use the structure of the problem to develop\na polynomial time algorithm to solve it approximately. To do so, we first replace (cid:80)n\ni /yi by an\nauxiliary variable S. Next, we consider the corresponding lagrangian problem. Using Karush-Kuhn-\nTucker (KKT) conditions, we establish a number of relations between problems\u2019 parameters, S, and\np, the lagrangian coefficient corresponding to S = (cid:80)n\ni /yi. Furthermore, we develop upper and\nlower bounds for S. Finally, we do a grid search to find the approximate optimal solution.\nTheorem 3. For any vector of reported privacy sensitivities and \u03f5 > 0, Algorithm 1 finds local privacy\nloss levels and the differentially private linear estimator of the two-part data acquisition mechanism\nwhose cost (i.e., platform\u2019s objective) is at most 1 + \u03b4 of the optimal cost in time poly(n, 1\n\ni=1 w2\n\ni=1 w2\n\n\u03b4 ).\n\nNotice that the approximation factor in Theorem 3 depends on the underlying parameters. Therefore,\nwe have a Polynomial Time Approximation Scheme (PTAS) for finding the optimal two-part data\nacquisition mechanism in the class of linear estimators.\n\nIn the Appendix, we provide a case study with two users to illustrate the performance of the optimal\ntwo-part data acquisition mechanism in terms of the guaranteed privacy levels and payments as\nfunctions of the reported privacy sensitivities.\n\n\n\nBased on the section(s), please answer the following question with yes, no, or n/a and provide a brief justification for your answer.\nQuestion: If the authors used crowdsourcing or conducted research with human subjects, do the authors include the estimated hourly wage paid to participants and the total amount spent on participant compensation?"}}}