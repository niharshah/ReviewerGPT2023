{"paper_index": 17, "title": "Learn to Explain: Multimodal Reasoning via Thought Chains for Science Question Answering", "abstract": "\n\nWhen answering a question, humans utilize the information available across differ-\nent modalities to synthesize a consistent and complete chain of thought (CoT). This\nprocess is normally a black box in the case of deep learning models like large-scale\nlanguage models. Recently, science question benchmarks have been used to diag-\nnose the multi-hop reasoning ability and interpretability of an AI system. However,\nexisting datasets fail to provide annotations for the answers, or are restricted to\nthe textual-only modality, small scales, and limited domain diversity. To this end,\nwe present Science Question Answering (SCIENCEQA), a new benchmark that\n21k multimodal multiple choice questions with diverse science topics\nconsists of\nand annotations of their answers with corresponding lectures and explanations. We\nfurther design language models to learn to generate lectures and explanations as the\nchain of thought (CoT) to mimic the multi-hop reasoning process when answering\nSCIENCEQA questions. SCIENCEQA demonstrates the utility of CoT in language\nmodels, as CoT improves the question answering performance by 1.20% in few-\nshot GPT-3 and 3.99% in \ufb01ne-tuned Uni\ufb01edQA. We also explore the upper bound\nfor models to leverage explanations by feeding those in the input; we observe that\nit improves the few-shot performance of GPT-3 by 18.96%. Our analysis further\nshows that language models, similar to humans, bene\ufb01t from explanations to learn\nfrom fewer data and achieve the same performance with just 40% of the data.1\n\n\u21e0\n\n1\n\n", "introduction": "\n\nA long-standing goal of AI systems is to act reliably and learn complex tasks ef\ufb01ciently like human\nbeings. In the process of reliable decision making, humans follow an explicit chain-of-thought (CoT)\nreasoning process that is typically expressed as an explanation. However, machine learning models\nare trained mostly using a large number of input-output examples to perform a speci\ufb01c task. These\nblack-box models only generate the \ufb01nal decision without reliably revealing the underlying reasoning\nprocess. Not surprisingly, it is unclear if they understand the task and can generalize even though\nthey perform well on the benchmark. On the other hand, humans are able to learn from instructions\nor explanations from past experience and generalize them to novel and unseen problems. This helps\nthem learn more quickly with fewer data. In this work, we explore if machines can be endowed with\nsuch reasoning abilities in the context of science-based question answering.\n\nRecently, science problem solving benchmarks [18] have been used to diagnose the multi-hop\nreasoning ability and interpretability of AI systems. To answer science questions, a model needs to\n\n1The data and code are available at https://scienceqa.github.io.\nWork was partially done while Pan Lu and Swaroop Mishra were interns at AI2.\n\n36th Conference on Neural Information Processing Systems (NeurIPS 2022).\n\n\f\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 1: We construct the SCIENCEQA dataset where a data example consists of multimodal\nquestion answering information and the grounded lecture and explanation. We study if QA models\ncan generate a reasonable explanation to reveal the chain-of-thought reasoning.\n\nnot only understand multimodal contents but also extract external knowledge to arrive at the correct\nanswer. Since these tasks require domain-speci\ufb01c knowledge and explicit multi-hop reasoning, a\nmodel would be not interpretable if it fails to provide explanations to reveal the reasoning process.\nHowever, current science question datasets [18, 17, 52] mostly lack annotated explanations for the\nanswers. To address this issue, other science datasets annotate the explanations, but they are restricted\nto the textual only modality and limited to small data scales [13, 6, 37] or a small set of topics [20, 14].\nTherefore, we collect Science Question Answering (SCIENCEQA), a large-scale multi-choice dataset\nthat contains multimodal science questions with explanations and features rich domain diversity.\n\nSCIENCEQA is collected from elementary and high school science curricula, and contains 21,208\nexamples along with lectures and explanations. Different from existing datasets [17, 18, 52], SCI-\nENCEQA has richer domain diversity from three different subjects: natural science, social science,\nand language science. A typical example consists of a question, multiple choices, multimodal con-\ntexts, a correct answer, as well as a lecture and an explanation. The lecture and explanation provide\ngeneral external knowledge and speci\ufb01c reasons, respectively, for arriving at the correct answer.\n\nConsider the thoughts one person might have when answering the question in Figure 1. One \ufb01rst\nrecalls the knowledge regarding the de\ufb01nition of a force learned from textbooks: \u201cA force is a push or\na pull that ... The direction of a push is ... The direction of a pull is ...\u201d, then forms a line of reasoning:\n\u201cThe baby\u2019s hand applies a force to the cabinet door.\nThe\ndirection of this force is toward the baby\u2019s hand.\u201d, and \ufb01nally arrives at the correct answer: \u201cThis\nforce is a pull.\u201d. Following [41], we formulate the task to output a natural explanation alongside the\npredicted answer. In this paper, we train language models to generate lectures and explanations as the\nchain of thought (CoT) to mimic the multi-hop reasoning process to answer SCIENCEQA questions.\n\nThis force causes the door to open.\n\n!\n\n!\n\nOur experiments show that current multimodal methods [55, 1, 21, 9, 26, 35] fail to achieve satisfac-\ntory performance on SCIENCEQA and do not generate correct explanations. Instead, we \ufb01nd that CoT\ncan help large language models not only in the few-shot learning setting but also in the \ufb01ne-tuning\nsetting. When combined with CoT to generate the lecture and explanation, the \ufb01ne-tuned Uni\ufb01edQA\n[19] achieves an improvement of 3.99% as opposed to not using CoT in the \ufb01ne-tuning stage. The\nfew-shot GPT-3 model [4] via chain-of-thought prompting can obtain 75.17% on SCIENCEQA with\nan improvement of 1.20% compared to the few-shot GPT-3 without CoT. Prompted with CoT, GPT-3\ncan generate reasonable explanations as evaluated by automated metrics, and promisingly, 65.2%\nof explanations meet the gold standard of human evaluations. We also investigate the upper bound\nfor models to harness explanations by including them in the input. We \ufb01nd that doing so improves\nGPT-3\u2019s few-shot performance by 18.96%, suggesting that explanations do aid models and are\ncurrently underutilized in the CoT framework. Further analysis shows that, like humans, language\nmodels bene\ufb01t from explanations to learn with less data: Uni\ufb01edQA with CoT obtains the same\nresults as Uni\ufb01edQA without CoT with only 40% of the training data.\n\nTo sum up, our contributions are three-fold: (a) To bridge the gap in existing datasets in the scienti\ufb01c\ndomain, we build Science Question Answering (SCIENCEQA), a new dataset containing 21,208\nmultimodal science questions with rich domain diversity. To the best of our knowledge, SCIENCEQA\nis the \ufb01rst large-scale multimodal dataset that annotates lectures and explanations for the answers.\n\n2\n\n\f(b) We show that CoT bene\ufb01ts large language models in both few-shot and \ufb01ne-tuning learning by\nimproving model performance and reliability via generating explanations. (c) We further explore the\nupper bound of GPT-3 and show that CoT helps language models learn from fewer data.\n\n", "methods": "\n\nIn this section, we establish baselines and develop two chain-of-thought models on SCIENCEQA.\n\n4.1 Baselines\n\nHeuristic baselines. The \ufb01rst heuristic baseline is random chance: we randomly select one from the\nmultiple options. Each trial is completed on the whole test set, and we take three different trials for\nan average result. The second heuristic baseline is human performance. We post the task to Amazon\nMechanical Turk and ask workers to answer SCIENCEQA questions. Only workers who obtain a\nhigh school or higher degree and pass the quali\ufb01cation examples are quali\ufb01ed for the study. Each\nworker needs to answer a set of 10 test questions, and each question is answered by three different\nworkers. For more details of the human performance study, see Appendix B.2.\n\nZero-shot and few-shot baselines. We establish the zero-shot baselines on top of Uni\ufb01edQA\n[19] and GPT-3 [4]. The zero-shot setup follows the format of QCM\nA where the input is the\nconcatenation of tokens of the question text (Q), the context text (C), and multiple options (M), while\nthe output is to predict the answer (A) from the option set. We extract the caption from the captioning\nmodel based on ViT [7] and GPT-2 [47] for the image as the visual context. In the few-shot setting, we\nfollow the standard prompting [4] where in-context examples from the training set are concatenated\nbefore the test instance. These in-context examples serve as an instruction for the language model to\nadjust to the speci\ufb01c task in SCIENCEQA.\n\n!\n\nFine-tuning baselines. We \ufb01rst consider the \ufb01ne-tuning baselines from VQA models [1, 21, 55, 9,\n22, 35, 26] proposed in recent years. These VQA baselines take the question, the context, and choices\nas the textual input, take the image as the visual input, and predict the score distribution over choice\ncandidates via a linear classi\ufb01er. In addition, we build the \ufb01ne-tuning baseline on top of the large\nlanguage model Uni\ufb01edQA [19]. Uni\ufb01edQA takes the textual information as the input and outputs\nthe answer option. Similarly, the image is converted into a caption that provides the visual semantics\nfor the language model.\n\n4.2 Language Models with the Chain of Thought\n\nA chain of thought refers to a coherent \ufb02ow of sentences that reveals the premises and conclusion of\na reasoning problem [54]. A chain of thought clearly decomposes a multi-hop reasoning task into\nintermediate steps instead of solving the task in a black-box way. The chain of thought can be the\nstep-by-step thought process [54] before arriving at the \ufb01nal answer or explanations [41] that come\nafter the answer. The annotated lectures and explanations in SCIENCEQA serve as demonstrations of\nthe chain of thought that mimics the multi-step reasoning steps of human beings. In this paper, we\nstudy if large language models can generate reasonable explanations as the chain of thought to reveal\nthe thought process when answering SCIENCEQA questions. Further, we explore how the chain of\nthought can improve the reasoning ability of language models on SCIENCEQA in both few-shot and\n\ufb01ne-tuning learning.\n\nUni\ufb01edQA with the chain of thought. Uni\ufb01edQA [19] is a state of the art model for multi-option\nquestion answering. The original architecture of Uni\ufb01edQA takes the question and options as the\ninput and outputs a short phrase as the \ufb01nal answer. We make a format modi\ufb01cation to develop\nUni\ufb01edQA with the chain of thought (CoT), i.e., Uni\ufb01edQA is \ufb01ne-tuned to generate a long sequence\nof text which consists of the answer followed by the lecture and explanation.\n\nGPT-3 via chain-of-thought prompting. Recent research work [4, 38, 34] has shown that GPT-3\n[4] can perform various tasks when provided with in-context examples in a standard prompt. Take\nmulti-option question answering as an example, the standard prompt [36, 57, 29] builds instructions\nusing in-context examples with components of the question text, options, and the correct answer\ntext. This style of few-shot learning enables the GPT-3 model to answer speci\ufb01c questions without\nparameter updates. Different from standard prompting, we build GPT-3 via chain-of-thought (CoT)\nprompting, as shown in Figure 5. To be speci\ufb01c, for each test problem t, we map the prompt\ninstruction I :\nIi}n refers to the instruction set of n-shot\n{\nin-context examples from the training set, while It denotes the test instruction. Instead of the way\nwhere the explanation comes before the answer [54], we feed the instruction I into the encoder-\n\nIi}n, It into a textual format where\n{\n\n6\n\n\fQuestion: question : I ques\nOptions: (A) option : I opt\nContext: context : I cont\nAnswer: The answer is answer : I a\n\ni\n\ni\n\ni1 (B) option : I opt\n\ni2 (C) option : I opt\n\ni3\n\ni . BECAUSE: lecture : I lect\n\ni\n\nexplanation : I exp\n\ni\n\nQuestion: question : I ques\nOptions: (A) option : I opt\nContext: context : I cont\nAnswer:\n\nt\n\nt\n\nt1 (B) option : I opt\n\nt2 (C) option : I opt\n\nt3 (D) option : I opt\n\nt4\n\nFigure 5: Prompt instruction encoding for the test example t in GPT-3 (CoT). The prompt above\nconsists of the instruction\n\nIi}1 for the 1-shot training example and It for the test example.\n\n{\n\ndecoder model GPT-3 to generate the answer a followed by the lecture lect and explanation exp:\nM :\n\na, lect, exp.\n\nIi}n, It !\n\n{\n\n", "experiments": "\n\n5.1 Experimental Setup\n\nEvaluation metrics. The heuristics and VQA baselines treat our SCIENCEQA task as a multi-class\nclassi\ufb01cation problem with multiple options and are evaluated with the accuracy metrics. Uni\ufb01edQA\nand GPT-3 treat SCIENCEQA as a text generation problem. So the most similar option is selected\nas the \ufb01nal prediction to evaluate the question answering accuracy. The generated lectures and\nexplanations are evaluated by automatic metrics [44, 28, 49] and human scores by annotators.\n\nImplementation details. The VQA baselines are trained for a maximum number of 50 epochs with a\nlearning rate of 5e\n5. We \ufb01ne-tune the Uni\ufb01edQA for 50k iterations and evaluate every 1k iteration.\nThe training process is stopped following the early stopping strategy with a patience period of three\nevaluations. For GPT-3, we use the text-davinci-002 engine, which is the most capable model\nversion suggested in the of\ufb01cial documentation. More details can be found in Appendix B.1.\n\n\u0000\n\n5.2 Results for Question Answering\n\nTable 3 demonstrates the empirical results for Science Question Answering.\n\nVQA baselines. We feed the VQA baseline models with the input of QCM format to predict answers\nA. Out of all the VQA models we benchmarked, VisualBERT [26, 27] performs the best on average\n(61.87%). Interestingly, Patch-TRM [35] beats VisualBERT in natural science (NAT) and language\nscience (LAN), and it also performs better in higher-grade questions (67.50% v.s. 59.92%). However,\nin the subject of social science (SOC), VisualBERT outperforms Patch-TRM by a large margin\n(+22.39%). Such drastic changes in performance might imply that current VQA models are not\ngeneralized to process the challenging questions in SCIENCEQA.\n\nLanguage models. We evaluate whether large-scale pretraining on text can help language models\nlearn scienti\ufb01c knowledge and thus perform better on the SCIENCEQA task. For this purpose, we\nhave tried two of the state-of-the-art pre-trained language models: Uni\ufb01edQA and GPT-3.\n\n(i) Uni\ufb01edQA. The results show that without any supervised \ufb01ne-tuning (zero-shot), Uni\ufb01edQA\ncannot beat any VQA baseline model, while the pretraining does help the model obtain some\nscienti\ufb01c knowledge to outperform the random baseline. When \ufb01ne-tuned with the answer labels in\nSCIENCEQA, Uni\ufb01edQABASE reports an accuracy of 70.12% on average. By further teaching the\nmodel to generate the answer along with lecture and explanation, the developed language model with\nAE)\nchain-of-thought (Uni\ufb01edQABASE (CoT)) brings additional improvements of +3.21% (QCM\nand +3.99% (QCM\nALE). These results show that generating the chain of thought along with the\nanswer bene\ufb01ts the reasoning ability of language models.\n\n!\n\n!\n\n(ii) GPT-3. The positive effect of pretraining is also proved by the surprisingly good results from\nGPT-3 in the same zero-shot setting as Uni\ufb01edQA. Without any \ufb01ne-tuning, GPT-3 already reaches\nalmost the best performance we can get. Interestingly, prompting the GPT-3 with two training\nexamples with only answers results in a negligible difference. However, if we prompt GPT-3 with\nchain-of-thought prompting (QCM\n\nALE), we obtain the state-of-the-art result so far (75.17%).\n\n!\n\n7\n\n\fModel\n\nLearning\n\nFormat\n\nNAT SOC LAN TXT IMG NO G1-6 G7-12 Avg\n\nRandom chance\n\n-\n\nM\n\nA\n\n!\n\n40.28 46.13 29.25 47.45 40.08 33.66 39.35 40.67 39.83\n\nQ only [1]\nCI only [1]\nQ+M only [1]\nQ+CT +M only [1]\nQ+CI +M only [1]\n\ntrain set\nA\nQ\n!\nA\ntrain set\nCI !\nA\nQM\ntrain set\n!\ntrain set QCT M\ntrain set QCI M\n\n41.34 27.22 47.00 41.79 35.15 44.60 39.28 40.87 39.85\n41.34 29.25 45.45 42.33 36.09 42.93 39.21 41.07 39.87\n52.66 51.86 60.18 55.57 50.37 57.42 52.53 57.88 54.44\nA 57.28 49.04 61.36 60.46 52.80 58.82 54.44 60.51 56.61\nA 58.97 53.77 60.45 62.85 54.49 57.63 56.72 61.04 58.26\n\n!\n!\n\nMCAN [55]\nTop-Down [1]\nBAN [21]\nDFAF [9]\nViLT [22]\nPatch-TRM [35]\nVisualBERT [26, 27]\n\ntrain set\ntrain set\ntrain set\ntrain set\ntrain set\ntrain set\ntrain set\n\nQCM\nQCM\nQCM\nQCM\nQCM\nQCM\nQCM\n\n!\n!\n!\n!\n!\n!\n!\n\nA 56.08 46.23 58.09 59.43 51.17 55.40 51.65 59.72 54.54\nA 59.50 54.33 61.82 62.90 54.88 59.79 57.27 62.16 59.02\nA 60.88 46.57 66.64 62.61 52.60 65.51 56.83 63.94 59.37\nA 64.03 48.82 63.55 65.88 54.49 64.11 57.12 67.17 60.72\nA 60.48 63.89 60.27 63.20 61.38 57.00 60.72 61.90 61.14\nA 65.19 46.79 65.55 66.96 55.28 64.95 58.04 67.50 61.42\nA 59.33 69.18 61.18 62.71 62.17 58.54 62.96 59.92 61.87\n\nUni\ufb01edQASMALL [48]\nUni\ufb01edQABASE [48]\nUni\ufb01edQASMALL [48]\nUni\ufb01edQABASE [48]\nUni\ufb01edQABASE (CoT)\nUni\ufb01edQABASE (CoT)\n\nGPT-3 [4]\nGPT-3 [4]\nGPT-3 (CoT)\nGPT-3 (CoT)\n\nzero-shot QCM\nzero-shot QCM\nQCM\ntrain set\ntrain set\nQCM\ntrain set QCM\ntrain set QCM\n\n!\n!\n!\n!\n!\n!\nzero-shot QCM\nQCM\n2-shot\n2-shot\nQCM\n2-shot QCM\n\n!\n!\n!\n!\nQCM\n\n!\n\nA 47.78 40.49 46.00 50.24 44.12 44.39 45.56 46.21 45.79\nA 50.13 44.54 48.18 53.08 48.09 46.69 47.58 50.03 48.46\nA 53.77 58.04 61.09 52.10 51.51 61.46 58.22 53.59 56.57\nA 68.16 69.18 74.91 63.78 61.38 77.84 72.98 65.00 70.12\nAE 70.60 74.02 78.36 65.69 64.80 81.53 75.48 69.48 73.333.21\nALE 71.00 76.04 78.91 66.42 66.53 81.81 77.06 68.82 74.113.99\n\nA 75.04 66.59 78.00 74.24 65.74 79.58 76.36 69.87 74.04\nA 74.64 69.74 76.00 74.44 67.28 77.42 76.80 68.89 73.97\nAE 76.60 65.92 77.55 75.51 66.09 79.58 78.49 67.63 74.610.64\nALE 75.44 70.87 78.09 74.68 67.43 79.93 78.23 69.68 75.171.20\n\n\"\n\n\"\n\n\"\n\n\"\n\nHuman\n\n-\n\nA 90.23 84.97 87.48 89.60 87.50 88.10 91.59 82.42 88.40\n\nTable 3: Evaluation of baselines over different classes in accuracy (%). Model names: Q = question,\nM = multiple options, C = context, CT = text context, CI = image context, CoT = chain of thought.\nFormat names: A = answer, AE = answer with explanation, ALE = answer with lecture and expla-\nnation. Question classes: NAT = natural science, SOC = social science, LAN = language science,\nTXT = text context, IMG = image context, NO = no context, G1-6 = grades 1-6, G7-12 = grades 7-12.\nSegments 1: Random chance; Segment 2: Ablation studies on top of Top-Down; Segment 3: VQA\nbaselines; Segment 4: Uni\ufb01edQA baselines and Uni\ufb01edQA with CoT; Segment 5: GPT-3 baselines\nand GPT-3 with CoT; Segment 6: Average human performance.\n\nFigure 6: One example of the predicted answer along with the chain of thought from GPT-3 (CoT).\n\nHuman performance. Humans outperform all benchmarks consistently across question classes,\ncontext types, and grades, e.g., a 20.07% gap for questions with the image context (IMG) between\nhumans and our best performing model. The gap is to be \ufb01lled by future research on multimodal\nreasoning for scienti\ufb01c question answering.\n\n5.3 Results for Generated Explanations\n\nOne prediction example of GPT-3 (CoT) is visualized in Figure 6. We can see that GPT-3 (CoT)\npredicts the correct answer and generates a reasonable lecture and explanation to mimic the human\nthought process. We further report automatic metrics (BLEU-1/4 [44], ROUGE-L [44], and (sentence)\n\n8\n\nQuestion: What do these two changes have in common?\\nbleaching clothes\\na piece of apple turning brown\\nOptions: (A) Both are only physical changes. (B) Both are caused by cooling. (C) Both are chemical changes. (D) Both are caused by heating.\\nAnswer: The answer is C. BECAUSE: A physical change is a change to a substance that does not change what the substance is. A physical change only changes the substance\u2019s physical properties, such as its size, shape, or color.\\nA chemical change is a change to a substance that changes what the substance is. A chemical change changes the substance\u2019s chemical properties, such as its chemical structure.\\nThe changes that you listed are both chemical changes.\\nWhen you bleach clothes, the chemical structure of the fabric changes. This change makes the fabric less likely to hold color.\\n When a piece of fruit turns brown, the chemical structure of the fruit changes. This change makes the fruit taste different.\fSimilarity [49] to evaluate the generated lectures and explanations, as shown in Table 4. The Similarity\nmetric computes the cosine-similarity of semantic embeddings between two sentences based on\nthe Sentence-BERT network [49]. The results show that Uni\ufb01edQABASE (CoT) generates the most\nsimilar explanations to the given ones. However, it\u2019s commonly agreed that automatic evaluation\nof generated texts only provides a partial view and has to be complemented by a human study. By\nasking annotators to rate the relevance, correctness, and completeness of generated explanations, we\n\ufb01nd that the explanations generated by GPT-3 (CoT) conform best to human judgment.\n\nFormat\n\nModel\nUni\ufb01edQABASE (CoT) QCM\nQCM\nGPT-3 (CoT)\nQCM\nGPT-3 (CoT)\n\n!\n!\n!\n\nBLEU-1 BLEU-4 ROUGE-L Similarity Relevant Correct Complete Gold\n80.4% 76.6% 76.1% 56.9%\n76.9% 73.0% 70.5% 52.5%\n88.5% 78.8% 84.5% 65.2%\n\n0.811\n0.561\n0.595\n\n0.370\n0.048\n0.052\n\n0.714\n0.351\n0.323\n\nALE 0.397\nAE\n0.234\nALE 0.192\n\nTable 4: Automatic metrics (BLEU-1/4, ROUGE-L, Similarity) and human evaluation of generated\nexplanations. Note that a gold explanation refers to one that is relevant, correct, and complete.\n\n5.4 Analysis\n\nBlind studies. Blind studies are conducted on top of the modi\ufb01cation of the full model, Top-Down [1].\nThe results achieved in blind studies of Q only and CI only are close to random chance, showing that\nthe SCIENCEQA dataset is robust and reliable in distribution. The performance drops in Q+M only,\nQ+CT +M only, and Q+CI +M only indicate that all input components provide critical information for\nanswering SCIENCEQA questions.\n\nPrompt types. We study the effect of prompt types and visualize the comparison in Figure 7 (a). It\nshows that prompting the GPT-3 model with both lectures and explanations (QCM\nALE) results\nin the highest accuracy on average and the smallest variance. In contrast, prompting with only\nexplanations (QCM\n\nAE) gives the largest variance, resulting in a less stable model.\n\n!\n\n!\n\n(a) Acc. v.s. different prompts with 4-shot examples.\n\n(b) Acc. v.s. different # of training examples.\n\nFigure 7: Accuracy of GPT-3 (CoT) cross different prompt types (a) and # of training examples (b).\n\nNumber of in-context examples. In Figure 7 (b), we further investigate how different numbers of\ntraining examples encoded in prompts can affect the prediction accuracy. The QCM\nALE prompt\ntype outperforms or performs comparably to the QCM\nA type with all numbers of examples. And\nALE with 2 training examples being prompted. After\nwe observe the peak performance of QCM\nthat, the accuracy goes down as more training examples are added to the model.\n\n!\n\n!\n\n!\n\nDynamic sampling. In Table 5, instead of ran-\ndom sampling, we try to dynamically select the\nin-context examples to prompt with the same\nclass as the test sample. However, slight differ-\nences in prediction accuracy are observed when\ncomparing them to simple random sampling.\n\nPrompt type Sampling\nQCM\nQCM\nQCM\n\nALE Dynamic (same topic)\nALE Dynamic (same category)\nALE Dynamic (same skill)\n\n!\n!\n!\n\nAcc. (%)\n75.15\n74.58\n75.10\n\nTable 5: Dynamic sampling for GPT-3 (CoT).\n\nUpper bound. We search the upper bound of the GPT-3 accuracy by feeding the gold lecture and\nexplanation in the test prompt. As reported in Table 6, QCME*\nALE\nA outperforms the QCM\nbaseline by 18.86% and QCMLE*\nALE by 18.96%, indicating a potential\nA outperforms QCM\nimprovement direction by generating correct explanations before answering science questions.\n\n!\n\n!\n\n!\n\n!\n\n9\n\n\fPrompt type\nA\nQCML*\n!\nAE\nQCML*\n!\nQCME*\nA\n!\nQCMLE*\nQCM\n\nSampling Acc. (%)\nRandom\nRandom\nRandom\nA Random\nRandom\n\n73.59\n74.32\n94.0318.86\n94.1318.96\n75.17\n\n!\nALE\n\n!\n\nPrompt type\nQCM\nQCM\nQCM\nQCM\nQCM\n\nLA\nEA\nLEA\nELA\nALE\n\n!\n!\n!\n!\n!\n\nSampling Acc. (%)\nRandom\nRandom\nRandom\nRandom\nRandom\n\n60.6\n56.0\n55.4\n51.5\n73.6\n\n\"\n\n\"\n\nTable 6: Upper bound of GPT-3 (CoT).\n\nTable 7: Different positions of L/E for GPT-3 (CoT).\n\nPositions of lectures and explanations. We study the performance of GPT-3 (CoT) in terms of\ndifferent positions of lectures and explanations on 1,000 test examples. The results are shown in\nTable 7. There could be huge accuracy decreases if GPT-3 (CoT) predicts lectures and explanations\nbefore answers. It is mainly because if GPT-3 (CoT) is formulated to generate the long lecture and\nexplanation \ufb01rst, there is a greater chance that it will stop generating the prediction early or use up\nthe maximum token limits before obtaining the required answer.\n\nCoT learns with fewer data. To study if the chain of\nthought helps language models learn more ef\ufb01ciently, we\nreport the accuracies of Uni\ufb01edQA and Uni\ufb01edQA (CoT)\n\ufb01ne-tuned on different sizes of the training set in Figure 8.\nUni\ufb01edQA (CoT) bene\ufb01ts language models by learning the\ncoherent reasoning path when answering questions, resulting\nin similar accuracy with fewer training examples.\n\nError analysis. GPT-3 via chain-of-thought prompting ob-\ntains promising results but still fails to answer a wide range\nof challenging questions in SCIENCEQA. See examples of\nfailure cases in Appendix B.4. The failure cases can be clas-\nsi\ufb01ed into two types: (a) the model fails to understand the\nmultimodal inputs and lacks domain-speci\ufb01c knowledge to arrive at the correct answer; (b) the model\ngenerates the wrong chain of thought with irrelevant, incorrect, or incomplete information.\n\nFigure 8: Uni\ufb01edQA (CoT) learns ef-\n\ufb01ciently with fewer training examples.\n\n", "conclusion": "\n\nIn this paper, we propose SCIENCEQA, a dataset that features 21,208 multi-option questions with\nmultimodal contexts from the science curriculum. To the best of our knowledge, SCIENCEQA is the\n\ufb01rst large-scale multimodal science dataset where most questions are annotated with corresponding\nlectures and explanations. We establish various baselines, including recent VQA models and large\nlanguage models on SCIENCEQA. We further study if language models can generate reasonable\nexplanations and then bene\ufb01t the reasoning ability. Experiments show that Uni\ufb01edQA with the chain\nof thought can achieve an improvement of 3.99% and few-shot GPT-3 via chain-of-thought (CoT)\nprompting can obtain a satisfactory accuracy of 75.17% on SCIENCEQA. 65.2% of the generated\nexplanations from GPT-3 (CoT) meet the gold standard by human evaluations.\n\n", "appendix": "Supplementary Materials for\nLearn to Explain: Multimodal Reasoning via\nThought Chains for Science Question Answering\n\nA Dataset Analysis\n\nA.1 Data Collection\n\nQuestions in the SCIENCEQA dataset are sourced from open resources managed by IXL Learning,\nan online learning platform curated by experts in the \ufb01eld of K-12 education. The dataset includes\nproblems that align with California Common Core Content Standards. To construct SCIENCEQA, we\ndownloaded the original science problems and then extracted individual components (e.g. questions,\nhints, images, options, answers, lectures, and solutions) from them based on heuristic rules.\n\nWe manually removed invalid questions, such as questions that have only one choice, questions that\ncontain faulty data, and questions that are duplicated, to comply with fair use and transformative\nuse of the law. If there were multiple correct answers that applied, we kept only one correct answer.\nAlso, we shuf\ufb02ed the answer options of each question to ensure the choices do not follow any\nspeci\ufb01c pattern. To make the dataset easy to use, we then used semi-automated scripts to reformat\nthe lectures and solutions. Therefore, special structures in the texts, such as tables and lists, are\neasily distinguishable from simple text passages. Similar to ImageNet, ReClor, and PMR datasets,\nSCIENCEQA is available for non-commercial research purposes only and the copyright belongs to\nthe original authors. To ensure data quality, we developed a data exploration tool to review examples\nin the collected dataset, and incorrect annotations were further manually revised by experts. The tool\ncan be accessed at https://scienceqa.github.io/explore.html.\n\nA.2 Question Statistics\n\nFigure 9 (a) is a word cloud showing the most frequently appeared words in the question texts.\nStopping words that do not contain any semantic meaning, such as \u201cwhat\u201d or \u201cand\u201d, are removed to\ngive us a clearer view of the semantic range of SCIENCEQA. The diagram shows that SCIENCEQA\ncovers a wide range of topics, with words from different topics showing up across the cloud.\n\nFigures 9 (b) (c) (d) show the word clouds for each of the three subjects. We can observe from the\nword clouds that the words are well-matched to the subject themes. In natural science questions,\nwords such as \u201ctrait\u201d, \u201cmagnet\u201d, and \u201cforce\u201d appear frequently. Words such as \u201ccapital\u201d and \u201cstate\u201d\nshow up frequently in social science questions, whereas words such as \u201cdictionary\u201d and \u201cpage\u201d are\ncommon in language science questions.\n\nA.3 Choice Statistics\n\nTable 8 shows the number of questions with each number of\ndifferent choices. Questions have a minimum of two options\nand a maximum of \ufb01ve options. Figure 10 shows the distribu-\ntion of choice length in SCIENCEQA. Most choices are short,\ncontaining up to \ufb01ve words. However, the distribution has a\nlong tail where about 5% of the choices contain more than 15\nwords. Hence, it requires models to have a high level of text\nunderstanding to address diversely distributed choices.\n\nChoice number\n\nSize\n\nPercent\n\n2\n3\n4\n5\n\n11,045\n5,078\n4,893\n192\n\n52.08%\n23.94%\n23.07%\n0.91%\n\nTable 8: Choice number distribution.\n\n36th Conference on Neural Information Processing Systems (NeurIPS 2022).\n\n16\n\n\f(a) Questions of all subjects.\n\n(b) Questions in natural science.\n\n(c) Questions in social science.\n\n(d) Questions in language science.\n\nFigure 9: Word cloud distributions of question texts in different subjects.\n\nA.4 Subject Statistics\n\nFigure 11 shows the question length distribution of each subject. The three subjects all feature\nlong-tail distributions in terms of the number of question words. On average, social science questions\nare the shortest, while language science questions are the longest. Language science questions are\ndistributed more evenly than other questions across different numbers of words. These features imply\nthat the SCIENCEQA dataset is rich in compositional diversity.\n\nFigure 10: Choice length distribution.\n\nFigure 11: Question distributions of diff. subjects.\n\nA.5 Grade Statistics\n\nThe grade distribution is shown in Figure 12. The majority of questions come from the middle\nlevel curriculum (i.e., from grade 3 to grade 8) while around 10% are taken from the high school\ncurriculum (i.e., from grade 9 to grade 12). These high school level questions are close to or at the\ndif\ufb01culty level of the U.S. standardized tests for college admissions. Machine algorithms need to\nmaster a large amount of scienti\ufb01c knowledge and perform complex reasoning in order to perform\nwell on SCIENCEQA.\n\n17\n\n\fGrades\n\nNumber\n\nPercent\n\nGrade 1\nGrade 2\nGrade 3\nGrade 4\nGrade 5\nGrade 6\nGrade 7\nGrade 8\nGrade 9\nGrade 10\nGrade 11\nGrade 12\n\n95\n1,678\n3,032\n3,544\n3,086\n2,450\n2,749\n2,546\n491\n558\n539\n440\n\n0.45%\n7.91%\n14.3%\n16.71%\n14.55%\n11.55%\n12.96%\n12.0%\n2.32%\n2.63%\n2.54%\n2.07%\n\n(a) Grade distribution statistics.\n\n(b) Grade distribution visualization.\n\nFigure 12: SCIENCEQA questions and their corresponding grades.\n\nB Experiments\n\nB.1 Experimental Details\n\nBelow are details on the experiments:\n\n\u2022 Fine-tuning on the dataset. Fine-tuning baselines (VQA baselines and Uni\ufb01edQA) are trained\n\non the training set, developed on the validation set, and evaluated on the test set.\n\n\u2022 Input sizes: For VQA baselines, we set the maximum number of input words or tokens as 100.\n\u2022 Batch sizes. We use batches of 64 and 4 for VQA baselines and \ufb01ne-tuned Uni\ufb01edQA, respec-\n\ntively.\n\n\u2022 Newline character. For language models, the newline separators (\\n) in the text are replaced\nwith \\\\n when encoding the inputs because \\n is normally used as a stop symbol, following the\noriginal works [4, 19].\n\n\u2022 Captioning model. We use the tool2 to generate captions for the images in the dataset. The\nmaximum length of generated captions is 16, the number of beams is 4, and the maximum\nnumber of output tokens is 512.\n\n\u2022 Compute resources. We use two GeForce RTX 3090 GPUs for \ufb01ne-tuning VQA baselines and\n\nUni\ufb01edQA on the dataset.\n\n\u2022 Questions without any context. For questions without any context, the context text is replaced\n\nwith an empty string.\n\n\u2022 GPT-3: Following default settings, we choose temperature, frequency penalty and presence\npenalty as 0.0, and top probability as 1.0. All experiments for GPT-3 are run via the online\nAPI. Experiments in Figure 7 are repeated four times with in-context examples listed in Table 9.\nExperiments in Table 3, 5, 6, and 7 are conducted using examples with the trial ID of 1.\n\nTrial IDs Random seeds\n\nIn-context example IDs\n\n1\n2\n3\n4\n\n3\n5\n7\n9\n\n6493, 16241, 14954, 3598, 10088\n17099, 6960, 20290, 9780, 18898\n8836, 4144, 10781, 17852, 1363\n12701, 16832, 10180, 7289, 3801\n\nTable 9: Training example candidates used in four trials for GPT-3 (CoT).\n\nB.2 Human Performance Study\n\nIn order to understand how humans perform on SCIENCEQA questions, we used Amazon Mechanical\nTurk (AMT) to crowdsource answers to the test set. The interface of instructions and one example\n\n2https://huggingface.co/nlpconnect/vit-gpt2-image-captioning\n\n18\n\n\f(a) Instructions to answer the SCIENCEQA questions.\n\nFigure 13: Interfaces of instructions and one test question example for AMT workers.\n\n(b) One test question example.\n\nFigure 14: Interface of instructions for AMT workers to evaluate the explanations generated from\nUni\ufb01edQA (CoT) and GPT-3 (CoT).\n\nof a test question is shown in Figure 13. A total of 4,241 test questions were shuf\ufb02ed and split into\n425 batches, with each batch having 10 questions (excluding the last one). For each batch, we also\nrandomly added \ufb01ve training questions as exam examples. Each set of 15 questions was then assigned\nto 3 AMT workers. Only workers who correctly answer 4 out of the 5 exam examples or more are\nquali\ufb01ed for the human performance study. In other words, workers who failed to pass the quali\ufb01ed\nexam were eliminated from the analysis. For each set of 15 questions, we provided the worker with\n$0.5 per HIT task. At the rate of 3 questions per minute, this amounts to $6.0 per hour.\n\nB.3 Human Evaluation of Generated Explanations\n\nWe also evaluated the quality of predictions from GPT-3 (CoT) and Uni\ufb01edQA (CoT) by asking\nAMT workers to rate the model-generated explanations. The interface is shown in Figure 14. Each\nsample\u2019s question text, contexts, choices, and answers were presented, along with the corresponding\nexplanation generated by language models. The workers were asked to decide whether the proposed\nexplanation is relevant (is related to the question), correct (gives a correct answer and explanation),\nand complete (fully explains the answer). Prediction outputs that contain textual explanations were\ngrouped into batches of 10, each assigned to 3 workers for evaluation. For each batch, we provided\n\n19\n\n\f\n\n\n\n\n\n\n\n\n\n\n\n(a) A natural science example with the correct answer and a gold explanation.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b) A social science example with a correct answer and a gold explanation.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(c) A language science example with a correct answer and a gold explanation.\n\nFigure 15: Three examples with a correct answer and a gold explanation predicted by GPT-3 (CoT).\n\n\n\nthe workers with a monetary compensation of $0.3. Finally, the human scores for each explanation\nwere determined by taking a majority vote.\n\nB.4 Case Study and Limitations\n\nFigure 15 shows three examples with correct answers and gold explanations predicted by GPT-3\nvia chain-of-thought prompting (CoT). We can see that GPT-3 (CoT) not only predicts the correct\nanswers but also generates reasonable explanations, which follow the multi-hop reasoning process\nof human beings. This suggests that large language models like GPT-3 have great promise for\nimplementing high-level reasoning abilities.\n\nFigure 16 visualizes three more examples with predictions from GPT-3 (CoT). In these examples,\nGPT-3 (CoT) is able to predict the correct answers but fails to generate gold explanations. For\nexample, GPT-3 (CoT) generates an irrelevant explanation because the context text does not include\n\ufb01ne-grained visual information in the image (Figure 16a). In the example shown in Figure 16b,\nGPT-3 (CoT) fails to predict the coherent thought chains, where there are an incorrect example and\nan incorrect statement for a chemical change. The third example is given in Figure 16c, where the\ngenerated explanation is just a repetition of the input question and the output answer, instead of\nfollowing the complete thought chain to arrive at the \ufb01nal answer.\n\nFour failure examples with wrong predicted answers are listed in Figure 17. We extract the image\ncaptions and feed them to the large language model as the visual content input. However, these\ncaptions lack \ufb01ne-grained semantics and usually do not work well for diagrams, which results in\n\n20\n\n\f\n\n\n\n\n\n\n\n\n\n(a) An example with a correct answer but an irrelevant explanation.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b) An example with a correct answer but an incorrect explanation.\n\n\n\n\n\n\n\n\n\n\n\n\n(c) An example with a correct answer but an incomplete explanation.\n\nFigure 16: Three examples with predictions from GPT-3 (CoT). The answers are correct but the\ngenerated explanations are irrelevant, incorrect, or incomplete.\n\ntwo failure cases shown in Figure 17a and 17b. Moreover, there exist challenges for large language\nmodels to reason about the questions that require them to understand complex and uncommon\ndomain knowledge. For example, GPT-3 (CoT) cannot understand accurately the terminology of\n\npersoni\ufb01cation in language science (Figure 17c) and a series of complex chemical changes happen in\n\nthe formation process of dinosaur fossils (Figure 17d).\n\nB.5 Broader Impacts\n\nSocietal impact. The SCIENCEQA dataset collects science questions sourced from textbooks and is\nproposed to diagnose the multimodal understanding and multi-hop reasoning abilities of AI systems.\nDue to the nature of data sources, SCIENCEQA does not contain any user usage data or personally\nsensitive information such as gender and race. After careful examination of our dataset, to our\nbest knowledge, we have not found any improper content, such as pornographic information, racial\nremarks, or harmful social bias. We adhere to the goal of AI for the common good, and any antisocial\ndata points will be removed from the dataset based on feedback.\n\nPotential usage. The proposed SCIENCEQA dataset and designed methods in this paper are bene\ufb01-\ncial to both follow-up research work and real-world applications. SCIENCEQA provides a useful\nbenchmark for multi-modal learning, multi-hop reasoning, and general arti\ufb01cial intelligence. Besides,\n\n21\n\n\f\n\n\n\n\n\n\n\n\n\n\n\n(a) An example with a wrong answer and a wrong explanation.\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b) An example with a wrong answer and a wrong explanation.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(c) An example with a wrong answer and a wrong explanation.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(d) An example with a wrong answer and a wrong explanation.\n\nFigure 17: Four failure examples with predictions from GPT-3 (CoT). The answers are wrong, and\n\nthe generated explanations fail to follow the right chain-of-thought reasoning process.\n\n\nSCIENCEQA will contribute to the development of K-12 education applications such as tutoring\nsystems. Furthermore, the designed methods with the chain of thought investigate the ability of large\nlanguage models to mimic the human mind process when reasoning about a challenging task.\n\n22\n\n\f", "system_prompts": [{"role": "system", "content": "You are a computer science researcher currently reviewing a paper titled \"Learn to Explain: Multimodal Reasoning via Thought Chains for Science Question Answering\" for the NeurIPS computer science conference. Your goal is to try to be as objective and truthful as possible in your answers about the paper provided. Your reviews will be used for causal reasoning in determining the quality of the paper."}], "problems_dict": {"q1b": [[{"role": "user", "content": "The following is the conclusion section of the paper you are reviewing:\n\n\nOne future direction is to consider incentives in our paradigm like the literature of information\nelicitation without verification [13, 18, 5, 22, 9]. We have asked a class of students at Peking\nUniversity: why are bar chairs high? using our paradigm. We cluster the answers by hand. The\nplurality answer is \u201cthe bar counter is high\u201d and our top-ranking answer is \u201cbetter eye contact with\npeople who stand\u201d. Thus, another future diction is to extend our approach to the scenario where\npeople\u2019s answers are sentences, where we can apply NLP to cluster them automatically. In summary,\nwe propose the first empirically validated method to learn the thinking hierarchy without any prior\nin the general problem-solving scenarios. Potentially, our paradigm can be used to make a better\ndecision when we crowd-source opinions in a new field with little prior information. Moreover, when\nwe elicit the crowds\u2019 opinions for a policy, with the thinking hierarchy information, it\u2019s possible to\nunderstand the crowds\u2019 opinions better. However, regarding the negative impact, it may be easier\nto implement a social media manipulation of public opinion with the full thinking hierarchy of the\ncrowds. One interesting future direction is to explore the impact of the thinking hierarchy information.\n\n\nBased on this section, do the authors describe limitations of their work?"}]], "q1c": [[{"role": "user", "content": "The following is the conclusion section of the paper you are reviewing:\n\n\nOne future direction is to consider incentives in our paradigm like the literature of information\nelicitation without verification [13, 18, 5, 22, 9]. We have asked a class of students at Peking\nUniversity: why are bar chairs high? using our paradigm. We cluster the answers by hand. The\nplurality answer is \u201cthe bar counter is high\u201d and our top-ranking answer is \u201cbetter eye contact with\npeople who stand\u201d. Thus, another future diction is to extend our approach to the scenario where\npeople\u2019s answers are sentences, where we can apply NLP to cluster them automatically. In summary,\nwe propose the first empirically validated method to learn the thinking hierarchy without any prior\nin the general problem-solving scenarios. Potentially, our paradigm can be used to make a better\ndecision when we crowd-source opinions in a new field with little prior information. Moreover, when\nwe elicit the crowds\u2019 opinions for a policy, with the thinking hierarchy information, it\u2019s possible to\nunderstand the crowds\u2019 opinions better. However, regarding the negative impact, it may be easier\nto implement a social media manipulation of public opinion with the full thinking hierarchy of the\ncrowds. One interesting future direction is to explore the impact of the thinking hierarchy information.\n\n\nBased on this section, do the authors address potential negative societal impacts of their work?"}]], "q2a": [[{"role": "user", "content": "The following is the abstract of the paper you are reviewing:\n\n\nWhen we use the wisdom of the crowds, we usually rank the answers according to\ntheir popularity, especially when we cannot verify the answers. However, this can\nbe very dangerous when the majority make systematic mistakes. A fundamental\nquestion arises: can we build a hierarchy among the answers without any prior\nwhere the higher-ranking answers, which may not be supported by the majority,\nare from more sophisticated people? To address the question, we propose 1) a\nnovel model to describe people\u2019s thinking hierarchy; 2) two algorithms to learn\nthe thinking hierarchy without any prior; 3) a novel open-response based crowd-\nsourcing approach based on the above theoretic framework. In addition to theoretic\njustifications, we conduct four empirical crowdsourcing studies and show that a)\nthe accuracy of the top-ranking answers learned by our approach is much higher\nthan that of plurality voting (In one question, the plurality answer is supported by\n74 respondents but the correct answer is only supported by 3 respondents. Our\napproach ranks the correct answer the highest without any prior); b) our model\nhas a high goodness-of-fit, especially for the questions where our top-ranking\nanswer is correct. To the best of our knowledge, we are the first to propose a\nthinking hierarchy model with empirical validations in the general problem-solving\nscenarios; and the first to propose a practical open-response based crowdsourcing\napproach that beats plurality voting without any prior.\n\n1\n\n\nBased on this abstract, do the authors have theoretical results? Please start your answer with \"Yes\", \"No\", or \"Unsure\"."}, {"role": "user", "content": "The following is the methods section of the paper you are reviewing:\n\n\nWe first introduce our model for thinking hierarchy. Tversky and Kahneman [27] propose that people\nhave two systems, a fast and intuitive system, and a slow and logical system. For example, Alice\nstarts to solve the circle problem. When she reads the question, she can run her intuitive system 1 and\nobtain answer \u201c3\u201d. However, when she starts to think carefully, she runs her more careful system 2 to\nobtain answer \u201c4\u201d.\n\nWe propose a more general model where people can run multiple oracles to approach the question\nduring their thinking process. Conceptually similar to the cognitive hierarchy theory [3], we assume\nthe oracles have different levels. People usually run lower level oracles before the higher level oracles.\nIn the previous example, system 1 is the lower level oracle and system 2 is the higher one. We assume\nthat Alice runs system 2 after system 1. Thus, in our model, people who know the answer is \u201c4\u201d can\n\n4\n\n\fpredict that other people answer \u201c3\u201d, which is hypothesized in Kong and Schoenebeck [10]. It\u2019s also\npossible that some people run the most sophisticated oracle directly without running the lower ones.\nThus, we design the model such that it does not require the higher-type to be able to predict ALL\nlower types. We also allow the oracles\u2019 outputs to be random.\n\nSection 2.1 introduces the model of thinking hierarchy. Section 2.2 reduces the model inference\nproblem to a matrix decomposition problem. Section 2.3 and section 2.4 show how to learn the\nthinking hierarchy.\n\n2.1 Thinking hierarchy\n\nFixing a question q (e.g. the circle problem), T denotes the set of thinking types. A denotes the set of\npossible answers. Both T and A are finite sets. \u2206A denotes all possible distributions over A. We\nsometimes use \u201cprob\u201d as a shorthand for probability.\n\nGenerating answers We will describe how people of different thinking types generate answers.\nDefinition 2.1 (Oracles of thinking types W ). An answer generating oracle maps the question to\nan (random) answer in A. Each type t corresponds to an oracle Ot. The output Ot(q) is a random\nvariable whose distribution is wt \u2208 \u2206A. W denotes a |T | \u00d7 |A| matrix where each row t is wt.\nEach respondent is type t with prob pt and (cid:80)\nrunning the oracle Ot. For all a \u2208 A, the probability that a respondent answers a will be (cid:80)\nWe assume the probability is positive for all a \u2208 A.\nExample 2.2 (A running example). There are two types T = {0, 1}. The answer space is A =\n{3, 4, 6}. O0 will output \u20183\u2019 with probability 0.8 and \u20186\u2019 with probability 0.2. O1 will output \u20184\u2019\n0\n1\n\nt pt = 1. A type t respondent generates the answer by\nt ptwt(a).\n\ndeterministically. In this example, W =\n\nwhere the first row is the distribution of\n\n(cid:20)0.8\n0\n\n0.2\n0\n\n(cid:21)\n\nO0\u2019s output and the second row is the distribution of O1\u2019s output.\n\nGenerating predictions We then describe how people of different thinking types predict what other\npeople will answer. Here the prediction is not a distribution, but an answer other people may report.\nWhen a type t respondent makes a prediction, she will run an oracle, which is Ot\u2032 with probability\npt\u2192t\u2032 where (cid:80)\n\nt\u2032 pt\u2192t\u2032 = 1. She uses the output of Ot\u2032 as the prediction g \u2208 A.\n\nCombination: answer-prediction joint distribution M M denotes a |A| \u00d7 |A| matrix where\nMa,g is the probability an respondent answers a and predicts g. \u039b denotes a |T | \u00d7 |T | matrix where\n\u039bt,t\u2032 = ptpt\u2192t\u2032 is the probability a respondent is type t and predicts type t\u2032.\nExample 2.3. In this example, when type 0 respondent makes a prediction, with prob 1, she will run\nO0 again. When type 1 respondent makes a prediction, with prob 0.5, she will run O1 again, with\nprob 0.5, she will run O0. Moreover, a respondent is type 0 with prob 0.7, and type 1 with prob 0.3.\n\nHere \u039b =\n\n(cid:20)p0p0\u21920\np1p1\u21920\n\np0p0\u21921\np1p1\u21921\n\n(cid:21)\n\n=\n\n(cid:20) 0.7 \u2217 1\n0.3 \u2217 0.5\n\n0.7 \u2217 0\n0.3 \u2217 0.5\n\n(cid:21)\n\n=\n\n(cid:20) 0.7\n0.15\n\n(cid:21)\n\n.\n\n0\n0.15\n\nClaim 2.4. Based on the above generating processes, M = W\u22a4\u039bW.\n\nProof. For each respondent, the probability she answers a and predicts g will be\n(cid:88)\n\n(cid:88)\n\n(cid:88)\n\nptwt(a)\n\npt\u2192t\u2032wt\u2032(g) =\n\nwt(a)ptpt\u2192t\u2032wt\u2032(g).\n\nMa,g =\n\nt\n\nt\u2032\n\nt,t\u2032\n\nWe sum over all possible types the respondent will be. Given she is type t, she runs oracle Ot to\ngenerate the answer and wt(a) is the probability that the answer is a. We sum over all possible\noracles she runs to predict. Given that she runs Ot\u2032, wt\u2032(g) is the probability the prediction is g.\n\nKey assumption: upper-triangular \u039b we assume that people of a less sophisticated level can\nnever run the oracles of more sophisticated levels. A linear ordering of types \u03c0 : {1, 2, \u00b7 \u00b7 \u00b7 , |T |} (cid:55)\u2192 T\nmaps a ranking position to a type. For example, \u03c0(1) \u2208 T is the top-ranking type.\nAssumption 2.5. We assume that with a proper ordering \u03c0 of the types, \u039b is an upper-triangular\nmatrix. Formally, there exists \u03c0 such that \u2200i > j, \u039b\u03c0(i),\u03c0(j) = 0. Any \u03c0 that makes \u039b upper-\ntriangular is a valid thinking hierarchy of the types.\n\n5\n\n\fIn the running example, the valid thinking hierarchy is \u03c0(1) = type 1, \u03c0(2) = type 0. Note that the\nabove assumption does not require \u2200i \u2264 j, \u039b\u03c0(i),\u03c0(j) > 0. When \u039b is a diagonal matrix, types cannot\npredict each other and are equally sophisticated, thus any ordering is a valid thinking hierarchy.\n\nAn algorithm finds the thinking hierarchy when the algorithm is given M which is generated by\nlatent (unknown) W, \u039b, and the algorithm will output a matrix W\u2217 which equals a row-permuted\nW where the row order is a valid thinking hierarchy. Formally, there exists a valid thinking hierarchy\n\u03c0 such that the ith row of W\u2217 is the \u03c0(i)th row of W, i.e, w\u2217\n\ni = w\u03c0(i).\n\n2.2 Non-negative Congruence Triangularization (NCT)\n\nWith the above model, inferring thinking hierarchy leads to a novel matrix decomposition problem,\nwhich is similar to the symmetric non-negative matrix factorization problem (NMF)6. A non-negative\nmatrix is a matrix whose elements are non-negative.\nDefinition 2.6 (Non-negative Congruence7 Triangularization (NCT)). Given a non-negative matrix\nM, NCT aims to find non-negative matrices W and non-negative upper-triangular matrix \u039b such\nthat M = W\u22a4\u039bW. In a Frobenius norm based approximated version, given a set of matrices W,\nNCT aims to find non-negative matrices W and non-negative upper-triangular matrix \u039b to minimize\n\nmin\nW\u2208W,\u039b\n\n||M \u2212 W\u22a4\u039bW||2\nF\n\nand the minimum is defined as the lack-of-fit of M regarding W 8.\n\nLike NMF, it\u2019s impossible to ask for the strict uniqueness of the results. Let P\u039b be the set of\npermutation matrices such that \u03a0\u22a4\u039b\u03a0 is still upper-triangular.\nIf (W, \u039b) is a solution, then\n(\u03a0\u22121DW, \u03a0\u22a4D\u22121\u039bD\u22121\u03a0) is also a solution where D is a diagonal matrix with positive elements\nand \u03a0 \u2208 P\u039b. We state the uniqueness results as follows and the proof is deferred to Appendix C.\nProposition 2.7 (Uniqueness). If |T | \u2264 |A| and T columns of W consist of a permuted positive\ndiagonal matrix, NCT for M = W\u22a4\u039bW is unique in the sense that then for all W\u2032\u22a4\u039b\u2032W\u2032 =\nW\u22a4\u039bW, there exists a positive diagonal matrix D and a |T | \u00d7 |T | permutation matrix \u03a0 \u2208 P\u039b\nsuch that W\u2032 = \u03a0\u22121DW.\n\nWhen we restrict W to be \u201csemi-orthogonal\u201d, we obtain a clean format of NCT without searching\nfor optimal \u039b. I is the set of all \u201csemi-orthogonal\u201d matrices W where each column of W has\nand only has one non-zero element and WW\u22a4 = I. For example, the W in Example 2.2 can be\nnormalized to a semi-orthogonal matrix. The following lemma follows from the expansion of the\nFrobenius norm and we defer the proof to Appendix C.\n\nLemma 2.8 (Semi-orthogonal: minimizing F-norm = maximizing upper-triangular\u2019s sum of the\nsquare). For all set of matrices W \u2282 I, minW\u2208W,\u039b ||M \u2212 W\u22a4\u039bW||2\nF is equivalent to solv-\ni,j and setting \u039b as Up(WMW\u22a4), the upper-triangular area of\ning maxW\u2208W\nWMW\u22a4.\n\ni\u2264j(WMW\u22a4)2\n\n(cid:80)\n\n2.3\n\nInferring the thinking hierarchy with answer-prediction joint distribution M\n\nGiven M, inferring the thinking hierarchy is equivalent to solving NCT in general. Though we do not\nhave M, later we will show a proxy for M. For simplicity of practical use, we introduce two simple\nranking algorithms by employing Lemma 2.8. The ranking algorithm takes M as input and outputs a\nlinear ordering of answers \u03c0 : {1, 2, \u00b7 \u00b7 \u00b7 , |A|} (cid:55)\u2192 A that maps a ranking position to an answer.\n\nAnswer-Ranking Algorithm (Default) AR(M) The algorithm computes\n\n\u03a0\u2217 \u2190 arg max\n\u03a0\u2208P\n\n(cid:88)\n\n(\u03a0M\u03a0\u22a4)2\ni,j\n\ni\u2264j\n\n6Symmetric NMF: minW ||M \u2212 W\u22a4W||2\nF\n7We use congruence here though it is not matrix congruence since W may not be a square matrix.\n8M = W\u22a4\u039bW, W \u2208 W has zero lack-of-fit.\n\n6\n\n\fwhere P is the set of all |A| \u00d7 |A| permutation matrices. There is a one to one mapping between each\npermutation matrix \u03a0 and a linear ordering \u03c0: \u03a0i,\u03c0(i) = 1, \u2200i. Therefore, the optimal \u03a0\u2217 leads to an\noptimal rank over answers directly and the default algorithm can be also represented as\n\n\u03c0\u2217 \u2190 arg max\n\n\u03c0\n\nM 2\n\n\u03c0(i),\u03c0(j).\n\n(cid:88)\n\ni\u2264j\n\nTo find the optimal rank, we use a dynamic programming based algorithm (see the supplementary\nmaterials) which takes O(2|A||A|2). In practice, |A| is usually at most 7 or 8. In our empirical study,\nthe default algorithm takes 91 milliseconds to finish the computation of all 152 questions.\n\nThe default algorithm implicitly assumes |T | = |A| and all oracles are deterministic. To allow\n|T | < |A| and non-deterministic oracles, we introduce a variant that generalizes P to a subset of\nsemi-orthogonal matrices I. Every |T | \u00d7 |A| semi-orthogonal W indicates a hard clustering. Each\ncluster t \u2208 T contains all answers a such that Wt,a > 0. For example, the W in Example 2.2 can\nbe normalized to a semi-orthogonal matrix and indicates a hard clustering {4}, {6, 3}. Therefore,\nthe variant algorithm will partition the answers into multiple clusters and assign a hierarchy to the\nclusters.\n\nAnswer-Ranking Algorithm (Variant) AR+(M, W) The algorithm computes\n\nW\u2217 \u2190 arg max\nW\u2208I\n\n(cid:88)\n\n(WMW\u22a4)2\n\ni,j.\n\ni\u2264j\n\nwhere W \u2282 I. W\u2217 is normalized such that every row sums to 1. This algorithm does not restrict\n|T | = |A| and learns the optimal |T |.\n\nW\u2217 \u21d2 Answer rank The output W\u2217 indicates a hard clustering of all answers. We rank all answer\nas follows: for any i < j, the answers in cluster i has a higher rank 9 than the answers in cluster j.\nFor all i, for any two answers a, a\u2032 in the same cluster i, a is ranked higher than a\u2032 if W \u2217\ni,a\u2032.\n\ni,a > W \u2217\n\nTheoretical justification When M perfectly fits the model with the restriction that the latent W is\na permutation or a hard clustering, we show that our algorithm finds the thinking hierarchy. Otherwise,\nour algorithm finds the \u201cclosest\u201d solution measured by the Frobenius norm.\nTheorem 2.9. When there exists \u03a00 \u2208 P and non-negative upper-triangular matrix \u039b0 such that\n0 \u039b0\u03a00, AR(M) finds the thinking hierarchy 10. In general, AR(M) will output \u03a0\u2217 where\nM = \u03a0\u22a4\n\u03a0\u2217, \u039b\u2217 = Up(\u03a0\u2217M\u03a0\u2217\u22a4) is a solution to arg min\u03a0\u2208P,\u039b ||M \u2212 \u03a0\u22a4\u039b\u03a0||2\nF . The above statement\nis still valid by replacing P by W \u2282 I and AR(M) by AR+(M, W).\n\nProposition 2.7 and Lemma 2.8 almost directly imply the above theorem. We defer the formal proof\nto Appendix C.\n\n2.4 A proxy for answer-prediction joint distribution M\n\nIn practice, we do not have perfect M. We use the following open-response paradigm to obtain a\nproxy for M.\n\nAnswer-prediction paradigm the respondents are asked Q1: What\u2019s your answer? Answer:\nQ2: What do you think other people will answer:\n\n.\n\nIn the circle example, the possible feedback can be \u201canswer: 4; prediction: 3\u201d, \u201canswer: 3; prediction:\n6,9,1\u201d... We collect all answers provided by the respondents 11 and denote the set of them by A. In\nthe circle example, A = {1, 2, 3, 4, 6, 9}. We also allow respondents to provide no prediction or\nmultiple predictions.\n\n9Rank 1 answer is in the highest position.\n10See definition in the last paragraph in Section 2.1.\n11In practice, we set a threshold \u03b8 and collect answers which are provided by at least \u03b8 fraction of the\nrespondents. We allow multiple predictions and also allow people to answer \u201cI do not know\u201d. See Section 3 for\nmore details.\n\n7\n\n\f(a) Accuracy of algorithms\n\n(b) Empirical distribution of lack-of-fit\n\nFigure 3: The results of our experiment\n\nAnswer-prediction matrix We aggregate the feedback and visualize it by an Answer-Prediction\nmatrix. The Answer-Prediction matrix A is a |A| \u00d7 |A| square matrix where |A| is the number of\ndistinct answers provided by the respondents. Each entry Aa,g, a, g \u2208 A is the number of respondents\nthat answer \u201ca\u201d and predict \u201cg\u201d.\n\nWe will show that with proper assumptions, the answer-prediction matrix A\u2019s expectation is pro-\nportional to M. First, for ease of analysis, we assume that each respondent\u2019s predictions are i.i.d.\nsamples12. Second, since we allow people to optionally provide predictions, we need to additionally\nassume that the number of predictions each respondent is willing to provide is independent of her\ntype and answer. We state the formal result as follows and the proof is deferred to Appendix C.\n\nProposition 2.10. When each respondent\u2019s predictions are i.i.d. samples, and the number of\npredictions she gives is independent of her type and answer, the answer-prediction matrix A\u2019s\nexpectation is proportional to M.\n\n\nBased on this section, do the authors state the assumptions for their theoretical results?"}]], "q2b": [[{"role": "user", "content": "The following is the abstract of the paper you are reviewing:\n\n\nWhen we use the wisdom of the crowds, we usually rank the answers according to\ntheir popularity, especially when we cannot verify the answers. However, this can\nbe very dangerous when the majority make systematic mistakes. A fundamental\nquestion arises: can we build a hierarchy among the answers without any prior\nwhere the higher-ranking answers, which may not be supported by the majority,\nare from more sophisticated people? To address the question, we propose 1) a\nnovel model to describe people\u2019s thinking hierarchy; 2) two algorithms to learn\nthe thinking hierarchy without any prior; 3) a novel open-response based crowd-\nsourcing approach based on the above theoretic framework. In addition to theoretic\njustifications, we conduct four empirical crowdsourcing studies and show that a)\nthe accuracy of the top-ranking answers learned by our approach is much higher\nthan that of plurality voting (In one question, the plurality answer is supported by\n74 respondents but the correct answer is only supported by 3 respondents. Our\napproach ranks the correct answer the highest without any prior); b) our model\nhas a high goodness-of-fit, especially for the questions where our top-ranking\nanswer is correct. To the best of our knowledge, we are the first to propose a\nthinking hierarchy model with empirical validations in the general problem-solving\nscenarios; and the first to propose a practical open-response based crowdsourcing\napproach that beats plurality voting without any prior.\n\n1\n\n\nBased on this abstract, do the authors have theoretical results? Please start your answer with \"Yes\", \"No\", or \"Unsure\"."}, {"role": "user", "content": "The following is the methods section of the paper you are reviewing:\n\n\nWe first introduce our model for thinking hierarchy. Tversky and Kahneman [27] propose that people\nhave two systems, a fast and intuitive system, and a slow and logical system. For example, Alice\nstarts to solve the circle problem. When she reads the question, she can run her intuitive system 1 and\nobtain answer \u201c3\u201d. However, when she starts to think carefully, she runs her more careful system 2 to\nobtain answer \u201c4\u201d.\n\nWe propose a more general model where people can run multiple oracles to approach the question\nduring their thinking process. Conceptually similar to the cognitive hierarchy theory [3], we assume\nthe oracles have different levels. People usually run lower level oracles before the higher level oracles.\nIn the previous example, system 1 is the lower level oracle and system 2 is the higher one. We assume\nthat Alice runs system 2 after system 1. Thus, in our model, people who know the answer is \u201c4\u201d can\n\n4\n\n\fpredict that other people answer \u201c3\u201d, which is hypothesized in Kong and Schoenebeck [10]. It\u2019s also\npossible that some people run the most sophisticated oracle directly without running the lower ones.\nThus, we design the model such that it does not require the higher-type to be able to predict ALL\nlower types. We also allow the oracles\u2019 outputs to be random.\n\nSection 2.1 introduces the model of thinking hierarchy. Section 2.2 reduces the model inference\nproblem to a matrix decomposition problem. Section 2.3 and section 2.4 show how to learn the\nthinking hierarchy.\n\n2.1 Thinking hierarchy\n\nFixing a question q (e.g. the circle problem), T denotes the set of thinking types. A denotes the set of\npossible answers. Both T and A are finite sets. \u2206A denotes all possible distributions over A. We\nsometimes use \u201cprob\u201d as a shorthand for probability.\n\nGenerating answers We will describe how people of different thinking types generate answers.\nDefinition 2.1 (Oracles of thinking types W ). An answer generating oracle maps the question to\nan (random) answer in A. Each type t corresponds to an oracle Ot. The output Ot(q) is a random\nvariable whose distribution is wt \u2208 \u2206A. W denotes a |T | \u00d7 |A| matrix where each row t is wt.\nEach respondent is type t with prob pt and (cid:80)\nrunning the oracle Ot. For all a \u2208 A, the probability that a respondent answers a will be (cid:80)\nWe assume the probability is positive for all a \u2208 A.\nExample 2.2 (A running example). There are two types T = {0, 1}. The answer space is A =\n{3, 4, 6}. O0 will output \u20183\u2019 with probability 0.8 and \u20186\u2019 with probability 0.2. O1 will output \u20184\u2019\n0\n1\n\nt pt = 1. A type t respondent generates the answer by\nt ptwt(a).\n\ndeterministically. In this example, W =\n\nwhere the first row is the distribution of\n\n(cid:20)0.8\n0\n\n0.2\n0\n\n(cid:21)\n\nO0\u2019s output and the second row is the distribution of O1\u2019s output.\n\nGenerating predictions We then describe how people of different thinking types predict what other\npeople will answer. Here the prediction is not a distribution, but an answer other people may report.\nWhen a type t respondent makes a prediction, she will run an oracle, which is Ot\u2032 with probability\npt\u2192t\u2032 where (cid:80)\n\nt\u2032 pt\u2192t\u2032 = 1. She uses the output of Ot\u2032 as the prediction g \u2208 A.\n\nCombination: answer-prediction joint distribution M M denotes a |A| \u00d7 |A| matrix where\nMa,g is the probability an respondent answers a and predicts g. \u039b denotes a |T | \u00d7 |T | matrix where\n\u039bt,t\u2032 = ptpt\u2192t\u2032 is the probability a respondent is type t and predicts type t\u2032.\nExample 2.3. In this example, when type 0 respondent makes a prediction, with prob 1, she will run\nO0 again. When type 1 respondent makes a prediction, with prob 0.5, she will run O1 again, with\nprob 0.5, she will run O0. Moreover, a respondent is type 0 with prob 0.7, and type 1 with prob 0.3.\n\nHere \u039b =\n\n(cid:20)p0p0\u21920\np1p1\u21920\n\np0p0\u21921\np1p1\u21921\n\n(cid:21)\n\n=\n\n(cid:20) 0.7 \u2217 1\n0.3 \u2217 0.5\n\n0.7 \u2217 0\n0.3 \u2217 0.5\n\n(cid:21)\n\n=\n\n(cid:20) 0.7\n0.15\n\n(cid:21)\n\n.\n\n0\n0.15\n\nClaim 2.4. Based on the above generating processes, M = W\u22a4\u039bW.\n\nProof. For each respondent, the probability she answers a and predicts g will be\n(cid:88)\n\n(cid:88)\n\n(cid:88)\n\nptwt(a)\n\npt\u2192t\u2032wt\u2032(g) =\n\nwt(a)ptpt\u2192t\u2032wt\u2032(g).\n\nMa,g =\n\nt\n\nt\u2032\n\nt,t\u2032\n\nWe sum over all possible types the respondent will be. Given she is type t, she runs oracle Ot to\ngenerate the answer and wt(a) is the probability that the answer is a. We sum over all possible\noracles she runs to predict. Given that she runs Ot\u2032, wt\u2032(g) is the probability the prediction is g.\n\nKey assumption: upper-triangular \u039b we assume that people of a less sophisticated level can\nnever run the oracles of more sophisticated levels. A linear ordering of types \u03c0 : {1, 2, \u00b7 \u00b7 \u00b7 , |T |} (cid:55)\u2192 T\nmaps a ranking position to a type. For example, \u03c0(1) \u2208 T is the top-ranking type.\nAssumption 2.5. We assume that with a proper ordering \u03c0 of the types, \u039b is an upper-triangular\nmatrix. Formally, there exists \u03c0 such that \u2200i > j, \u039b\u03c0(i),\u03c0(j) = 0. Any \u03c0 that makes \u039b upper-\ntriangular is a valid thinking hierarchy of the types.\n\n5\n\n\fIn the running example, the valid thinking hierarchy is \u03c0(1) = type 1, \u03c0(2) = type 0. Note that the\nabove assumption does not require \u2200i \u2264 j, \u039b\u03c0(i),\u03c0(j) > 0. When \u039b is a diagonal matrix, types cannot\npredict each other and are equally sophisticated, thus any ordering is a valid thinking hierarchy.\n\nAn algorithm finds the thinking hierarchy when the algorithm is given M which is generated by\nlatent (unknown) W, \u039b, and the algorithm will output a matrix W\u2217 which equals a row-permuted\nW where the row order is a valid thinking hierarchy. Formally, there exists a valid thinking hierarchy\n\u03c0 such that the ith row of W\u2217 is the \u03c0(i)th row of W, i.e, w\u2217\n\ni = w\u03c0(i).\n\n2.2 Non-negative Congruence Triangularization (NCT)\n\nWith the above model, inferring thinking hierarchy leads to a novel matrix decomposition problem,\nwhich is similar to the symmetric non-negative matrix factorization problem (NMF)6. A non-negative\nmatrix is a matrix whose elements are non-negative.\nDefinition 2.6 (Non-negative Congruence7 Triangularization (NCT)). Given a non-negative matrix\nM, NCT aims to find non-negative matrices W and non-negative upper-triangular matrix \u039b such\nthat M = W\u22a4\u039bW. In a Frobenius norm based approximated version, given a set of matrices W,\nNCT aims to find non-negative matrices W and non-negative upper-triangular matrix \u039b to minimize\n\nmin\nW\u2208W,\u039b\n\n||M \u2212 W\u22a4\u039bW||2\nF\n\nand the minimum is defined as the lack-of-fit of M regarding W 8.\n\nLike NMF, it\u2019s impossible to ask for the strict uniqueness of the results. Let P\u039b be the set of\npermutation matrices such that \u03a0\u22a4\u039b\u03a0 is still upper-triangular.\nIf (W, \u039b) is a solution, then\n(\u03a0\u22121DW, \u03a0\u22a4D\u22121\u039bD\u22121\u03a0) is also a solution where D is a diagonal matrix with positive elements\nand \u03a0 \u2208 P\u039b. We state the uniqueness results as follows and the proof is deferred to Appendix C.\nProposition 2.7 (Uniqueness). If |T | \u2264 |A| and T columns of W consist of a permuted positive\ndiagonal matrix, NCT for M = W\u22a4\u039bW is unique in the sense that then for all W\u2032\u22a4\u039b\u2032W\u2032 =\nW\u22a4\u039bW, there exists a positive diagonal matrix D and a |T | \u00d7 |T | permutation matrix \u03a0 \u2208 P\u039b\nsuch that W\u2032 = \u03a0\u22121DW.\n\nWhen we restrict W to be \u201csemi-orthogonal\u201d, we obtain a clean format of NCT without searching\nfor optimal \u039b. I is the set of all \u201csemi-orthogonal\u201d matrices W where each column of W has\nand only has one non-zero element and WW\u22a4 = I. For example, the W in Example 2.2 can be\nnormalized to a semi-orthogonal matrix. The following lemma follows from the expansion of the\nFrobenius norm and we defer the proof to Appendix C.\n\nLemma 2.8 (Semi-orthogonal: minimizing F-norm = maximizing upper-triangular\u2019s sum of the\nsquare). For all set of matrices W \u2282 I, minW\u2208W,\u039b ||M \u2212 W\u22a4\u039bW||2\nF is equivalent to solv-\ni,j and setting \u039b as Up(WMW\u22a4), the upper-triangular area of\ning maxW\u2208W\nWMW\u22a4.\n\ni\u2264j(WMW\u22a4)2\n\n(cid:80)\n\n2.3\n\nInferring the thinking hierarchy with answer-prediction joint distribution M\n\nGiven M, inferring the thinking hierarchy is equivalent to solving NCT in general. Though we do not\nhave M, later we will show a proxy for M. For simplicity of practical use, we introduce two simple\nranking algorithms by employing Lemma 2.8. The ranking algorithm takes M as input and outputs a\nlinear ordering of answers \u03c0 : {1, 2, \u00b7 \u00b7 \u00b7 , |A|} (cid:55)\u2192 A that maps a ranking position to an answer.\n\nAnswer-Ranking Algorithm (Default) AR(M) The algorithm computes\n\n\u03a0\u2217 \u2190 arg max\n\u03a0\u2208P\n\n(cid:88)\n\n(\u03a0M\u03a0\u22a4)2\ni,j\n\ni\u2264j\n\n6Symmetric NMF: minW ||M \u2212 W\u22a4W||2\nF\n7We use congruence here though it is not matrix congruence since W may not be a square matrix.\n8M = W\u22a4\u039bW, W \u2208 W has zero lack-of-fit.\n\n6\n\n\fwhere P is the set of all |A| \u00d7 |A| permutation matrices. There is a one to one mapping between each\npermutation matrix \u03a0 and a linear ordering \u03c0: \u03a0i,\u03c0(i) = 1, \u2200i. Therefore, the optimal \u03a0\u2217 leads to an\noptimal rank over answers directly and the default algorithm can be also represented as\n\n\u03c0\u2217 \u2190 arg max\n\n\u03c0\n\nM 2\n\n\u03c0(i),\u03c0(j).\n\n(cid:88)\n\ni\u2264j\n\nTo find the optimal rank, we use a dynamic programming based algorithm (see the supplementary\nmaterials) which takes O(2|A||A|2). In practice, |A| is usually at most 7 or 8. In our empirical study,\nthe default algorithm takes 91 milliseconds to finish the computation of all 152 questions.\n\nThe default algorithm implicitly assumes |T | = |A| and all oracles are deterministic. To allow\n|T | < |A| and non-deterministic oracles, we introduce a variant that generalizes P to a subset of\nsemi-orthogonal matrices I. Every |T | \u00d7 |A| semi-orthogonal W indicates a hard clustering. Each\ncluster t \u2208 T contains all answers a such that Wt,a > 0. For example, the W in Example 2.2 can\nbe normalized to a semi-orthogonal matrix and indicates a hard clustering {4}, {6, 3}. Therefore,\nthe variant algorithm will partition the answers into multiple clusters and assign a hierarchy to the\nclusters.\n\nAnswer-Ranking Algorithm (Variant) AR+(M, W) The algorithm computes\n\nW\u2217 \u2190 arg max\nW\u2208I\n\n(cid:88)\n\n(WMW\u22a4)2\n\ni,j.\n\ni\u2264j\n\nwhere W \u2282 I. W\u2217 is normalized such that every row sums to 1. This algorithm does not restrict\n|T | = |A| and learns the optimal |T |.\n\nW\u2217 \u21d2 Answer rank The output W\u2217 indicates a hard clustering of all answers. We rank all answer\nas follows: for any i < j, the answers in cluster i has a higher rank 9 than the answers in cluster j.\nFor all i, for any two answers a, a\u2032 in the same cluster i, a is ranked higher than a\u2032 if W \u2217\ni,a\u2032.\n\ni,a > W \u2217\n\nTheoretical justification When M perfectly fits the model with the restriction that the latent W is\na permutation or a hard clustering, we show that our algorithm finds the thinking hierarchy. Otherwise,\nour algorithm finds the \u201cclosest\u201d solution measured by the Frobenius norm.\nTheorem 2.9. When there exists \u03a00 \u2208 P and non-negative upper-triangular matrix \u039b0 such that\n0 \u039b0\u03a00, AR(M) finds the thinking hierarchy 10. In general, AR(M) will output \u03a0\u2217 where\nM = \u03a0\u22a4\n\u03a0\u2217, \u039b\u2217 = Up(\u03a0\u2217M\u03a0\u2217\u22a4) is a solution to arg min\u03a0\u2208P,\u039b ||M \u2212 \u03a0\u22a4\u039b\u03a0||2\nF . The above statement\nis still valid by replacing P by W \u2282 I and AR(M) by AR+(M, W).\n\nProposition 2.7 and Lemma 2.8 almost directly imply the above theorem. We defer the formal proof\nto Appendix C.\n\n2.4 A proxy for answer-prediction joint distribution M\n\nIn practice, we do not have perfect M. We use the following open-response paradigm to obtain a\nproxy for M.\n\nAnswer-prediction paradigm the respondents are asked Q1: What\u2019s your answer? Answer:\nQ2: What do you think other people will answer:\n\n.\n\nIn the circle example, the possible feedback can be \u201canswer: 4; prediction: 3\u201d, \u201canswer: 3; prediction:\n6,9,1\u201d... We collect all answers provided by the respondents 11 and denote the set of them by A. In\nthe circle example, A = {1, 2, 3, 4, 6, 9}. We also allow respondents to provide no prediction or\nmultiple predictions.\n\n9Rank 1 answer is in the highest position.\n10See definition in the last paragraph in Section 2.1.\n11In practice, we set a threshold \u03b8 and collect answers which are provided by at least \u03b8 fraction of the\nrespondents. We allow multiple predictions and also allow people to answer \u201cI do not know\u201d. See Section 3 for\nmore details.\n\n7\n\n\f(a) Accuracy of algorithms\n\n(b) Empirical distribution of lack-of-fit\n\nFigure 3: The results of our experiment\n\nAnswer-prediction matrix We aggregate the feedback and visualize it by an Answer-Prediction\nmatrix. The Answer-Prediction matrix A is a |A| \u00d7 |A| square matrix where |A| is the number of\ndistinct answers provided by the respondents. Each entry Aa,g, a, g \u2208 A is the number of respondents\nthat answer \u201ca\u201d and predict \u201cg\u201d.\n\nWe will show that with proper assumptions, the answer-prediction matrix A\u2019s expectation is pro-\nportional to M. First, for ease of analysis, we assume that each respondent\u2019s predictions are i.i.d.\nsamples12. Second, since we allow people to optionally provide predictions, we need to additionally\nassume that the number of predictions each respondent is willing to provide is independent of her\ntype and answer. We state the formal result as follows and the proof is deferred to Appendix C.\n\nProposition 2.10. When each respondent\u2019s predictions are i.i.d. samples, and the number of\npredictions she gives is independent of her type and answer, the answer-prediction matrix A\u2019s\nexpectation is proportional to M.\n\n\nBased on this section, do the authors include complete proofs for all their theoretical results?"}]], "q3a": [[{"role": "user", "content": "The following is the experiments section of the paper you are reviewing:\n\n\nWe conduct four studies, study 1 (35 math problems), study 2 (30 Go problems), study 3 (44 general\nknowledge questions), and study 4 (43 Chinese character pronunciation questions).\n\nData collection All studies are performed by online questionnaires. We recruit the respondents by\nan online announcement13 or from an online platform that is similar to Amazon Mechanical Turk. We\nget respondents\u2019 consent for using and sharing their data for research. Respondents are asked not to\nsearch for the answers to the questions or communicate with other people. We allow the respondents\nto answer \u201cI do not know\u201d for all questions. Except for Go problems, all questionnaires use flat\npayment. We illustrate the data collection process in detail in Appendix A. We allow respondents\nto participate in more than one study because our algorithms analyze each question separately and\nindependently.\n\nData processing We merge answers which are the same, like \u20180.5\u2019 and \u201850%\u2019. We omit the answers\nthat are reported by less than (\u2264) 3% of respondents or one person. The remaining answers, excluding\n\u201cI do not know\u201d, form the answer set A whose size is |A|. We then construct the Answer-Prediction\nmatrix and perform our algorithms. Pseudo-codes are attached in Appendix B. Our algorithms do not\nrequire any prior or the respondents\u2019 expertise levels.\n\n3.1 Results\n\n12This may not be a very good assumption since i.i.d. samples can repeat but respondents usually do not\nrepeat their predictions. If we do not want this assumption, we can choose to only use the first prediction from\neach respondent (if there exists) to construct the answer-prediction matrix.\n\n13Many are students from top universities in China. See Appendix A for more details.\n\n8\n\n\fType\nMath\nGo\nGeneral knowledge\nChinese character\n\nTotal Our algorithm(Default) Our algorithm(Variant)\n\n35\n30\n44\n43\n\n29\n28\n41\n36\n\n29\n28\n41\n35\n\nPlurality voting\n24\n23\n35\n34\n\nTable 1: The number of questions our algorithms/baseline are correct.\n\n(a) the Monty Hall problem: you can select one\nclosed door of three. A prize, a car, is behind\none of the doors. The other two doors hide goats.\nAfter you have made your choice, Monty Hall\nwill open one of the remaining doors and show\nthat it does not contain the prize. He then asks\nyou if you would like to switch your choice to\nthe other unopened door. What is the probability\nto get the prize if you switch?\n\n(b) the Taxicab problem: 85% of taxis in this\ncity are green, the others are blue. A witness\nsees a blue taxi. She is usually correct with\nprobability 80%. What is the probability that\nthe taxi saw by the witness is blue?\n\n(c) Pick a move for black such that they can be\nalive.\n\n(d) Pick a move for black such that they can be\nalive by ko.\n\n(e) the boundary question: what river forms the\nboundary between North Korea and China?\n\n(f) the Middle Age New Year question: when\nwas the new year in middle age?\n\n(g) the pronunciation of \u7762\n\n(h) the pronunciation of \u6ec2\n\nFigure 4: The ranked answer-prediction matrices\n\n9\n\n2/31/21/32/31/21/3Answer34261413517146Prediction415080121520415080121520Answer160500008201200801619501104818000522130010218PredictionD3C1B2C2B1D3C1B2C2B1Answer62131091410042012115300002PredictionB8A8B6B7B8A8B6B7Answer112540630001230035PredictionYaluandDoomanYaluSonghuaYaluandDoomanYaluSonghuaAnswer33007410005PredictionApr1Dec25Jan1Apr1Dec25Jan1Answer15410028180633Predictionsui1ju1zhi4zhui1sui1ju1zhi4zhui1Answer115023161300210107Predictionpang1pang2pang1pang2Answer1511216Prediction\fWe compare our approach to the baseline, the plurality voting, regarding the accuracy of the top-\nranking answers. Both of the algorithms beat plurality voting for all studies and the default is slightly\nbetter. Among all 152 questions, in 138 questions, the variant algorithm outputs the same hierarchy as\nthe default algorithm. For other questions, the top-ranking type of the variant algorithm may contain\nmore than one answer. The top-ranking answer is the answer supported by more people among all\nanswers in the top-ranking type. In one question the variant is wrong but the default is correct, the\nvariant algorithm assigns both the correct answer and the incorrect plurality answer to the top-ranking\ntype, thus outputting the incorrect answer as the top-ranking answer.\n\nWe also compute the lack-of-fit index (Definition 2.6) of the algorithms and find that the questions\nthe algorithm outputs the correct answer have a smaller lack-of-fit thus fitting the thinking hierarchy\nmodel better. Therefore, we can use the lack-of-fit index as a reliability index of the algorithms.\n\nWe additionally pick several representative examples for each study (Figure 4) where the matrices\nare ranked by the default algorithm and the diagonal area modified like Example 1.1. In all of these\nexamples, the plurality is incorrect while our approach is correct. Results of other questions are\nillustrated at https://elicitation.info/classroom/1/. Detailed explanations are illustrated\nin Appendix D and here we provide some highlights. First, our approach elicits a rich hierarchy. For\nexample, the taxicab problem is borrowed from Kahneman [8] and previous studies show that people\nusually ignore the base rate and report \u201880%\u2019. The imagined levels can be 41%\u219280%. We elicit a\nmuch richer level \u201c41%\u219250%\u219280%\u219212%\u219215%\u219220%\u201d. Second, the most sophisticated level\nmay fail to predict the least one. In the Taxicab problem, the correct \u201c41%\u201d supporters successfully\npredict the common wrong answer \u201c80%\u201d. However, they fail to predict the surprisingly wrong\nanswers \u201c12%,20%\u201d, which are in contrast successfully predicted by \u201c80%\u201d supporters. Our model\nis sufficiently general to allow this situation. Third, even for problems (like Go) without famous\nmistakes, our approach still works. Moreover, in the boundary question, we identify the correct\nanswer without any prior when only 3 respondents are correct.\n\n\nBased on this section, do the authors include code, data, and instructions needed to reproduce the main experimental results, or provide them in a URL?"}]], "q3b": [[{"role": "user", "content": "The following is the experiments section of the paper you are reviewing:\n\n\nWe conduct four studies, study 1 (35 math problems), study 2 (30 Go problems), study 3 (44 general\nknowledge questions), and study 4 (43 Chinese character pronunciation questions).\n\nData collection All studies are performed by online questionnaires. We recruit the respondents by\nan online announcement13 or from an online platform that is similar to Amazon Mechanical Turk. We\nget respondents\u2019 consent for using and sharing their data for research. Respondents are asked not to\nsearch for the answers to the questions or communicate with other people. We allow the respondents\nto answer \u201cI do not know\u201d for all questions. Except for Go problems, all questionnaires use flat\npayment. We illustrate the data collection process in detail in Appendix A. We allow respondents\nto participate in more than one study because our algorithms analyze each question separately and\nindependently.\n\nData processing We merge answers which are the same, like \u20180.5\u2019 and \u201850%\u2019. We omit the answers\nthat are reported by less than (\u2264) 3% of respondents or one person. The remaining answers, excluding\n\u201cI do not know\u201d, form the answer set A whose size is |A|. We then construct the Answer-Prediction\nmatrix and perform our algorithms. Pseudo-codes are attached in Appendix B. Our algorithms do not\nrequire any prior or the respondents\u2019 expertise levels.\n\n3.1 Results\n\n12This may not be a very good assumption since i.i.d. samples can repeat but respondents usually do not\nrepeat their predictions. If we do not want this assumption, we can choose to only use the first prediction from\neach respondent (if there exists) to construct the answer-prediction matrix.\n\n13Many are students from top universities in China. See Appendix A for more details.\n\n8\n\n\fType\nMath\nGo\nGeneral knowledge\nChinese character\n\nTotal Our algorithm(Default) Our algorithm(Variant)\n\n35\n30\n44\n43\n\n29\n28\n41\n36\n\n29\n28\n41\n35\n\nPlurality voting\n24\n23\n35\n34\n\nTable 1: The number of questions our algorithms/baseline are correct.\n\n(a) the Monty Hall problem: you can select one\nclosed door of three. A prize, a car, is behind\none of the doors. The other two doors hide goats.\nAfter you have made your choice, Monty Hall\nwill open one of the remaining doors and show\nthat it does not contain the prize. He then asks\nyou if you would like to switch your choice to\nthe other unopened door. What is the probability\nto get the prize if you switch?\n\n(b) the Taxicab problem: 85% of taxis in this\ncity are green, the others are blue. A witness\nsees a blue taxi. She is usually correct with\nprobability 80%. What is the probability that\nthe taxi saw by the witness is blue?\n\n(c) Pick a move for black such that they can be\nalive.\n\n(d) Pick a move for black such that they can be\nalive by ko.\n\n(e) the boundary question: what river forms the\nboundary between North Korea and China?\n\n(f) the Middle Age New Year question: when\nwas the new year in middle age?\n\n(g) the pronunciation of \u7762\n\n(h) the pronunciation of \u6ec2\n\nFigure 4: The ranked answer-prediction matrices\n\n9\n\n2/31/21/32/31/21/3Answer34261413517146Prediction415080121520415080121520Answer160500008201200801619501104818000522130010218PredictionD3C1B2C2B1D3C1B2C2B1Answer62131091410042012115300002PredictionB8A8B6B7B8A8B6B7Answer112540630001230035PredictionYaluandDoomanYaluSonghuaYaluandDoomanYaluSonghuaAnswer33007410005PredictionApr1Dec25Jan1Apr1Dec25Jan1Answer15410028180633Predictionsui1ju1zhi4zhui1sui1ju1zhi4zhui1Answer115023161300210107Predictionpang1pang2pang1pang2Answer1511216Prediction\fWe compare our approach to the baseline, the plurality voting, regarding the accuracy of the top-\nranking answers. Both of the algorithms beat plurality voting for all studies and the default is slightly\nbetter. Among all 152 questions, in 138 questions, the variant algorithm outputs the same hierarchy as\nthe default algorithm. For other questions, the top-ranking type of the variant algorithm may contain\nmore than one answer. The top-ranking answer is the answer supported by more people among all\nanswers in the top-ranking type. In one question the variant is wrong but the default is correct, the\nvariant algorithm assigns both the correct answer and the incorrect plurality answer to the top-ranking\ntype, thus outputting the incorrect answer as the top-ranking answer.\n\nWe also compute the lack-of-fit index (Definition 2.6) of the algorithms and find that the questions\nthe algorithm outputs the correct answer have a smaller lack-of-fit thus fitting the thinking hierarchy\nmodel better. Therefore, we can use the lack-of-fit index as a reliability index of the algorithms.\n\nWe additionally pick several representative examples for each study (Figure 4) where the matrices\nare ranked by the default algorithm and the diagonal area modified like Example 1.1. In all of these\nexamples, the plurality is incorrect while our approach is correct. Results of other questions are\nillustrated at https://elicitation.info/classroom/1/. Detailed explanations are illustrated\nin Appendix D and here we provide some highlights. First, our approach elicits a rich hierarchy. For\nexample, the taxicab problem is borrowed from Kahneman [8] and previous studies show that people\nusually ignore the base rate and report \u201880%\u2019. The imagined levels can be 41%\u219280%. We elicit a\nmuch richer level \u201c41%\u219250%\u219280%\u219212%\u219215%\u219220%\u201d. Second, the most sophisticated level\nmay fail to predict the least one. In the Taxicab problem, the correct \u201c41%\u201d supporters successfully\npredict the common wrong answer \u201c80%\u201d. However, they fail to predict the surprisingly wrong\nanswers \u201c12%,20%\u201d, which are in contrast successfully predicted by \u201c80%\u201d supporters. Our model\nis sufficiently general to allow this situation. Third, even for problems (like Go) without famous\nmistakes, our approach still works. Moreover, in the boundary question, we identify the correct\nanswer without any prior when only 3 respondents are correct.\n\nBased on this section, do the authors specify all training details (e.g., data splits, hyperparameters, how they were chosen) for their results?"}]], "q3c": [[{"role": "user", "content": "The following is the experiments section of the paper you are reviewing:\n\n\nWe conduct four studies, study 1 (35 math problems), study 2 (30 Go problems), study 3 (44 general\nknowledge questions), and study 4 (43 Chinese character pronunciation questions).\n\nData collection All studies are performed by online questionnaires. We recruit the respondents by\nan online announcement13 or from an online platform that is similar to Amazon Mechanical Turk. We\nget respondents\u2019 consent for using and sharing their data for research. Respondents are asked not to\nsearch for the answers to the questions or communicate with other people. We allow the respondents\nto answer \u201cI do not know\u201d for all questions. Except for Go problems, all questionnaires use flat\npayment. We illustrate the data collection process in detail in Appendix A. We allow respondents\nto participate in more than one study because our algorithms analyze each question separately and\nindependently.\n\nData processing We merge answers which are the same, like \u20180.5\u2019 and \u201850%\u2019. We omit the answers\nthat are reported by less than (\u2264) 3% of respondents or one person. The remaining answers, excluding\n\u201cI do not know\u201d, form the answer set A whose size is |A|. We then construct the Answer-Prediction\nmatrix and perform our algorithms. Pseudo-codes are attached in Appendix B. Our algorithms do not\nrequire any prior or the respondents\u2019 expertise levels.\n\n3.1 Results\n\n12This may not be a very good assumption since i.i.d. samples can repeat but respondents usually do not\nrepeat their predictions. If we do not want this assumption, we can choose to only use the first prediction from\neach respondent (if there exists) to construct the answer-prediction matrix.\n\n13Many are students from top universities in China. See Appendix A for more details.\n\n8\n\n\fType\nMath\nGo\nGeneral knowledge\nChinese character\n\nTotal Our algorithm(Default) Our algorithm(Variant)\n\n35\n30\n44\n43\n\n29\n28\n41\n36\n\n29\n28\n41\n35\n\nPlurality voting\n24\n23\n35\n34\n\nTable 1: The number of questions our algorithms/baseline are correct.\n\n(a) the Monty Hall problem: you can select one\nclosed door of three. A prize, a car, is behind\none of the doors. The other two doors hide goats.\nAfter you have made your choice, Monty Hall\nwill open one of the remaining doors and show\nthat it does not contain the prize. He then asks\nyou if you would like to switch your choice to\nthe other unopened door. What is the probability\nto get the prize if you switch?\n\n(b) the Taxicab problem: 85% of taxis in this\ncity are green, the others are blue. A witness\nsees a blue taxi. She is usually correct with\nprobability 80%. What is the probability that\nthe taxi saw by the witness is blue?\n\n(c) Pick a move for black such that they can be\nalive.\n\n(d) Pick a move for black such that they can be\nalive by ko.\n\n(e) the boundary question: what river forms the\nboundary between North Korea and China?\n\n(f) the Middle Age New Year question: when\nwas the new year in middle age?\n\n(g) the pronunciation of \u7762\n\n(h) the pronunciation of \u6ec2\n\nFigure 4: The ranked answer-prediction matrices\n\n9\n\n2/31/21/32/31/21/3Answer34261413517146Prediction415080121520415080121520Answer160500008201200801619501104818000522130010218PredictionD3C1B2C2B1D3C1B2C2B1Answer62131091410042012115300002PredictionB8A8B6B7B8A8B6B7Answer112540630001230035PredictionYaluandDoomanYaluSonghuaYaluandDoomanYaluSonghuaAnswer33007410005PredictionApr1Dec25Jan1Apr1Dec25Jan1Answer15410028180633Predictionsui1ju1zhi4zhui1sui1ju1zhi4zhui1Answer115023161300210107Predictionpang1pang2pang1pang2Answer1511216Prediction\fWe compare our approach to the baseline, the plurality voting, regarding the accuracy of the top-\nranking answers. Both of the algorithms beat plurality voting for all studies and the default is slightly\nbetter. Among all 152 questions, in 138 questions, the variant algorithm outputs the same hierarchy as\nthe default algorithm. For other questions, the top-ranking type of the variant algorithm may contain\nmore than one answer. The top-ranking answer is the answer supported by more people among all\nanswers in the top-ranking type. In one question the variant is wrong but the default is correct, the\nvariant algorithm assigns both the correct answer and the incorrect plurality answer to the top-ranking\ntype, thus outputting the incorrect answer as the top-ranking answer.\n\nWe also compute the lack-of-fit index (Definition 2.6) of the algorithms and find that the questions\nthe algorithm outputs the correct answer have a smaller lack-of-fit thus fitting the thinking hierarchy\nmodel better. Therefore, we can use the lack-of-fit index as a reliability index of the algorithms.\n\nWe additionally pick several representative examples for each study (Figure 4) where the matrices\nare ranked by the default algorithm and the diagonal area modified like Example 1.1. In all of these\nexamples, the plurality is incorrect while our approach is correct. Results of other questions are\nillustrated at https://elicitation.info/classroom/1/. Detailed explanations are illustrated\nin Appendix D and here we provide some highlights. First, our approach elicits a rich hierarchy. For\nexample, the taxicab problem is borrowed from Kahneman [8] and previous studies show that people\nusually ignore the base rate and report \u201880%\u2019. The imagined levels can be 41%\u219280%. We elicit a\nmuch richer level \u201c41%\u219250%\u219280%\u219212%\u219215%\u219220%\u201d. Second, the most sophisticated level\nmay fail to predict the least one. In the Taxicab problem, the correct \u201c41%\u201d supporters successfully\npredict the common wrong answer \u201c80%\u201d. However, they fail to predict the surprisingly wrong\nanswers \u201c12%,20%\u201d, which are in contrast successfully predicted by \u201c80%\u201d supporters. Our model\nis sufficiently general to allow this situation. Third, even for problems (like Go) without famous\nmistakes, our approach still works. Moreover, in the boundary question, we identify the correct\nanswer without any prior when only 3 respondents are correct.\n\nBased on this section, do the authors report error bars (e.g., with respect to the random seed after running experiments multiple times)?"}]], "q3d": [[{"role": "user", "content": "The following is the experiments section of the paper you are reviewing:\n\n\nWe conduct four studies, study 1 (35 math problems), study 2 (30 Go problems), study 3 (44 general\nknowledge questions), and study 4 (43 Chinese character pronunciation questions).\n\nData collection All studies are performed by online questionnaires. We recruit the respondents by\nan online announcement13 or from an online platform that is similar to Amazon Mechanical Turk. We\nget respondents\u2019 consent for using and sharing their data for research. Respondents are asked not to\nsearch for the answers to the questions or communicate with other people. We allow the respondents\nto answer \u201cI do not know\u201d for all questions. Except for Go problems, all questionnaires use flat\npayment. We illustrate the data collection process in detail in Appendix A. We allow respondents\nto participate in more than one study because our algorithms analyze each question separately and\nindependently.\n\nData processing We merge answers which are the same, like \u20180.5\u2019 and \u201850%\u2019. We omit the answers\nthat are reported by less than (\u2264) 3% of respondents or one person. The remaining answers, excluding\n\u201cI do not know\u201d, form the answer set A whose size is |A|. We then construct the Answer-Prediction\nmatrix and perform our algorithms. Pseudo-codes are attached in Appendix B. Our algorithms do not\nrequire any prior or the respondents\u2019 expertise levels.\n\n3.1 Results\n\n12This may not be a very good assumption since i.i.d. samples can repeat but respondents usually do not\nrepeat their predictions. If we do not want this assumption, we can choose to only use the first prediction from\neach respondent (if there exists) to construct the answer-prediction matrix.\n\n13Many are students from top universities in China. See Appendix A for more details.\n\n8\n\n\fType\nMath\nGo\nGeneral knowledge\nChinese character\n\nTotal Our algorithm(Default) Our algorithm(Variant)\n\n35\n30\n44\n43\n\n29\n28\n41\n36\n\n29\n28\n41\n35\n\nPlurality voting\n24\n23\n35\n34\n\nTable 1: The number of questions our algorithms/baseline are correct.\n\n(a) the Monty Hall problem: you can select one\nclosed door of three. A prize, a car, is behind\none of the doors. The other two doors hide goats.\nAfter you have made your choice, Monty Hall\nwill open one of the remaining doors and show\nthat it does not contain the prize. He then asks\nyou if you would like to switch your choice to\nthe other unopened door. What is the probability\nto get the prize if you switch?\n\n(b) the Taxicab problem: 85% of taxis in this\ncity are green, the others are blue. A witness\nsees a blue taxi. She is usually correct with\nprobability 80%. What is the probability that\nthe taxi saw by the witness is blue?\n\n(c) Pick a move for black such that they can be\nalive.\n\n(d) Pick a move for black such that they can be\nalive by ko.\n\n(e) the boundary question: what river forms the\nboundary between North Korea and China?\n\n(f) the Middle Age New Year question: when\nwas the new year in middle age?\n\n(g) the pronunciation of \u7762\n\n(h) the pronunciation of \u6ec2\n\nFigure 4: The ranked answer-prediction matrices\n\n9\n\n2/31/21/32/31/21/3Answer34261413517146Prediction415080121520415080121520Answer160500008201200801619501104818000522130010218PredictionD3C1B2C2B1D3C1B2C2B1Answer62131091410042012115300002PredictionB8A8B6B7B8A8B6B7Answer112540630001230035PredictionYaluandDoomanYaluSonghuaYaluandDoomanYaluSonghuaAnswer33007410005PredictionApr1Dec25Jan1Apr1Dec25Jan1Answer15410028180633Predictionsui1ju1zhi4zhui1sui1ju1zhi4zhui1Answer115023161300210107Predictionpang1pang2pang1pang2Answer1511216Prediction\fWe compare our approach to the baseline, the plurality voting, regarding the accuracy of the top-\nranking answers. Both of the algorithms beat plurality voting for all studies and the default is slightly\nbetter. Among all 152 questions, in 138 questions, the variant algorithm outputs the same hierarchy as\nthe default algorithm. For other questions, the top-ranking type of the variant algorithm may contain\nmore than one answer. The top-ranking answer is the answer supported by more people among all\nanswers in the top-ranking type. In one question the variant is wrong but the default is correct, the\nvariant algorithm assigns both the correct answer and the incorrect plurality answer to the top-ranking\ntype, thus outputting the incorrect answer as the top-ranking answer.\n\nWe also compute the lack-of-fit index (Definition 2.6) of the algorithms and find that the questions\nthe algorithm outputs the correct answer have a smaller lack-of-fit thus fitting the thinking hierarchy\nmodel better. Therefore, we can use the lack-of-fit index as a reliability index of the algorithms.\n\nWe additionally pick several representative examples for each study (Figure 4) where the matrices\nare ranked by the default algorithm and the diagonal area modified like Example 1.1. In all of these\nexamples, the plurality is incorrect while our approach is correct. Results of other questions are\nillustrated at https://elicitation.info/classroom/1/. Detailed explanations are illustrated\nin Appendix D and here we provide some highlights. First, our approach elicits a rich hierarchy. For\nexample, the taxicab problem is borrowed from Kahneman [8] and previous studies show that people\nusually ignore the base rate and report \u201880%\u2019. The imagined levels can be 41%\u219280%. We elicit a\nmuch richer level \u201c41%\u219250%\u219280%\u219212%\u219215%\u219220%\u201d. Second, the most sophisticated level\nmay fail to predict the least one. In the Taxicab problem, the correct \u201c41%\u201d supporters successfully\npredict the common wrong answer \u201c80%\u201d. However, they fail to predict the surprisingly wrong\nanswers \u201c12%,20%\u201d, which are in contrast successfully predicted by \u201c80%\u201d supporters. Our model\nis sufficiently general to allow this situation. Third, even for problems (like Go) without famous\nmistakes, our approach still works. Moreover, in the boundary question, we identify the correct\nanswer without any prior when only 3 respondents are correct.\n\nBased on this section, do the authors include the amount of compute and type of resources used (e.g., type of GPUs, internal cluster, or cloud provider)?"}]], "q4a": [[{"role": "user", "content": "The following is the methods section of the paper you are reviewing:\n\n\nWe first introduce our model for thinking hierarchy. Tversky and Kahneman [27] propose that people\nhave two systems, a fast and intuitive system, and a slow and logical system. For example, Alice\nstarts to solve the circle problem. When she reads the question, she can run her intuitive system 1 and\nobtain answer \u201c3\u201d. However, when she starts to think carefully, she runs her more careful system 2 to\nobtain answer \u201c4\u201d.\n\nWe propose a more general model where people can run multiple oracles to approach the question\nduring their thinking process. Conceptually similar to the cognitive hierarchy theory [3], we assume\nthe oracles have different levels. People usually run lower level oracles before the higher level oracles.\nIn the previous example, system 1 is the lower level oracle and system 2 is the higher one. We assume\nthat Alice runs system 2 after system 1. Thus, in our model, people who know the answer is \u201c4\u201d can\n\n4\n\n\fpredict that other people answer \u201c3\u201d, which is hypothesized in Kong and Schoenebeck [10]. It\u2019s also\npossible that some people run the most sophisticated oracle directly without running the lower ones.\nThus, we design the model such that it does not require the higher-type to be able to predict ALL\nlower types. We also allow the oracles\u2019 outputs to be random.\n\nSection 2.1 introduces the model of thinking hierarchy. Section 2.2 reduces the model inference\nproblem to a matrix decomposition problem. Section 2.3 and section 2.4 show how to learn the\nthinking hierarchy.\n\n2.1 Thinking hierarchy\n\nFixing a question q (e.g. the circle problem), T denotes the set of thinking types. A denotes the set of\npossible answers. Both T and A are finite sets. \u2206A denotes all possible distributions over A. We\nsometimes use \u201cprob\u201d as a shorthand for probability.\n\nGenerating answers We will describe how people of different thinking types generate answers.\nDefinition 2.1 (Oracles of thinking types W ). An answer generating oracle maps the question to\nan (random) answer in A. Each type t corresponds to an oracle Ot. The output Ot(q) is a random\nvariable whose distribution is wt \u2208 \u2206A. W denotes a |T | \u00d7 |A| matrix where each row t is wt.\nEach respondent is type t with prob pt and (cid:80)\nrunning the oracle Ot. For all a \u2208 A, the probability that a respondent answers a will be (cid:80)\nWe assume the probability is positive for all a \u2208 A.\nExample 2.2 (A running example). There are two types T = {0, 1}. The answer space is A =\n{3, 4, 6}. O0 will output \u20183\u2019 with probability 0.8 and \u20186\u2019 with probability 0.2. O1 will output \u20184\u2019\n0\n1\n\nt pt = 1. A type t respondent generates the answer by\nt ptwt(a).\n\ndeterministically. In this example, W =\n\nwhere the first row is the distribution of\n\n(cid:20)0.8\n0\n\n0.2\n0\n\n(cid:21)\n\nO0\u2019s output and the second row is the distribution of O1\u2019s output.\n\nGenerating predictions We then describe how people of different thinking types predict what other\npeople will answer. Here the prediction is not a distribution, but an answer other people may report.\nWhen a type t respondent makes a prediction, she will run an oracle, which is Ot\u2032 with probability\npt\u2192t\u2032 where (cid:80)\n\nt\u2032 pt\u2192t\u2032 = 1. She uses the output of Ot\u2032 as the prediction g \u2208 A.\n\nCombination: answer-prediction joint distribution M M denotes a |A| \u00d7 |A| matrix where\nMa,g is the probability an respondent answers a and predicts g. \u039b denotes a |T | \u00d7 |T | matrix where\n\u039bt,t\u2032 = ptpt\u2192t\u2032 is the probability a respondent is type t and predicts type t\u2032.\nExample 2.3. In this example, when type 0 respondent makes a prediction, with prob 1, she will run\nO0 again. When type 1 respondent makes a prediction, with prob 0.5, she will run O1 again, with\nprob 0.5, she will run O0. Moreover, a respondent is type 0 with prob 0.7, and type 1 with prob 0.3.\n\nHere \u039b =\n\n(cid:20)p0p0\u21920\np1p1\u21920\n\np0p0\u21921\np1p1\u21921\n\n(cid:21)\n\n=\n\n(cid:20) 0.7 \u2217 1\n0.3 \u2217 0.5\n\n0.7 \u2217 0\n0.3 \u2217 0.5\n\n(cid:21)\n\n=\n\n(cid:20) 0.7\n0.15\n\n(cid:21)\n\n.\n\n0\n0.15\n\nClaim 2.4. Based on the above generating processes, M = W\u22a4\u039bW.\n\nProof. For each respondent, the probability she answers a and predicts g will be\n(cid:88)\n\n(cid:88)\n\n(cid:88)\n\nptwt(a)\n\npt\u2192t\u2032wt\u2032(g) =\n\nwt(a)ptpt\u2192t\u2032wt\u2032(g).\n\nMa,g =\n\nt\n\nt\u2032\n\nt,t\u2032\n\nWe sum over all possible types the respondent will be. Given she is type t, she runs oracle Ot to\ngenerate the answer and wt(a) is the probability that the answer is a. We sum over all possible\noracles she runs to predict. Given that she runs Ot\u2032, wt\u2032(g) is the probability the prediction is g.\n\nKey assumption: upper-triangular \u039b we assume that people of a less sophisticated level can\nnever run the oracles of more sophisticated levels. A linear ordering of types \u03c0 : {1, 2, \u00b7 \u00b7 \u00b7 , |T |} (cid:55)\u2192 T\nmaps a ranking position to a type. For example, \u03c0(1) \u2208 T is the top-ranking type.\nAssumption 2.5. We assume that with a proper ordering \u03c0 of the types, \u039b is an upper-triangular\nmatrix. Formally, there exists \u03c0 such that \u2200i > j, \u039b\u03c0(i),\u03c0(j) = 0. Any \u03c0 that makes \u039b upper-\ntriangular is a valid thinking hierarchy of the types.\n\n5\n\n\fIn the running example, the valid thinking hierarchy is \u03c0(1) = type 1, \u03c0(2) = type 0. Note that the\nabove assumption does not require \u2200i \u2264 j, \u039b\u03c0(i),\u03c0(j) > 0. When \u039b is a diagonal matrix, types cannot\npredict each other and are equally sophisticated, thus any ordering is a valid thinking hierarchy.\n\nAn algorithm finds the thinking hierarchy when the algorithm is given M which is generated by\nlatent (unknown) W, \u039b, and the algorithm will output a matrix W\u2217 which equals a row-permuted\nW where the row order is a valid thinking hierarchy. Formally, there exists a valid thinking hierarchy\n\u03c0 such that the ith row of W\u2217 is the \u03c0(i)th row of W, i.e, w\u2217\n\ni = w\u03c0(i).\n\n2.2 Non-negative Congruence Triangularization (NCT)\n\nWith the above model, inferring thinking hierarchy leads to a novel matrix decomposition problem,\nwhich is similar to the symmetric non-negative matrix factorization problem (NMF)6. A non-negative\nmatrix is a matrix whose elements are non-negative.\nDefinition 2.6 (Non-negative Congruence7 Triangularization (NCT)). Given a non-negative matrix\nM, NCT aims to find non-negative matrices W and non-negative upper-triangular matrix \u039b such\nthat M = W\u22a4\u039bW. In a Frobenius norm based approximated version, given a set of matrices W,\nNCT aims to find non-negative matrices W and non-negative upper-triangular matrix \u039b to minimize\n\nmin\nW\u2208W,\u039b\n\n||M \u2212 W\u22a4\u039bW||2\nF\n\nand the minimum is defined as the lack-of-fit of M regarding W 8.\n\nLike NMF, it\u2019s impossible to ask for the strict uniqueness of the results. Let P\u039b be the set of\npermutation matrices such that \u03a0\u22a4\u039b\u03a0 is still upper-triangular.\nIf (W, \u039b) is a solution, then\n(\u03a0\u22121DW, \u03a0\u22a4D\u22121\u039bD\u22121\u03a0) is also a solution where D is a diagonal matrix with positive elements\nand \u03a0 \u2208 P\u039b. We state the uniqueness results as follows and the proof is deferred to Appendix C.\nProposition 2.7 (Uniqueness). If |T | \u2264 |A| and T columns of W consist of a permuted positive\ndiagonal matrix, NCT for M = W\u22a4\u039bW is unique in the sense that then for all W\u2032\u22a4\u039b\u2032W\u2032 =\nW\u22a4\u039bW, there exists a positive diagonal matrix D and a |T | \u00d7 |T | permutation matrix \u03a0 \u2208 P\u039b\nsuch that W\u2032 = \u03a0\u22121DW.\n\nWhen we restrict W to be \u201csemi-orthogonal\u201d, we obtain a clean format of NCT without searching\nfor optimal \u039b. I is the set of all \u201csemi-orthogonal\u201d matrices W where each column of W has\nand only has one non-zero element and WW\u22a4 = I. For example, the W in Example 2.2 can be\nnormalized to a semi-orthogonal matrix. The following lemma follows from the expansion of the\nFrobenius norm and we defer the proof to Appendix C.\n\nLemma 2.8 (Semi-orthogonal: minimizing F-norm = maximizing upper-triangular\u2019s sum of the\nsquare). For all set of matrices W \u2282 I, minW\u2208W,\u039b ||M \u2212 W\u22a4\u039bW||2\nF is equivalent to solv-\ni,j and setting \u039b as Up(WMW\u22a4), the upper-triangular area of\ning maxW\u2208W\nWMW\u22a4.\n\ni\u2264j(WMW\u22a4)2\n\n(cid:80)\n\n2.3\n\nInferring the thinking hierarchy with answer-prediction joint distribution M\n\nGiven M, inferring the thinking hierarchy is equivalent to solving NCT in general. Though we do not\nhave M, later we will show a proxy for M. For simplicity of practical use, we introduce two simple\nranking algorithms by employing Lemma 2.8. The ranking algorithm takes M as input and outputs a\nlinear ordering of answers \u03c0 : {1, 2, \u00b7 \u00b7 \u00b7 , |A|} (cid:55)\u2192 A that maps a ranking position to an answer.\n\nAnswer-Ranking Algorithm (Default) AR(M) The algorithm computes\n\n\u03a0\u2217 \u2190 arg max\n\u03a0\u2208P\n\n(cid:88)\n\n(\u03a0M\u03a0\u22a4)2\ni,j\n\ni\u2264j\n\n6Symmetric NMF: minW ||M \u2212 W\u22a4W||2\nF\n7We use congruence here though it is not matrix congruence since W may not be a square matrix.\n8M = W\u22a4\u039bW, W \u2208 W has zero lack-of-fit.\n\n6\n\n\fwhere P is the set of all |A| \u00d7 |A| permutation matrices. There is a one to one mapping between each\npermutation matrix \u03a0 and a linear ordering \u03c0: \u03a0i,\u03c0(i) = 1, \u2200i. Therefore, the optimal \u03a0\u2217 leads to an\noptimal rank over answers directly and the default algorithm can be also represented as\n\n\u03c0\u2217 \u2190 arg max\n\n\u03c0\n\nM 2\n\n\u03c0(i),\u03c0(j).\n\n(cid:88)\n\ni\u2264j\n\nTo find the optimal rank, we use a dynamic programming based algorithm (see the supplementary\nmaterials) which takes O(2|A||A|2). In practice, |A| is usually at most 7 or 8. In our empirical study,\nthe default algorithm takes 91 milliseconds to finish the computation of all 152 questions.\n\nThe default algorithm implicitly assumes |T | = |A| and all oracles are deterministic. To allow\n|T | < |A| and non-deterministic oracles, we introduce a variant that generalizes P to a subset of\nsemi-orthogonal matrices I. Every |T | \u00d7 |A| semi-orthogonal W indicates a hard clustering. Each\ncluster t \u2208 T contains all answers a such that Wt,a > 0. For example, the W in Example 2.2 can\nbe normalized to a semi-orthogonal matrix and indicates a hard clustering {4}, {6, 3}. Therefore,\nthe variant algorithm will partition the answers into multiple clusters and assign a hierarchy to the\nclusters.\n\nAnswer-Ranking Algorithm (Variant) AR+(M, W) The algorithm computes\n\nW\u2217 \u2190 arg max\nW\u2208I\n\n(cid:88)\n\n(WMW\u22a4)2\n\ni,j.\n\ni\u2264j\n\nwhere W \u2282 I. W\u2217 is normalized such that every row sums to 1. This algorithm does not restrict\n|T | = |A| and learns the optimal |T |.\n\nW\u2217 \u21d2 Answer rank The output W\u2217 indicates a hard clustering of all answers. We rank all answer\nas follows: for any i < j, the answers in cluster i has a higher rank 9 than the answers in cluster j.\nFor all i, for any two answers a, a\u2032 in the same cluster i, a is ranked higher than a\u2032 if W \u2217\ni,a\u2032.\n\ni,a > W \u2217\n\nTheoretical justification When M perfectly fits the model with the restriction that the latent W is\na permutation or a hard clustering, we show that our algorithm finds the thinking hierarchy. Otherwise,\nour algorithm finds the \u201cclosest\u201d solution measured by the Frobenius norm.\nTheorem 2.9. When there exists \u03a00 \u2208 P and non-negative upper-triangular matrix \u039b0 such that\n0 \u039b0\u03a00, AR(M) finds the thinking hierarchy 10. In general, AR(M) will output \u03a0\u2217 where\nM = \u03a0\u22a4\n\u03a0\u2217, \u039b\u2217 = Up(\u03a0\u2217M\u03a0\u2217\u22a4) is a solution to arg min\u03a0\u2208P,\u039b ||M \u2212 \u03a0\u22a4\u039b\u03a0||2\nF . The above statement\nis still valid by replacing P by W \u2282 I and AR(M) by AR+(M, W).\n\nProposition 2.7 and Lemma 2.8 almost directly imply the above theorem. We defer the formal proof\nto Appendix C.\n\n2.4 A proxy for answer-prediction joint distribution M\n\nIn practice, we do not have perfect M. We use the following open-response paradigm to obtain a\nproxy for M.\n\nAnswer-prediction paradigm the respondents are asked Q1: What\u2019s your answer? Answer:\nQ2: What do you think other people will answer:\n\n.\n\nIn the circle example, the possible feedback can be \u201canswer: 4; prediction: 3\u201d, \u201canswer: 3; prediction:\n6,9,1\u201d... We collect all answers provided by the respondents 11 and denote the set of them by A. In\nthe circle example, A = {1, 2, 3, 4, 6, 9}. We also allow respondents to provide no prediction or\nmultiple predictions.\n\n9Rank 1 answer is in the highest position.\n10See definition in the last paragraph in Section 2.1.\n11In practice, we set a threshold \u03b8 and collect answers which are provided by at least \u03b8 fraction of the\nrespondents. We allow multiple predictions and also allow people to answer \u201cI do not know\u201d. See Section 3 for\nmore details.\n\n7\n\n\f(a) Accuracy of algorithms\n\n(b) Empirical distribution of lack-of-fit\n\nFigure 3: The results of our experiment\n\nAnswer-prediction matrix We aggregate the feedback and visualize it by an Answer-Prediction\nmatrix. The Answer-Prediction matrix A is a |A| \u00d7 |A| square matrix where |A| is the number of\ndistinct answers provided by the respondents. Each entry Aa,g, a, g \u2208 A is the number of respondents\nthat answer \u201ca\u201d and predict \u201cg\u201d.\n\nWe will show that with proper assumptions, the answer-prediction matrix A\u2019s expectation is pro-\nportional to M. First, for ease of analysis, we assume that each respondent\u2019s predictions are i.i.d.\nsamples12. Second, since we allow people to optionally provide predictions, we need to additionally\nassume that the number of predictions each respondent is willing to provide is independent of her\ntype and answer. We state the formal result as follows and the proof is deferred to Appendix C.\n\nProposition 2.10. When each respondent\u2019s predictions are i.i.d. samples, and the number of\npredictions she gives is independent of her type and answer, the answer-prediction matrix A\u2019s\nexpectation is proportional to M.\n\n\nBased on this section, do the authors use existing assets (e.g., code, data, models) that are not new contributions in their paper? Please start your answer with \"Yes\", \"No\", or \"Unsure\"."}, {"role": "user", "content": "The following is the experiments section of the paper you are reviewing:\nexperiments section: \n\nWe conduct four studies, study 1 (35 math problems), study 2 (30 Go problems), study 3 (44 general\nknowledge questions), and study 4 (43 Chinese character pronunciation questions).\n\nData collection All studies are performed by online questionnaires. We recruit the respondents by\nan online announcement13 or from an online platform that is similar to Amazon Mechanical Turk. We\nget respondents\u2019 consent for using and sharing their data for research. Respondents are asked not to\nsearch for the answers to the questions or communicate with other people. We allow the respondents\nto answer \u201cI do not know\u201d for all questions. Except for Go problems, all questionnaires use flat\npayment. We illustrate the data collection process in detail in Appendix A. We allow respondents\nto participate in more than one study because our algorithms analyze each question separately and\nindependently.\n\nData processing We merge answers which are the same, like \u20180.5\u2019 and \u201850%\u2019. We omit the answers\nthat are reported by less than (\u2264) 3% of respondents or one person. The remaining answers, excluding\n\u201cI do not know\u201d, form the answer set A whose size is |A|. We then construct the Answer-Prediction\nmatrix and perform our algorithms. Pseudo-codes are attached in Appendix B. Our algorithms do not\nrequire any prior or the respondents\u2019 expertise levels.\n\n3.1 Results\n\n12This may not be a very good assumption since i.i.d. samples can repeat but respondents usually do not\nrepeat their predictions. If we do not want this assumption, we can choose to only use the first prediction from\neach respondent (if there exists) to construct the answer-prediction matrix.\n\n13Many are students from top universities in China. See Appendix A for more details.\n\n8\n\n\fType\nMath\nGo\nGeneral knowledge\nChinese character\n\nTotal Our algorithm(Default) Our algorithm(Variant)\n\n35\n30\n44\n43\n\n29\n28\n41\n36\n\n29\n28\n41\n35\n\nPlurality voting\n24\n23\n35\n34\n\nTable 1: The number of questions our algorithms/baseline are correct.\n\n(a) the Monty Hall problem: you can select one\nclosed door of three. A prize, a car, is behind\none of the doors. The other two doors hide goats.\nAfter you have made your choice, Monty Hall\nwill open one of the remaining doors and show\nthat it does not contain the prize. He then asks\nyou if you would like to switch your choice to\nthe other unopened door. What is the probability\nto get the prize if you switch?\n\n(b) the Taxicab problem: 85% of taxis in this\ncity are green, the others are blue. A witness\nsees a blue taxi. She is usually correct with\nprobability 80%. What is the probability that\nthe taxi saw by the witness is blue?\n\n(c) Pick a move for black such that they can be\nalive.\n\n(d) Pick a move for black such that they can be\nalive by ko.\n\n(e) the boundary question: what river forms the\nboundary between North Korea and China?\n\n(f) the Middle Age New Year question: when\nwas the new year in middle age?\n\n(g) the pronunciation of \u7762\n\n(h) the pronunciation of \u6ec2\n\nFigure 4: The ranked answer-prediction matrices\n\n9\n\n2/31/21/32/31/21/3Answer34261413517146Prediction415080121520415080121520Answer160500008201200801619501104818000522130010218PredictionD3C1B2C2B1D3C1B2C2B1Answer62131091410042012115300002PredictionB8A8B6B7B8A8B6B7Answer112540630001230035PredictionYaluandDoomanYaluSonghuaYaluandDoomanYaluSonghuaAnswer33007410005PredictionApr1Dec25Jan1Apr1Dec25Jan1Answer15410028180633Predictionsui1ju1zhi4zhui1sui1ju1zhi4zhui1Answer115023161300210107Predictionpang1pang2pang1pang2Answer1511216Prediction\fWe compare our approach to the baseline, the plurality voting, regarding the accuracy of the top-\nranking answers. Both of the algorithms beat plurality voting for all studies and the default is slightly\nbetter. Among all 152 questions, in 138 questions, the variant algorithm outputs the same hierarchy as\nthe default algorithm. For other questions, the top-ranking type of the variant algorithm may contain\nmore than one answer. The top-ranking answer is the answer supported by more people among all\nanswers in the top-ranking type. In one question the variant is wrong but the default is correct, the\nvariant algorithm assigns both the correct answer and the incorrect plurality answer to the top-ranking\ntype, thus outputting the incorrect answer as the top-ranking answer.\n\nWe also compute the lack-of-fit index (Definition 2.6) of the algorithms and find that the questions\nthe algorithm outputs the correct answer have a smaller lack-of-fit thus fitting the thinking hierarchy\nmodel better. Therefore, we can use the lack-of-fit index as a reliability index of the algorithms.\n\nWe additionally pick several representative examples for each study (Figure 4) where the matrices\nare ranked by the default algorithm and the diagonal area modified like Example 1.1. In all of these\nexamples, the plurality is incorrect while our approach is correct. Results of other questions are\nillustrated at https://elicitation.info/classroom/1/. Detailed explanations are illustrated\nin Appendix D and here we provide some highlights. First, our approach elicits a rich hierarchy. For\nexample, the taxicab problem is borrowed from Kahneman [8] and previous studies show that people\nusually ignore the base rate and report \u201880%\u2019. The imagined levels can be 41%\u219280%. We elicit a\nmuch richer level \u201c41%\u219250%\u219280%\u219212%\u219215%\u219220%\u201d. Second, the most sophisticated level\nmay fail to predict the least one. In the Taxicab problem, the correct \u201c41%\u201d supporters successfully\npredict the common wrong answer \u201c80%\u201d. However, they fail to predict the surprisingly wrong\nanswers \u201c12%,20%\u201d, which are in contrast successfully predicted by \u201c80%\u201d supporters. Our model\nis sufficiently general to allow this situation. Third, even for problems (like Go) without famous\nmistakes, our approach still works. Moreover, in the boundary question, we identify the correct\nanswer without any prior when only 3 respondents are correct.\n\n\nBased on the section, do the authors cite the creators of the existing assets that they use?"}]], "q4b": [[{"role": "user", "content": "The following is the experiments section of the paper you are reviewing:\n\n\nWe conduct four studies, study 1 (35 math problems), study 2 (30 Go problems), study 3 (44 general\nknowledge questions), and study 4 (43 Chinese character pronunciation questions).\n\nData collection All studies are performed by online questionnaires. We recruit the respondents by\nan online announcement13 or from an online platform that is similar to Amazon Mechanical Turk. We\nget respondents\u2019 consent for using and sharing their data for research. Respondents are asked not to\nsearch for the answers to the questions or communicate with other people. We allow the respondents\nto answer \u201cI do not know\u201d for all questions. Except for Go problems, all questionnaires use flat\npayment. We illustrate the data collection process in detail in Appendix A. We allow respondents\nto participate in more than one study because our algorithms analyze each question separately and\nindependently.\n\nData processing We merge answers which are the same, like \u20180.5\u2019 and \u201850%\u2019. We omit the answers\nthat are reported by less than (\u2264) 3% of respondents or one person. The remaining answers, excluding\n\u201cI do not know\u201d, form the answer set A whose size is |A|. We then construct the Answer-Prediction\nmatrix and perform our algorithms. Pseudo-codes are attached in Appendix B. Our algorithms do not\nrequire any prior or the respondents\u2019 expertise levels.\n\n3.1 Results\n\n12This may not be a very good assumption since i.i.d. samples can repeat but respondents usually do not\nrepeat their predictions. If we do not want this assumption, we can choose to only use the first prediction from\neach respondent (if there exists) to construct the answer-prediction matrix.\n\n13Many are students from top universities in China. See Appendix A for more details.\n\n8\n\n\fType\nMath\nGo\nGeneral knowledge\nChinese character\n\nTotal Our algorithm(Default) Our algorithm(Variant)\n\n35\n30\n44\n43\n\n29\n28\n41\n36\n\n29\n28\n41\n35\n\nPlurality voting\n24\n23\n35\n34\n\nTable 1: The number of questions our algorithms/baseline are correct.\n\n(a) the Monty Hall problem: you can select one\nclosed door of three. A prize, a car, is behind\none of the doors. The other two doors hide goats.\nAfter you have made your choice, Monty Hall\nwill open one of the remaining doors and show\nthat it does not contain the prize. He then asks\nyou if you would like to switch your choice to\nthe other unopened door. What is the probability\nto get the prize if you switch?\n\n(b) the Taxicab problem: 85% of taxis in this\ncity are green, the others are blue. A witness\nsees a blue taxi. She is usually correct with\nprobability 80%. What is the probability that\nthe taxi saw by the witness is blue?\n\n(c) Pick a move for black such that they can be\nalive.\n\n(d) Pick a move for black such that they can be\nalive by ko.\n\n(e) the boundary question: what river forms the\nboundary between North Korea and China?\n\n(f) the Middle Age New Year question: when\nwas the new year in middle age?\n\n(g) the pronunciation of \u7762\n\n(h) the pronunciation of \u6ec2\n\nFigure 4: The ranked answer-prediction matrices\n\n9\n\n2/31/21/32/31/21/3Answer34261413517146Prediction415080121520415080121520Answer160500008201200801619501104818000522130010218PredictionD3C1B2C2B1D3C1B2C2B1Answer62131091410042012115300002PredictionB8A8B6B7B8A8B6B7Answer112540630001230035PredictionYaluandDoomanYaluSonghuaYaluandDoomanYaluSonghuaAnswer33007410005PredictionApr1Dec25Jan1Apr1Dec25Jan1Answer15410028180633Predictionsui1ju1zhi4zhui1sui1ju1zhi4zhui1Answer115023161300210107Predictionpang1pang2pang1pang2Answer1511216Prediction\fWe compare our approach to the baseline, the plurality voting, regarding the accuracy of the top-\nranking answers. Both of the algorithms beat plurality voting for all studies and the default is slightly\nbetter. Among all 152 questions, in 138 questions, the variant algorithm outputs the same hierarchy as\nthe default algorithm. For other questions, the top-ranking type of the variant algorithm may contain\nmore than one answer. The top-ranking answer is the answer supported by more people among all\nanswers in the top-ranking type. In one question the variant is wrong but the default is correct, the\nvariant algorithm assigns both the correct answer and the incorrect plurality answer to the top-ranking\ntype, thus outputting the incorrect answer as the top-ranking answer.\n\nWe also compute the lack-of-fit index (Definition 2.6) of the algorithms and find that the questions\nthe algorithm outputs the correct answer have a smaller lack-of-fit thus fitting the thinking hierarchy\nmodel better. Therefore, we can use the lack-of-fit index as a reliability index of the algorithms.\n\nWe additionally pick several representative examples for each study (Figure 4) where the matrices\nare ranked by the default algorithm and the diagonal area modified like Example 1.1. In all of these\nexamples, the plurality is incorrect while our approach is correct. Results of other questions are\nillustrated at https://elicitation.info/classroom/1/. Detailed explanations are illustrated\nin Appendix D and here we provide some highlights. First, our approach elicits a rich hierarchy. For\nexample, the taxicab problem is borrowed from Kahneman [8] and previous studies show that people\nusually ignore the base rate and report \u201880%\u2019. The imagined levels can be 41%\u219280%. We elicit a\nmuch richer level \u201c41%\u219250%\u219280%\u219212%\u219215%\u219220%\u201d. Second, the most sophisticated level\nmay fail to predict the least one. In the Taxicab problem, the correct \u201c41%\u201d supporters successfully\npredict the common wrong answer \u201c80%\u201d. However, they fail to predict the surprisingly wrong\nanswers \u201c12%,20%\u201d, which are in contrast successfully predicted by \u201c80%\u201d supporters. Our model\nis sufficiently general to allow this situation. Third, even for problems (like Go) without famous\nmistakes, our approach still works. Moreover, in the boundary question, we identify the correct\nanswer without any prior when only 3 respondents are correct.\n\n\nBased on this section, do the authors use existing assets (e.g., code, data, models) that are not new contributions in their paper? Please start your answer with \"Yes\", \"No\", or \"Unsure\"."}, {"role": "user", "content": "The following is the experiments section of the paper you are reviewing:\n\n\nWe conduct four studies, study 1 (35 math problems), study 2 (30 Go problems), study 3 (44 general\nknowledge questions), and study 4 (43 Chinese character pronunciation questions).\n\nData collection All studies are performed by online questionnaires. We recruit the respondents by\nan online announcement13 or from an online platform that is similar to Amazon Mechanical Turk. We\nget respondents\u2019 consent for using and sharing their data for research. Respondents are asked not to\nsearch for the answers to the questions or communicate with other people. We allow the respondents\nto answer \u201cI do not know\u201d for all questions. Except for Go problems, all questionnaires use flat\npayment. We illustrate the data collection process in detail in Appendix A. We allow respondents\nto participate in more than one study because our algorithms analyze each question separately and\nindependently.\n\nData processing We merge answers which are the same, like \u20180.5\u2019 and \u201850%\u2019. We omit the answers\nthat are reported by less than (\u2264) 3% of respondents or one person. The remaining answers, excluding\n\u201cI do not know\u201d, form the answer set A whose size is |A|. We then construct the Answer-Prediction\nmatrix and perform our algorithms. Pseudo-codes are attached in Appendix B. Our algorithms do not\nrequire any prior or the respondents\u2019 expertise levels.\n\n3.1 Results\n\n12This may not be a very good assumption since i.i.d. samples can repeat but respondents usually do not\nrepeat their predictions. If we do not want this assumption, we can choose to only use the first prediction from\neach respondent (if there exists) to construct the answer-prediction matrix.\n\n13Many are students from top universities in China. See Appendix A for more details.\n\n8\n\n\fType\nMath\nGo\nGeneral knowledge\nChinese character\n\nTotal Our algorithm(Default) Our algorithm(Variant)\n\n35\n30\n44\n43\n\n29\n28\n41\n36\n\n29\n28\n41\n35\n\nPlurality voting\n24\n23\n35\n34\n\nTable 1: The number of questions our algorithms/baseline are correct.\n\n(a) the Monty Hall problem: you can select one\nclosed door of three. A prize, a car, is behind\none of the doors. The other two doors hide goats.\nAfter you have made your choice, Monty Hall\nwill open one of the remaining doors and show\nthat it does not contain the prize. He then asks\nyou if you would like to switch your choice to\nthe other unopened door. What is the probability\nto get the prize if you switch?\n\n(b) the Taxicab problem: 85% of taxis in this\ncity are green, the others are blue. A witness\nsees a blue taxi. She is usually correct with\nprobability 80%. What is the probability that\nthe taxi saw by the witness is blue?\n\n(c) Pick a move for black such that they can be\nalive.\n\n(d) Pick a move for black such that they can be\nalive by ko.\n\n(e) the boundary question: what river forms the\nboundary between North Korea and China?\n\n(f) the Middle Age New Year question: when\nwas the new year in middle age?\n\n(g) the pronunciation of \u7762\n\n(h) the pronunciation of \u6ec2\n\nFigure 4: The ranked answer-prediction matrices\n\n9\n\n2/31/21/32/31/21/3Answer34261413517146Prediction415080121520415080121520Answer160500008201200801619501104818000522130010218PredictionD3C1B2C2B1D3C1B2C2B1Answer62131091410042012115300002PredictionB8A8B6B7B8A8B6B7Answer112540630001230035PredictionYaluandDoomanYaluSonghuaYaluandDoomanYaluSonghuaAnswer33007410005PredictionApr1Dec25Jan1Apr1Dec25Jan1Answer15410028180633Predictionsui1ju1zhi4zhui1sui1ju1zhi4zhui1Answer115023161300210107Predictionpang1pang2pang1pang2Answer1511216Prediction\fWe compare our approach to the baseline, the plurality voting, regarding the accuracy of the top-\nranking answers. Both of the algorithms beat plurality voting for all studies and the default is slightly\nbetter. Among all 152 questions, in 138 questions, the variant algorithm outputs the same hierarchy as\nthe default algorithm. For other questions, the top-ranking type of the variant algorithm may contain\nmore than one answer. The top-ranking answer is the answer supported by more people among all\nanswers in the top-ranking type. In one question the variant is wrong but the default is correct, the\nvariant algorithm assigns both the correct answer and the incorrect plurality answer to the top-ranking\ntype, thus outputting the incorrect answer as the top-ranking answer.\n\nWe also compute the lack-of-fit index (Definition 2.6) of the algorithms and find that the questions\nthe algorithm outputs the correct answer have a smaller lack-of-fit thus fitting the thinking hierarchy\nmodel better. Therefore, we can use the lack-of-fit index as a reliability index of the algorithms.\n\nWe additionally pick several representative examples for each study (Figure 4) where the matrices\nare ranked by the default algorithm and the diagonal area modified like Example 1.1. In all of these\nexamples, the plurality is incorrect while our approach is correct. Results of other questions are\nillustrated at https://elicitation.info/classroom/1/. Detailed explanations are illustrated\nin Appendix D and here we provide some highlights. First, our approach elicits a rich hierarchy. For\nexample, the taxicab problem is borrowed from Kahneman [8] and previous studies show that people\nusually ignore the base rate and report \u201880%\u2019. The imagined levels can be 41%\u219280%. We elicit a\nmuch richer level \u201c41%\u219250%\u219280%\u219212%\u219215%\u219220%\u201d. Second, the most sophisticated level\nmay fail to predict the least one. In the Taxicab problem, the correct \u201c41%\u201d supporters successfully\npredict the common wrong answer \u201c80%\u201d. However, they fail to predict the surprisingly wrong\nanswers \u201c12%,20%\u201d, which are in contrast successfully predicted by \u201c80%\u201d supporters. Our model\nis sufficiently general to allow this situation. Third, even for problems (like Go) without famous\nmistakes, our approach still works. Moreover, in the boundary question, we identify the correct\nanswer without any prior when only 3 respondents are correct.\n\n\nBased on the section, do the authors mention the licenses of the existing assets that they use?"}]], "q4c": [[{"role": "user", "content": "The following is the abstract section of the paper you are reviewing:\n\n\nWhen we use the wisdom of the crowds, we usually rank the answers according to\ntheir popularity, especially when we cannot verify the answers. However, this can\nbe very dangerous when the majority make systematic mistakes. A fundamental\nquestion arises: can we build a hierarchy among the answers without any prior\nwhere the higher-ranking answers, which may not be supported by the majority,\nare from more sophisticated people? To address the question, we propose 1) a\nnovel model to describe people\u2019s thinking hierarchy; 2) two algorithms to learn\nthe thinking hierarchy without any prior; 3) a novel open-response based crowd-\nsourcing approach based on the above theoretic framework. In addition to theoretic\njustifications, we conduct four empirical crowdsourcing studies and show that a)\nthe accuracy of the top-ranking answers learned by our approach is much higher\nthan that of plurality voting (In one question, the plurality answer is supported by\n74 respondents but the correct answer is only supported by 3 respondents. Our\napproach ranks the correct answer the highest without any prior); b) our model\nhas a high goodness-of-fit, especially for the questions where our top-ranking\nanswer is correct. To the best of our knowledge, we are the first to propose a\nthinking hierarchy model with empirical validations in the general problem-solving\nscenarios; and the first to propose a practical open-response based crowdsourcing\napproach that beats plurality voting without any prior.\n\n1\n\n\nBased on this abstract, do the authors curate/release new assets (e.g., code, data, models)? Please start your answer with \"Yes\", \"No\", or \"Unsure\"."}, {"role": "user", "content": "The following is the experiments section of the paper you are reviewing:\n\n\nWe conduct four studies, study 1 (35 math problems), study 2 (30 Go problems), study 3 (44 general\nknowledge questions), and study 4 (43 Chinese character pronunciation questions).\n\nData collection All studies are performed by online questionnaires. We recruit the respondents by\nan online announcement13 or from an online platform that is similar to Amazon Mechanical Turk. We\nget respondents\u2019 consent for using and sharing their data for research. Respondents are asked not to\nsearch for the answers to the questions or communicate with other people. We allow the respondents\nto answer \u201cI do not know\u201d for all questions. Except for Go problems, all questionnaires use flat\npayment. We illustrate the data collection process in detail in Appendix A. We allow respondents\nto participate in more than one study because our algorithms analyze each question separately and\nindependently.\n\nData processing We merge answers which are the same, like \u20180.5\u2019 and \u201850%\u2019. We omit the answers\nthat are reported by less than (\u2264) 3% of respondents or one person. The remaining answers, excluding\n\u201cI do not know\u201d, form the answer set A whose size is |A|. We then construct the Answer-Prediction\nmatrix and perform our algorithms. Pseudo-codes are attached in Appendix B. Our algorithms do not\nrequire any prior or the respondents\u2019 expertise levels.\n\n3.1 Results\n\n12This may not be a very good assumption since i.i.d. samples can repeat but respondents usually do not\nrepeat their predictions. If we do not want this assumption, we can choose to only use the first prediction from\neach respondent (if there exists) to construct the answer-prediction matrix.\n\n13Many are students from top universities in China. See Appendix A for more details.\n\n8\n\n\fType\nMath\nGo\nGeneral knowledge\nChinese character\n\nTotal Our algorithm(Default) Our algorithm(Variant)\n\n35\n30\n44\n43\n\n29\n28\n41\n36\n\n29\n28\n41\n35\n\nPlurality voting\n24\n23\n35\n34\n\nTable 1: The number of questions our algorithms/baseline are correct.\n\n(a) the Monty Hall problem: you can select one\nclosed door of three. A prize, a car, is behind\none of the doors. The other two doors hide goats.\nAfter you have made your choice, Monty Hall\nwill open one of the remaining doors and show\nthat it does not contain the prize. He then asks\nyou if you would like to switch your choice to\nthe other unopened door. What is the probability\nto get the prize if you switch?\n\n(b) the Taxicab problem: 85% of taxis in this\ncity are green, the others are blue. A witness\nsees a blue taxi. She is usually correct with\nprobability 80%. What is the probability that\nthe taxi saw by the witness is blue?\n\n(c) Pick a move for black such that they can be\nalive.\n\n(d) Pick a move for black such that they can be\nalive by ko.\n\n(e) the boundary question: what river forms the\nboundary between North Korea and China?\n\n(f) the Middle Age New Year question: when\nwas the new year in middle age?\n\n(g) the pronunciation of \u7762\n\n(h) the pronunciation of \u6ec2\n\nFigure 4: The ranked answer-prediction matrices\n\n9\n\n2/31/21/32/31/21/3Answer34261413517146Prediction415080121520415080121520Answer160500008201200801619501104818000522130010218PredictionD3C1B2C2B1D3C1B2C2B1Answer62131091410042012115300002PredictionB8A8B6B7B8A8B6B7Answer112540630001230035PredictionYaluandDoomanYaluSonghuaYaluandDoomanYaluSonghuaAnswer33007410005PredictionApr1Dec25Jan1Apr1Dec25Jan1Answer15410028180633Predictionsui1ju1zhi4zhui1sui1ju1zhi4zhui1Answer115023161300210107Predictionpang1pang2pang1pang2Answer1511216Prediction\fWe compare our approach to the baseline, the plurality voting, regarding the accuracy of the top-\nranking answers. Both of the algorithms beat plurality voting for all studies and the default is slightly\nbetter. Among all 152 questions, in 138 questions, the variant algorithm outputs the same hierarchy as\nthe default algorithm. For other questions, the top-ranking type of the variant algorithm may contain\nmore than one answer. The top-ranking answer is the answer supported by more people among all\nanswers in the top-ranking type. In one question the variant is wrong but the default is correct, the\nvariant algorithm assigns both the correct answer and the incorrect plurality answer to the top-ranking\ntype, thus outputting the incorrect answer as the top-ranking answer.\n\nWe also compute the lack-of-fit index (Definition 2.6) of the algorithms and find that the questions\nthe algorithm outputs the correct answer have a smaller lack-of-fit thus fitting the thinking hierarchy\nmodel better. Therefore, we can use the lack-of-fit index as a reliability index of the algorithms.\n\nWe additionally pick several representative examples for each study (Figure 4) where the matrices\nare ranked by the default algorithm and the diagonal area modified like Example 1.1. In all of these\nexamples, the plurality is incorrect while our approach is correct. Results of other questions are\nillustrated at https://elicitation.info/classroom/1/. Detailed explanations are illustrated\nin Appendix D and here we provide some highlights. First, our approach elicits a rich hierarchy. For\nexample, the taxicab problem is borrowed from Kahneman [8] and previous studies show that people\nusually ignore the base rate and report \u201880%\u2019. The imagined levels can be 41%\u219280%. We elicit a\nmuch richer level \u201c41%\u219250%\u219280%\u219212%\u219215%\u219220%\u201d. Second, the most sophisticated level\nmay fail to predict the least one. In the Taxicab problem, the correct \u201c41%\u201d supporters successfully\npredict the common wrong answer \u201c80%\u201d. However, they fail to predict the surprisingly wrong\nanswers \u201c12%,20%\u201d, which are in contrast successfully predicted by \u201c80%\u201d supporters. Our model\nis sufficiently general to allow this situation. Third, even for problems (like Go) without famous\nmistakes, our approach still works. Moreover, in the boundary question, we identify the correct\nanswer without any prior when only 3 respondents are correct.\n\n\nBased on this section, do the authors include any new assets either in the supplemental material or as a URL?"}]], "q4d": [[{"role": "user", "content": "The following is the abstract section of the paper you are reviewing:\n\n\nWhen we use the wisdom of the crowds, we usually rank the answers according to\ntheir popularity, especially when we cannot verify the answers. However, this can\nbe very dangerous when the majority make systematic mistakes. A fundamental\nquestion arises: can we build a hierarchy among the answers without any prior\nwhere the higher-ranking answers, which may not be supported by the majority,\nare from more sophisticated people? To address the question, we propose 1) a\nnovel model to describe people\u2019s thinking hierarchy; 2) two algorithms to learn\nthe thinking hierarchy without any prior; 3) a novel open-response based crowd-\nsourcing approach based on the above theoretic framework. In addition to theoretic\njustifications, we conduct four empirical crowdsourcing studies and show that a)\nthe accuracy of the top-ranking answers learned by our approach is much higher\nthan that of plurality voting (In one question, the plurality answer is supported by\n74 respondents but the correct answer is only supported by 3 respondents. Our\napproach ranks the correct answer the highest without any prior); b) our model\nhas a high goodness-of-fit, especially for the questions where our top-ranking\nanswer is correct. To the best of our knowledge, we are the first to propose a\nthinking hierarchy model with empirical validations in the general problem-solving\nscenarios; and the first to propose a practical open-response based crowdsourcing\napproach that beats plurality voting without any prior.\n\n1\n\n\nBased on this abstract, do the authors curate/release new assets (e.g., code, data, models)? Please start your answer with \"Yes\", \"No\", or \"Unsure\"."}, {"role": "user", "content": "The following is the experiments section of the paper you are reviewing:\n\n\nWe conduct four studies, study 1 (35 math problems), study 2 (30 Go problems), study 3 (44 general\nknowledge questions), and study 4 (43 Chinese character pronunciation questions).\n\nData collection All studies are performed by online questionnaires. We recruit the respondents by\nan online announcement13 or from an online platform that is similar to Amazon Mechanical Turk. We\nget respondents\u2019 consent for using and sharing their data for research. Respondents are asked not to\nsearch for the answers to the questions or communicate with other people. We allow the respondents\nto answer \u201cI do not know\u201d for all questions. Except for Go problems, all questionnaires use flat\npayment. We illustrate the data collection process in detail in Appendix A. We allow respondents\nto participate in more than one study because our algorithms analyze each question separately and\nindependently.\n\nData processing We merge answers which are the same, like \u20180.5\u2019 and \u201850%\u2019. We omit the answers\nthat are reported by less than (\u2264) 3% of respondents or one person. The remaining answers, excluding\n\u201cI do not know\u201d, form the answer set A whose size is |A|. We then construct the Answer-Prediction\nmatrix and perform our algorithms. Pseudo-codes are attached in Appendix B. Our algorithms do not\nrequire any prior or the respondents\u2019 expertise levels.\n\n3.1 Results\n\n12This may not be a very good assumption since i.i.d. samples can repeat but respondents usually do not\nrepeat their predictions. If we do not want this assumption, we can choose to only use the first prediction from\neach respondent (if there exists) to construct the answer-prediction matrix.\n\n13Many are students from top universities in China. See Appendix A for more details.\n\n8\n\n\fType\nMath\nGo\nGeneral knowledge\nChinese character\n\nTotal Our algorithm(Default) Our algorithm(Variant)\n\n35\n30\n44\n43\n\n29\n28\n41\n36\n\n29\n28\n41\n35\n\nPlurality voting\n24\n23\n35\n34\n\nTable 1: The number of questions our algorithms/baseline are correct.\n\n(a) the Monty Hall problem: you can select one\nclosed door of three. A prize, a car, is behind\none of the doors. The other two doors hide goats.\nAfter you have made your choice, Monty Hall\nwill open one of the remaining doors and show\nthat it does not contain the prize. He then asks\nyou if you would like to switch your choice to\nthe other unopened door. What is the probability\nto get the prize if you switch?\n\n(b) the Taxicab problem: 85% of taxis in this\ncity are green, the others are blue. A witness\nsees a blue taxi. She is usually correct with\nprobability 80%. What is the probability that\nthe taxi saw by the witness is blue?\n\n(c) Pick a move for black such that they can be\nalive.\n\n(d) Pick a move for black such that they can be\nalive by ko.\n\n(e) the boundary question: what river forms the\nboundary between North Korea and China?\n\n(f) the Middle Age New Year question: when\nwas the new year in middle age?\n\n(g) the pronunciation of \u7762\n\n(h) the pronunciation of \u6ec2\n\nFigure 4: The ranked answer-prediction matrices\n\n9\n\n2/31/21/32/31/21/3Answer34261413517146Prediction415080121520415080121520Answer160500008201200801619501104818000522130010218PredictionD3C1B2C2B1D3C1B2C2B1Answer62131091410042012115300002PredictionB8A8B6B7B8A8B6B7Answer112540630001230035PredictionYaluandDoomanYaluSonghuaYaluandDoomanYaluSonghuaAnswer33007410005PredictionApr1Dec25Jan1Apr1Dec25Jan1Answer15410028180633Predictionsui1ju1zhi4zhui1sui1ju1zhi4zhui1Answer115023161300210107Predictionpang1pang2pang1pang2Answer1511216Prediction\fWe compare our approach to the baseline, the plurality voting, regarding the accuracy of the top-\nranking answers. Both of the algorithms beat plurality voting for all studies and the default is slightly\nbetter. Among all 152 questions, in 138 questions, the variant algorithm outputs the same hierarchy as\nthe default algorithm. For other questions, the top-ranking type of the variant algorithm may contain\nmore than one answer. The top-ranking answer is the answer supported by more people among all\nanswers in the top-ranking type. In one question the variant is wrong but the default is correct, the\nvariant algorithm assigns both the correct answer and the incorrect plurality answer to the top-ranking\ntype, thus outputting the incorrect answer as the top-ranking answer.\n\nWe also compute the lack-of-fit index (Definition 2.6) of the algorithms and find that the questions\nthe algorithm outputs the correct answer have a smaller lack-of-fit thus fitting the thinking hierarchy\nmodel better. Therefore, we can use the lack-of-fit index as a reliability index of the algorithms.\n\nWe additionally pick several representative examples for each study (Figure 4) where the matrices\nare ranked by the default algorithm and the diagonal area modified like Example 1.1. In all of these\nexamples, the plurality is incorrect while our approach is correct. Results of other questions are\nillustrated at https://elicitation.info/classroom/1/. Detailed explanations are illustrated\nin Appendix D and here we provide some highlights. First, our approach elicits a rich hierarchy. For\nexample, the taxicab problem is borrowed from Kahneman [8] and previous studies show that people\nusually ignore the base rate and report \u201880%\u2019. The imagined levels can be 41%\u219280%. We elicit a\nmuch richer level \u201c41%\u219250%\u219280%\u219212%\u219215%\u219220%\u201d. Second, the most sophisticated level\nmay fail to predict the least one. In the Taxicab problem, the correct \u201c41%\u201d supporters successfully\npredict the common wrong answer \u201c80%\u201d. However, they fail to predict the surprisingly wrong\nanswers \u201c12%,20%\u201d, which are in contrast successfully predicted by \u201c80%\u201d supporters. Our model\nis sufficiently general to allow this situation. Third, even for problems (like Go) without famous\nmistakes, our approach still works. Moreover, in the boundary question, we identify the correct\nanswer without any prior when only 3 respondents are correct.\n\n\nBased on this section, do the authors discuss whether and how consent was obtained from people whose data the authors are using/curating?"}]], "q4e": [[{"role": "user", "content": "The following is the abstract section of the paper you are reviewing:\n\n\nWhen we use the wisdom of the crowds, we usually rank the answers according to\ntheir popularity, especially when we cannot verify the answers. However, this can\nbe very dangerous when the majority make systematic mistakes. A fundamental\nquestion arises: can we build a hierarchy among the answers without any prior\nwhere the higher-ranking answers, which may not be supported by the majority,\nare from more sophisticated people? To address the question, we propose 1) a\nnovel model to describe people\u2019s thinking hierarchy; 2) two algorithms to learn\nthe thinking hierarchy without any prior; 3) a novel open-response based crowd-\nsourcing approach based on the above theoretic framework. In addition to theoretic\njustifications, we conduct four empirical crowdsourcing studies and show that a)\nthe accuracy of the top-ranking answers learned by our approach is much higher\nthan that of plurality voting (In one question, the plurality answer is supported by\n74 respondents but the correct answer is only supported by 3 respondents. Our\napproach ranks the correct answer the highest without any prior); b) our model\nhas a high goodness-of-fit, especially for the questions where our top-ranking\nanswer is correct. To the best of our knowledge, we are the first to propose a\nthinking hierarchy model with empirical validations in the general problem-solving\nscenarios; and the first to propose a practical open-response based crowdsourcing\napproach that beats plurality voting without any prior.\n\n1\n\n\nBased on this abstract, do the authors curate/release new assets (e.g., code, data, models)? Please start your answer with \"Yes\", \"No\", or \"Unsure\"."}, {"role": "user", "content": "The following is the experiments section of the paper you are reviewing:\n\n\nWe conduct four studies, study 1 (35 math problems), study 2 (30 Go problems), study 3 (44 general\nknowledge questions), and study 4 (43 Chinese character pronunciation questions).\n\nData collection All studies are performed by online questionnaires. We recruit the respondents by\nan online announcement13 or from an online platform that is similar to Amazon Mechanical Turk. We\nget respondents\u2019 consent for using and sharing their data for research. Respondents are asked not to\nsearch for the answers to the questions or communicate with other people. We allow the respondents\nto answer \u201cI do not know\u201d for all questions. Except for Go problems, all questionnaires use flat\npayment. We illustrate the data collection process in detail in Appendix A. We allow respondents\nto participate in more than one study because our algorithms analyze each question separately and\nindependently.\n\nData processing We merge answers which are the same, like \u20180.5\u2019 and \u201850%\u2019. We omit the answers\nthat are reported by less than (\u2264) 3% of respondents or one person. The remaining answers, excluding\n\u201cI do not know\u201d, form the answer set A whose size is |A|. We then construct the Answer-Prediction\nmatrix and perform our algorithms. Pseudo-codes are attached in Appendix B. Our algorithms do not\nrequire any prior or the respondents\u2019 expertise levels.\n\n3.1 Results\n\n12This may not be a very good assumption since i.i.d. samples can repeat but respondents usually do not\nrepeat their predictions. If we do not want this assumption, we can choose to only use the first prediction from\neach respondent (if there exists) to construct the answer-prediction matrix.\n\n13Many are students from top universities in China. See Appendix A for more details.\n\n8\n\n\fType\nMath\nGo\nGeneral knowledge\nChinese character\n\nTotal Our algorithm(Default) Our algorithm(Variant)\n\n35\n30\n44\n43\n\n29\n28\n41\n36\n\n29\n28\n41\n35\n\nPlurality voting\n24\n23\n35\n34\n\nTable 1: The number of questions our algorithms/baseline are correct.\n\n(a) the Monty Hall problem: you can select one\nclosed door of three. A prize, a car, is behind\none of the doors. The other two doors hide goats.\nAfter you have made your choice, Monty Hall\nwill open one of the remaining doors and show\nthat it does not contain the prize. He then asks\nyou if you would like to switch your choice to\nthe other unopened door. What is the probability\nto get the prize if you switch?\n\n(b) the Taxicab problem: 85% of taxis in this\ncity are green, the others are blue. A witness\nsees a blue taxi. She is usually correct with\nprobability 80%. What is the probability that\nthe taxi saw by the witness is blue?\n\n(c) Pick a move for black such that they can be\nalive.\n\n(d) Pick a move for black such that they can be\nalive by ko.\n\n(e) the boundary question: what river forms the\nboundary between North Korea and China?\n\n(f) the Middle Age New Year question: when\nwas the new year in middle age?\n\n(g) the pronunciation of \u7762\n\n(h) the pronunciation of \u6ec2\n\nFigure 4: The ranked answer-prediction matrices\n\n9\n\n2/31/21/32/31/21/3Answer34261413517146Prediction415080121520415080121520Answer160500008201200801619501104818000522130010218PredictionD3C1B2C2B1D3C1B2C2B1Answer62131091410042012115300002PredictionB8A8B6B7B8A8B6B7Answer112540630001230035PredictionYaluandDoomanYaluSonghuaYaluandDoomanYaluSonghuaAnswer33007410005PredictionApr1Dec25Jan1Apr1Dec25Jan1Answer15410028180633Predictionsui1ju1zhi4zhui1sui1ju1zhi4zhui1Answer115023161300210107Predictionpang1pang2pang1pang2Answer1511216Prediction\fWe compare our approach to the baseline, the plurality voting, regarding the accuracy of the top-\nranking answers. Both of the algorithms beat plurality voting for all studies and the default is slightly\nbetter. Among all 152 questions, in 138 questions, the variant algorithm outputs the same hierarchy as\nthe default algorithm. For other questions, the top-ranking type of the variant algorithm may contain\nmore than one answer. The top-ranking answer is the answer supported by more people among all\nanswers in the top-ranking type. In one question the variant is wrong but the default is correct, the\nvariant algorithm assigns both the correct answer and the incorrect plurality answer to the top-ranking\ntype, thus outputting the incorrect answer as the top-ranking answer.\n\nWe also compute the lack-of-fit index (Definition 2.6) of the algorithms and find that the questions\nthe algorithm outputs the correct answer have a smaller lack-of-fit thus fitting the thinking hierarchy\nmodel better. Therefore, we can use the lack-of-fit index as a reliability index of the algorithms.\n\nWe additionally pick several representative examples for each study (Figure 4) where the matrices\nare ranked by the default algorithm and the diagonal area modified like Example 1.1. In all of these\nexamples, the plurality is incorrect while our approach is correct. Results of other questions are\nillustrated at https://elicitation.info/classroom/1/. Detailed explanations are illustrated\nin Appendix D and here we provide some highlights. First, our approach elicits a rich hierarchy. For\nexample, the taxicab problem is borrowed from Kahneman [8] and previous studies show that people\nusually ignore the base rate and report \u201880%\u2019. The imagined levels can be 41%\u219280%. We elicit a\nmuch richer level \u201c41%\u219250%\u219280%\u219212%\u219215%\u219220%\u201d. Second, the most sophisticated level\nmay fail to predict the least one. In the Taxicab problem, the correct \u201c41%\u201d supporters successfully\npredict the common wrong answer \u201c80%\u201d. However, they fail to predict the surprisingly wrong\nanswers \u201c12%,20%\u201d, which are in contrast successfully predicted by \u201c80%\u201d supporters. Our model\nis sufficiently general to allow this situation. Third, even for problems (like Go) without famous\nmistakes, our approach still works. Moreover, in the boundary question, we identify the correct\nanswer without any prior when only 3 respondents are correct.\n\n\nBased on this section, do the authors discuss whether the data used/curated contains personally identifiable information or offensive content?"}]], "q5a": [[{"role": "user", "content": "The following is the abstract of the paper you are reviewing:\n\n\nWhen we use the wisdom of the crowds, we usually rank the answers according to\ntheir popularity, especially when we cannot verify the answers. However, this can\nbe very dangerous when the majority make systematic mistakes. A fundamental\nquestion arises: can we build a hierarchy among the answers without any prior\nwhere the higher-ranking answers, which may not be supported by the majority,\nare from more sophisticated people? To address the question, we propose 1) a\nnovel model to describe people\u2019s thinking hierarchy; 2) two algorithms to learn\nthe thinking hierarchy without any prior; 3) a novel open-response based crowd-\nsourcing approach based on the above theoretic framework. In addition to theoretic\njustifications, we conduct four empirical crowdsourcing studies and show that a)\nthe accuracy of the top-ranking answers learned by our approach is much higher\nthan that of plurality voting (In one question, the plurality answer is supported by\n74 respondents but the correct answer is only supported by 3 respondents. Our\napproach ranks the correct answer the highest without any prior); b) our model\nhas a high goodness-of-fit, especially for the questions where our top-ranking\nanswer is correct. To the best of our knowledge, we are the first to propose a\nthinking hierarchy model with empirical validations in the general problem-solving\nscenarios; and the first to propose a practical open-response based crowdsourcing\napproach that beats plurality voting without any prior.\n\n1\n\n\nBased on this abstract, do the authors use crowdsourcing or conduct research with human subjects? Please start your answer with \"Yes\", \"No\", or \"Unsure\"."}, {"role": "user", "content": "The following is the experiments section of the paper you are reviewing:\n\n\nWe conduct four studies, study 1 (35 math problems), study 2 (30 Go problems), study 3 (44 general\nknowledge questions), and study 4 (43 Chinese character pronunciation questions).\n\nData collection All studies are performed by online questionnaires. We recruit the respondents by\nan online announcement13 or from an online platform that is similar to Amazon Mechanical Turk. We\nget respondents\u2019 consent for using and sharing their data for research. Respondents are asked not to\nsearch for the answers to the questions or communicate with other people. We allow the respondents\nto answer \u201cI do not know\u201d for all questions. Except for Go problems, all questionnaires use flat\npayment. We illustrate the data collection process in detail in Appendix A. We allow respondents\nto participate in more than one study because our algorithms analyze each question separately and\nindependently.\n\nData processing We merge answers which are the same, like \u20180.5\u2019 and \u201850%\u2019. We omit the answers\nthat are reported by less than (\u2264) 3% of respondents or one person. The remaining answers, excluding\n\u201cI do not know\u201d, form the answer set A whose size is |A|. We then construct the Answer-Prediction\nmatrix and perform our algorithms. Pseudo-codes are attached in Appendix B. Our algorithms do not\nrequire any prior or the respondents\u2019 expertise levels.\n\n3.1 Results\n\n12This may not be a very good assumption since i.i.d. samples can repeat but respondents usually do not\nrepeat their predictions. If we do not want this assumption, we can choose to only use the first prediction from\neach respondent (if there exists) to construct the answer-prediction matrix.\n\n13Many are students from top universities in China. See Appendix A for more details.\n\n8\n\n\fType\nMath\nGo\nGeneral knowledge\nChinese character\n\nTotal Our algorithm(Default) Our algorithm(Variant)\n\n35\n30\n44\n43\n\n29\n28\n41\n36\n\n29\n28\n41\n35\n\nPlurality voting\n24\n23\n35\n34\n\nTable 1: The number of questions our algorithms/baseline are correct.\n\n(a) the Monty Hall problem: you can select one\nclosed door of three. A prize, a car, is behind\none of the doors. The other two doors hide goats.\nAfter you have made your choice, Monty Hall\nwill open one of the remaining doors and show\nthat it does not contain the prize. He then asks\nyou if you would like to switch your choice to\nthe other unopened door. What is the probability\nto get the prize if you switch?\n\n(b) the Taxicab problem: 85% of taxis in this\ncity are green, the others are blue. A witness\nsees a blue taxi. She is usually correct with\nprobability 80%. What is the probability that\nthe taxi saw by the witness is blue?\n\n(c) Pick a move for black such that they can be\nalive.\n\n(d) Pick a move for black such that they can be\nalive by ko.\n\n(e) the boundary question: what river forms the\nboundary between North Korea and China?\n\n(f) the Middle Age New Year question: when\nwas the new year in middle age?\n\n(g) the pronunciation of \u7762\n\n(h) the pronunciation of \u6ec2\n\nFigure 4: The ranked answer-prediction matrices\n\n9\n\n2/31/21/32/31/21/3Answer34261413517146Prediction415080121520415080121520Answer160500008201200801619501104818000522130010218PredictionD3C1B2C2B1D3C1B2C2B1Answer62131091410042012115300002PredictionB8A8B6B7B8A8B6B7Answer112540630001230035PredictionYaluandDoomanYaluSonghuaYaluandDoomanYaluSonghuaAnswer33007410005PredictionApr1Dec25Jan1Apr1Dec25Jan1Answer15410028180633Predictionsui1ju1zhi4zhui1sui1ju1zhi4zhui1Answer115023161300210107Predictionpang1pang2pang1pang2Answer1511216Prediction\fWe compare our approach to the baseline, the plurality voting, regarding the accuracy of the top-\nranking answers. Both of the algorithms beat plurality voting for all studies and the default is slightly\nbetter. Among all 152 questions, in 138 questions, the variant algorithm outputs the same hierarchy as\nthe default algorithm. For other questions, the top-ranking type of the variant algorithm may contain\nmore than one answer. The top-ranking answer is the answer supported by more people among all\nanswers in the top-ranking type. In one question the variant is wrong but the default is correct, the\nvariant algorithm assigns both the correct answer and the incorrect plurality answer to the top-ranking\ntype, thus outputting the incorrect answer as the top-ranking answer.\n\nWe also compute the lack-of-fit index (Definition 2.6) of the algorithms and find that the questions\nthe algorithm outputs the correct answer have a smaller lack-of-fit thus fitting the thinking hierarchy\nmodel better. Therefore, we can use the lack-of-fit index as a reliability index of the algorithms.\n\nWe additionally pick several representative examples for each study (Figure 4) where the matrices\nare ranked by the default algorithm and the diagonal area modified like Example 1.1. In all of these\nexamples, the plurality is incorrect while our approach is correct. Results of other questions are\nillustrated at https://elicitation.info/classroom/1/. Detailed explanations are illustrated\nin Appendix D and here we provide some highlights. First, our approach elicits a rich hierarchy. For\nexample, the taxicab problem is borrowed from Kahneman [8] and previous studies show that people\nusually ignore the base rate and report \u201880%\u2019. The imagined levels can be 41%\u219280%. We elicit a\nmuch richer level \u201c41%\u219250%\u219280%\u219212%\u219215%\u219220%\u201d. Second, the most sophisticated level\nmay fail to predict the least one. In the Taxicab problem, the correct \u201c41%\u201d supporters successfully\npredict the common wrong answer \u201c80%\u201d. However, they fail to predict the surprisingly wrong\nanswers \u201c12%,20%\u201d, which are in contrast successfully predicted by \u201c80%\u201d supporters. Our model\nis sufficiently general to allow this situation. Third, even for problems (like Go) without famous\nmistakes, our approach still works. Moreover, in the boundary question, we identify the correct\nanswer without any prior when only 3 respondents are correct.\n\n\nBased on this section, do the authors include the full text of instructions given to participants and screenshots, if applicable?"}]], "q5b": [[{"role": "user", "content": "The following is the abstract of the paper you are reviewing:\n\n\nWhen we use the wisdom of the crowds, we usually rank the answers according to\ntheir popularity, especially when we cannot verify the answers. However, this can\nbe very dangerous when the majority make systematic mistakes. A fundamental\nquestion arises: can we build a hierarchy among the answers without any prior\nwhere the higher-ranking answers, which may not be supported by the majority,\nare from more sophisticated people? To address the question, we propose 1) a\nnovel model to describe people\u2019s thinking hierarchy; 2) two algorithms to learn\nthe thinking hierarchy without any prior; 3) a novel open-response based crowd-\nsourcing approach based on the above theoretic framework. In addition to theoretic\njustifications, we conduct four empirical crowdsourcing studies and show that a)\nthe accuracy of the top-ranking answers learned by our approach is much higher\nthan that of plurality voting (In one question, the plurality answer is supported by\n74 respondents but the correct answer is only supported by 3 respondents. Our\napproach ranks the correct answer the highest without any prior); b) our model\nhas a high goodness-of-fit, especially for the questions where our top-ranking\nanswer is correct. To the best of our knowledge, we are the first to propose a\nthinking hierarchy model with empirical validations in the general problem-solving\nscenarios; and the first to propose a practical open-response based crowdsourcing\napproach that beats plurality voting without any prior.\n\n1\n\n\nBased on this abstract, do the authors use crowdsourcing or conduct research with human subjects? Please start your answer with \"Yes\", \"No\", or \"Unsure\"."}, {"role": "user", "content": "The following is the experiments section of the paper you are reviewing:\n\n\nWe conduct four studies, study 1 (35 math problems), study 2 (30 Go problems), study 3 (44 general\nknowledge questions), and study 4 (43 Chinese character pronunciation questions).\n\nData collection All studies are performed by online questionnaires. We recruit the respondents by\nan online announcement13 or from an online platform that is similar to Amazon Mechanical Turk. We\nget respondents\u2019 consent for using and sharing their data for research. Respondents are asked not to\nsearch for the answers to the questions or communicate with other people. We allow the respondents\nto answer \u201cI do not know\u201d for all questions. Except for Go problems, all questionnaires use flat\npayment. We illustrate the data collection process in detail in Appendix A. We allow respondents\nto participate in more than one study because our algorithms analyze each question separately and\nindependently.\n\nData processing We merge answers which are the same, like \u20180.5\u2019 and \u201850%\u2019. We omit the answers\nthat are reported by less than (\u2264) 3% of respondents or one person. The remaining answers, excluding\n\u201cI do not know\u201d, form the answer set A whose size is |A|. We then construct the Answer-Prediction\nmatrix and perform our algorithms. Pseudo-codes are attached in Appendix B. Our algorithms do not\nrequire any prior or the respondents\u2019 expertise levels.\n\n3.1 Results\n\n12This may not be a very good assumption since i.i.d. samples can repeat but respondents usually do not\nrepeat their predictions. If we do not want this assumption, we can choose to only use the first prediction from\neach respondent (if there exists) to construct the answer-prediction matrix.\n\n13Many are students from top universities in China. See Appendix A for more details.\n\n8\n\n\fType\nMath\nGo\nGeneral knowledge\nChinese character\n\nTotal Our algorithm(Default) Our algorithm(Variant)\n\n35\n30\n44\n43\n\n29\n28\n41\n36\n\n29\n28\n41\n35\n\nPlurality voting\n24\n23\n35\n34\n\nTable 1: The number of questions our algorithms/baseline are correct.\n\n(a) the Monty Hall problem: you can select one\nclosed door of three. A prize, a car, is behind\none of the doors. The other two doors hide goats.\nAfter you have made your choice, Monty Hall\nwill open one of the remaining doors and show\nthat it does not contain the prize. He then asks\nyou if you would like to switch your choice to\nthe other unopened door. What is the probability\nto get the prize if you switch?\n\n(b) the Taxicab problem: 85% of taxis in this\ncity are green, the others are blue. A witness\nsees a blue taxi. She is usually correct with\nprobability 80%. What is the probability that\nthe taxi saw by the witness is blue?\n\n(c) Pick a move for black such that they can be\nalive.\n\n(d) Pick a move for black such that they can be\nalive by ko.\n\n(e) the boundary question: what river forms the\nboundary between North Korea and China?\n\n(f) the Middle Age New Year question: when\nwas the new year in middle age?\n\n(g) the pronunciation of \u7762\n\n(h) the pronunciation of \u6ec2\n\nFigure 4: The ranked answer-prediction matrices\n\n9\n\n2/31/21/32/31/21/3Answer34261413517146Prediction415080121520415080121520Answer160500008201200801619501104818000522130010218PredictionD3C1B2C2B1D3C1B2C2B1Answer62131091410042012115300002PredictionB8A8B6B7B8A8B6B7Answer112540630001230035PredictionYaluandDoomanYaluSonghuaYaluandDoomanYaluSonghuaAnswer33007410005PredictionApr1Dec25Jan1Apr1Dec25Jan1Answer15410028180633Predictionsui1ju1zhi4zhui1sui1ju1zhi4zhui1Answer115023161300210107Predictionpang1pang2pang1pang2Answer1511216Prediction\fWe compare our approach to the baseline, the plurality voting, regarding the accuracy of the top-\nranking answers. Both of the algorithms beat plurality voting for all studies and the default is slightly\nbetter. Among all 152 questions, in 138 questions, the variant algorithm outputs the same hierarchy as\nthe default algorithm. For other questions, the top-ranking type of the variant algorithm may contain\nmore than one answer. The top-ranking answer is the answer supported by more people among all\nanswers in the top-ranking type. In one question the variant is wrong but the default is correct, the\nvariant algorithm assigns both the correct answer and the incorrect plurality answer to the top-ranking\ntype, thus outputting the incorrect answer as the top-ranking answer.\n\nWe also compute the lack-of-fit index (Definition 2.6) of the algorithms and find that the questions\nthe algorithm outputs the correct answer have a smaller lack-of-fit thus fitting the thinking hierarchy\nmodel better. Therefore, we can use the lack-of-fit index as a reliability index of the algorithms.\n\nWe additionally pick several representative examples for each study (Figure 4) where the matrices\nare ranked by the default algorithm and the diagonal area modified like Example 1.1. In all of these\nexamples, the plurality is incorrect while our approach is correct. Results of other questions are\nillustrated at https://elicitation.info/classroom/1/. Detailed explanations are illustrated\nin Appendix D and here we provide some highlights. First, our approach elicits a rich hierarchy. For\nexample, the taxicab problem is borrowed from Kahneman [8] and previous studies show that people\nusually ignore the base rate and report \u201880%\u2019. The imagined levels can be 41%\u219280%. We elicit a\nmuch richer level \u201c41%\u219250%\u219280%\u219212%\u219215%\u219220%\u201d. Second, the most sophisticated level\nmay fail to predict the least one. In the Taxicab problem, the correct \u201c41%\u201d supporters successfully\npredict the common wrong answer \u201c80%\u201d. However, they fail to predict the surprisingly wrong\nanswers \u201c12%,20%\u201d, which are in contrast successfully predicted by \u201c80%\u201d supporters. Our model\nis sufficiently general to allow this situation. Third, even for problems (like Go) without famous\nmistakes, our approach still works. Moreover, in the boundary question, we identify the correct\nanswer without any prior when only 3 respondents are correct.\n\n\nBased on this section, do the authors describe any potential participant risks, with links to Institutional Review Board (IRB) approvals, if applicable?"}]], "q5c": [[{"role": "user", "content": "The following is the abstract of the paper you are reviewing:\n\n\nWhen we use the wisdom of the crowds, we usually rank the answers according to\ntheir popularity, especially when we cannot verify the answers. However, this can\nbe very dangerous when the majority make systematic mistakes. A fundamental\nquestion arises: can we build a hierarchy among the answers without any prior\nwhere the higher-ranking answers, which may not be supported by the majority,\nare from more sophisticated people? To address the question, we propose 1) a\nnovel model to describe people\u2019s thinking hierarchy; 2) two algorithms to learn\nthe thinking hierarchy without any prior; 3) a novel open-response based crowd-\nsourcing approach based on the above theoretic framework. In addition to theoretic\njustifications, we conduct four empirical crowdsourcing studies and show that a)\nthe accuracy of the top-ranking answers learned by our approach is much higher\nthan that of plurality voting (In one question, the plurality answer is supported by\n74 respondents but the correct answer is only supported by 3 respondents. Our\napproach ranks the correct answer the highest without any prior); b) our model\nhas a high goodness-of-fit, especially for the questions where our top-ranking\nanswer is correct. To the best of our knowledge, we are the first to propose a\nthinking hierarchy model with empirical validations in the general problem-solving\nscenarios; and the first to propose a practical open-response based crowdsourcing\napproach that beats plurality voting without any prior.\n\n1\n\n\nBased on this abstract, do the authors use crowdsourcing or conduct research with human subjects? Please start your answer with \"Yes\", \"No\", or \"Unsure\"."}, {"role": "user", "content": "The following is the experiments section of the paper you are reviewing:\n\n\nWe conduct four studies, study 1 (35 math problems), study 2 (30 Go problems), study 3 (44 general\nknowledge questions), and study 4 (43 Chinese character pronunciation questions).\n\nData collection All studies are performed by online questionnaires. We recruit the respondents by\nan online announcement13 or from an online platform that is similar to Amazon Mechanical Turk. We\nget respondents\u2019 consent for using and sharing their data for research. Respondents are asked not to\nsearch for the answers to the questions or communicate with other people. We allow the respondents\nto answer \u201cI do not know\u201d for all questions. Except for Go problems, all questionnaires use flat\npayment. We illustrate the data collection process in detail in Appendix A. We allow respondents\nto participate in more than one study because our algorithms analyze each question separately and\nindependently.\n\nData processing We merge answers which are the same, like \u20180.5\u2019 and \u201850%\u2019. We omit the answers\nthat are reported by less than (\u2264) 3% of respondents or one person. The remaining answers, excluding\n\u201cI do not know\u201d, form the answer set A whose size is |A|. We then construct the Answer-Prediction\nmatrix and perform our algorithms. Pseudo-codes are attached in Appendix B. Our algorithms do not\nrequire any prior or the respondents\u2019 expertise levels.\n\n3.1 Results\n\n12This may not be a very good assumption since i.i.d. samples can repeat but respondents usually do not\nrepeat their predictions. If we do not want this assumption, we can choose to only use the first prediction from\neach respondent (if there exists) to construct the answer-prediction matrix.\n\n13Many are students from top universities in China. See Appendix A for more details.\n\n8\n\n\fType\nMath\nGo\nGeneral knowledge\nChinese character\n\nTotal Our algorithm(Default) Our algorithm(Variant)\n\n35\n30\n44\n43\n\n29\n28\n41\n36\n\n29\n28\n41\n35\n\nPlurality voting\n24\n23\n35\n34\n\nTable 1: The number of questions our algorithms/baseline are correct.\n\n(a) the Monty Hall problem: you can select one\nclosed door of three. A prize, a car, is behind\none of the doors. The other two doors hide goats.\nAfter you have made your choice, Monty Hall\nwill open one of the remaining doors and show\nthat it does not contain the prize. He then asks\nyou if you would like to switch your choice to\nthe other unopened door. What is the probability\nto get the prize if you switch?\n\n(b) the Taxicab problem: 85% of taxis in this\ncity are green, the others are blue. A witness\nsees a blue taxi. She is usually correct with\nprobability 80%. What is the probability that\nthe taxi saw by the witness is blue?\n\n(c) Pick a move for black such that they can be\nalive.\n\n(d) Pick a move for black such that they can be\nalive by ko.\n\n(e) the boundary question: what river forms the\nboundary between North Korea and China?\n\n(f) the Middle Age New Year question: when\nwas the new year in middle age?\n\n(g) the pronunciation of \u7762\n\n(h) the pronunciation of \u6ec2\n\nFigure 4: The ranked answer-prediction matrices\n\n9\n\n2/31/21/32/31/21/3Answer34261413517146Prediction415080121520415080121520Answer160500008201200801619501104818000522130010218PredictionD3C1B2C2B1D3C1B2C2B1Answer62131091410042012115300002PredictionB8A8B6B7B8A8B6B7Answer112540630001230035PredictionYaluandDoomanYaluSonghuaYaluandDoomanYaluSonghuaAnswer33007410005PredictionApr1Dec25Jan1Apr1Dec25Jan1Answer15410028180633Predictionsui1ju1zhi4zhui1sui1ju1zhi4zhui1Answer115023161300210107Predictionpang1pang2pang1pang2Answer1511216Prediction\fWe compare our approach to the baseline, the plurality voting, regarding the accuracy of the top-\nranking answers. Both of the algorithms beat plurality voting for all studies and the default is slightly\nbetter. Among all 152 questions, in 138 questions, the variant algorithm outputs the same hierarchy as\nthe default algorithm. For other questions, the top-ranking type of the variant algorithm may contain\nmore than one answer. The top-ranking answer is the answer supported by more people among all\nanswers in the top-ranking type. In one question the variant is wrong but the default is correct, the\nvariant algorithm assigns both the correct answer and the incorrect plurality answer to the top-ranking\ntype, thus outputting the incorrect answer as the top-ranking answer.\n\nWe also compute the lack-of-fit index (Definition 2.6) of the algorithms and find that the questions\nthe algorithm outputs the correct answer have a smaller lack-of-fit thus fitting the thinking hierarchy\nmodel better. Therefore, we can use the lack-of-fit index as a reliability index of the algorithms.\n\nWe additionally pick several representative examples for each study (Figure 4) where the matrices\nare ranked by the default algorithm and the diagonal area modified like Example 1.1. In all of these\nexamples, the plurality is incorrect while our approach is correct. Results of other questions are\nillustrated at https://elicitation.info/classroom/1/. Detailed explanations are illustrated\nin Appendix D and here we provide some highlights. First, our approach elicits a rich hierarchy. For\nexample, the taxicab problem is borrowed from Kahneman [8] and previous studies show that people\nusually ignore the base rate and report \u201880%\u2019. The imagined levels can be 41%\u219280%. We elicit a\nmuch richer level \u201c41%\u219250%\u219280%\u219212%\u219215%\u219220%\u201d. Second, the most sophisticated level\nmay fail to predict the least one. In the Taxicab problem, the correct \u201c41%\u201d supporters successfully\npredict the common wrong answer \u201c80%\u201d. However, they fail to predict the surprisingly wrong\nanswers \u201c12%,20%\u201d, which are in contrast successfully predicted by \u201c80%\u201d supporters. Our model\nis sufficiently general to allow this situation. Third, even for problems (like Go) without famous\nmistakes, our approach still works. Moreover, in the boundary question, we identify the correct\nanswer without any prior when only 3 respondents are correct.\n\n\nBased on this section, do the authors include the estimated hourly wage paid to participants and the total amount spent on participant compensation?"}]]}}