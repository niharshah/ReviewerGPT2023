{"paper_index": 4, "title": "Leveraging Inter-Layer Dependency for Post -Training Quantization", "abstract": "", "introduction": "\n\nDeep learning has seen a boom of architectures with ever-increasing capabilities and capacity. Huge\ncapacity, however, always comes with boosting parameters, resulting in large computation cost\nand memory footprint. Researchers have utilized strategies like neural architecture search (NAS)\n[4, 11, 29, 49], network pruning [12, 19], and knowledge distillation (KD) [18, 21] to design compact\nnetworks and improve their accuracy. Furthermore, neural network quantization [7, 15, 22, 25, 28]\nis typically applied along with the methods mentioned above to reduce the computation cost and\nmemory footprint. Quantization represents neural networks with lower bit precision, usually 8 bits in\npractice, and saves 75% of memory while offering 2 \u223c 4\u00d7 speedup[26].\n\nNetwork quantization techniques are typically categorized into Quantization Aware Training (QAT)\nand Post-training Quantization(PTQ) based on the dependency on labeled data and the necessity of\nweight tuning. Generally, PTQ techniques are preferred in industry because they rely less on labeled\ndata and take into account privacy concerns as well as compute resources. This paper focuses on\nPTQ.\n\nPTQ usually suffers from significant accuracy degradation, especially for low-bit quantization which\noffers much more memory reduction and speedup. To improve the accuracy of PTQ, much effort\n\n36th Conference on Neural Information Processing Systems (NeurIPS 2022).\n\n\f(a) layer-wise\n\n(b) block-wise\n\n(c) network-wise\n\nlayer-wise vs. block-wise vs. network-wise. Gray nodes are frozen while highlighted nodes\nFigure 1:\nare learnable (quantization parameters only). The term block here denotes residual unit in modern CNN (e.g.\nResidual Bottleneck for ResNets).\n\nhas been devoted [5, 32, 34, 35, 39]. AdaRound[32] has recently discovered that round-to-nearest\nappears to be optimal for single weight but not for the entire layer. The rounding errors of weights in\na single layer have the capability of compensating for each other, contributing to a smaller layer-wise\nquantization error. We term this property as intra-layer dependency. Motivated by this property,\nAdaRound[32] proposes to learn the round policy layer by layer to improve accuracy.\n\nInspired by AdaRound[32], we hypothesize that the quantization errors of all layers in a network are\nable to compensate for each other, resulting in a reduced quantization error. We refer to this capability\nas inter-layer dependency. However, AdaRound[32] assumes that layers are mutual-independent\nand derives a layer-diagonal Hessian, leading to a layer-wise solution(Figure 1a). BRECQ[27]\npoints out that the dependency should not be ignored and the assumption of AdaRound[32] is not\naccurate, they instead utilize the inter-layer dependency within each block to build a block-wise\napproach(Figure 1b). However, BRECQ[27] assumes that blocks are mutual-independent. We\nargue that neither the approximations of AdaRound nor that of BRECQ are accurate enough, the\ninter-layer dependency should be leveraged in a network-wise manner. To achieve this, we propose a\nNetwork-Wise Quantization (NWQ, Figure 1c) scheme to fully leverage the inter-layer dependency.\nAdaRound and BRECQ separate a network into sub-nets and train them sequentially, this process\nmakes the preceding layers unaware of succeeding layers. NWQ solves this problem via training\nthe entire network end-to-end, where layers are aware of each other and they are able to cooperate\nfor a more optimal solution. However, quantizing the entire network end-to-end is a combinatorial\noptimization problem where huge number of discrete parameters from all layers are combined\ntogether for optimization. This optimization problem will raise two major challenges that result in\nsignificant accuracy degradation.\n\nThe first challenge is over-fitting. Extending the reconstruction unit from layer/block to entire network\nwill increase parameters dramatically and thus raises the risk of over-fitting over a typically small\ncalibration set. To alleviate this issue, NWQ applies Activation Regularization (AR), a variant of\nL2 regularization, on intermediate representation to keep the quantized activations as close to the\nfloating-point counterpart as possible. The benefits of AR are two folds. First, AR regularizes\nquantized activation flow to prevent over-fitting. Second, it works as a quantization loss function to\nencourage minimal intermediate quantization error. AR turns out to be both simple and effective.\n\nAnother challenge is discrete optimization. Based on AdaRound, NWQ learns the weight round\npolicy and activation quantization parameters, both of which involve in discrete variables. However,\nthe substantially increased depth and discrete variables make it more difficult to converge compared\nto layer-wise[23, 32] and block-wise[27, 40] approach. The insufficient convergence will leave the\ndiscrete variables (weight round policy) in a state of continuity rather than discretization, leading\nto great performance drop. To solve this problem, we propose Annealing Sofmtax (ASoftmax)\nand Annealing Mixup (AMixup) to better optimize discrete weights and activations. Specifically,\nASoftmax provides a mechanism to guarantee the discritization of round policy learning via annealing\ntemperature. AMixup resolves the train-test inconsistency problem indtroduced by QDROP[40] via\ngradually decreasing the floating-point percentage in mixed activations.\n\n2\n\nlayer 1layer 2layer N\u2026block 1block 2block N\u2026layer 1layer 2layer 3modellayer 1layer 2layer N\u2026layer 1layer 2layer N\u2026block 1block 2block N\u2026layer 1layer 2layer 3modellayer 1layer 2layer N\u2026layer 1layer 2layer N\u2026block 1block 2block N\u2026layer 1layer 2layer 3modellayer 1layer 2layer N\u2026\fEquipped with AR, ASoftmax and AMixup, we are able to build our NWQ framework that fully\nleverages inter-layer dependency to achieve higher accuracy than existing works.\n\nTo summarize, this paper makes the following contributions:\n\n1. We propose a novel network-wise quantization framework for PTQ to fully leverage inter-\n\nlayer dependency.\n\n2. We analyze the combinatorial optimization problem introduced by network-wise quan-\ntization and propose AR, ASoftmax and AMixup to solve the over-fitting and discrete\noptimization problem.\n\n3. We establish a new state-of-the-art through significantly improving PTQ by a large margin:\n20.24% for MobileNetV2 with 2 bits and thus push the limit of extremely low-bit PTQ from\nfeasibility to usability.\n\n", "methods": "\n\nNetwork-Wise Quantization (NWQ) is a simple end-to-end post-training quantization approach.\nThe structure of NWQ is illustrated in Figure 2. The optimization objective is composed of output\nquantization loss and layer-wise AR loss. Meanwhile, the annealing approaches (Asoftmax and\nAMixup) are introduced in training process, which enable the quantized weights and activations to\ngently converge from continuous state to discrete state, lowering training complexity and improving\naccuracy.\n\n3.1 Activation Regularization\n\nNWQ leverages Activation Regularization (AR) to penalize quantized intermediate activations that\nare substantially away from the floating-point counterparts, thereby encouraging the activations to\nremain unchanged.\n\nFormally, we define AR by:\n\nLar(G, G) =\n\nN \u22121\n(cid:88)\n\nl=1\n\n\u2225al \u2212 al\u22252\nF ,\n\n(1)\n\nwhile G represents floating-point network and G is the simulated quantized counterpart. al and al\ndenote the floating-point and the simulated quantized activation of the l-th layer, respectively. N\nrepresents the number of total layers and \u2225\u00b7\u2225F denotes the Frobenius norm.\nOur network quantization error Lquant(aN , aN ) is defined by:\n\nLquant(G, G) = \u2225aN \u2212 aN \u22252\nF ,\n\nthen the final objective can be formulated as:\n\nL(G, G) = Lar + Lquant =\n\nN\n(cid:88)\n\nl=1\n\n\u2225al \u2212 al\u22252\nF .\n\n(2)\n\nPractically, Lar is not restricted to be utilized layer by layer, it can also be performed block by\nblock and stage by stage, as BRECQ [27] defined. We will discuss how the granularity affects the\nperformance in Sec. 4.2.1. By default, we employ layer-wise AR in our experiments.\n\n4\n\nCaption: Overview of NWQ. Asoftmaxis applied on weight quantization to encourage discretization, while Adrophelp activation quantization to improve flatness.The layer-wise Activation Regularization(AR) works as a regularizeras well as a quantizationloss.Layer 1AMixupLayer N\u2026AMixupLayer 1Layer N\u2026\ud835\udc34\ud835\udc459\ud835\udc3f:;<=>+\u2026\ud835\udc34\ud835\udc45?@9\ud835\udc59\ud835\udc5c\ud835\udc60\ud835\udc60ASoftmaxASoftmax\ud835\udc3f<A+Layer N-1ASoftmaxAMixupLayer N-1+\f3.2 Annealing Softmax\n\nFor a weight vector w, AdaRound [32] proposes to learn the round policy by:\n\n\u02c6wi = clip\n\n(cid:16)\n\n\u230a\n\nwi\ns\n\n\u230b + h(vi), q\u2212, q+\n\n(cid:17)\n\n,\n\n(3)\n\nwhere \u230a\u00b7\u230b gives as output of the integral part and clip(\u00b7) clamps all values between q\u2212 and q+. \u02c6wi\ndenotes the quantized weight for the i-th element of w. We will omit i below for simplicity. s is the\nquantizer step size. v is a learnable continuous vector w.r.t. w and h(v) is recitified sigmoid function\n(see Eqn.(4)) which is regularized to converge to {0, 1}.\n\nh(v) = clip(\n\n1.2\n1 + exp(\u2212v)\n\n\u2212 0.1, 0, 1).\n\n(4)\n\nBased on Eqn.(4), AdaRound proposes Rectified Sigmoid with an explicit Regularization (RSeR)\nloss to discrete h(v):\n\nLRSeR(h(v)) = 1 \u2212 |2h(v) \u2212 1|\u03b2,\nHowever, there are two limitations of RSeR. First, RSeR works only for binary round policy. Second,\nRSeR is adversarial to quantization loss. For example, for a linear transformation network, the\nquantization loss is\n\n(5)\n\nLq = \u2225w\u22bax \u2212 s \u02c6w\u22bax\u22252\nF\nw\ns\n\n= s2\u2225(\n\nw\ns\n\n\u2212 \u230a\n\n\u230b \u2212 h(v))\u22bax\u22252\nF ,\n\n(6)\n\nwhere x denotes the input and we ignore the clip function in Eqn. (3).\n\nEqn. (6) indicates that quantization loss will prevent LRSeR from convergence (i.e., LRSeR encour-\nages h(v) to converge to 0 or 1 while Lq prefers h(v) to remain w\ns \u230b). Besides, this adversarial\nbecome more severe when extending layer/block-wise methods to network-wise quantization(see Sec.\n4.2.2 and Table 4).\n\ns \u2212 \u230a w\n\n3.2.1 Formulation\n\nTo resolve the two problems of RSeR, we propose Annealing Softmax (ASoftmax). First, we extend\nthe discrete optimization space of h(\u00b7) from {0, 1} to K, where K is a integer range and defined by:\nK = {n, n + 1, ..., m \u2212 1, m}, n \u2208 Z\u22640, m \u2208 Z>0,\n\n(7)\n\nwhere n and m are hyper-parameters that specify the discrete optimization space. Then, ASoftmax\ncan be formulated as:\n\n\u02c6wt = clip\n\n\u230b + ht(V), q\u2212, q+\n\n,\n\n(8)\n\n(cid:17)\n\n(cid:16)\n\n\u230a\n\nw\ns\n\nwhere V is a continuous learnable matrix and represents the logits of probability over K. ht(V) is\ndefined by:\n\nht(V) = Ek\u223cp(k|V,\u03c4 t)\n\n(cid:104)\nk\n\n(cid:105)\n,\n\n(9)\n\np(k = Ki|V, \u03c4 t) \u221d exp(Vi/\u03c4 t), 0 \u2264 i < m \u2212 n + 1,\n(10)\nwhere t represents for t-th iteration. \u03c4 t is a temperature that decays linearly to transition ht(V)\nfrom continuity to discretization. For example, when \u03c4 t is large, p(k|V, \u03c4 t) is a continuous vector\nof probability and ht(V) is the expectation of the discretization. When \u03c4 t turns tiny, p(k|V, \u03c4 t)\nbecomes a one-hot vector and ht(V) selects k by p(k|V, \u03c4 t) = 1. \u03c4 t is simply decayed by:\n\n\u03c4 t =\n\n(\u03c4 T \u2212 \u03c4 0) \u00b7 t\nT\n\n+ \u03c4 0,\n\n(11)\n\nwhere \u03c4 0 and \u03c4 T are hyper-parameters of initial and final temperature.\n\nASoftmax is a general formation of learning wider range of discrete parameters than binary. It easily\ndegenerates to round policy learning by setting n = 0 and m = 1. We use \u03c4 T = 0.01 and \u03c4 0 = 1 by\ndefault.\n\n5\n\n\f3.2.2 Initialization\n\nAccording to our observations, initialization has significant impacts on learning round policy. Given\nthe weight quantizer step size s, which has been computed using statistic-based PTQ methods, we\nattempt to initialize V in order to make s \u00b7 \u02c6w0 as close to floating-point w as possible at the start. To\nachieve this goal, we first calculate the un-normalized probability by:\n\n\u03c3\u2032(V)i =\n\n1\ns \u2212 \u230a w\n|Ki \u2212 ( w\n\ns \u230b)|\n\n, 0 \u2264 i < m \u2212 n + 1.\n\nThen, we normalize the probability:\n\n\u03c3(V)i =\n\n\u03c3\u2032(V)i\nk=0 \u03c3\u2032(V)k\n\n(cid:80)m\u2212n\n\n,\n\n(12)\n\n(13)\n\nFinally, the initialization for Vi is log(\u03c3(V)i). It can be easily proved (see Appendix A) that the\ninitialized s \u00b7 \u02c6w0 equals w as long as the integer range K is symmetric w.r.t. 0.5 (i.e. n = 1 \u2212 m).\n\n3.3 Annealing Mixup\n\nNWQ considers activation quantization as a discrete optimization problem and solves it via continuous\nrelaxation. Different from ASoftmax which is applied on element-wise level, AMixup works on\ntensor-wise level: we randomly mix up the floating-point activations with the simulated quantized\nactivations. To encourage continuous relaxation converge to discretization, NWQ progressively\ndecreases the percentage of floating-point activations, so the mixed activations will become fully\nquantized activation gradually.\n\nFormally, the percentage of floating-point activations is defined by:\n\n(Pe \u2212 Ps) \u00b7 t\nT\nwhere P (t) is the percentage of floating-point activation in the mixed activation for the t-th iteration.\nPs and Pe denote the percentage at start and end respectively, and T is the number of iterations. We\nuse Ps = 0.5 and Pe = 0 by default.\n\nP (t) =\n\n+ Ps,\n\n(14)\n\nAMixup is similar to QDROP [40] in the mixup operation. The key difference, however, is that\nAMixup introduces an annealing process. This is very important for NWQ. QDROP trains with mixed\nactivations while testing with fully quantized activations, thus introducing inconsistency bias. This\nbias accmulates as network depth increases and leads to great accuracy degradation in network-wise\nquantization. AMixup is able to eliminate the inconsistency bias via the annealing process.\n\n", "experiments": "\n\nIn this section, we conduct extensive experiments with various architectures to verify our NWQ.\nWe first compare our method with previous state-of-the-art, and then we delve into our important\ndesigns and fully explore the scalability, effectiveness, and efficiency of our solution. We report top-1\nclassification accuracy on the ImageNet[9] validation set.\n\nImplementation Details. Our code is based on open-source codes BRECQ and QDrop. We\napply layer-wise AR. For ASoftmax, we set n = 0, m = 1 and \u03c4 is decayed from 1 to 0.01. For\nAMixup, we set default Ps as 0.5 and Pe as 0. We randomly sample 1024 images from ImageNet\ntrain set and employ Cutmix [45] and Mixup[46] as data augmentation. The learning rates are\n0.01 for round policy and 0.0004 for activation quantizer step size. We train for 20000 iterations\nwith a mini-batch size of 32 in 8 Tesla V100 GPUs, taking \u223c 30 minutes for ResNet18, which\nis on par with BRECQ and QDROP. Our experiments are conducted on 5 architectures, including\nResNet18(Res18), MobileNetV2(MNV2), RegNet-600MF(Reg600MF), RegNet-3.2GF(Reg3.2GF)\nand MnasNet(Mnas). Other settings remain same as QDROP[40] if not specified.\n\n4.1 Comprehensive Comparison\n\nTable 1 compares our NWQ with several main-stream PTQ approaches. Overall, NWQ outperforms\nprevious works across all neural network architectures and bit widths, and our performance improves\neven more as the bit width decreases.\n\n6\n\n\fFull Prec\nACIQ-Mix\u2217 [35]\nZeroQ\u2217 [5]\nLAPQ\u2217 [34]\nAdaQuant\u2217 [23]\nBit-Split\u2217 [39]\nAdaRound\u2217 [32]\nBRECQ\u2217 [27]\nQDROP\u2217 [40]\nQDROP\u2020 [40]\nNWQ(Ours)\nNWQ\u2020(Ours)\nAdaQuant\u2217 [23]\nAdaRound\u2217 [32]\nBRECQ\u2217 [27]\nQDROP\u2217 [40]\nQDROP\u2020 [40]\nNWQ(Ours)\nNWQ\u2020(Ours)\nBRECQ\u2217 [27]\nQDROP\u2217 [40]\nQDROP\u2020 [40]\nNWQ(Ours)\nNWQ\u2020(Ours)\n\nBits(W/A)\n32/32\n4/4\n4/4\n4/4\n4/4\n4/4\n4/4\n4/4\n4/4\n4/4\n4/4\n4/4\n3/3\n3/3\n3/3\n3/3\n3/3\n3/3\n3/3\n2/2\n2/2\n2/2\n2/2\n2/2\n\nRes18\n71.06\n67.00\n21.71\n60.30\n69.60\n67.56\n67.96\n69.60\n69.62\n69.88\n69.85\u00b10.04\n69.95\u00b10.07\n60.09\n64.66\n65.87\n66.75\n67.51\n67.58\u00b10.14\n68.15\u00b10.10\n42.54\n54.72\n57.84\n59.14\u00b10.05\n61.65\u00b10.12\n\nMobileNetV2\n72.49\n-\n26.24\n49.70\n47.16\n-\n61.52\n66.57\n68.84\n69.15\n69.14\u00b10.12\n69.60\u00b10.03\n2.23\n15.20\n23.41\n57.98\n58.54\n61.24\u00b10.21\n62.53\u00b10.05\n0.24\n13.05\n14.89\n26.42\u00b11.03\n35.13\u00b12.16\n\nReg600MF\n73.71\n-\n28.54\n57.71\n-\n-\n68.20\n68.33\n71.18\n71.45\n71.92\u00b10.07\n72.03\u00b10.09\n-\n51.01\n55.16\n65.54\n66.75\n67.38\u00b10.13\n68.31\u00b10.09\n3.58\n41.47\n45.93\n48.49\u00b10.03\n56.36\u00b10.01\n\nReg3.2GF\n78.36\n-\n12.24\n55.89\n-\n-\n73.85\n74.21\n76.66\n77.02\n77.40\u00b10.04\n77.42\u00b10.03\n-\n56.79\n57.12\n72.51\n73.48\n74.79\u00b10.09\n75.44\u00b10.08\n3.62\n55.11\n57.98\n62.85\u00b10.13\n67.02\u00b10.01\n\nMnasNet2.0\n76.68\n-\n3.89\n65.32\n-\n-\n68.86\n73.56\n73.71\n74.09\n74.60\u00b10.09\n74.86\u00b10.03\n-\n47.89\n49.78\n66.81\n68.28\n68.85\u00b10.18\n70.77\u00b10.10\n0.61\n28.77\n34.19\n41.17\u00b10.35\n53.55\u00b10.20\n\nTable 1: Comprehensive comparison. We report top1 accuracy on the ImageNet validation set. \u2217 represents for\nreference from QDROP[40] and \u2020 means scaling up the calibration set to 10240 images. QDROP\u2020 results are\nproduced by open-source codes from QDrop. We use n = 0, m = 1 for NWQ and n = \u22121, m = 2 for NWQ\u2020.\nWe follow BRECQ[27] to set the first and the last layer to 8-bit.\n\nClassical Calibration Set. Following previous works, we compare the performance on a small\ncalibration set of 1024 images. For start, we achieve 0.2%\u223c 0.9% improvement even compared with\nstrong baselines on W4A4, which are rather close to full precision accuracy. For W3A3, NWQ yields\n0.83%\u223c 3.26% and reduces the gap between W3A3 and W4A4. For the challenging W2A2, our\nmethod surpasses the previous state-of-the-art by a large margin of 13.37% for MobileNetV2 and\n12.4% for MnasNet, pushing the limit of extremely low-bit PTQ step further.\n\nScaled-up Calibration Set. To further explore the potential of NWQ, we scale up the calibration\nset by 10\u00d7 and reproduce QDROP on the scaled-up calibration set, building a very strong baseline.\nInterestingly, NWQ outperforms QDROP even with one-tenth data. When equipping NWQ with\nthe scaled-up calibration set, we observe significant improvement under all settings. Particularly,\nthe accuracy gaps of W2A2 between NWQ and QDROP are enlarged even more as the calibration\nset scales up. For example, the gap of MnasNet increases from 12.4% to 19.36%, and that of\nMobileNetV2 increases from 13.37% to 20.24%, showing the excellent scalability of our method.\n\n4.2 Ablation Study\n\n4.2.1 AR\n\nThe Effect of AR. We study the effect of AR via compar-\ning the experiments with layer-wise AR to the experiments\nwithout AR on W2A2. Table 2 shows that layer-wise AR\nimproves the accuracy significantly.\nIt\u2019s worth noting\nthat layer-wise AR improves 5.22% for MobileNetV2 and\n10.42% for MnasNet. To understand how AR works, we\nanalyze the loss curves during training in Figure 3. First,\nour AR experiment consistently converges to lower loss\n\n7\n\nFigure 3: AR vs. w/o AR on MNV2 with W2A2. Quanti-\nzation loss curves on train set of 1024 images and valida-\ntion set of 5 \u00d7 104 images.\n\n2.55.07.510.012.515.017.520.0iterations/10381012141618loss/103AR-trainAR-valw/o AR-trainw/o AR-val\fAR Granularity\nlayer-wise AR\nblock-wise AR\nstage-wise AR\nw/o AR\n\nRes18\n59.14\n59.12\n59.10\n57.94\n\nMNV2\n26.42\n26.52\n25.52\n21.20\n\nReg600M\n48.49\n48.89\n47.28\n46.82\n\nReg3.2G\n62.85\n61.31\n60.30\n59.82\n\nMnas\n41.17\n41.89\n41.25\n30.75\n\nAR\n\u2717\n\u2717\n\u2713\n\u2713\n\nAA\n\u2717\n\u2713\n\u2717\n\u2713\n\nRes18\n57.94\n58.99\n59.14\n59.52\n\nMNV2\n21.20\n24.86\n26.42\n26.90\n\nReg600M\n46.82\n47.92\n48.49\n48.62\n\nReg3.2G\n59.82\n62.19\n62.85\n63.52\n\nMnas\n30.75\n38.16\n41.17\n42.92\n\nTable 2: AR vs. w/o AR on W2A2 with our NWQ approach. Up:\nComparison of different AR granularities (i.e. Apply AR after each\nlayer/block/stage). Bottom: Experiments without AR.\n\nTable 3: AR vs. AutoAugment on W2A2. For AutoAugment, we use\nthe open-source codes from torchvision. AA is short for AutoAug-\nment.\n\nMethod\nLW\nRes18\nMNV2\n\n0.01\n0.96\n0.10\n\n0.1\n3.49\n0.11\n\nRSeR\n1\n34.75\n0.28\n\n10\n46.56\n0.87\n\n100\n44.54\n4.86\n\n1000\n43.83\n2.95\n\nASoftmax\n-\n59.14\n26.42\n\nInitialization\nrandom\nOurs\n\nBits(W/A)\n2/2\n2/2\n\nResNet18\n55.33\n59.14\n\nMobileNetV2\n22.00\n26.42\n\nTable 4: ASoftmax vs. RSeR on W2A2 with our NWQ approach. LW\nrepresents the regularization loss weight for AdaRound. We simply\nreplace ASoftmax with RSeR in our NWQ solution.\n\nTable 5: Comparison of different initialization methods. We compare\nrandom initialization with our method described in Sec. 3.2.2.\n\non both the train and test datasets, resulting in higher accuracy. Second, AR accelerates convergence\nsignificantly and thus allows NWQ to reach competitive accuracy at a substantially lower computing\ncost. Third, even though AR does not totally eliminate over-fitting, it effectively bridges the loss gap\nbetween train and test datasets.\n\nThe Effect of Granularity. Table 2 studies how AR granularity affects accuracy. AR granularity\nrepresents for applying AR after each layer/block/stage, while training(reconstruction) granularity\nrepresents for optimizing each layer/block/stage sequentially and independently. We discover that\nlightweight networks prefer coarse-grained AR, whereas heavyweight networks prefer fine-grained\nAR. MobileNetV2, RegNet-600MF, and MnasNet, for example, achieve the best performance with\nblock-wise AR, whereas ResNet18 and RegNet-3.2GF achieve the maximum accuracy with layer-wise\nAR.\n\nComparison with Data Augmentation. Data augmentation is a classical strategy used to relieve\nover-fitting due to its simplicity and effectiveness. We compare AR with strong augmentation (e.g.\nAutoAugment[8]) in Table 3. Notably, both AR and AutoAugment significantly improve accuracy,\nand AR contributes more than AutoAugment, particularly for sparse networks like MobileNetV2 and\nMnasNet. Coupling these two techniques enables us to achieve better performance.\n\n4.2.2 ASoftmax Study\n\nThe Effect of ASoftmax. Table 4 compares ASoftmax to Rectified Sigmoid with explicit Regulariza-\ntion (RSeR) which is used in AdaRound. We simply replace ASoftmax with RSeR in our NWQ and\nrun a grid search on regularization loss weight to explore the potential of RSeR. We discover that\nRSeR requires a huge loss weight for NWQ, which is 103 \u223c 105\u00d7 larger than that of subnet-wise\napproaches. Even with exhausting hyper-parameter search, RSeR does not perform as well as in the\nsubnet-wise method. ASoftmax, on the other hand, outperforms RSeR by 12.58% and 21.56% on\nResNet18 and MobileNetV2, respectively, without any additional hyper-parameter tuning.\n\nThe Effect of Initialization. Table 5 demonstrates the effectiveness of our initialization, which keeps\nthe quantization model close to the float-point at the start to steady the training. It is shown that our\ninitialization method significantly outperforms random initialization.\n\nThe Robustness of \u03c4 . We investigate the robustness of \u03c4 in Table 6. \u03c4 starts with a large number\nto encourage continuous relaxation and ends with a small value to favor one-hot distribution. The\nfinal \u03c4 T determines the decaying speed of \u03c4 . It can be observed that \u03c4 T has a robustness range of\n10\u22122 \u223c 10\u22124, making it easily adaptable to a wider range of neural network architectures.\n\n\u03c4 T\n1e-1\n1e-2\n1e-3\n1e-4\n\nRes18\n59.08\n59.14\n59.05\n58.98\n\nMNV2\n12.19\n26.42\n26.75\n24.74\n\nReg600M\n47.84\n48.49\n49.77\n49.42\n\nReg3.2G\n60.10\n62.85\n62.09\n62.06\n\nMnas\n33.92\n41.17\n41.34\n41.32\n\n#Images\n1024\n1024\n10240\n10240\n\nn/m Res18 MNV2\n26.42\n0/1\n28.73\n-1/2\n29.54\n0/1\n35.13\n-1/2\n\n59.14\n59.10\n61.61\n61.65\n\nReg600M Reg3.2G Mnas\n41.17\n41.07\n51.15\n53.55\n\n48.49\n49.73\n53.83\n56.36\n\n62.85\n61.68\n67.02\n67.02\n\nTable 6: Robustness of \u03c4 T for ASoftmax on W2A2. Experiments\nare conducted with \u03c4 0 = 1.0\n\nTable 7: Extensiveness of ASoftmax on W2A2. MNV2 is short for\nMobileNetV2.\n\n8\n\n\fMethod\nQDROP-style\nw/o Drop\nAMixup\n\nBits(W/A)\n2/2\n2/2\n2/2\n\nResNet18\n56.47\n58.42\n59.12\n\nMobileNetV2\n5.13\n25.09\n26.42\n\nTable 8: QDROP-style vs. w/o Drop vs. AMixup on W2A2 with\nour NWQ approach. QDROP-style represents for dropping methods\nof QDROP coupled with NWQ. w/o Drop represents for training\nwithout dropping or mixup operation.\n\nPs\n1\n0.8\n0.6\n0.5\n0.4\n0.2\n\nRes18 MNV2\n59.47\n25.34\n25.57\n59.39\n24.18\n59.32\n26.42\n59.14\n25.79\n59.08\n25.44\n58.96\n\nReg600M Reg3.2G Mnas\n40.55\n40.66\n40.86\n41.17\n40.63\n39.71\n\n50.53\n50.05\n49.84\n48.49\n48.83\n48.81\n\n63.06\n62.65\n61.90\n62.85\n61.73\n60.76\n\nTable 9: Decay Policy of AMixup on W2A2. We use Pe = 0 for all\nof the experiments.\n\nThe Extensiveness of ASoftmax. As described in Sec.3.2.1, ASoftmax can easily extend the discrete\nparameter space from {0, 1} to a larger integer range, offering the opportunity for a better optimum.\nHowever, extending discrete range benefits MobileNetV2 and RegNet-600MF while degrading\nRegNet-3.2GF on a small calibration set, as shown in Table 7 upper group. We argue that the enlarged\ndiscrete range raises the risk of over-fitting for large networks. When we scale up the calibration set,\nthe performance no longer degrades and the performance improves even more.\n\n4.2.3 AMixup Study\n\nThe Effect of AMixup. QDROP presents good performance in block-wise reconstruction, but fails\nin our network-wise scheme. As illustrated in Table 8, we achieve 1.95% improvement for ResNet18\nand 19.96% for MobileNetV2 by simply removing QDROP (w/o Drop row). We hypothesize\nthat randomly mixing activations up introduces inconsistency bias between the training and test\nphase, which is concealed by the improvement of flatness in block-wise reconstruction. However,\nas the reconstruction unit goes deeper (from block to network) and sparser, the problem becomes\nmore pronounced, resulting in significant degeneration. Over-parameterized ResNet18 is able to\ncompensate for the side effect, but MobileNetV2 suffers a great performance drop. Remarkably, our\nAMixup is able to overcome the inconsistency while taking advantage of the flatness, contributing to\nhigher accuracy.\n\nThe Policy of Decay. In Table 9, we run a grid search for Ps to explores the influence of different\ndecay policies. It\u2019s worth noting that MobileNetV2 and MnasNet prefer 0.5, whereas ResNet18 and\nRegNet prefer 1. The basic law is very similar to that of AR granularity, which states that heavyweight\nnetworks require more regularization, implying that AMixup and AR are both capable of regularizing\naccuracy.\n\n4.3 Efficiency\n\nWe compare the efficiency of several approaches\non ResNet18. According to Figure 4, NWQ out-\nperforms QDROP and BRECQ under various it-\nerations. Specifically, NWQ achieve competitive\nresults with only 10% computation cost, demon-\nstrating the efficiency of NWQ. As the number of\niterations increases, NWQ continues to improve\naccuracy, whereas BRECQ and QDROP reach\ntheir saturation at \u223c 1.6 \u00d7 104 iterations.\n\n", "conclusion": "\n\nFigure 4: Accuracy w.r.t. iterations. The experiments are con-\nducted on ResNet18 with W2A2.\n\nSimple algorithms that scale well are the core of\ndeep learning. Prior works tend to quantize net-\nworks in layer-wise and block-wise manner. We prove that network-wise quantization is a simpler\nand more effective way than layer/block-wise solutions. To accomplish this, we analyze the problems\nintroduced by the network-wise regime and propose three key designs to address them. Based on that,\nwe construct a simple and effective end-to-end network-wise PTQ framework. Extensive experiments\ndemonstrate that our solution is scalable, effective, and efficient. However, there is still a large\naccuracy gap between low-bit PTQ and full precision network, and we believe that there is still much\nroom for further improvement for PTQ, which is a key direction for future research.\n\n9\n\n248121620Iterations/103354045505560Accuracy/%53.2443.4735.3156.0449.7239.7357.7652.8941.19858.1353.7442.4658.9254.3342.52259.1454.2842.58NWQ(Ours)QDROPBRECQ\fBroder impacts. The proposed method uses GPU resources for training, thus resulting in a certain\namount of energy consumption and carbon footprints. For example, it takes about 4 GPU-hours for\nour method to quantize ResNet18, which would consume around 1 kWh and produce around 1 lbs of\nCO2, accelerating climate change and global warming in some degree.\n\n", "appendix": "", "system_prompts": [{"role": "system", "content": "You are a computer science researcher currently reviewing a paper titled \"Leveraging Inter-Layer Dependency for Post -Training Quantization\" for the NeurIPS computer science conference. Your goal is to try to be as objective and truthful as possible in your answers about the paper provided. Your reviews will be used for causal reasoning in determining the quality of the paper."}], "prompts": {"1b": {"role": "user", "content": "The following is the conclusion section of the paper you are reviewing:\n\n\nFigure 4: Accuracy w.r.t. iterations. The experiments are con-\nducted on ResNet18 with W2A2.\n\nSimple algorithms that scale well are the core of\ndeep learning. Prior works tend to quantize net-\nworks in layer-wise and block-wise manner. We prove that network-wise quantization is a simpler\nand more effective way than layer/block-wise solutions. To accomplish this, we analyze the problems\nintroduced by the network-wise regime and propose three key designs to address them. Based on that,\nwe construct a simple and effective end-to-end network-wise PTQ framework. Extensive experiments\ndemonstrate that our solution is scalable, effective, and efficient. However, there is still a large\naccuracy gap between low-bit PTQ and full precision network, and we believe that there is still much\nroom for further improvement for PTQ, which is a key direction for future research.\n\n9\n\n248121620Iterations/103354045505560Accuracy/%53.2443.4735.3156.0449.7239.7357.7652.8941.19858.1353.7442.4658.9254.3342.52259.1454.2842.58NWQ(Ours)QDROPBRECQ\fBroder impacts. The proposed method uses GPU resources for training, thus resulting in a certain\namount of energy consumption and carbon footprints. For example, it takes about 4 GPU-hours for\nour method to quantize ResNet18, which would consume around 1 kWh and produce around 1 lbs of\nCO2, accelerating climate change and global warming in some degree.\n\n\n\nBased on the section(s), please answer the following question with yes, no, or n/a and provide a brief justification for your answer.\nQuestion: Do the authors describe the limitations of their work?"}, "1c": {"role": "user", "content": "The following is the conclusion section of the paper you are reviewing:\n\n\nFigure 4: Accuracy w.r.t. iterations. The experiments are con-\nducted on ResNet18 with W2A2.\n\nSimple algorithms that scale well are the core of\ndeep learning. Prior works tend to quantize net-\nworks in layer-wise and block-wise manner. We prove that network-wise quantization is a simpler\nand more effective way than layer/block-wise solutions. To accomplish this, we analyze the problems\nintroduced by the network-wise regime and propose three key designs to address them. Based on that,\nwe construct a simple and effective end-to-end network-wise PTQ framework. Extensive experiments\ndemonstrate that our solution is scalable, effective, and efficient. However, there is still a large\naccuracy gap between low-bit PTQ and full precision network, and we believe that there is still much\nroom for further improvement for PTQ, which is a key direction for future research.\n\n9\n\n248121620Iterations/103354045505560Accuracy/%53.2443.4735.3156.0449.7239.7357.7652.8941.19858.1353.7442.4658.9254.3342.52259.1454.2842.58NWQ(Ours)QDROPBRECQ\fBroder impacts. The proposed method uses GPU resources for training, thus resulting in a certain\namount of energy consumption and carbon footprints. For example, it takes about 4 GPU-hours for\nour method to quantize ResNet18, which would consume around 1 kWh and produce around 1 lbs of\nCO2, accelerating climate change and global warming in some degree.\n\n\n\nBased on the section(s), please answer the following question with yes, no, or n/a and provide a brief justification for your answer.\nQuestion: Do the authors discuss any potential negative societal impacts of their work?"}, "3b": {"role": "user", "content": "The following is the experiments section of the paper you are reviewing:\n\n\nIn this section, we conduct extensive experiments with various architectures to verify our NWQ.\nWe first compare our method with previous state-of-the-art, and then we delve into our important\ndesigns and fully explore the scalability, effectiveness, and efficiency of our solution. We report top-1\nclassification accuracy on the ImageNet[9] validation set.\n\nImplementation Details. Our code is based on open-source codes BRECQ and QDrop. We\napply layer-wise AR. For ASoftmax, we set n = 0, m = 1 and \u03c4 is decayed from 1 to 0.01. For\nAMixup, we set default Ps as 0.5 and Pe as 0. We randomly sample 1024 images from ImageNet\ntrain set and employ Cutmix [45] and Mixup[46] as data augmentation. The learning rates are\n0.01 for round policy and 0.0004 for activation quantizer step size. We train for 20000 iterations\nwith a mini-batch size of 32 in 8 Tesla V100 GPUs, taking \u223c 30 minutes for ResNet18, which\nis on par with BRECQ and QDROP. Our experiments are conducted on 5 architectures, including\nResNet18(Res18), MobileNetV2(MNV2), RegNet-600MF(Reg600MF), RegNet-3.2GF(Reg3.2GF)\nand MnasNet(Mnas). Other settings remain same as QDROP[40] if not specified.\n\n4.1 Comprehensive Comparison\n\nTable 1 compares our NWQ with several main-stream PTQ approaches. Overall, NWQ outperforms\nprevious works across all neural network architectures and bit widths, and our performance improves\neven more as the bit width decreases.\n\n6\n\n\fFull Prec\nACIQ-Mix\u2217 [35]\nZeroQ\u2217 [5]\nLAPQ\u2217 [34]\nAdaQuant\u2217 [23]\nBit-Split\u2217 [39]\nAdaRound\u2217 [32]\nBRECQ\u2217 [27]\nQDROP\u2217 [40]\nQDROP\u2020 [40]\nNWQ(Ours)\nNWQ\u2020(Ours)\nAdaQuant\u2217 [23]\nAdaRound\u2217 [32]\nBRECQ\u2217 [27]\nQDROP\u2217 [40]\nQDROP\u2020 [40]\nNWQ(Ours)\nNWQ\u2020(Ours)\nBRECQ\u2217 [27]\nQDROP\u2217 [40]\nQDROP\u2020 [40]\nNWQ(Ours)\nNWQ\u2020(Ours)\n\nBits(W/A)\n32/32\n4/4\n4/4\n4/4\n4/4\n4/4\n4/4\n4/4\n4/4\n4/4\n4/4\n4/4\n3/3\n3/3\n3/3\n3/3\n3/3\n3/3\n3/3\n2/2\n2/2\n2/2\n2/2\n2/2\n\nRes18\n71.06\n67.00\n21.71\n60.30\n69.60\n67.56\n67.96\n69.60\n69.62\n69.88\n69.85\u00b10.04\n69.95\u00b10.07\n60.09\n64.66\n65.87\n66.75\n67.51\n67.58\u00b10.14\n68.15\u00b10.10\n42.54\n54.72\n57.84\n59.14\u00b10.05\n61.65\u00b10.12\n\nMobileNetV2\n72.49\n-\n26.24\n49.70\n47.16\n-\n61.52\n66.57\n68.84\n69.15\n69.14\u00b10.12\n69.60\u00b10.03\n2.23\n15.20\n23.41\n57.98\n58.54\n61.24\u00b10.21\n62.53\u00b10.05\n0.24\n13.05\n14.89\n26.42\u00b11.03\n35.13\u00b12.16\n\nReg600MF\n73.71\n-\n28.54\n57.71\n-\n-\n68.20\n68.33\n71.18\n71.45\n71.92\u00b10.07\n72.03\u00b10.09\n-\n51.01\n55.16\n65.54\n66.75\n67.38\u00b10.13\n68.31\u00b10.09\n3.58\n41.47\n45.93\n48.49\u00b10.03\n56.36\u00b10.01\n\nReg3.2GF\n78.36\n-\n12.24\n55.89\n-\n-\n73.85\n74.21\n76.66\n77.02\n77.40\u00b10.04\n77.42\u00b10.03\n-\n56.79\n57.12\n72.51\n73.48\n74.79\u00b10.09\n75.44\u00b10.08\n3.62\n55.11\n57.98\n62.85\u00b10.13\n67.02\u00b10.01\n\nMnasNet2.0\n76.68\n-\n3.89\n65.32\n-\n-\n68.86\n73.56\n73.71\n74.09\n74.60\u00b10.09\n74.86\u00b10.03\n-\n47.89\n49.78\n66.81\n68.28\n68.85\u00b10.18\n70.77\u00b10.10\n0.61\n28.77\n34.19\n41.17\u00b10.35\n53.55\u00b10.20\n\nTable 1: Comprehensive comparison. We report top1 accuracy on the ImageNet validation set. \u2217 represents for\nreference from QDROP[40] and \u2020 means scaling up the calibration set to 10240 images. QDROP\u2020 results are\nproduced by open-source codes from QDrop. We use n = 0, m = 1 for NWQ and n = \u22121, m = 2 for NWQ\u2020.\nWe follow BRECQ[27] to set the first and the last layer to 8-bit.\n\nClassical Calibration Set. Following previous works, we compare the performance on a small\ncalibration set of 1024 images. For start, we achieve 0.2%\u223c 0.9% improvement even compared with\nstrong baselines on W4A4, which are rather close to full precision accuracy. For W3A3, NWQ yields\n0.83%\u223c 3.26% and reduces the gap between W3A3 and W4A4. For the challenging W2A2, our\nmethod surpasses the previous state-of-the-art by a large margin of 13.37% for MobileNetV2 and\n12.4% for MnasNet, pushing the limit of extremely low-bit PTQ step further.\n\nScaled-up Calibration Set. To further explore the potential of NWQ, we scale up the calibration\nset by 10\u00d7 and reproduce QDROP on the scaled-up calibration set, building a very strong baseline.\nInterestingly, NWQ outperforms QDROP even with one-tenth data. When equipping NWQ with\nthe scaled-up calibration set, we observe significant improvement under all settings. Particularly,\nthe accuracy gaps of W2A2 between NWQ and QDROP are enlarged even more as the calibration\nset scales up. For example, the gap of MnasNet increases from 12.4% to 19.36%, and that of\nMobileNetV2 increases from 13.37% to 20.24%, showing the excellent scalability of our method.\n\n4.2 Ablation Study\n\n4.2.1 AR\n\nThe Effect of AR. We study the effect of AR via compar-\ning the experiments with layer-wise AR to the experiments\nwithout AR on W2A2. Table 2 shows that layer-wise AR\nimproves the accuracy significantly.\nIt\u2019s worth noting\nthat layer-wise AR improves 5.22% for MobileNetV2 and\n10.42% for MnasNet. To understand how AR works, we\nanalyze the loss curves during training in Figure 3. First,\nour AR experiment consistently converges to lower loss\n\n7\n\nFigure 3: AR vs. w/o AR on MNV2 with W2A2. Quanti-\nzation loss curves on train set of 1024 images and valida-\ntion set of 5 \u00d7 104 images.\n\n2.55.07.510.012.515.017.520.0iterations/10381012141618loss/103AR-trainAR-valw/o AR-trainw/o AR-val\fAR Granularity\nlayer-wise AR\nblock-wise AR\nstage-wise AR\nw/o AR\n\nRes18\n59.14\n59.12\n59.10\n57.94\n\nMNV2\n26.42\n26.52\n25.52\n21.20\n\nReg600M\n48.49\n48.89\n47.28\n46.82\n\nReg3.2G\n62.85\n61.31\n60.30\n59.82\n\nMnas\n41.17\n41.89\n41.25\n30.75\n\nAR\n\u2717\n\u2717\n\u2713\n\u2713\n\nAA\n\u2717\n\u2713\n\u2717\n\u2713\n\nRes18\n57.94\n58.99\n59.14\n59.52\n\nMNV2\n21.20\n24.86\n26.42\n26.90\n\nReg600M\n46.82\n47.92\n48.49\n48.62\n\nReg3.2G\n59.82\n62.19\n62.85\n63.52\n\nMnas\n30.75\n38.16\n41.17\n42.92\n\nTable 2: AR vs. w/o AR on W2A2 with our NWQ approach. Up:\nComparison of different AR granularities (i.e. Apply AR after each\nlayer/block/stage). Bottom: Experiments without AR.\n\nTable 3: AR vs. AutoAugment on W2A2. For AutoAugment, we use\nthe open-source codes from torchvision. AA is short for AutoAug-\nment.\n\nMethod\nLW\nRes18\nMNV2\n\n0.01\n0.96\n0.10\n\n0.1\n3.49\n0.11\n\nRSeR\n1\n34.75\n0.28\n\n10\n46.56\n0.87\n\n100\n44.54\n4.86\n\n1000\n43.83\n2.95\n\nASoftmax\n-\n59.14\n26.42\n\nInitialization\nrandom\nOurs\n\nBits(W/A)\n2/2\n2/2\n\nResNet18\n55.33\n59.14\n\nMobileNetV2\n22.00\n26.42\n\nTable 4: ASoftmax vs. RSeR on W2A2 with our NWQ approach. LW\nrepresents the regularization loss weight for AdaRound. We simply\nreplace ASoftmax with RSeR in our NWQ solution.\n\nTable 5: Comparison of different initialization methods. We compare\nrandom initialization with our method described in Sec. 3.2.2.\n\non both the train and test datasets, resulting in higher accuracy. Second, AR accelerates convergence\nsignificantly and thus allows NWQ to reach competitive accuracy at a substantially lower computing\ncost. Third, even though AR does not totally eliminate over-fitting, it effectively bridges the loss gap\nbetween train and test datasets.\n\nThe Effect of Granularity. Table 2 studies how AR granularity affects accuracy. AR granularity\nrepresents for applying AR after each layer/block/stage, while training(reconstruction) granularity\nrepresents for optimizing each layer/block/stage sequentially and independently. We discover that\nlightweight networks prefer coarse-grained AR, whereas heavyweight networks prefer fine-grained\nAR. MobileNetV2, RegNet-600MF, and MnasNet, for example, achieve the best performance with\nblock-wise AR, whereas ResNet18 and RegNet-3.2GF achieve the maximum accuracy with layer-wise\nAR.\n\nComparison with Data Augmentation. Data augmentation is a classical strategy used to relieve\nover-fitting due to its simplicity and effectiveness. We compare AR with strong augmentation (e.g.\nAutoAugment[8]) in Table 3. Notably, both AR and AutoAugment significantly improve accuracy,\nand AR contributes more than AutoAugment, particularly for sparse networks like MobileNetV2 and\nMnasNet. Coupling these two techniques enables us to achieve better performance.\n\n4.2.2 ASoftmax Study\n\nThe Effect of ASoftmax. Table 4 compares ASoftmax to Rectified Sigmoid with explicit Regulariza-\ntion (RSeR) which is used in AdaRound. We simply replace ASoftmax with RSeR in our NWQ and\nrun a grid search on regularization loss weight to explore the potential of RSeR. We discover that\nRSeR requires a huge loss weight for NWQ, which is 103 \u223c 105\u00d7 larger than that of subnet-wise\napproaches. Even with exhausting hyper-parameter search, RSeR does not perform as well as in the\nsubnet-wise method. ASoftmax, on the other hand, outperforms RSeR by 12.58% and 21.56% on\nResNet18 and MobileNetV2, respectively, without any additional hyper-parameter tuning.\n\nThe Effect of Initialization. Table 5 demonstrates the effectiveness of our initialization, which keeps\nthe quantization model close to the float-point at the start to steady the training. It is shown that our\ninitialization method significantly outperforms random initialization.\n\nThe Robustness of \u03c4 . We investigate the robustness of \u03c4 in Table 6. \u03c4 starts with a large number\nto encourage continuous relaxation and ends with a small value to favor one-hot distribution. The\nfinal \u03c4 T determines the decaying speed of \u03c4 . It can be observed that \u03c4 T has a robustness range of\n10\u22122 \u223c 10\u22124, making it easily adaptable to a wider range of neural network architectures.\n\n\u03c4 T\n1e-1\n1e-2\n1e-3\n1e-4\n\nRes18\n59.08\n59.14\n59.05\n58.98\n\nMNV2\n12.19\n26.42\n26.75\n24.74\n\nReg600M\n47.84\n48.49\n49.77\n49.42\n\nReg3.2G\n60.10\n62.85\n62.09\n62.06\n\nMnas\n33.92\n41.17\n41.34\n41.32\n\n#Images\n1024\n1024\n10240\n10240\n\nn/m Res18 MNV2\n26.42\n0/1\n28.73\n-1/2\n29.54\n0/1\n35.13\n-1/2\n\n59.14\n59.10\n61.61\n61.65\n\nReg600M Reg3.2G Mnas\n41.17\n41.07\n51.15\n53.55\n\n48.49\n49.73\n53.83\n56.36\n\n62.85\n61.68\n67.02\n67.02\n\nTable 6: Robustness of \u03c4 T for ASoftmax on W2A2. Experiments\nare conducted with \u03c4 0 = 1.0\n\nTable 7: Extensiveness of ASoftmax on W2A2. MNV2 is short for\nMobileNetV2.\n\n8\n\n\fMethod\nQDROP-style\nw/o Drop\nAMixup\n\nBits(W/A)\n2/2\n2/2\n2/2\n\nResNet18\n56.47\n58.42\n59.12\n\nMobileNetV2\n5.13\n25.09\n26.42\n\nTable 8: QDROP-style vs. w/o Drop vs. AMixup on W2A2 with\nour NWQ approach. QDROP-style represents for dropping methods\nof QDROP coupled with NWQ. w/o Drop represents for training\nwithout dropping or mixup operation.\n\nPs\n1\n0.8\n0.6\n0.5\n0.4\n0.2\n\nRes18 MNV2\n59.47\n25.34\n25.57\n59.39\n24.18\n59.32\n26.42\n59.14\n25.79\n59.08\n25.44\n58.96\n\nReg600M Reg3.2G Mnas\n40.55\n40.66\n40.86\n41.17\n40.63\n39.71\n\n50.53\n50.05\n49.84\n48.49\n48.83\n48.81\n\n63.06\n62.65\n61.90\n62.85\n61.73\n60.76\n\nTable 9: Decay Policy of AMixup on W2A2. We use Pe = 0 for all\nof the experiments.\n\nThe Extensiveness of ASoftmax. As described in Sec.3.2.1, ASoftmax can easily extend the discrete\nparameter space from {0, 1} to a larger integer range, offering the opportunity for a better optimum.\nHowever, extending discrete range benefits MobileNetV2 and RegNet-600MF while degrading\nRegNet-3.2GF on a small calibration set, as shown in Table 7 upper group. We argue that the enlarged\ndiscrete range raises the risk of over-fitting for large networks. When we scale up the calibration set,\nthe performance no longer degrades and the performance improves even more.\n\n4.2.3 AMixup Study\n\nThe Effect of AMixup. QDROP presents good performance in block-wise reconstruction, but fails\nin our network-wise scheme. As illustrated in Table 8, we achieve 1.95% improvement for ResNet18\nand 19.96% for MobileNetV2 by simply removing QDROP (w/o Drop row). We hypothesize\nthat randomly mixing activations up introduces inconsistency bias between the training and test\nphase, which is concealed by the improvement of flatness in block-wise reconstruction. However,\nas the reconstruction unit goes deeper (from block to network) and sparser, the problem becomes\nmore pronounced, resulting in significant degeneration. Over-parameterized ResNet18 is able to\ncompensate for the side effect, but MobileNetV2 suffers a great performance drop. Remarkably, our\nAMixup is able to overcome the inconsistency while taking advantage of the flatness, contributing to\nhigher accuracy.\n\nThe Policy of Decay. In Table 9, we run a grid search for Ps to explores the influence of different\ndecay policies. It\u2019s worth noting that MobileNetV2 and MnasNet prefer 0.5, whereas ResNet18 and\nRegNet prefer 1. The basic law is very similar to that of AR granularity, which states that heavyweight\nnetworks require more regularization, implying that AMixup and AR are both capable of regularizing\naccuracy.\n\n4.3 Efficiency\n\nWe compare the efficiency of several approaches\non ResNet18. According to Figure 4, NWQ out-\nperforms QDROP and BRECQ under various it-\nerations. Specifically, NWQ achieve competitive\nresults with only 10% computation cost, demon-\nstrating the efficiency of NWQ. As the number of\niterations increases, NWQ continues to improve\naccuracy, whereas BRECQ and QDROP reach\ntheir saturation at \u223c 1.6 \u00d7 104 iterations.\n\n\n\nBased on the section(s), please answer the following question with yes, no, or n/a and provide a brief justification for your answer.\nQuestion: If the authors ran experiments, do the authors specify all the training details (e.g., data splits, hyperparameters, how they were chosen)?"}, "3c": {"role": "user", "content": "The following is the experiments section of the paper you are reviewing:\n\n\nIn this section, we conduct extensive experiments with various architectures to verify our NWQ.\nWe first compare our method with previous state-of-the-art, and then we delve into our important\ndesigns and fully explore the scalability, effectiveness, and efficiency of our solution. We report top-1\nclassification accuracy on the ImageNet[9] validation set.\n\nImplementation Details. Our code is based on open-source codes BRECQ and QDrop. We\napply layer-wise AR. For ASoftmax, we set n = 0, m = 1 and \u03c4 is decayed from 1 to 0.01. For\nAMixup, we set default Ps as 0.5 and Pe as 0. We randomly sample 1024 images from ImageNet\ntrain set and employ Cutmix [45] and Mixup[46] as data augmentation. The learning rates are\n0.01 for round policy and 0.0004 for activation quantizer step size. We train for 20000 iterations\nwith a mini-batch size of 32 in 8 Tesla V100 GPUs, taking \u223c 30 minutes for ResNet18, which\nis on par with BRECQ and QDROP. Our experiments are conducted on 5 architectures, including\nResNet18(Res18), MobileNetV2(MNV2), RegNet-600MF(Reg600MF), RegNet-3.2GF(Reg3.2GF)\nand MnasNet(Mnas). Other settings remain same as QDROP[40] if not specified.\n\n4.1 Comprehensive Comparison\n\nTable 1 compares our NWQ with several main-stream PTQ approaches. Overall, NWQ outperforms\nprevious works across all neural network architectures and bit widths, and our performance improves\neven more as the bit width decreases.\n\n6\n\n\fFull Prec\nACIQ-Mix\u2217 [35]\nZeroQ\u2217 [5]\nLAPQ\u2217 [34]\nAdaQuant\u2217 [23]\nBit-Split\u2217 [39]\nAdaRound\u2217 [32]\nBRECQ\u2217 [27]\nQDROP\u2217 [40]\nQDROP\u2020 [40]\nNWQ(Ours)\nNWQ\u2020(Ours)\nAdaQuant\u2217 [23]\nAdaRound\u2217 [32]\nBRECQ\u2217 [27]\nQDROP\u2217 [40]\nQDROP\u2020 [40]\nNWQ(Ours)\nNWQ\u2020(Ours)\nBRECQ\u2217 [27]\nQDROP\u2217 [40]\nQDROP\u2020 [40]\nNWQ(Ours)\nNWQ\u2020(Ours)\n\nBits(W/A)\n32/32\n4/4\n4/4\n4/4\n4/4\n4/4\n4/4\n4/4\n4/4\n4/4\n4/4\n4/4\n3/3\n3/3\n3/3\n3/3\n3/3\n3/3\n3/3\n2/2\n2/2\n2/2\n2/2\n2/2\n\nRes18\n71.06\n67.00\n21.71\n60.30\n69.60\n67.56\n67.96\n69.60\n69.62\n69.88\n69.85\u00b10.04\n69.95\u00b10.07\n60.09\n64.66\n65.87\n66.75\n67.51\n67.58\u00b10.14\n68.15\u00b10.10\n42.54\n54.72\n57.84\n59.14\u00b10.05\n61.65\u00b10.12\n\nMobileNetV2\n72.49\n-\n26.24\n49.70\n47.16\n-\n61.52\n66.57\n68.84\n69.15\n69.14\u00b10.12\n69.60\u00b10.03\n2.23\n15.20\n23.41\n57.98\n58.54\n61.24\u00b10.21\n62.53\u00b10.05\n0.24\n13.05\n14.89\n26.42\u00b11.03\n35.13\u00b12.16\n\nReg600MF\n73.71\n-\n28.54\n57.71\n-\n-\n68.20\n68.33\n71.18\n71.45\n71.92\u00b10.07\n72.03\u00b10.09\n-\n51.01\n55.16\n65.54\n66.75\n67.38\u00b10.13\n68.31\u00b10.09\n3.58\n41.47\n45.93\n48.49\u00b10.03\n56.36\u00b10.01\n\nReg3.2GF\n78.36\n-\n12.24\n55.89\n-\n-\n73.85\n74.21\n76.66\n77.02\n77.40\u00b10.04\n77.42\u00b10.03\n-\n56.79\n57.12\n72.51\n73.48\n74.79\u00b10.09\n75.44\u00b10.08\n3.62\n55.11\n57.98\n62.85\u00b10.13\n67.02\u00b10.01\n\nMnasNet2.0\n76.68\n-\n3.89\n65.32\n-\n-\n68.86\n73.56\n73.71\n74.09\n74.60\u00b10.09\n74.86\u00b10.03\n-\n47.89\n49.78\n66.81\n68.28\n68.85\u00b10.18\n70.77\u00b10.10\n0.61\n28.77\n34.19\n41.17\u00b10.35\n53.55\u00b10.20\n\nTable 1: Comprehensive comparison. We report top1 accuracy on the ImageNet validation set. \u2217 represents for\nreference from QDROP[40] and \u2020 means scaling up the calibration set to 10240 images. QDROP\u2020 results are\nproduced by open-source codes from QDrop. We use n = 0, m = 1 for NWQ and n = \u22121, m = 2 for NWQ\u2020.\nWe follow BRECQ[27] to set the first and the last layer to 8-bit.\n\nClassical Calibration Set. Following previous works, we compare the performance on a small\ncalibration set of 1024 images. For start, we achieve 0.2%\u223c 0.9% improvement even compared with\nstrong baselines on W4A4, which are rather close to full precision accuracy. For W3A3, NWQ yields\n0.83%\u223c 3.26% and reduces the gap between W3A3 and W4A4. For the challenging W2A2, our\nmethod surpasses the previous state-of-the-art by a large margin of 13.37% for MobileNetV2 and\n12.4% for MnasNet, pushing the limit of extremely low-bit PTQ step further.\n\nScaled-up Calibration Set. To further explore the potential of NWQ, we scale up the calibration\nset by 10\u00d7 and reproduce QDROP on the scaled-up calibration set, building a very strong baseline.\nInterestingly, NWQ outperforms QDROP even with one-tenth data. When equipping NWQ with\nthe scaled-up calibration set, we observe significant improvement under all settings. Particularly,\nthe accuracy gaps of W2A2 between NWQ and QDROP are enlarged even more as the calibration\nset scales up. For example, the gap of MnasNet increases from 12.4% to 19.36%, and that of\nMobileNetV2 increases from 13.37% to 20.24%, showing the excellent scalability of our method.\n\n4.2 Ablation Study\n\n4.2.1 AR\n\nThe Effect of AR. We study the effect of AR via compar-\ning the experiments with layer-wise AR to the experiments\nwithout AR on W2A2. Table 2 shows that layer-wise AR\nimproves the accuracy significantly.\nIt\u2019s worth noting\nthat layer-wise AR improves 5.22% for MobileNetV2 and\n10.42% for MnasNet. To understand how AR works, we\nanalyze the loss curves during training in Figure 3. First,\nour AR experiment consistently converges to lower loss\n\n7\n\nFigure 3: AR vs. w/o AR on MNV2 with W2A2. Quanti-\nzation loss curves on train set of 1024 images and valida-\ntion set of 5 \u00d7 104 images.\n\n2.55.07.510.012.515.017.520.0iterations/10381012141618loss/103AR-trainAR-valw/o AR-trainw/o AR-val\fAR Granularity\nlayer-wise AR\nblock-wise AR\nstage-wise AR\nw/o AR\n\nRes18\n59.14\n59.12\n59.10\n57.94\n\nMNV2\n26.42\n26.52\n25.52\n21.20\n\nReg600M\n48.49\n48.89\n47.28\n46.82\n\nReg3.2G\n62.85\n61.31\n60.30\n59.82\n\nMnas\n41.17\n41.89\n41.25\n30.75\n\nAR\n\u2717\n\u2717\n\u2713\n\u2713\n\nAA\n\u2717\n\u2713\n\u2717\n\u2713\n\nRes18\n57.94\n58.99\n59.14\n59.52\n\nMNV2\n21.20\n24.86\n26.42\n26.90\n\nReg600M\n46.82\n47.92\n48.49\n48.62\n\nReg3.2G\n59.82\n62.19\n62.85\n63.52\n\nMnas\n30.75\n38.16\n41.17\n42.92\n\nTable 2: AR vs. w/o AR on W2A2 with our NWQ approach. Up:\nComparison of different AR granularities (i.e. Apply AR after each\nlayer/block/stage). Bottom: Experiments without AR.\n\nTable 3: AR vs. AutoAugment on W2A2. For AutoAugment, we use\nthe open-source codes from torchvision. AA is short for AutoAug-\nment.\n\nMethod\nLW\nRes18\nMNV2\n\n0.01\n0.96\n0.10\n\n0.1\n3.49\n0.11\n\nRSeR\n1\n34.75\n0.28\n\n10\n46.56\n0.87\n\n100\n44.54\n4.86\n\n1000\n43.83\n2.95\n\nASoftmax\n-\n59.14\n26.42\n\nInitialization\nrandom\nOurs\n\nBits(W/A)\n2/2\n2/2\n\nResNet18\n55.33\n59.14\n\nMobileNetV2\n22.00\n26.42\n\nTable 4: ASoftmax vs. RSeR on W2A2 with our NWQ approach. LW\nrepresents the regularization loss weight for AdaRound. We simply\nreplace ASoftmax with RSeR in our NWQ solution.\n\nTable 5: Comparison of different initialization methods. We compare\nrandom initialization with our method described in Sec. 3.2.2.\n\non both the train and test datasets, resulting in higher accuracy. Second, AR accelerates convergence\nsignificantly and thus allows NWQ to reach competitive accuracy at a substantially lower computing\ncost. Third, even though AR does not totally eliminate over-fitting, it effectively bridges the loss gap\nbetween train and test datasets.\n\nThe Effect of Granularity. Table 2 studies how AR granularity affects accuracy. AR granularity\nrepresents for applying AR after each layer/block/stage, while training(reconstruction) granularity\nrepresents for optimizing each layer/block/stage sequentially and independently. We discover that\nlightweight networks prefer coarse-grained AR, whereas heavyweight networks prefer fine-grained\nAR. MobileNetV2, RegNet-600MF, and MnasNet, for example, achieve the best performance with\nblock-wise AR, whereas ResNet18 and RegNet-3.2GF achieve the maximum accuracy with layer-wise\nAR.\n\nComparison with Data Augmentation. Data augmentation is a classical strategy used to relieve\nover-fitting due to its simplicity and effectiveness. We compare AR with strong augmentation (e.g.\nAutoAugment[8]) in Table 3. Notably, both AR and AutoAugment significantly improve accuracy,\nand AR contributes more than AutoAugment, particularly for sparse networks like MobileNetV2 and\nMnasNet. Coupling these two techniques enables us to achieve better performance.\n\n4.2.2 ASoftmax Study\n\nThe Effect of ASoftmax. Table 4 compares ASoftmax to Rectified Sigmoid with explicit Regulariza-\ntion (RSeR) which is used in AdaRound. We simply replace ASoftmax with RSeR in our NWQ and\nrun a grid search on regularization loss weight to explore the potential of RSeR. We discover that\nRSeR requires a huge loss weight for NWQ, which is 103 \u223c 105\u00d7 larger than that of subnet-wise\napproaches. Even with exhausting hyper-parameter search, RSeR does not perform as well as in the\nsubnet-wise method. ASoftmax, on the other hand, outperforms RSeR by 12.58% and 21.56% on\nResNet18 and MobileNetV2, respectively, without any additional hyper-parameter tuning.\n\nThe Effect of Initialization. Table 5 demonstrates the effectiveness of our initialization, which keeps\nthe quantization model close to the float-point at the start to steady the training. It is shown that our\ninitialization method significantly outperforms random initialization.\n\nThe Robustness of \u03c4 . We investigate the robustness of \u03c4 in Table 6. \u03c4 starts with a large number\nto encourage continuous relaxation and ends with a small value to favor one-hot distribution. The\nfinal \u03c4 T determines the decaying speed of \u03c4 . It can be observed that \u03c4 T has a robustness range of\n10\u22122 \u223c 10\u22124, making it easily adaptable to a wider range of neural network architectures.\n\n\u03c4 T\n1e-1\n1e-2\n1e-3\n1e-4\n\nRes18\n59.08\n59.14\n59.05\n58.98\n\nMNV2\n12.19\n26.42\n26.75\n24.74\n\nReg600M\n47.84\n48.49\n49.77\n49.42\n\nReg3.2G\n60.10\n62.85\n62.09\n62.06\n\nMnas\n33.92\n41.17\n41.34\n41.32\n\n#Images\n1024\n1024\n10240\n10240\n\nn/m Res18 MNV2\n26.42\n0/1\n28.73\n-1/2\n29.54\n0/1\n35.13\n-1/2\n\n59.14\n59.10\n61.61\n61.65\n\nReg600M Reg3.2G Mnas\n41.17\n41.07\n51.15\n53.55\n\n48.49\n49.73\n53.83\n56.36\n\n62.85\n61.68\n67.02\n67.02\n\nTable 6: Robustness of \u03c4 T for ASoftmax on W2A2. Experiments\nare conducted with \u03c4 0 = 1.0\n\nTable 7: Extensiveness of ASoftmax on W2A2. MNV2 is short for\nMobileNetV2.\n\n8\n\n\fMethod\nQDROP-style\nw/o Drop\nAMixup\n\nBits(W/A)\n2/2\n2/2\n2/2\n\nResNet18\n56.47\n58.42\n59.12\n\nMobileNetV2\n5.13\n25.09\n26.42\n\nTable 8: QDROP-style vs. w/o Drop vs. AMixup on W2A2 with\nour NWQ approach. QDROP-style represents for dropping methods\nof QDROP coupled with NWQ. w/o Drop represents for training\nwithout dropping or mixup operation.\n\nPs\n1\n0.8\n0.6\n0.5\n0.4\n0.2\n\nRes18 MNV2\n59.47\n25.34\n25.57\n59.39\n24.18\n59.32\n26.42\n59.14\n25.79\n59.08\n25.44\n58.96\n\nReg600M Reg3.2G Mnas\n40.55\n40.66\n40.86\n41.17\n40.63\n39.71\n\n50.53\n50.05\n49.84\n48.49\n48.83\n48.81\n\n63.06\n62.65\n61.90\n62.85\n61.73\n60.76\n\nTable 9: Decay Policy of AMixup on W2A2. We use Pe = 0 for all\nof the experiments.\n\nThe Extensiveness of ASoftmax. As described in Sec.3.2.1, ASoftmax can easily extend the discrete\nparameter space from {0, 1} to a larger integer range, offering the opportunity for a better optimum.\nHowever, extending discrete range benefits MobileNetV2 and RegNet-600MF while degrading\nRegNet-3.2GF on a small calibration set, as shown in Table 7 upper group. We argue that the enlarged\ndiscrete range raises the risk of over-fitting for large networks. When we scale up the calibration set,\nthe performance no longer degrades and the performance improves even more.\n\n4.2.3 AMixup Study\n\nThe Effect of AMixup. QDROP presents good performance in block-wise reconstruction, but fails\nin our network-wise scheme. As illustrated in Table 8, we achieve 1.95% improvement for ResNet18\nand 19.96% for MobileNetV2 by simply removing QDROP (w/o Drop row). We hypothesize\nthat randomly mixing activations up introduces inconsistency bias between the training and test\nphase, which is concealed by the improvement of flatness in block-wise reconstruction. However,\nas the reconstruction unit goes deeper (from block to network) and sparser, the problem becomes\nmore pronounced, resulting in significant degeneration. Over-parameterized ResNet18 is able to\ncompensate for the side effect, but MobileNetV2 suffers a great performance drop. Remarkably, our\nAMixup is able to overcome the inconsistency while taking advantage of the flatness, contributing to\nhigher accuracy.\n\nThe Policy of Decay. In Table 9, we run a grid search for Ps to explores the influence of different\ndecay policies. It\u2019s worth noting that MobileNetV2 and MnasNet prefer 0.5, whereas ResNet18 and\nRegNet prefer 1. The basic law is very similar to that of AR granularity, which states that heavyweight\nnetworks require more regularization, implying that AMixup and AR are both capable of regularizing\naccuracy.\n\n4.3 Efficiency\n\nWe compare the efficiency of several approaches\non ResNet18. According to Figure 4, NWQ out-\nperforms QDROP and BRECQ under various it-\nerations. Specifically, NWQ achieve competitive\nresults with only 10% computation cost, demon-\nstrating the efficiency of NWQ. As the number of\niterations increases, NWQ continues to improve\naccuracy, whereas BRECQ and QDROP reach\ntheir saturation at \u223c 1.6 \u00d7 104 iterations.\n\n\n\nBased on the section(s), please answer the following question with yes, no, or n/a and provide a brief justification for your answer.\nQuestion: If the authors ran experiments, do the authors report error bars (e.g., with respect to the random seed after running experiments multiple times)?"}, "3d": {"role": "user", "content": "The following is the experiments section of the paper you are reviewing:\n\n\nIn this section, we conduct extensive experiments with various architectures to verify our NWQ.\nWe first compare our method with previous state-of-the-art, and then we delve into our important\ndesigns and fully explore the scalability, effectiveness, and efficiency of our solution. We report top-1\nclassification accuracy on the ImageNet[9] validation set.\n\nImplementation Details. Our code is based on open-source codes BRECQ and QDrop. We\napply layer-wise AR. For ASoftmax, we set n = 0, m = 1 and \u03c4 is decayed from 1 to 0.01. For\nAMixup, we set default Ps as 0.5 and Pe as 0. We randomly sample 1024 images from ImageNet\ntrain set and employ Cutmix [45] and Mixup[46] as data augmentation. The learning rates are\n0.01 for round policy and 0.0004 for activation quantizer step size. We train for 20000 iterations\nwith a mini-batch size of 32 in 8 Tesla V100 GPUs, taking \u223c 30 minutes for ResNet18, which\nis on par with BRECQ and QDROP. Our experiments are conducted on 5 architectures, including\nResNet18(Res18), MobileNetV2(MNV2), RegNet-600MF(Reg600MF), RegNet-3.2GF(Reg3.2GF)\nand MnasNet(Mnas). Other settings remain same as QDROP[40] if not specified.\n\n4.1 Comprehensive Comparison\n\nTable 1 compares our NWQ with several main-stream PTQ approaches. Overall, NWQ outperforms\nprevious works across all neural network architectures and bit widths, and our performance improves\neven more as the bit width decreases.\n\n6\n\n\fFull Prec\nACIQ-Mix\u2217 [35]\nZeroQ\u2217 [5]\nLAPQ\u2217 [34]\nAdaQuant\u2217 [23]\nBit-Split\u2217 [39]\nAdaRound\u2217 [32]\nBRECQ\u2217 [27]\nQDROP\u2217 [40]\nQDROP\u2020 [40]\nNWQ(Ours)\nNWQ\u2020(Ours)\nAdaQuant\u2217 [23]\nAdaRound\u2217 [32]\nBRECQ\u2217 [27]\nQDROP\u2217 [40]\nQDROP\u2020 [40]\nNWQ(Ours)\nNWQ\u2020(Ours)\nBRECQ\u2217 [27]\nQDROP\u2217 [40]\nQDROP\u2020 [40]\nNWQ(Ours)\nNWQ\u2020(Ours)\n\nBits(W/A)\n32/32\n4/4\n4/4\n4/4\n4/4\n4/4\n4/4\n4/4\n4/4\n4/4\n4/4\n4/4\n3/3\n3/3\n3/3\n3/3\n3/3\n3/3\n3/3\n2/2\n2/2\n2/2\n2/2\n2/2\n\nRes18\n71.06\n67.00\n21.71\n60.30\n69.60\n67.56\n67.96\n69.60\n69.62\n69.88\n69.85\u00b10.04\n69.95\u00b10.07\n60.09\n64.66\n65.87\n66.75\n67.51\n67.58\u00b10.14\n68.15\u00b10.10\n42.54\n54.72\n57.84\n59.14\u00b10.05\n61.65\u00b10.12\n\nMobileNetV2\n72.49\n-\n26.24\n49.70\n47.16\n-\n61.52\n66.57\n68.84\n69.15\n69.14\u00b10.12\n69.60\u00b10.03\n2.23\n15.20\n23.41\n57.98\n58.54\n61.24\u00b10.21\n62.53\u00b10.05\n0.24\n13.05\n14.89\n26.42\u00b11.03\n35.13\u00b12.16\n\nReg600MF\n73.71\n-\n28.54\n57.71\n-\n-\n68.20\n68.33\n71.18\n71.45\n71.92\u00b10.07\n72.03\u00b10.09\n-\n51.01\n55.16\n65.54\n66.75\n67.38\u00b10.13\n68.31\u00b10.09\n3.58\n41.47\n45.93\n48.49\u00b10.03\n56.36\u00b10.01\n\nReg3.2GF\n78.36\n-\n12.24\n55.89\n-\n-\n73.85\n74.21\n76.66\n77.02\n77.40\u00b10.04\n77.42\u00b10.03\n-\n56.79\n57.12\n72.51\n73.48\n74.79\u00b10.09\n75.44\u00b10.08\n3.62\n55.11\n57.98\n62.85\u00b10.13\n67.02\u00b10.01\n\nMnasNet2.0\n76.68\n-\n3.89\n65.32\n-\n-\n68.86\n73.56\n73.71\n74.09\n74.60\u00b10.09\n74.86\u00b10.03\n-\n47.89\n49.78\n66.81\n68.28\n68.85\u00b10.18\n70.77\u00b10.10\n0.61\n28.77\n34.19\n41.17\u00b10.35\n53.55\u00b10.20\n\nTable 1: Comprehensive comparison. We report top1 accuracy on the ImageNet validation set. \u2217 represents for\nreference from QDROP[40] and \u2020 means scaling up the calibration set to 10240 images. QDROP\u2020 results are\nproduced by open-source codes from QDrop. We use n = 0, m = 1 for NWQ and n = \u22121, m = 2 for NWQ\u2020.\nWe follow BRECQ[27] to set the first and the last layer to 8-bit.\n\nClassical Calibration Set. Following previous works, we compare the performance on a small\ncalibration set of 1024 images. For start, we achieve 0.2%\u223c 0.9% improvement even compared with\nstrong baselines on W4A4, which are rather close to full precision accuracy. For W3A3, NWQ yields\n0.83%\u223c 3.26% and reduces the gap between W3A3 and W4A4. For the challenging W2A2, our\nmethod surpasses the previous state-of-the-art by a large margin of 13.37% for MobileNetV2 and\n12.4% for MnasNet, pushing the limit of extremely low-bit PTQ step further.\n\nScaled-up Calibration Set. To further explore the potential of NWQ, we scale up the calibration\nset by 10\u00d7 and reproduce QDROP on the scaled-up calibration set, building a very strong baseline.\nInterestingly, NWQ outperforms QDROP even with one-tenth data. When equipping NWQ with\nthe scaled-up calibration set, we observe significant improvement under all settings. Particularly,\nthe accuracy gaps of W2A2 between NWQ and QDROP are enlarged even more as the calibration\nset scales up. For example, the gap of MnasNet increases from 12.4% to 19.36%, and that of\nMobileNetV2 increases from 13.37% to 20.24%, showing the excellent scalability of our method.\n\n4.2 Ablation Study\n\n4.2.1 AR\n\nThe Effect of AR. We study the effect of AR via compar-\ning the experiments with layer-wise AR to the experiments\nwithout AR on W2A2. Table 2 shows that layer-wise AR\nimproves the accuracy significantly.\nIt\u2019s worth noting\nthat layer-wise AR improves 5.22% for MobileNetV2 and\n10.42% for MnasNet. To understand how AR works, we\nanalyze the loss curves during training in Figure 3. First,\nour AR experiment consistently converges to lower loss\n\n7\n\nFigure 3: AR vs. w/o AR on MNV2 with W2A2. Quanti-\nzation loss curves on train set of 1024 images and valida-\ntion set of 5 \u00d7 104 images.\n\n2.55.07.510.012.515.017.520.0iterations/10381012141618loss/103AR-trainAR-valw/o AR-trainw/o AR-val\fAR Granularity\nlayer-wise AR\nblock-wise AR\nstage-wise AR\nw/o AR\n\nRes18\n59.14\n59.12\n59.10\n57.94\n\nMNV2\n26.42\n26.52\n25.52\n21.20\n\nReg600M\n48.49\n48.89\n47.28\n46.82\n\nReg3.2G\n62.85\n61.31\n60.30\n59.82\n\nMnas\n41.17\n41.89\n41.25\n30.75\n\nAR\n\u2717\n\u2717\n\u2713\n\u2713\n\nAA\n\u2717\n\u2713\n\u2717\n\u2713\n\nRes18\n57.94\n58.99\n59.14\n59.52\n\nMNV2\n21.20\n24.86\n26.42\n26.90\n\nReg600M\n46.82\n47.92\n48.49\n48.62\n\nReg3.2G\n59.82\n62.19\n62.85\n63.52\n\nMnas\n30.75\n38.16\n41.17\n42.92\n\nTable 2: AR vs. w/o AR on W2A2 with our NWQ approach. Up:\nComparison of different AR granularities (i.e. Apply AR after each\nlayer/block/stage). Bottom: Experiments without AR.\n\nTable 3: AR vs. AutoAugment on W2A2. For AutoAugment, we use\nthe open-source codes from torchvision. AA is short for AutoAug-\nment.\n\nMethod\nLW\nRes18\nMNV2\n\n0.01\n0.96\n0.10\n\n0.1\n3.49\n0.11\n\nRSeR\n1\n34.75\n0.28\n\n10\n46.56\n0.87\n\n100\n44.54\n4.86\n\n1000\n43.83\n2.95\n\nASoftmax\n-\n59.14\n26.42\n\nInitialization\nrandom\nOurs\n\nBits(W/A)\n2/2\n2/2\n\nResNet18\n55.33\n59.14\n\nMobileNetV2\n22.00\n26.42\n\nTable 4: ASoftmax vs. RSeR on W2A2 with our NWQ approach. LW\nrepresents the regularization loss weight for AdaRound. We simply\nreplace ASoftmax with RSeR in our NWQ solution.\n\nTable 5: Comparison of different initialization methods. We compare\nrandom initialization with our method described in Sec. 3.2.2.\n\non both the train and test datasets, resulting in higher accuracy. Second, AR accelerates convergence\nsignificantly and thus allows NWQ to reach competitive accuracy at a substantially lower computing\ncost. Third, even though AR does not totally eliminate over-fitting, it effectively bridges the loss gap\nbetween train and test datasets.\n\nThe Effect of Granularity. Table 2 studies how AR granularity affects accuracy. AR granularity\nrepresents for applying AR after each layer/block/stage, while training(reconstruction) granularity\nrepresents for optimizing each layer/block/stage sequentially and independently. We discover that\nlightweight networks prefer coarse-grained AR, whereas heavyweight networks prefer fine-grained\nAR. MobileNetV2, RegNet-600MF, and MnasNet, for example, achieve the best performance with\nblock-wise AR, whereas ResNet18 and RegNet-3.2GF achieve the maximum accuracy with layer-wise\nAR.\n\nComparison with Data Augmentation. Data augmentation is a classical strategy used to relieve\nover-fitting due to its simplicity and effectiveness. We compare AR with strong augmentation (e.g.\nAutoAugment[8]) in Table 3. Notably, both AR and AutoAugment significantly improve accuracy,\nand AR contributes more than AutoAugment, particularly for sparse networks like MobileNetV2 and\nMnasNet. Coupling these two techniques enables us to achieve better performance.\n\n4.2.2 ASoftmax Study\n\nThe Effect of ASoftmax. Table 4 compares ASoftmax to Rectified Sigmoid with explicit Regulariza-\ntion (RSeR) which is used in AdaRound. We simply replace ASoftmax with RSeR in our NWQ and\nrun a grid search on regularization loss weight to explore the potential of RSeR. We discover that\nRSeR requires a huge loss weight for NWQ, which is 103 \u223c 105\u00d7 larger than that of subnet-wise\napproaches. Even with exhausting hyper-parameter search, RSeR does not perform as well as in the\nsubnet-wise method. ASoftmax, on the other hand, outperforms RSeR by 12.58% and 21.56% on\nResNet18 and MobileNetV2, respectively, without any additional hyper-parameter tuning.\n\nThe Effect of Initialization. Table 5 demonstrates the effectiveness of our initialization, which keeps\nthe quantization model close to the float-point at the start to steady the training. It is shown that our\ninitialization method significantly outperforms random initialization.\n\nThe Robustness of \u03c4 . We investigate the robustness of \u03c4 in Table 6. \u03c4 starts with a large number\nto encourage continuous relaxation and ends with a small value to favor one-hot distribution. The\nfinal \u03c4 T determines the decaying speed of \u03c4 . It can be observed that \u03c4 T has a robustness range of\n10\u22122 \u223c 10\u22124, making it easily adaptable to a wider range of neural network architectures.\n\n\u03c4 T\n1e-1\n1e-2\n1e-3\n1e-4\n\nRes18\n59.08\n59.14\n59.05\n58.98\n\nMNV2\n12.19\n26.42\n26.75\n24.74\n\nReg600M\n47.84\n48.49\n49.77\n49.42\n\nReg3.2G\n60.10\n62.85\n62.09\n62.06\n\nMnas\n33.92\n41.17\n41.34\n41.32\n\n#Images\n1024\n1024\n10240\n10240\n\nn/m Res18 MNV2\n26.42\n0/1\n28.73\n-1/2\n29.54\n0/1\n35.13\n-1/2\n\n59.14\n59.10\n61.61\n61.65\n\nReg600M Reg3.2G Mnas\n41.17\n41.07\n51.15\n53.55\n\n48.49\n49.73\n53.83\n56.36\n\n62.85\n61.68\n67.02\n67.02\n\nTable 6: Robustness of \u03c4 T for ASoftmax on W2A2. Experiments\nare conducted with \u03c4 0 = 1.0\n\nTable 7: Extensiveness of ASoftmax on W2A2. MNV2 is short for\nMobileNetV2.\n\n8\n\n\fMethod\nQDROP-style\nw/o Drop\nAMixup\n\nBits(W/A)\n2/2\n2/2\n2/2\n\nResNet18\n56.47\n58.42\n59.12\n\nMobileNetV2\n5.13\n25.09\n26.42\n\nTable 8: QDROP-style vs. w/o Drop vs. AMixup on W2A2 with\nour NWQ approach. QDROP-style represents for dropping methods\nof QDROP coupled with NWQ. w/o Drop represents for training\nwithout dropping or mixup operation.\n\nPs\n1\n0.8\n0.6\n0.5\n0.4\n0.2\n\nRes18 MNV2\n59.47\n25.34\n25.57\n59.39\n24.18\n59.32\n26.42\n59.14\n25.79\n59.08\n25.44\n58.96\n\nReg600M Reg3.2G Mnas\n40.55\n40.66\n40.86\n41.17\n40.63\n39.71\n\n50.53\n50.05\n49.84\n48.49\n48.83\n48.81\n\n63.06\n62.65\n61.90\n62.85\n61.73\n60.76\n\nTable 9: Decay Policy of AMixup on W2A2. We use Pe = 0 for all\nof the experiments.\n\nThe Extensiveness of ASoftmax. As described in Sec.3.2.1, ASoftmax can easily extend the discrete\nparameter space from {0, 1} to a larger integer range, offering the opportunity for a better optimum.\nHowever, extending discrete range benefits MobileNetV2 and RegNet-600MF while degrading\nRegNet-3.2GF on a small calibration set, as shown in Table 7 upper group. We argue that the enlarged\ndiscrete range raises the risk of over-fitting for large networks. When we scale up the calibration set,\nthe performance no longer degrades and the performance improves even more.\n\n4.2.3 AMixup Study\n\nThe Effect of AMixup. QDROP presents good performance in block-wise reconstruction, but fails\nin our network-wise scheme. As illustrated in Table 8, we achieve 1.95% improvement for ResNet18\nand 19.96% for MobileNetV2 by simply removing QDROP (w/o Drop row). We hypothesize\nthat randomly mixing activations up introduces inconsistency bias between the training and test\nphase, which is concealed by the improvement of flatness in block-wise reconstruction. However,\nas the reconstruction unit goes deeper (from block to network) and sparser, the problem becomes\nmore pronounced, resulting in significant degeneration. Over-parameterized ResNet18 is able to\ncompensate for the side effect, but MobileNetV2 suffers a great performance drop. Remarkably, our\nAMixup is able to overcome the inconsistency while taking advantage of the flatness, contributing to\nhigher accuracy.\n\nThe Policy of Decay. In Table 9, we run a grid search for Ps to explores the influence of different\ndecay policies. It\u2019s worth noting that MobileNetV2 and MnasNet prefer 0.5, whereas ResNet18 and\nRegNet prefer 1. The basic law is very similar to that of AR granularity, which states that heavyweight\nnetworks require more regularization, implying that AMixup and AR are both capable of regularizing\naccuracy.\n\n4.3 Efficiency\n\nWe compare the efficiency of several approaches\non ResNet18. According to Figure 4, NWQ out-\nperforms QDROP and BRECQ under various it-\nerations. Specifically, NWQ achieve competitive\nresults with only 10% computation cost, demon-\nstrating the efficiency of NWQ. As the number of\niterations increases, NWQ continues to improve\naccuracy, whereas BRECQ and QDROP reach\ntheir saturation at \u223c 1.6 \u00d7 104 iterations.\n\n\n\nBased on the section(s), please answer the following question with yes, no, or n/a and provide a brief justification for your answer.\nQuestion: If the authors ran experiments, do the authors include the total amount of compute and the type of resources used (e.g., type of GPUs, internal cluster, or cloud provider)?"}, "4a": {"role": "user", "content": "The following is the experiments section of the paper you are reviewing:\n\n\nIn this section, we conduct extensive experiments with various architectures to verify our NWQ.\nWe first compare our method with previous state-of-the-art, and then we delve into our important\ndesigns and fully explore the scalability, effectiveness, and efficiency of our solution. We report top-1\nclassification accuracy on the ImageNet[9] validation set.\n\nImplementation Details. Our code is based on open-source codes BRECQ and QDrop. We\napply layer-wise AR. For ASoftmax, we set n = 0, m = 1 and \u03c4 is decayed from 1 to 0.01. For\nAMixup, we set default Ps as 0.5 and Pe as 0. We randomly sample 1024 images from ImageNet\ntrain set and employ Cutmix [45] and Mixup[46] as data augmentation. The learning rates are\n0.01 for round policy and 0.0004 for activation quantizer step size. We train for 20000 iterations\nwith a mini-batch size of 32 in 8 Tesla V100 GPUs, taking \u223c 30 minutes for ResNet18, which\nis on par with BRECQ and QDROP. Our experiments are conducted on 5 architectures, including\nResNet18(Res18), MobileNetV2(MNV2), RegNet-600MF(Reg600MF), RegNet-3.2GF(Reg3.2GF)\nand MnasNet(Mnas). Other settings remain same as QDROP[40] if not specified.\n\n4.1 Comprehensive Comparison\n\nTable 1 compares our NWQ with several main-stream PTQ approaches. Overall, NWQ outperforms\nprevious works across all neural network architectures and bit widths, and our performance improves\neven more as the bit width decreases.\n\n6\n\n\fFull Prec\nACIQ-Mix\u2217 [35]\nZeroQ\u2217 [5]\nLAPQ\u2217 [34]\nAdaQuant\u2217 [23]\nBit-Split\u2217 [39]\nAdaRound\u2217 [32]\nBRECQ\u2217 [27]\nQDROP\u2217 [40]\nQDROP\u2020 [40]\nNWQ(Ours)\nNWQ\u2020(Ours)\nAdaQuant\u2217 [23]\nAdaRound\u2217 [32]\nBRECQ\u2217 [27]\nQDROP\u2217 [40]\nQDROP\u2020 [40]\nNWQ(Ours)\nNWQ\u2020(Ours)\nBRECQ\u2217 [27]\nQDROP\u2217 [40]\nQDROP\u2020 [40]\nNWQ(Ours)\nNWQ\u2020(Ours)\n\nBits(W/A)\n32/32\n4/4\n4/4\n4/4\n4/4\n4/4\n4/4\n4/4\n4/4\n4/4\n4/4\n4/4\n3/3\n3/3\n3/3\n3/3\n3/3\n3/3\n3/3\n2/2\n2/2\n2/2\n2/2\n2/2\n\nRes18\n71.06\n67.00\n21.71\n60.30\n69.60\n67.56\n67.96\n69.60\n69.62\n69.88\n69.85\u00b10.04\n69.95\u00b10.07\n60.09\n64.66\n65.87\n66.75\n67.51\n67.58\u00b10.14\n68.15\u00b10.10\n42.54\n54.72\n57.84\n59.14\u00b10.05\n61.65\u00b10.12\n\nMobileNetV2\n72.49\n-\n26.24\n49.70\n47.16\n-\n61.52\n66.57\n68.84\n69.15\n69.14\u00b10.12\n69.60\u00b10.03\n2.23\n15.20\n23.41\n57.98\n58.54\n61.24\u00b10.21\n62.53\u00b10.05\n0.24\n13.05\n14.89\n26.42\u00b11.03\n35.13\u00b12.16\n\nReg600MF\n73.71\n-\n28.54\n57.71\n-\n-\n68.20\n68.33\n71.18\n71.45\n71.92\u00b10.07\n72.03\u00b10.09\n-\n51.01\n55.16\n65.54\n66.75\n67.38\u00b10.13\n68.31\u00b10.09\n3.58\n41.47\n45.93\n48.49\u00b10.03\n56.36\u00b10.01\n\nReg3.2GF\n78.36\n-\n12.24\n55.89\n-\n-\n73.85\n74.21\n76.66\n77.02\n77.40\u00b10.04\n77.42\u00b10.03\n-\n56.79\n57.12\n72.51\n73.48\n74.79\u00b10.09\n75.44\u00b10.08\n3.62\n55.11\n57.98\n62.85\u00b10.13\n67.02\u00b10.01\n\nMnasNet2.0\n76.68\n-\n3.89\n65.32\n-\n-\n68.86\n73.56\n73.71\n74.09\n74.60\u00b10.09\n74.86\u00b10.03\n-\n47.89\n49.78\n66.81\n68.28\n68.85\u00b10.18\n70.77\u00b10.10\n0.61\n28.77\n34.19\n41.17\u00b10.35\n53.55\u00b10.20\n\nTable 1: Comprehensive comparison. We report top1 accuracy on the ImageNet validation set. \u2217 represents for\nreference from QDROP[40] and \u2020 means scaling up the calibration set to 10240 images. QDROP\u2020 results are\nproduced by open-source codes from QDrop. We use n = 0, m = 1 for NWQ and n = \u22121, m = 2 for NWQ\u2020.\nWe follow BRECQ[27] to set the first and the last layer to 8-bit.\n\nClassical Calibration Set. Following previous works, we compare the performance on a small\ncalibration set of 1024 images. For start, we achieve 0.2%\u223c 0.9% improvement even compared with\nstrong baselines on W4A4, which are rather close to full precision accuracy. For W3A3, NWQ yields\n0.83%\u223c 3.26% and reduces the gap between W3A3 and W4A4. For the challenging W2A2, our\nmethod surpasses the previous state-of-the-art by a large margin of 13.37% for MobileNetV2 and\n12.4% for MnasNet, pushing the limit of extremely low-bit PTQ step further.\n\nScaled-up Calibration Set. To further explore the potential of NWQ, we scale up the calibration\nset by 10\u00d7 and reproduce QDROP on the scaled-up calibration set, building a very strong baseline.\nInterestingly, NWQ outperforms QDROP even with one-tenth data. When equipping NWQ with\nthe scaled-up calibration set, we observe significant improvement under all settings. Particularly,\nthe accuracy gaps of W2A2 between NWQ and QDROP are enlarged even more as the calibration\nset scales up. For example, the gap of MnasNet increases from 12.4% to 19.36%, and that of\nMobileNetV2 increases from 13.37% to 20.24%, showing the excellent scalability of our method.\n\n4.2 Ablation Study\n\n4.2.1 AR\n\nThe Effect of AR. We study the effect of AR via compar-\ning the experiments with layer-wise AR to the experiments\nwithout AR on W2A2. Table 2 shows that layer-wise AR\nimproves the accuracy significantly.\nIt\u2019s worth noting\nthat layer-wise AR improves 5.22% for MobileNetV2 and\n10.42% for MnasNet. To understand how AR works, we\nanalyze the loss curves during training in Figure 3. First,\nour AR experiment consistently converges to lower loss\n\n7\n\nFigure 3: AR vs. w/o AR on MNV2 with W2A2. Quanti-\nzation loss curves on train set of 1024 images and valida-\ntion set of 5 \u00d7 104 images.\n\n2.55.07.510.012.515.017.520.0iterations/10381012141618loss/103AR-trainAR-valw/o AR-trainw/o AR-val\fAR Granularity\nlayer-wise AR\nblock-wise AR\nstage-wise AR\nw/o AR\n\nRes18\n59.14\n59.12\n59.10\n57.94\n\nMNV2\n26.42\n26.52\n25.52\n21.20\n\nReg600M\n48.49\n48.89\n47.28\n46.82\n\nReg3.2G\n62.85\n61.31\n60.30\n59.82\n\nMnas\n41.17\n41.89\n41.25\n30.75\n\nAR\n\u2717\n\u2717\n\u2713\n\u2713\n\nAA\n\u2717\n\u2713\n\u2717\n\u2713\n\nRes18\n57.94\n58.99\n59.14\n59.52\n\nMNV2\n21.20\n24.86\n26.42\n26.90\n\nReg600M\n46.82\n47.92\n48.49\n48.62\n\nReg3.2G\n59.82\n62.19\n62.85\n63.52\n\nMnas\n30.75\n38.16\n41.17\n42.92\n\nTable 2: AR vs. w/o AR on W2A2 with our NWQ approach. Up:\nComparison of different AR granularities (i.e. Apply AR after each\nlayer/block/stage). Bottom: Experiments without AR.\n\nTable 3: AR vs. AutoAugment on W2A2. For AutoAugment, we use\nthe open-source codes from torchvision. AA is short for AutoAug-\nment.\n\nMethod\nLW\nRes18\nMNV2\n\n0.01\n0.96\n0.10\n\n0.1\n3.49\n0.11\n\nRSeR\n1\n34.75\n0.28\n\n10\n46.56\n0.87\n\n100\n44.54\n4.86\n\n1000\n43.83\n2.95\n\nASoftmax\n-\n59.14\n26.42\n\nInitialization\nrandom\nOurs\n\nBits(W/A)\n2/2\n2/2\n\nResNet18\n55.33\n59.14\n\nMobileNetV2\n22.00\n26.42\n\nTable 4: ASoftmax vs. RSeR on W2A2 with our NWQ approach. LW\nrepresents the regularization loss weight for AdaRound. We simply\nreplace ASoftmax with RSeR in our NWQ solution.\n\nTable 5: Comparison of different initialization methods. We compare\nrandom initialization with our method described in Sec. 3.2.2.\n\non both the train and test datasets, resulting in higher accuracy. Second, AR accelerates convergence\nsignificantly and thus allows NWQ to reach competitive accuracy at a substantially lower computing\ncost. Third, even though AR does not totally eliminate over-fitting, it effectively bridges the loss gap\nbetween train and test datasets.\n\nThe Effect of Granularity. Table 2 studies how AR granularity affects accuracy. AR granularity\nrepresents for applying AR after each layer/block/stage, while training(reconstruction) granularity\nrepresents for optimizing each layer/block/stage sequentially and independently. We discover that\nlightweight networks prefer coarse-grained AR, whereas heavyweight networks prefer fine-grained\nAR. MobileNetV2, RegNet-600MF, and MnasNet, for example, achieve the best performance with\nblock-wise AR, whereas ResNet18 and RegNet-3.2GF achieve the maximum accuracy with layer-wise\nAR.\n\nComparison with Data Augmentation. Data augmentation is a classical strategy used to relieve\nover-fitting due to its simplicity and effectiveness. We compare AR with strong augmentation (e.g.\nAutoAugment[8]) in Table 3. Notably, both AR and AutoAugment significantly improve accuracy,\nand AR contributes more than AutoAugment, particularly for sparse networks like MobileNetV2 and\nMnasNet. Coupling these two techniques enables us to achieve better performance.\n\n4.2.2 ASoftmax Study\n\nThe Effect of ASoftmax. Table 4 compares ASoftmax to Rectified Sigmoid with explicit Regulariza-\ntion (RSeR) which is used in AdaRound. We simply replace ASoftmax with RSeR in our NWQ and\nrun a grid search on regularization loss weight to explore the potential of RSeR. We discover that\nRSeR requires a huge loss weight for NWQ, which is 103 \u223c 105\u00d7 larger than that of subnet-wise\napproaches. Even with exhausting hyper-parameter search, RSeR does not perform as well as in the\nsubnet-wise method. ASoftmax, on the other hand, outperforms RSeR by 12.58% and 21.56% on\nResNet18 and MobileNetV2, respectively, without any additional hyper-parameter tuning.\n\nThe Effect of Initialization. Table 5 demonstrates the effectiveness of our initialization, which keeps\nthe quantization model close to the float-point at the start to steady the training. It is shown that our\ninitialization method significantly outperforms random initialization.\n\nThe Robustness of \u03c4 . We investigate the robustness of \u03c4 in Table 6. \u03c4 starts with a large number\nto encourage continuous relaxation and ends with a small value to favor one-hot distribution. The\nfinal \u03c4 T determines the decaying speed of \u03c4 . It can be observed that \u03c4 T has a robustness range of\n10\u22122 \u223c 10\u22124, making it easily adaptable to a wider range of neural network architectures.\n\n\u03c4 T\n1e-1\n1e-2\n1e-3\n1e-4\n\nRes18\n59.08\n59.14\n59.05\n58.98\n\nMNV2\n12.19\n26.42\n26.75\n24.74\n\nReg600M\n47.84\n48.49\n49.77\n49.42\n\nReg3.2G\n60.10\n62.85\n62.09\n62.06\n\nMnas\n33.92\n41.17\n41.34\n41.32\n\n#Images\n1024\n1024\n10240\n10240\n\nn/m Res18 MNV2\n26.42\n0/1\n28.73\n-1/2\n29.54\n0/1\n35.13\n-1/2\n\n59.14\n59.10\n61.61\n61.65\n\nReg600M Reg3.2G Mnas\n41.17\n41.07\n51.15\n53.55\n\n48.49\n49.73\n53.83\n56.36\n\n62.85\n61.68\n67.02\n67.02\n\nTable 6: Robustness of \u03c4 T for ASoftmax on W2A2. Experiments\nare conducted with \u03c4 0 = 1.0\n\nTable 7: Extensiveness of ASoftmax on W2A2. MNV2 is short for\nMobileNetV2.\n\n8\n\n\fMethod\nQDROP-style\nw/o Drop\nAMixup\n\nBits(W/A)\n2/2\n2/2\n2/2\n\nResNet18\n56.47\n58.42\n59.12\n\nMobileNetV2\n5.13\n25.09\n26.42\n\nTable 8: QDROP-style vs. w/o Drop vs. AMixup on W2A2 with\nour NWQ approach. QDROP-style represents for dropping methods\nof QDROP coupled with NWQ. w/o Drop represents for training\nwithout dropping or mixup operation.\n\nPs\n1\n0.8\n0.6\n0.5\n0.4\n0.2\n\nRes18 MNV2\n59.47\n25.34\n25.57\n59.39\n24.18\n59.32\n26.42\n59.14\n25.79\n59.08\n25.44\n58.96\n\nReg600M Reg3.2G Mnas\n40.55\n40.66\n40.86\n41.17\n40.63\n39.71\n\n50.53\n50.05\n49.84\n48.49\n48.83\n48.81\n\n63.06\n62.65\n61.90\n62.85\n61.73\n60.76\n\nTable 9: Decay Policy of AMixup on W2A2. We use Pe = 0 for all\nof the experiments.\n\nThe Extensiveness of ASoftmax. As described in Sec.3.2.1, ASoftmax can easily extend the discrete\nparameter space from {0, 1} to a larger integer range, offering the opportunity for a better optimum.\nHowever, extending discrete range benefits MobileNetV2 and RegNet-600MF while degrading\nRegNet-3.2GF on a small calibration set, as shown in Table 7 upper group. We argue that the enlarged\ndiscrete range raises the risk of over-fitting for large networks. When we scale up the calibration set,\nthe performance no longer degrades and the performance improves even more.\n\n4.2.3 AMixup Study\n\nThe Effect of AMixup. QDROP presents good performance in block-wise reconstruction, but fails\nin our network-wise scheme. As illustrated in Table 8, we achieve 1.95% improvement for ResNet18\nand 19.96% for MobileNetV2 by simply removing QDROP (w/o Drop row). We hypothesize\nthat randomly mixing activations up introduces inconsistency bias between the training and test\nphase, which is concealed by the improvement of flatness in block-wise reconstruction. However,\nas the reconstruction unit goes deeper (from block to network) and sparser, the problem becomes\nmore pronounced, resulting in significant degeneration. Over-parameterized ResNet18 is able to\ncompensate for the side effect, but MobileNetV2 suffers a great performance drop. Remarkably, our\nAMixup is able to overcome the inconsistency while taking advantage of the flatness, contributing to\nhigher accuracy.\n\nThe Policy of Decay. In Table 9, we run a grid search for Ps to explores the influence of different\ndecay policies. It\u2019s worth noting that MobileNetV2 and MnasNet prefer 0.5, whereas ResNet18 and\nRegNet prefer 1. The basic law is very similar to that of AR granularity, which states that heavyweight\nnetworks require more regularization, implying that AMixup and AR are both capable of regularizing\naccuracy.\n\n4.3 Efficiency\n\nWe compare the efficiency of several approaches\non ResNet18. According to Figure 4, NWQ out-\nperforms QDROP and BRECQ under various it-\nerations. Specifically, NWQ achieve competitive\nresults with only 10% computation cost, demon-\nstrating the efficiency of NWQ. As the number of\niterations increases, NWQ continues to improve\naccuracy, whereas BRECQ and QDROP reach\ntheir saturation at \u223c 1.6 \u00d7 104 iterations.\n\n\n\nBased on the section(s), please answer the following question with yes, no, or n/a and provide a brief justification for your answer.\nQuestion: If the authors use existing assets (e.g., code, data, models), do the authors cite the creators?"}, "4b": {"role": "user", "content": "The following is the experiments section of the paper you are reviewing:\n\n\nIn this section, we conduct extensive experiments with various architectures to verify our NWQ.\nWe first compare our method with previous state-of-the-art, and then we delve into our important\ndesigns and fully explore the scalability, effectiveness, and efficiency of our solution. We report top-1\nclassification accuracy on the ImageNet[9] validation set.\n\nImplementation Details. Our code is based on open-source codes BRECQ and QDrop. We\napply layer-wise AR. For ASoftmax, we set n = 0, m = 1 and \u03c4 is decayed from 1 to 0.01. For\nAMixup, we set default Ps as 0.5 and Pe as 0. We randomly sample 1024 images from ImageNet\ntrain set and employ Cutmix [45] and Mixup[46] as data augmentation. The learning rates are\n0.01 for round policy and 0.0004 for activation quantizer step size. We train for 20000 iterations\nwith a mini-batch size of 32 in 8 Tesla V100 GPUs, taking \u223c 30 minutes for ResNet18, which\nis on par with BRECQ and QDROP. Our experiments are conducted on 5 architectures, including\nResNet18(Res18), MobileNetV2(MNV2), RegNet-600MF(Reg600MF), RegNet-3.2GF(Reg3.2GF)\nand MnasNet(Mnas). Other settings remain same as QDROP[40] if not specified.\n\n4.1 Comprehensive Comparison\n\nTable 1 compares our NWQ with several main-stream PTQ approaches. Overall, NWQ outperforms\nprevious works across all neural network architectures and bit widths, and our performance improves\neven more as the bit width decreases.\n\n6\n\n\fFull Prec\nACIQ-Mix\u2217 [35]\nZeroQ\u2217 [5]\nLAPQ\u2217 [34]\nAdaQuant\u2217 [23]\nBit-Split\u2217 [39]\nAdaRound\u2217 [32]\nBRECQ\u2217 [27]\nQDROP\u2217 [40]\nQDROP\u2020 [40]\nNWQ(Ours)\nNWQ\u2020(Ours)\nAdaQuant\u2217 [23]\nAdaRound\u2217 [32]\nBRECQ\u2217 [27]\nQDROP\u2217 [40]\nQDROP\u2020 [40]\nNWQ(Ours)\nNWQ\u2020(Ours)\nBRECQ\u2217 [27]\nQDROP\u2217 [40]\nQDROP\u2020 [40]\nNWQ(Ours)\nNWQ\u2020(Ours)\n\nBits(W/A)\n32/32\n4/4\n4/4\n4/4\n4/4\n4/4\n4/4\n4/4\n4/4\n4/4\n4/4\n4/4\n3/3\n3/3\n3/3\n3/3\n3/3\n3/3\n3/3\n2/2\n2/2\n2/2\n2/2\n2/2\n\nRes18\n71.06\n67.00\n21.71\n60.30\n69.60\n67.56\n67.96\n69.60\n69.62\n69.88\n69.85\u00b10.04\n69.95\u00b10.07\n60.09\n64.66\n65.87\n66.75\n67.51\n67.58\u00b10.14\n68.15\u00b10.10\n42.54\n54.72\n57.84\n59.14\u00b10.05\n61.65\u00b10.12\n\nMobileNetV2\n72.49\n-\n26.24\n49.70\n47.16\n-\n61.52\n66.57\n68.84\n69.15\n69.14\u00b10.12\n69.60\u00b10.03\n2.23\n15.20\n23.41\n57.98\n58.54\n61.24\u00b10.21\n62.53\u00b10.05\n0.24\n13.05\n14.89\n26.42\u00b11.03\n35.13\u00b12.16\n\nReg600MF\n73.71\n-\n28.54\n57.71\n-\n-\n68.20\n68.33\n71.18\n71.45\n71.92\u00b10.07\n72.03\u00b10.09\n-\n51.01\n55.16\n65.54\n66.75\n67.38\u00b10.13\n68.31\u00b10.09\n3.58\n41.47\n45.93\n48.49\u00b10.03\n56.36\u00b10.01\n\nReg3.2GF\n78.36\n-\n12.24\n55.89\n-\n-\n73.85\n74.21\n76.66\n77.02\n77.40\u00b10.04\n77.42\u00b10.03\n-\n56.79\n57.12\n72.51\n73.48\n74.79\u00b10.09\n75.44\u00b10.08\n3.62\n55.11\n57.98\n62.85\u00b10.13\n67.02\u00b10.01\n\nMnasNet2.0\n76.68\n-\n3.89\n65.32\n-\n-\n68.86\n73.56\n73.71\n74.09\n74.60\u00b10.09\n74.86\u00b10.03\n-\n47.89\n49.78\n66.81\n68.28\n68.85\u00b10.18\n70.77\u00b10.10\n0.61\n28.77\n34.19\n41.17\u00b10.35\n53.55\u00b10.20\n\nTable 1: Comprehensive comparison. We report top1 accuracy on the ImageNet validation set. \u2217 represents for\nreference from QDROP[40] and \u2020 means scaling up the calibration set to 10240 images. QDROP\u2020 results are\nproduced by open-source codes from QDrop. We use n = 0, m = 1 for NWQ and n = \u22121, m = 2 for NWQ\u2020.\nWe follow BRECQ[27] to set the first and the last layer to 8-bit.\n\nClassical Calibration Set. Following previous works, we compare the performance on a small\ncalibration set of 1024 images. For start, we achieve 0.2%\u223c 0.9% improvement even compared with\nstrong baselines on W4A4, which are rather close to full precision accuracy. For W3A3, NWQ yields\n0.83%\u223c 3.26% and reduces the gap between W3A3 and W4A4. For the challenging W2A2, our\nmethod surpasses the previous state-of-the-art by a large margin of 13.37% for MobileNetV2 and\n12.4% for MnasNet, pushing the limit of extremely low-bit PTQ step further.\n\nScaled-up Calibration Set. To further explore the potential of NWQ, we scale up the calibration\nset by 10\u00d7 and reproduce QDROP on the scaled-up calibration set, building a very strong baseline.\nInterestingly, NWQ outperforms QDROP even with one-tenth data. When equipping NWQ with\nthe scaled-up calibration set, we observe significant improvement under all settings. Particularly,\nthe accuracy gaps of W2A2 between NWQ and QDROP are enlarged even more as the calibration\nset scales up. For example, the gap of MnasNet increases from 12.4% to 19.36%, and that of\nMobileNetV2 increases from 13.37% to 20.24%, showing the excellent scalability of our method.\n\n4.2 Ablation Study\n\n4.2.1 AR\n\nThe Effect of AR. We study the effect of AR via compar-\ning the experiments with layer-wise AR to the experiments\nwithout AR on W2A2. Table 2 shows that layer-wise AR\nimproves the accuracy significantly.\nIt\u2019s worth noting\nthat layer-wise AR improves 5.22% for MobileNetV2 and\n10.42% for MnasNet. To understand how AR works, we\nanalyze the loss curves during training in Figure 3. First,\nour AR experiment consistently converges to lower loss\n\n7\n\nFigure 3: AR vs. w/o AR on MNV2 with W2A2. Quanti-\nzation loss curves on train set of 1024 images and valida-\ntion set of 5 \u00d7 104 images.\n\n2.55.07.510.012.515.017.520.0iterations/10381012141618loss/103AR-trainAR-valw/o AR-trainw/o AR-val\fAR Granularity\nlayer-wise AR\nblock-wise AR\nstage-wise AR\nw/o AR\n\nRes18\n59.14\n59.12\n59.10\n57.94\n\nMNV2\n26.42\n26.52\n25.52\n21.20\n\nReg600M\n48.49\n48.89\n47.28\n46.82\n\nReg3.2G\n62.85\n61.31\n60.30\n59.82\n\nMnas\n41.17\n41.89\n41.25\n30.75\n\nAR\n\u2717\n\u2717\n\u2713\n\u2713\n\nAA\n\u2717\n\u2713\n\u2717\n\u2713\n\nRes18\n57.94\n58.99\n59.14\n59.52\n\nMNV2\n21.20\n24.86\n26.42\n26.90\n\nReg600M\n46.82\n47.92\n48.49\n48.62\n\nReg3.2G\n59.82\n62.19\n62.85\n63.52\n\nMnas\n30.75\n38.16\n41.17\n42.92\n\nTable 2: AR vs. w/o AR on W2A2 with our NWQ approach. Up:\nComparison of different AR granularities (i.e. Apply AR after each\nlayer/block/stage). Bottom: Experiments without AR.\n\nTable 3: AR vs. AutoAugment on W2A2. For AutoAugment, we use\nthe open-source codes from torchvision. AA is short for AutoAug-\nment.\n\nMethod\nLW\nRes18\nMNV2\n\n0.01\n0.96\n0.10\n\n0.1\n3.49\n0.11\n\nRSeR\n1\n34.75\n0.28\n\n10\n46.56\n0.87\n\n100\n44.54\n4.86\n\n1000\n43.83\n2.95\n\nASoftmax\n-\n59.14\n26.42\n\nInitialization\nrandom\nOurs\n\nBits(W/A)\n2/2\n2/2\n\nResNet18\n55.33\n59.14\n\nMobileNetV2\n22.00\n26.42\n\nTable 4: ASoftmax vs. RSeR on W2A2 with our NWQ approach. LW\nrepresents the regularization loss weight for AdaRound. We simply\nreplace ASoftmax with RSeR in our NWQ solution.\n\nTable 5: Comparison of different initialization methods. We compare\nrandom initialization with our method described in Sec. 3.2.2.\n\non both the train and test datasets, resulting in higher accuracy. Second, AR accelerates convergence\nsignificantly and thus allows NWQ to reach competitive accuracy at a substantially lower computing\ncost. Third, even though AR does not totally eliminate over-fitting, it effectively bridges the loss gap\nbetween train and test datasets.\n\nThe Effect of Granularity. Table 2 studies how AR granularity affects accuracy. AR granularity\nrepresents for applying AR after each layer/block/stage, while training(reconstruction) granularity\nrepresents for optimizing each layer/block/stage sequentially and independently. We discover that\nlightweight networks prefer coarse-grained AR, whereas heavyweight networks prefer fine-grained\nAR. MobileNetV2, RegNet-600MF, and MnasNet, for example, achieve the best performance with\nblock-wise AR, whereas ResNet18 and RegNet-3.2GF achieve the maximum accuracy with layer-wise\nAR.\n\nComparison with Data Augmentation. Data augmentation is a classical strategy used to relieve\nover-fitting due to its simplicity and effectiveness. We compare AR with strong augmentation (e.g.\nAutoAugment[8]) in Table 3. Notably, both AR and AutoAugment significantly improve accuracy,\nand AR contributes more than AutoAugment, particularly for sparse networks like MobileNetV2 and\nMnasNet. Coupling these two techniques enables us to achieve better performance.\n\n4.2.2 ASoftmax Study\n\nThe Effect of ASoftmax. Table 4 compares ASoftmax to Rectified Sigmoid with explicit Regulariza-\ntion (RSeR) which is used in AdaRound. We simply replace ASoftmax with RSeR in our NWQ and\nrun a grid search on regularization loss weight to explore the potential of RSeR. We discover that\nRSeR requires a huge loss weight for NWQ, which is 103 \u223c 105\u00d7 larger than that of subnet-wise\napproaches. Even with exhausting hyper-parameter search, RSeR does not perform as well as in the\nsubnet-wise method. ASoftmax, on the other hand, outperforms RSeR by 12.58% and 21.56% on\nResNet18 and MobileNetV2, respectively, without any additional hyper-parameter tuning.\n\nThe Effect of Initialization. Table 5 demonstrates the effectiveness of our initialization, which keeps\nthe quantization model close to the float-point at the start to steady the training. It is shown that our\ninitialization method significantly outperforms random initialization.\n\nThe Robustness of \u03c4 . We investigate the robustness of \u03c4 in Table 6. \u03c4 starts with a large number\nto encourage continuous relaxation and ends with a small value to favor one-hot distribution. The\nfinal \u03c4 T determines the decaying speed of \u03c4 . It can be observed that \u03c4 T has a robustness range of\n10\u22122 \u223c 10\u22124, making it easily adaptable to a wider range of neural network architectures.\n\n\u03c4 T\n1e-1\n1e-2\n1e-3\n1e-4\n\nRes18\n59.08\n59.14\n59.05\n58.98\n\nMNV2\n12.19\n26.42\n26.75\n24.74\n\nReg600M\n47.84\n48.49\n49.77\n49.42\n\nReg3.2G\n60.10\n62.85\n62.09\n62.06\n\nMnas\n33.92\n41.17\n41.34\n41.32\n\n#Images\n1024\n1024\n10240\n10240\n\nn/m Res18 MNV2\n26.42\n0/1\n28.73\n-1/2\n29.54\n0/1\n35.13\n-1/2\n\n59.14\n59.10\n61.61\n61.65\n\nReg600M Reg3.2G Mnas\n41.17\n41.07\n51.15\n53.55\n\n48.49\n49.73\n53.83\n56.36\n\n62.85\n61.68\n67.02\n67.02\n\nTable 6: Robustness of \u03c4 T for ASoftmax on W2A2. Experiments\nare conducted with \u03c4 0 = 1.0\n\nTable 7: Extensiveness of ASoftmax on W2A2. MNV2 is short for\nMobileNetV2.\n\n8\n\n\fMethod\nQDROP-style\nw/o Drop\nAMixup\n\nBits(W/A)\n2/2\n2/2\n2/2\n\nResNet18\n56.47\n58.42\n59.12\n\nMobileNetV2\n5.13\n25.09\n26.42\n\nTable 8: QDROP-style vs. w/o Drop vs. AMixup on W2A2 with\nour NWQ approach. QDROP-style represents for dropping methods\nof QDROP coupled with NWQ. w/o Drop represents for training\nwithout dropping or mixup operation.\n\nPs\n1\n0.8\n0.6\n0.5\n0.4\n0.2\n\nRes18 MNV2\n59.47\n25.34\n25.57\n59.39\n24.18\n59.32\n26.42\n59.14\n25.79\n59.08\n25.44\n58.96\n\nReg600M Reg3.2G Mnas\n40.55\n40.66\n40.86\n41.17\n40.63\n39.71\n\n50.53\n50.05\n49.84\n48.49\n48.83\n48.81\n\n63.06\n62.65\n61.90\n62.85\n61.73\n60.76\n\nTable 9: Decay Policy of AMixup on W2A2. We use Pe = 0 for all\nof the experiments.\n\nThe Extensiveness of ASoftmax. As described in Sec.3.2.1, ASoftmax can easily extend the discrete\nparameter space from {0, 1} to a larger integer range, offering the opportunity for a better optimum.\nHowever, extending discrete range benefits MobileNetV2 and RegNet-600MF while degrading\nRegNet-3.2GF on a small calibration set, as shown in Table 7 upper group. We argue that the enlarged\ndiscrete range raises the risk of over-fitting for large networks. When we scale up the calibration set,\nthe performance no longer degrades and the performance improves even more.\n\n4.2.3 AMixup Study\n\nThe Effect of AMixup. QDROP presents good performance in block-wise reconstruction, but fails\nin our network-wise scheme. As illustrated in Table 8, we achieve 1.95% improvement for ResNet18\nand 19.96% for MobileNetV2 by simply removing QDROP (w/o Drop row). We hypothesize\nthat randomly mixing activations up introduces inconsistency bias between the training and test\nphase, which is concealed by the improvement of flatness in block-wise reconstruction. However,\nas the reconstruction unit goes deeper (from block to network) and sparser, the problem becomes\nmore pronounced, resulting in significant degeneration. Over-parameterized ResNet18 is able to\ncompensate for the side effect, but MobileNetV2 suffers a great performance drop. Remarkably, our\nAMixup is able to overcome the inconsistency while taking advantage of the flatness, contributing to\nhigher accuracy.\n\nThe Policy of Decay. In Table 9, we run a grid search for Ps to explores the influence of different\ndecay policies. It\u2019s worth noting that MobileNetV2 and MnasNet prefer 0.5, whereas ResNet18 and\nRegNet prefer 1. The basic law is very similar to that of AR granularity, which states that heavyweight\nnetworks require more regularization, implying that AMixup and AR are both capable of regularizing\naccuracy.\n\n4.3 Efficiency\n\nWe compare the efficiency of several approaches\non ResNet18. According to Figure 4, NWQ out-\nperforms QDROP and BRECQ under various it-\nerations. Specifically, NWQ achieve competitive\nresults with only 10% computation cost, demon-\nstrating the efficiency of NWQ. As the number of\niterations increases, NWQ continues to improve\naccuracy, whereas BRECQ and QDROP reach\ntheir saturation at \u223c 1.6 \u00d7 104 iterations.\n\n\n\nBased on the section(s), please answer the following question with yes, no, or n/a and provide a brief justification for your answer.\nQuestion: If the authors use existing assets (e.g., code, data, models) or curate/release new assets, do the authors mention the license of the assets?"}}}