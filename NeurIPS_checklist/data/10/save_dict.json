{"paper_index": 15, "title": "Assistive Teaching of Motor Control Tasks to Humans", "abstract": "\n\nRecent works on shared autonomy and assistive-AI technologies, such as assistive\nrobot teleoperation, seek to model and help human users with limited ability in a\nfixed task. However, these approaches often fail to account for humans\u2019 ability to\nadapt and eventually learn how to execute a control task themselves. Furthermore,\nin applications where it may be desirable for a human to intervene, these methods\nmay inhibit their ability to learn how to succeed with full self-control. In this\npaper, we focus on the problem of assistive teaching of motor control tasks such as\nparking a car or landing an aircraft. Despite their ubiquitous role in humans\u2019 daily\nactivities and occupations, motor tasks are rarely taught in a uniform way due to\ntheir high complexity and variance. We propose an AI-assisted teaching algorithm\nthat leverages skill discovery methods from reinforcement learning (RL) to (i)\nbreak down any motor control task into teachable skills, (ii) construct novel drill\nsequences, and (iii) individualize curricula to students with different capabilities.\nThrough an extensive mix of synthetic and user studies on two motor control tasks\u2014\nparking a car with a joystick and writing characters from the Balinese alphabet\u2014we\nshow that assisted teaching with skills improves student performance by around\n40% compared to practicing full trajectories without skills, and practicing with\nindividualized drills can result in up to 25% further improvement.1\n\n1\n\n", "introduction": "\n\nImagine a novice human is tasked with operating a machine with challenging or unfamiliar controls,\nsuch as landing an aircraft or parking an oversized vehicle with novel transmission. A fully au-\ntonomous form of assistance, such as a self-driving car, would aid this user by entirely replacing their\ncontrol, seeking interventions only when necessary. Shared autonomous systems, such as those used\nin assistive robot teleoperation [1, 2, 3], would seek to model the human\u2019s goals and capabilities, and\nprovide assistance in the form of corrections [4] or simplified control spaces [3]. However, neither\nform of assistance would actually help the user learn how to successfully operate the machine, and\nmay even serve as a crutch that prevents their ability to ever perform the task independently.\n\nAlthough humans can learn a variety of complex motor control tasks (e.g., driving a car, or operating\nsurgical robots), they often rely on the assistance of specialized teachers. Receiving fine-grained\ninstruction from these specialized teachers can often be non-uniform, costly, and limited by their\navailability. In this work, we are interested in a more accessible and efficient approach: we wish to\ndevelop an AI-assisted teaching algorithm capable of teaching motor control tasks in an individualized\nmanner. While several works have leveraged AI techniques to aid instruction in traditional education\ntasks such as arithmetic [5, 6, 7] and foreign language learning [8, 9], motor control tasks introduce\n\n1Our source code is available at https://github.com/Stanford-ILIAD/teaching.\n\n36th Conference on Neural Information Processing Systems (NeurIPS 2022).\n\n\fseveral unique challenges, such as the high complexity of the input and output space for motor control\ntasks (e.g., controlling a high degree of freedom robotic arm with a 6 degrees of freedom joystick or\nthe continuous output space of trajectories in driving), as well as the need for generalization across\ndifferent scenarios in these high-dimensional and complex spaces.\n\nTo address these challenges, our insight is to utilize compositional motor skills for teaching: skills are\nmore manageable to teach, and can be used to construct novel compositional drills for an individual.\nWe take inspiration from how specialized human teachers teach motor control tasks. For example, a\npiano teacher may assign a student structured exercises (e.g., musical scales or arpeggios) to build up\ntheir general technique. Further, they may guide the student to break down complex musical phrases\ninto more manageable chunks, repeatedly practice those, and then compose them together [10].\n\nHowever, such fine-grained teaching often requires access to a specialized teacher capable of identi-\nfying skills and creating individualized drills based on a student\u2019s unique expertise. This is not only\nexpensive, but can be highly suboptimal for many motor control tasks where a teacher may not have\nfull visibility into the student\u2019s various inputs for a task (e.g. how much the student pushed the brake\npedal), but only observability of the final behavior of the system (e.g. how close the vehicle is to the\ncurbside), leading to less meaningful feedback. Can we leverage the rich action data provided by\nstudents learning motor control tasks to develop more efficient and reliable individualized instruction?\n\nOur key insight is to leverage automated skill discovery methods from the RL literature to break\ndown motor control tasks performed by an expert into teachable skills. We then identify which\nof the expert\u2019s skills are skills that the student is struggling with, and construct novel drills that\nfocus on those particular skills to individualize their curriculum. Our contributions include: 1)\nDeveloping an algorithm to identify which skills of a motor control task a student struggles with,\n2) Automatically creating individualized drills based on students\u2019 expertise in various skills, and\n3) Empirically validating the helpfulness of our AI-assisted teaching framework in two different\nmotor control tasks: controlling a simulated vehicle to park with an unintuitive joystick, and writing\nBalinese words using a computer mouse or trackpad. Our results show that assisted teaching with\nskills improve student performance by around 40% compared to practicing full trajectories without\nskills, and practicing with individualized drills can result in up to 25% further improvement.\n\n", "methods": "\n\nIn this section, we formalize the problem of Student-Aware AI-Assisted Teaching. We go over\npreliminaries and formalize skill discovery, and then define the problem statement leading to our\n3-step approach of teaching complex motor control tasks: (i) distilling tasks into underlying skills,\n(ii) identifying the student\u2019s proficiency, and (iii) creating novel and individualized curricula.\n\n3.1 Preliminaries\n\nWe formalize the target motor control task as a standard Markov decision process (MDP)\n\u27e8S, A, \u03c1, f, R, T \u27e9 with finite horizon T , where \u03c1 is the initial state distribution and f is a deter-\nministic transition function. In our formulation, R is a random variable for reward functions such\nthat each r \u223c R is a function r : S \u00d7 A \u2192 R. Given an initial state s0 \u223c \u03c1 and a reward function\nr \u223c R (both of which are revealed at the beginning of the task), the goal is to maximize the cumula-\ntive reward over the horizon. We call any (s0, r) pair a scenario \u03be drawn from a set of all possible\nscenarios \u039e. We assume access to a set of expert demonstrations, e.g., demonstrations drawn from a\npre-trained RL agent that optimizes the cumulative reward for any given scenario.\n\nAn inexperienced human would lack the expertise to perform optimally in a given scenario \u03be. Our goal\nis to develop teaching algorithms that leverage expert demonstrations to improve student performance.\nFor example, we want a human unfamiliar with Balinese to learn to write any Balinese word, which\nwould correspond to one r. For this, we develop an algorithm that uses skill discovery methods to\nautomatically create individualized drills based on the discovered skills and the student performance.\n\nSkill discovery. Humans often break down complex tasks into relevant skills (i.e., compositional\naction sequences that lead to high performance)\u2014when learning how to write, we first master how\nto write letters, and then compose them to write words and sentences. We define a skill as a latent\nvariable m that lies in some finite discrete latent space M. Each skill is an embedding that corresponds\nto an action sequence \u039b = (a1, a2, . . . , a|\u039b|) optimal at some state s \u2208 S under some reward r \u223c R:\n\n\u2203s \u2208 S, \u2203r : P (R = r) > 0, \u039b = (a\u2217\n1, a\u2217\n\n2, . . . , a\u2217\n\n(a\u2217\n\n|\u039b|, . . . ) \u2208 arg max\n\n1, a\u2217\n\n2, . . . , a\u2217\n\n|\u039b|),\n(r(s, a1) + r(f (s, a1), a2) + . . . ) ,\n\na1,a2,...,a|\u039b|,...\n\n(1)\n\nand |\u039b| \u2208 [Hmin, Hmax], where Hmin and Hmax \u226a T are hyperparameters.\n\nIn many tasks, it is difficult to enumerate all possible skills, and it is often not clear when a skill starts\nand ends in a trajectory. In this work, we use skill discovery algorithms to automatically extract skills\nrelevant to a task, and then teach the student those generated skills. While our teaching method is\nagnostic to the skill discovery algorithm, we use CompILE [28] as it learns skills directly from expert\ntrajectories. Our formulation assumes access to expert trajectories, so it would be unnecessary to\nlearn an expert policy in parallel with the skill discovery process, as several other methods do.\n\nWe use the trained CompILE as a function: SKILLEXTRACTOR(\u03c4 ) where \u03c4 is a trajectory.\noutputs a sequence of skills M \u03c4 = (m\u03c4\nsuch that\nj\u22121, b\u03c4\n0 = b\u03c4\n1 < \u00b7 \u00b7 \u00b7 < b\u03c4\nj )\nNseg\ncorrespond to skill m\u03c4\nj \u2208 M. Effectively, CompILE learns to divide trajectories into segments, and\ncluster segments with similar actions into skills. We refer to [28] and the Appendix for more details.\n\n2 , . . .) and boundaries B\u03c4 =\n= T , dividing \u03c4 into Nseg segments. Actions in \u03c4 between steps [b\u03c4\n\n1 , . . ., b\u03c4\n\n0 < b\u03c4\n\n1 , m\u03c4\n\n0 , b\u03c4\nb\u03c4\n\nNseg\n\nIt\n\n)\ufe02\n\n(\ufe02\n\n3\n\n\f3.2 Problem statement\nHaving formalized SKILLEXTRACTOR, which, given a trajectory \u03c4 , outputs M \u03c4 and B\u03c4 , we now\npresent our problem statement for AI-assisted teaching of motor control tasks.\nWhat should the student do? Given a set of expert demonstrations in scenarios \u039ee \u2286 \u039e, where\neach \u03be \u2208 \u039ee is sampled from (\u03c1, R), we would ideally teach the student to eventually have a policy\n\u03c0\u2217 : S \u2192 A that minimizes the difference between their behavior and the expert demonstrations:\n\u03be , \u03c4 \u03c0\n\u03be denotes the student trajectory performed by policy \u03c0 in scenario \u03be. Similarly, \u03c4 e\nwhere \u03c4 \u03c0\n\u03be denotes\nthe expert trajectory in \u03be.2 L is a task-dependent loss function. While it is a straightforward goal, it is\nnot clear how to teach a human to optimize this objective to achieve expert-like trajectories for any \u03be.\n\n\u03be ) \u2248 arg min\n\n\u03c0\u2217 = arg min\n\nP (\u03be)L(\u03c4 e\n\n\u03be , \u03c4 \u03c0\n\nL(\u03c4 e\n\n\u03be ) ,\n\n\u2211\ufe02\n\n\u2211\ufe02\n\n\u03be\u2208\u039ee\n\n(2)\n\n\u03be\u2208\u039e\n\n\u03c0\n\n\u03c0\n\nWhat should the teacher do? We can formulate\nthe teaching problem as a partially observable MDP\n(POMDP), where the state of this POMDP is the stu-\ndent\u2019s policy (initially \u03c00), and the teacher\u2019s actions\nguide the student to the policy \u03c0\u2217. The teacher\u2019s ob-\nservations would be student behavior expressed by\ntheir trajectories, or \u03c4 \u03c0k\nfor some \u03be for state \u03c0k at\n\u03be\nteaching round k. The teacher can then select actions\n(e.g. practice scenarios) in this POMDP. A teaching\naction at round k moves the student from \u03c0k to \u03c0k+1,\ngiving the teacher a reward of \u2211\ufe01\n, i.e., the decrease in the\nloss value according to Eq. (2). Thus, the teacher should take the optimal teaching actions that maxi-\nmize its cumulative reward over teaching rounds, whose global optimum would move the student to\n\u03c0\u2217 (see Fig. 1).\n\nFigure 1: An optimal teacher should select a teach-\ning action (e.g. which practice scenario to provide)\nbased on each student\u2019s individual policy to help\nguide them towards the expert policy \u03c0\u2217.\n\n\u03be ) \u2212 L(\u03c4 e\n\n\u03be , \u03c4 \u03c0k+1\n\n\u03be\u2208\u039e P (\u03be)\n\n\u03be , \u03c4 \u03c0k\n\nL(\u03c4 e\n\n(\ufe02\n\n)\ufe02\n\n)\n\n\u03be\n\nUnfortunately, we lack computational models of human learning necessary for making the POMDP\u2019s\nstate transition function (i.e., how a student updates its policy from \u03c0k to \u03c0k+1 for any teaching\naction the teacher could take) tractable for motor control tasks. Recent works exploring data-driven\nframeworks for knowledge tracing [34] and RL-based education [6] usually require large amounts of\ndata for simpler problems, and are thus not scalable to motor control tasks with large trajectory spaces.\n\nApproach overview. Our insight is that once we identify important skills of a motor control\ntask, an AI assisted teaching framework can efficiently leverage the skills to teach the task. The\nSKILLEXTRACTOR function allows us to identify the skills used in a given trajectory. Applying\nit to all expert demonstrations in scenarios \u039ee \u2286 \u039e gives us a set of skills relevant in the task.\nSay we label the expert demonstration \u03c4 e\n\u03be for each scenario \u03be \u2208 \u039ee with skills and their intervals:\n(M e\n\u03be ) = SKILLEXTRACTOR(\u03c4 e\n\u03be ). We would then want the student to perform each relevant skill\nas close as possible to the expert to solve the following optimization:\n\n\u03be , Be\n\n\u03c0\u2217 = arg min\n\n\u2211\ufe02\n\n\u03c0\n\n\u2211\ufe02\n\nP (\u03be)\n\n\u03be\u2208\u039e\n\nm\u2208M e\n\u03be\n\nLm(\u03c4 e\n\n\u03be , \u03c4 \u03c0\n\n\u03be ) \u2248 arg min\n\n\u03c0\n\n\u2211\ufe02\n\n\u2211\ufe02\n\n\u03be\u2208\u039ee\n\nm\u2208M e\n\u03be\n\nLm(\u03c4 e\n\n\u03be , \u03c4 \u03c0\n\n\u03be ) ,\n\n(3)\n\nwhere Lm is some loss function that compares expert and student trajectories in terms of the skill\nm. The intuition behind this objective is two-fold: (i) we want the student to perform the same set\nof skills as the expert for any \u03be \u2208 \u039e (this is why we sum over m \u2208 M e\n\u03be ), and (ii) we want them to\nperform each skill as close as possible to how the expert performs it. The underlying assumption is\nthat student will gain expertise if they perform the same skills in the most similar way to the expert.\n\nOverall, (3) is a simpler objective as the space of action sequences for each skill is much smaller than\nthe space of trajectories or scenarios. We can now try to teach a small number of skills to the student\ninstead of unrealistically trying to match them with the expert in the large trajectory space.\n\nOnly teaching popular skills of a task may suffer from the problem that the student does not learn\nhow to compose different skills to achieve the target task. We thus propose to create novel drills,\nwhich consist of repetitions of multiple skills in a single trajectory. Drills help the students (i) learn\nhow to connect action sequences for different skills, and (ii) develop muscle memory, which is\n\n2For concision, we assume a single expert demonstration \u03c4 e\n\n\u03be and at most a single student trajectory \u03c4 \u03c0\n\n\u03be for any \u03be \u2208 \u039ee.\n\nWithout this assumption, we would take expectations over expert and student trajectories for each \u03be.\n\n4\n\n\fFigure 2: Overview of our approach. We train a CompILE model over expert demonstrations (e.g., handwritten\nBalinese text) for our SKILLEXTRACTOR function, which we use to select a diverse set of scenarios (e.g. words)\ncovering many skills. After a student provides trajectories for these scenarios, we use SKILLEXTRACTOR again\nto identify their individual expertise for each skill, which informs the set of drills we provide them to practice.\n\ncrucial in motor control tasks, for the action sequences of each skill. The generated drills can also\nbe personalized based on what skills each student is struggling with. For instance, as in Fig. 1, two\ndifferent students (in green and blue) might have different policies with different skill sets. Therefore,\na teacher should generate individualized drills informed by the student\u2019s initial level of expertise.\n\nTo efficiently solve (3), we propose our student-aware teaching approach that follows three steps:\n1) Diverse scenario selection: Identify scenarios that yield a high diversity of popular skills when\nan expert controls the system, 2) Expertise estimation: Have the student control the system under\nthese scenarios to estimate their expertise in different skills, and 3) Individualized drill generation:\nBased on the estimates, create individualized drills to have the student practice the skills that they are\nstruggling with. In the subsequent section, we describe our procedure for each of these steps.\n\n4 Student-aware assisted teaching\n\nAlgorithm 1 Diverse Scenario Selection\nInput: Skill labels M e\n\u03be for each \u03be \u2208 \u039ee\nInput: Number of scenarios to be selected N s\n1: \u039es, Mcovered \u2190 \u2205, \u2205\n2: for i = 1, 2, . . . , N s do\n3:\n\n1) Diverse scenario selection. We select diverse\nscenarios that cover many different skills in or-\nder flexibly estimate students\u2019 expertise and in-\ndividualize our teaching strategy. For example,\nin English, we may have expert demonstrations\nof writing the words (scenarios) \u201cexpert\u201d, \u201cper-\nson\u201d, \u201cskills\u201d, from which we would select \u201cper-\nson\u201d to cover more diverse letter strokes (skills).\nFormally, we treat the problem of choosing sce-\nnarios with diverse skills as a maximum set coverage problem, which we solve using a simple greedy\nalgorithm (see Algorithm 1), outputting a set of scenarios that led the expert to demonstrate a variety\nof skills.\n\n\u03be\u2217 \u2190 arg max\u03be\u2208\u039ee\\\u039es |Mcovered \u222a M e\n\u03be |\n\u039es \u2190 \u039es \u222a {\u03be\u2217}\nMcovered \u2190 Mcovered \u222a M e\n\u03be\n\n4:\n5:\n6: return \u039es\n\n2) Identifying individual expertise in skills. To\nefficiently teach complex tasks, we seek to esti-\nmate student\u2019s expertise in different skills, such\nas identifying which strokes they struggle with\nwhen writing Balinese words. Requiring a di-\nverse set of skills, the scenarios \u039es we select en-\nable us to infer student\u2019s expertise efficiently: we\nask the student to perform the task in those sce-\nnarios, and based on their data, construct a labels\nvector E that quantifies the student\u2019s expertise\nEm for each skill m \u2208M.\n\nAlgorithm 2 Individual Expertise Identification\nInput: Selected scenarios \u039es \u2286 \u039ee\nInput: Skill labels M e\nInput: Student demonstration \u03c4 s\n\n\u03be for each \u03be \u2208 \u039ee\n\n\u03be for each \u03be \u2208 \u039es\n\n\u03be \u2190 SKILLEXTRACTOR(\u03c4 s\n\u03be )\n\n1: Em \u2190 0 for \u2200m \u2208 M\n2: for \u03be \u2208 \u039es do\nM s\n\u03be , Bs\n3:\nj \u2190 0\n4:\nfor m \u2208 M e\n5:\n\n\u03be do\nif m \u0338\u2208 M s\n\n6:\n\n7:\n\n\u03be then\nEm \u2190 Em + r\u03be(\u03c4 s\n\n\u03be )/j\n\nj \u2190 j + 1\n\n8:\n9: return E\n\nThe discrepancy between skills used by the stu-\ndent and the expert provides a useful signal to\nestimate the student\u2019s expertise. Intuitively, we\nexpect the student to demonstrate similar skills\nto the expert if they are proficient in those skills, while, otherwise, they might rely on a simpler or\nincorrect set of skills. For example, a student driver who lacks the expertise of reverse driving might\nattempt to park a vehicle by only driving forward. We therefore utilize the expert demonstrations\nin the same scenarios the student performed the task by extracting the skills the student used via\nSKILLEXTRACTOR and comparing them with the expert skills in those scenarios. The student\u2019s\nexpertise in a skill m is then: Em = \u2212 \u2211\ufe01\n\u03be\u2208\u039es \u2206m(\u03c4 e\n\u03be is the student trajectory in \u03be, and\n\u2206m is a function that computes the discrepancy between the two trajectories in terms of m.\n\n\u03be ), where \u03c4 s\n\n\u03be , \u03c4 s\n\n5\n\n\fIn our implementation, if a skill is used only by the expert, we assume the student did not attempt or\nfailed that skill. We increase the discrepancy based on how far in the trajectory they failed. We assign\nhigher penalty to failures in the beginning of the scenario for two reasons: 1) Skills relevant early in\nthe task are often more important: other skills would not even be needed without first achieving these\nskills. 2) Failing at early skills might mean the student never had a chance or a need to perform the\nlater skills due to compounding errors, so our uncertainty about the later skills is higher. Thus, we\ndecide to be more conservative about the later skills. Specifically, we use:\n{\ufe04\n\n\u2206m(\u03c4 e\n\n\u03be , \u03c4 s\n\n\u03be ) =\n\n\u03be )/j\n\n\u2212r\u03be(\u03c4 s\n0\n\nif m \u0338\u2208 M s\notherwise.\n\n\u03be and m\n\n\u03c4 e\n\u03be\nj = m,\n\n(4)\n\nHere r\u03be is the reward function under scenario \u03be, which we assume to be non-positive.3 We increase\nthe discrepancy if a skill is only performed by the expert, and we discount this value over time (j in\nEq. (4)). We also scale discrepancy with r\u03be(\u03c4 s\n\u03be ) so that student trajectories with lower reward will be\npenalized more in terms of expertise. Alg. 2 presents the pseudocode for this step. In applications\nwhere the reward function is not readily available, one could design the skill-discrepancy function\n\u2206m independent of the rewards by using a divergence metric between expert and student trajectories.\n\n3) Creating individualized drills. Referring to Eq. (3), a student should master the skills they\nstruggle with to get better at the overall task. Therefore, the teacher should first identify those skills,\nand then have the student practice them. For example, if a teacher thinks a student who attempted to\nwrite the word \u201cperson\u201d struggled with writing the letter \u201cp\u201d, they could provide the word \u201cpuppet\u201d\nas a drill for the student. Using the expertise vector E for a student, which is the output of Alg. 2, we\ncan select the skills m\u2217 with low Em\u2217 as the skills that the student needs to practice.\nGiven a skill m\u2217, the student should ideally prac-\ntice it in the context of skills that frequently\ngo together with m\u2217, rather than in isolation\nor with arbitrary skills. Our approach uses the\nexpert demonstrations to identify frequent skill\nsequences containing skill m\u2217. We create drills\nconsisting of these sequences that enable us to\nhave the student practice (i) the target skill m\u2217,\nand (ii) how skill m\u2217 connects with other skills\nthat co-exist in various scenarios. Each drill re-\npeats such a sequence Nrep times. A natural ap-\nproach is to iterate over all n-grams, borrowing\nterminology from natural language processing,\nof skills from the expert demonstrations. Within\neach n-gram, we check if m\u2217 exists to identify\nthe skills that often co-occur with m\u2217. Mathe-\nmatically, frequency of an n-gram of skills x is\n(where I is an indicator for the particular n-gram\nx):\n\nAlgorithm 3 Individualized Drill Creation\nInput: Individual expertise vector E\nInput: Number of skills to create drills for Ntarget\nInput: Hyperparameters n, Ndrills, Nrep\nInput: Expert demonstration \u03c4 e\nInput: Skill labels M e\n\n\u2020 arg setn minm Em returns the set of n distinct indices\nthat individually minimize Em.\n\u2021 \u03c4 e\nterval [b\u03c4 e\nj ) such that m\u03c4 e\nconcatenates the actions of trajectory segments.\n\n1: Compute f (x) for all n-grams x\n2: for m\u2217 \u2190 arg setNtarget\nX \u2190 arg setNdrills\n3:\nDm\u2217 \u2190 \u22c3\ufe01\n\n\u25b7 Eq. (5)\nminm Em do \u25b7 see \u2020\nmaxx\u220bm\u2217 f (x) \u25b7 see \u2020\nm\u2208x \u03c4 e\nm \u25b7 see \u2021\n\nm is the segment of an expert demonstration from in-\nj = m. The operator \u2295\n\n4:\n5: return D\n\n\u03be for each \u03be \u2208 \u039ee\n\n\u03be for each \u03be \u2208 \u039ee\n\nj\u22121, b\u03c4 e\n\n\u2a01\ufe01Nrep\ni=1\n\nx\u2208X\n\n\u2a01\ufe01\n\nf (x) =\n\n(5)\nWe can then ask the student to practice the Ndrills n-grams involving the target skill m\u2217 with the\nhighest frequency.\n\nx = (m\n\n\u03be\u2208\u039ee\n\n, m\n\n,\n\n\u2211\ufe02\n\n\u2211\ufe02|M e\ni=1\n\n\u03be |\u2212n+1\n\n(\ufe02\n\nI\n\n\u03c4 e\n\u03be\ni\n\n\u03c4 e\n\u03be\ni+1, . . . , m\n\n)\ufe02\n\u03c4 e\n\u03be\ni+n\u22121)\n\nNext, we formalize a drill as a repetition of the skills in the n-grams. This ensures the student gets\nenough practice of skill m\u2217 with the most common skill sequence containing m\u2217. We then project\nthis constructed skill sequence back to the trajectory space by again using the expert demonstrations.\nWhile different segments of the expert demonstrations may correspond to the same skill, we randomly\nselect one of those segments to project back to the trajectory space.4 Alg. 3 presents the pseudocode.\nIt returns a set of drills Dm\u2217 for every target skill m\u2217. Though these drills can be used in several ways,\ne.g., explaining them to a student, here we let the student observe and then imitate each drill trajectory.\n\n3This is a mild assumption, since we can always subtract a constant offset from any finite reward function.\n4We set the initial state of each drill as the initial state of the first trajectory segment in the drill. Since the transition\n\nfunction f is deterministic, the initial state and action sequence specify a full trajectory.\n\n6\n\n\f", "experiments": "\n\nEnvironments. We consider two motor control tasks: parking a car in simulation with a joystick\ncontroller (PARKING), and writing Balinese characters (WRITING) using a computer mouse 5.\nWRITING: We introduce a novel task of writing Balinese character sequences6 from the Omniglot\ndataset [41], which contains action sequences for 1623 characters spanning 50 alphabets provided\nby crowdworkers from Amazon Mechanical Turk. We sample 5 Balinese characters, introduce a\nconnector character \u201c-\u201d, and create goal sequences of up to 8 characters. An agent in this environment\noperates in 2D (x,y) continuous state and action space, and receives reward corresponding to the\ndegree of overlap between the agent\u2019s and the gold trajectory. We train CompILE with 1000 random\nsequences based on trajectories provided by human contributors to the Omniglot dataset.\n\nPARKING: We use the Parking environment from HighwayEnv [42], a goal-based continuous control\ntask where an agent must park a car in a specified parking spot by controlling its heading and\nacceleration (2D action space). The 6D state space corresponds to the car\u2019s position, velocity, and\nheading and at each time step, and the agent receives a reward representing how close the current state\nis to the goal state. Scenarios differ across initial heading angles and goal parking spots, resulting in a\ndiverse set of skills needed. We train CompILE with rollouts across random scenarios from an optimal\nRL agent trained for 106 epochs using the StableBaselines [43] implementation of Soft Actor-Critic.\n\nFor both tasks, reward is negative with an optimal value of 0, but reported with a positive offset. Fur-\nther details, as well as necessary pre-processing steps for both environments and all hyperparameters\nused for training CompILE models as part of SKILLEXTRACTOR, are in the Appendix.\n\nDrill Creation. We create one drill for each latent skill identified by SKILLEXTRACTOR for both\nPARKING (n = 3, Nrep = 1, Ntarget = 7, Ndrills = 1) and WRITING (n = 2, Nrep = 3, Ntarget =\n8, Ndrills = 1) tasks. Fig. 6 shows examples of these drills.\nUser Study. We design web applications for our user study for both environments. For PARKING,\nstudents see the environment rendered on the interface, and control the car\u2019s steering and acceleration\nwith a 2D joystick. For WRITING, students use their computer mouse or trackpad to write on an\nHTML5 canvas, with the goal sequence of Balinese characters displayed underneath. Students in both\nenvironments follow a sequence of pre-test scenarios, practice sessions (including skills or drill-based\npractice), and evaluation scenarios. We recruit students through the Prolific platform, and include\nfurther information about pay rate, average experiment times, and task interfaces in the Appendix.\n\n5.1 Results\n\nDoes individualization help synthetic students? To demonstrate our full approach, we first show\nresults for synthetic \u201cstudents\u201d in the PARKING environment. We train two different synthetic students\nusing standard behavior cloning on goal-conditioned (s, a) tuples from expert demonstrations:\n\n1. Half-trained: We train a 4-layer feed-forward neural network for only 50 epochs, resulting in\n\naround 50% increase in Mean Squared Error than a student trained for 400 epochs.\n\n2. Reversing Difficulty: We train a 4-layer feed-forward neural network for 400 epochs, but with\nonly 20% of the data containing reverse acceleration actions, to simulate difficulty with reversing.\n\nNext, following the approach described in Sec. 3, we create a pool of 25 scenarios covering a diverse\nset of skills returned by our trained SKILLEXTRACTOR. We then collect rollouts from each synthetic\nstudent across all scenarios, use Alg. 2 to identify which 3 skills each synthetic student has least\nexpertise in, and select drills from our offline drill dataset that target these skills. Using an equal\ndataset size, we compare and report average student reward across 15 different evaluation sets, where\nthe student is fine-tuned on (s, a) sampled from these individualized drills (Ind. Drills) vs. random\ndrills (Drills) vs. entire (s, a) trajectories from the original expert demonstrations (Full Trajectory).\n\nOur results in Fig. 3 show that for the synthetic Half-trained IL Student, fine-tuning on data from\nboth Ind. Drills and randomly selected Drills significantly outperforms the Full Trajectory setting\n(p < .05, Wilcoxon signed-rank test), while the difference between Ind. Drills and Drills is not\nsignificant. This may be due to a half-trained IL agent not having low \u201cexpertise\u201d in any particular\nregion, but more generally benefiting from training further on good quality data. Interestingly, for\n\n5Note that the skills extracted are specific to operating a joystick/pen with a computer mouse.\n6We select Balinese because, in comparison to other scripts available in the Omniglot dataset, the writing trajectories were\nconsistent across users. Assisted teaching when there are multiple ways of performing a task is an important future direction.\n\n7\n\n\fthe Student with Reverse Action Difficulty, Ind. Drills significantly outperforms both randomly\nselected Drills and Full Trajectory settings, but randomly selecting Drills slightly underperforms\nFull Trajectory, perhaps due to several drills not containing as many reverse actions as in the full-\ntrajectory data. These results suggest that while drills can benefit both types of synthetic students in\ngeneral, due to being composed of very common skill sequences, the synthetic student designed to\nlack a certain kind of \u201cskill\u201d is more likely to strongly benefit from individualization. This presents\na promising signal that our CompILE-based SKILLEXTRACTOR learns skills that align with our\nintuitions for what skills are necessary for the PARKING task. However, our eventual goal is to teach\nhumans motor control tasks, so we next test the usefulness of our approach with human students.\n\nDoes assisted skill-based practice improve human learning\noutcomes? Before constructing and individualizing drills, it\nis important to verify whether the skills returned by SKILLEX-\nTRACTOR\u2014the building blocks of our approach\u2014do indeed\nbenefit student performance, as well as in comparison to simpler\nheuristics. For example, CompILE, as well as other skill dis-\ncovery methods, take as input the expected number of skills K\nper expert demonstration. A simpler Time Heuristic approach\ncould instead be to assume that skills are temporally ordered\nand have equal length, and just split each expert demonstration\ninto K equally-sized segments. If such a heuristic sufficed, then\nit may not be necessary to leverage more complex skill discovery algorithms.We therefore run user\nstudies for both PARKING and WRITING environments comparing three different settings:\n\nFigure 3: The effect of Ind. Drills and\nrandom Drills for synthetic students.\n\n1. Skills: Each user practices trajectories corresponding to the 3 most common skills returned by\n\nSKILLEXTRACTOR based on CompILE parameterized with K, for 3 sessions each.\n\n2. Time Heuristic: We temporally split each trajectory into K intervals of equal length, and each\n\nuser practices 3 different intervals for 3 sessions each.\n\n3. Full Trajectory: Each user practices 3 different full trajectories of an expert demonstration (e.g.\n\nparking a car from the start state in PARKING, an entire character sequence in WRITING).\n\nWe set K = 3 for PARKING and K = 8 for WRITING. For fair comparison, the number of time-steps\nusers are able to practice in the full trajectory and time-heuristic modes is roughly equivalent. Because\nour users may differ widely in prior experience (e.g. a video game player may be more familiar with\nthe joystick in PARKING), we measure and report Reward Improvement, or the difference in average\nreward across 5 random evaluation scenarios and 2 random pre-test scenarios.\n\nOur results in Fig. 4 show significantly\n(p < .05) higher reward improvement for\nstudents in both WRITING (N = 25) and\nPARKING (N = 20) in the Skills setting vs.\nFull Trajectory setting. This suggests that\nteaching students the necessary skills for a\nmotor control task is extremely beneficial\nFigure 4: Students receive significantly higher reward im-\nto improve learning. However, comparison\nprovement when practicing with the Skills setting than Full\nbetween the Skills and Time Heuristic is\nTrajectory. A simple Time Heuristic results in inconsistent\nperformance across environments, demonstrating the neces-\ninterestingly inconsistent between environ-\nsity for a more reliable way to identifying skills for teaching.\nments, where Time Heuristic significantly\nleads to worse student learning outcomes in PARKING, but not for WRITING. One possible expla-\nnation might be that individual skills in WRITING, such as characters or certain curves, might be\nsemantically intuitive on their own, and therefore even if Time Heuristic presents different characters\nas the same overall \u201cskill\u201d, there is less confusion for users. On the other hand, because of the chal-\nlenging dynamics in PARKING, a user may rely more on practicing skill trajectories that have clearly\nsimilar actions. Because heuristics are often environment-specific and complex, we view these results\nas an encouraging sign of the suitability of intelligent skill-discovery methods for teaching humans.\n\nFinally, we ask all participants in a post-study survey \u201cWhat else would have been helpful for you to\nlearn how to write the characters or how to park a car?\u201d. One user for the WRITING task who was\nassigned practice without skills responded \u201cSeparating the characters\u201d. Responses from users who\nwere assigned skill-based practice included \u201cPracticing a string of 2 characters\u201d and \u201cMore time\nand more repetition\u201d. These responses motivate our next set of experiments on the effect of drills.\n\n8\n\n\fDoes practice with novel drill sequences outperform skill-based practice? Though our previous\nresults demonstrate skill-focused practice leads to stronger learning outcomes in both environments,\nwe hypothesize it is equally important to learn how to combine skills that often co-occur via drills.\nFor example, a qualitative look at the skills in Fig. 6 suggests some of the discovered skills consist of\nfew actions, such as a short dot in WRITING or only a few time steps in PARKING. On their own\nthese may not be useful skills to practice, but may be important to teach in the context of other skills.\n\nTo compare with our individualization\nmethod, we create a fixed set of 5 pre-\ntest scenarios covering a diverse set of\nskills returned by SKILLEXTRACTOR for\nboth environments. We select 2 random\ndrills, and compare the change in student\nreward when practicing with drills (Drills)\nvs. practicing the same underlying skill se-\nquences used to composed the drills as sep-\narate practice sessions (Skills). Overall, as\nshown in Fig. 5, we find no statistically sig-\nnificant difference in reward improvement\nwhen students practice repetitive drill se-\nquences composed of multiple skills (Blue\nbar) vs. skills in separation (Pink bar). Possible explanations include the increased number of prac-\ntice sessions in the Skills setting (because we keep the number of total time-steps equivalent across\nsettings) mitigating benefits of repetition or practicing skill transitions in Drills. However, we next\nask whether we may observe benefit from drills if we were to individualize them to students.\n\nFigure 5: Students marginally improve reward more when\npracticing with Ind. Drills than Skills for both tasks. In\nWRITING, students significantly prefer Ind. Drills over both\nrandom Drills and Skills, yet this does not hold in PARKING,\nwhere targeted practice of reversing may frustrate students.\n\nDoes individualizing drills improve student learning outcomes? For each student, we run a\nCompILE-based SKILLEXTRACTOR immediately after receiving all pre-test student trajectories.\nFig. 6 demonstrates the top 2 skills identified as \u201clow expertise\u201d across all students for both PARKING\n(N = 20) and WRITING (N = 25) environments. SKILLEXTRACTOR combined with Alg. 2 is\nindeed able to identify a wide set of low-expertise skills in student trajectories, ranging from more\npopular skills (e.g. \u201chorn\u201d shape in WRITING) to rarely difficult skills (e.g. steering the car forward\nin a straight line in PARKING). Importantly, this is despite the likely strong differences between\nhuman student and expert trajectories, emphasizing the viability of our approach to teaching humans.\n\nNext, we select 2 unique drills targeting each\nidentified skill, and provide them to students for\npractice before measuring reward improvement\non an evaluation set. Fig. 5 shows individualized\ndrills (Ind. Drills) lead to stronger reward im-\nprovements for students than Skills with marginal\nsignificance (p \u2248 .06 WRITING, p \u2248 .09 PARK-\nING), supporting our hypothesis that individual-\nizing drills leads to better learning outcomes. We\nvisualize all student trajectories before and after\npracticing Ind. Drills in the Appendix.\n\nFinally, we ask students to rate the helpfulness of\npractice sessions from 1-7 (1 = least helpful), and\nshow in Fig. 5 for WRITING, students strongly\npreferred practice sessions from Ind. Drills over\nDrills and Skills. However, despite the higher\naverage reward improvement for Ind. Drills in\nPARKING, students who participated in Skills\npractice rated the helpfulness of their sessions\nsignificantly higher. Though there may exist\nmany explanations for this result, including stu-\ndent preference for shorter sequences, we observe\nin evaluation rounds that users who received Ind.\nDrills were around twice as likely to try reversing into a parking spot than other students (53% vs.\n27%). This is likely due to our individualization method providing drills with reversing skills, which\n\nFigure 6: Alg. 2 identifies a wide range of skills as\nlow expertise across students for both environments.\nReverse actions are identified as the most common\nskills for students to improve on in PARKING, while\ncommon curved-shapes are identified for WRITING.\n\n9\n\n\fwere more common targeted skills (see Fig. 6). Crucially, in these scenarios the expert action is also\nto reverse, suggesting that while individualization may create a more frustrating learning experience,\nit may still help students achieve stronger learning outcomes by teaching more challenging skills.\n\n", "conclusion": "\n\nOur work is a first step towards building AI-assisted systems to more efficiently teach humans motor\ncontrol tasks. However, there exist a few important limitations, which we now outline below.\n\n6.1 Extracting \u201cHuman Teachable\u201d Skills\n\nA possible limitation of our work is whether skills extracted via SKILLEXTRACTOR are suitable for\nteaching human learners. While one benefit of our framework is its agnosticism to the type of the\nexpert, we could imagine that using an automated RL-trained expert, for example, might result in\nteaching skills that are unsuitable for human learners. Addressing this requires stronger cognitive\nmodels of human motor learning in order to iterate different types of skill decomposition and optimize\nfor some notion of \u201cteachability.\u201d We could consider adapting existing methods used to model\nlearning over time in other education domains (e.g. mathematics), such as Deep Knowledge Tracing\n([34]) and Item Response Theory, which typically represent questions as discrete items or with simple\nfeatures, and student responses as binary (correct or incorrect). Unfortunately, adapting such methods\nto handle the rich information in student trajectories is quite unstable, likely due to high dimension\nand scale (e.g. > 1K timesteps). Furthermore, student response variation for the same scenario is a lot\nhigher for motor tasks than in other tasks such as in mathematics, where within a short period a student\nlikely answers the same question the same way. Finally, such approaches require large amounts of\nstudent data (on the scale of thousands). Further discussion, as well as results demonstrating the\nunreliability of asking human experts to identify skills instead, is in Appendix Section E.\n\n6.2 Accounting for Student & Expert Multimodality\n\nOne important aspect of many motor control tasks is multimodality - there may exist many optimal\nways to complete the task. Although one can naively extend our approach to handle this by collecting\ndemonstrations from a diverse set of experts and identifying which expert to \u201cmatch\u201d a student with,\nunderstanding the different control preferences present in a motor task is complex. For example,\nin writing, while different stroke orders for the same character clearly constitute different modes,\nthere exist more subtle differences, such as the degree of rotation used, or the sharpness of the\ntrajectory. These may occur due to physical preferences or conditions such as arthritis, but are hard to\nautomatically distinguish as separate modes versus noise. Cleanly defining the distinct modes of a\ncontrol task likely requires strong domain knowledge combined with large amounts of data across a\ndiverse population. Furthermore, inferring which mode to teach a novice student is challenging, as\npre-test trajectories only provide a limited amount of information. In traditional education settings,\nwe often ask students questions such as whether one is left or right handed, and use that to inform how\nwe teach. However, there may exist more obscure forms of preferences that require longer interaction\nwith a student to elicit, and identifying such preferences is an important direction for future work.\n\n6.3 Potential Failure Cases\n\nAlthough we provide a general framework for assistive teaching of any arbitrary motor control task,\nincluding those that lack many expert instructors like teleoperating a novel surgical robot system,\nthere are some settings that may be incompatible with our approach. These include situations where\nimportant skills do not appear in observed trajectories (e.g. the positioning of a hand on a ball before\npitching in baseball), or there exists extreme stochasticity such that the same actions lead to drastically\ndifferent outcomes, resulting in unintuitive dynamics for learners. Furthermore, in some tasks learning\nmay not be feasible just from practice, but requires stronger forms of teacher-student interaction, such\nas physical guidance. We believe understanding the limits of demonstration-based teaching for motor\ntasks necessitate further insights from several areas within the broader NeurIPS research community,\nincluding cognitive science, reinforcement learning, and social aspects of machine learning.\n\n", "appendix": "Note: Additional visualizations of our experiments can be found here: https://sites.google.\ncom/view/assistive-teaching-2022/home\n\nA Broader Impact & Ethics Statement\n\nAI-assisted teaching of motor control tasks can provide significant benefits such as more reliable\nteaching to individual students with different abilities (e.g. by leveraging more granular information\nabout student actions), adaptability to any type of motor task or expert agent, and improved safety\nby reducing burden on human teachers for safety-critical tasks. However, we emphasize that our\napproach is solely meant to assist human teaching, as there exist many important aspects of human\ninstruction that would be challenging to replace, including providing inspiration and motivation, in\ndepth knowledge of human physical limitations, and an awareness of the broader context of a specific\nmotor control task. Further risks of our approach, and avenues to address them, include:\n\n\u2022 Bias of the expert agent. The suitability of the skills we use for teaching relies on how diverse\nthe set of demonstrations from an expert is. For example, if a writing task only contained demon-\nstrations from right-handed experts, certain action sequences that may be harmful for left-handed\nstudents\u2019 learning may be chosen as skills. Understanding how CompILE performs over a mix\nof expert types, and how to enable more complex adaptation to a specific student\u2019s needs at the\nskill-identification step itself are important future directions.\n\n\u2022 Over-reliance on the expert. Our work currently assumes that in order to learn a task, the student\nshould practice drills built from how an expert performs the task. However, a student should also\nbe encouraged to learn when it may be appropriate to differ from the expert\u2019s actions if it helps the\nstudent learn better. This requires knowledge of how individual skills serve the ultimate task\u2019s goal\n(e.g. understanding why we first turn to enter a parking spot), which future work on incorporating\ninterpretability methods and natural language instructions into our approach can address.\n\n\u2022 Student physical constraints during learning. In many tasks, certain action sequences may\nphysically be easier for an expert to perform than a student, and may perhaps even be dangerous\nfor a student to practice without building up necessary techniques. This can be addressed by\nleveraging more complex hierarchical approaches to skill discovery (e.g. which skills should be\nmastered before attempting others) and incorporating knowledge of human physical constraints (e.g.\ndegree of feasible wrist rotation). Another interesting direction for future work is to compare skills\nidentified at different levels of expertise, and ascertain whether skills identified from a \u201cmedium-\nlevel student\u201d may actually be easier, and less physically demanding, to teach with than those\nidentified from an expert.\n\nB Notation Glossary\n\nFor convenience, we provide a glossary of all mathematical notation used in our framework.\n\nTerm\n\u039es/\u039ee\n\u03c4 s\n\u03be /\u03c4 e\n\u03be\nM e\n\u03be\nE\nb\u03c4\n\u03c4 e\nm\n\u2295\n\nMeaning\nStudent/Expert Scenarios\nStudent/Expert Trajectories for scenario \u03be\nSet of skill labels corresponding to an Expert e\u2019s trajectory for scenario \u03be\nExpertise vector for a given student\nBoundary of a skill subsequence in a trajectory \u03c4 corresponding to a particular timestep\n\nSegment of an expert e\u2019s trajectory from interval [b\u03c4 e\nj = m\nConcatenation operator that stitches together action sequences when creating drills\n\nj ) such that m\u03c4 e\n\nj\u22121, b\u03c4 e\n\n14\n\n\fC Student Trajectories Before and After Practicing w/ Individualized Drills\n\nFigure 7: Overlay of all student trajectories of 3 Balinese characters for the WRITING task before teaching with\nindividualized drills and after. For the rightmost character (\u201cna\u201d in Balinese), students learn to draw a smaller\nsecond loop at a lower angle, following the expert more closely. For other characters, we find that students\nexhibit less noise, and off-character strokes more closely following the original character.\n\nWe note that while Fig. 7 compares student trajectories before and after teaching for one particular\nset of Balinese characters, the reported values are averaged across all pre-test and evaluation rounds.\nFurthermore, common failure modes in the pre-test rounds before teaching that are not captured by\nthis visualization include students giving up early and letting go of the mouse, and students re-tracing\ntheir characters.\n\nD Comparison Between Student and Expert CompILE Outputs\n\nFigure 8: SKILLEXTRACTOR outputs for Expert (left) and Student (right) trajectories for both the Parking (top)\nand Writing (bottom) tasks. Black dots represent skill boundaries identified by CompILE.\n\nComparing SKILLEXTRACTOR output boundaries on trajectories from both our experts and students\nfor both tasks, we can see that CompILE is able to segment both types of trajectories, but the noise\nin student trajectories leads to a failure of identifying the necessary skills for the task. We leverage\nthis information to identify which skills students are struggling with. For example, in PARKING, the\nstudent is clearly unable to park the car, but the initial movement towards the upper left is segmented\nsimilarly to the expert, so the student will largely be penalized for later parts of the trajectory.\n\nE Challenges in Human Expert Skill Identification\n\nTo address the challenges of extracting \u201chuman teachable\u201d skills discussed above, one may consider\nusing human experts as part of the SKILLEXTRACTOR function. However, the key idea behind our\nwork is that people who are experts at performing a task may not be expert at teaching it, and may\nstruggle to identify skills consistently.\n\nTo observe this clearly, we asked 3 different experts to annotate 10 successful trajectories of the\nPARKING task with boundaries corresponding to skills under unlimited time. The set of 10 trajectories\nonly contained 3 unique demonstrations, allowing us to measure whether experts were consistent\nwhen providing skill annotations for the same expert demonstration. Fig. 9 shows that even for the\n\n15\n\n\fsame trajectory, the same expert user may provide a completely different skill segmentation, and even\nidentify a different number of skills. Even Expert 3, the most consistent expert across all duplicate\ntrajectories, showed slight variation across trials - however, we note that CompILE\u2019s segmentations\nclosely matched theirs. Moroever, by design, CompILE returns the exact same segmentations for the\nsame action sequence. Overall, our user study results showed that human expert provided skills are\noften unreliable, leading to skill sequences with high variations that would be challenging to teach in\na uniform way.\n\nFigure 9: Skill segmentations from our CompILE-based SkillExtractor and 3 different human experts at the\nPARKING task. For a given trajectory, experts provided 2 segment boundary annotations (columns), with each row\ncorresponding to a different expert. Expert Users 1 and 2 show high variability among their own segmentations,\nwhile Expert User 3 is more consistent and provides skill segmentations similar to the output of CompILE.\n\nFinally, we note that a key aspect of our approach is scaling the ability to identify skills within a wide\nrange of student trajectories for individualization. This is an even higher burden for human experts,\nwho need to identify skills over trajectories that may widely differ from each other and the way the\nexpert knows how to complete the task.\n\nTherefore, in this work we attempted to incoporporate preliminary notions of \u201chuman teachability\"\nwhen selecting between hyperparameter settings of our CompILE-based SKILLEXTRACTOR. Specif-\nically, we filtered out skills corresponding to trajectories below a minimum length (due to human\nperceptual limits), and then chose the parameters that corresponded to the set of skills with highest\nentropy, with the intuition that a sufficiently diverse set of scenarios for a task would require a large\nvariety of skills, and to minimize the risk of SKILLEXTRACTOR grouping two distinct skills as just\none latent skill.\n\nF Impact of Training Time on Synthetic Student\n\nAlthough we report results for both half-trained and reverse difficulty synthetic students after fine-\ntuning on 100 epochs, one natural question is the effect of training time. We examine this more\nclosely with the \"reversing difficulty\" student, where Fig. 10 shows that as we increase the number\nof training epochs (equivalent to adjusting the Nrep parameter in our IL setting) for a fixed set of 3\n\n16\n\n\fdrills, student reward starts to plateau close to the average reward the expert receives. This shows that\nfor our synthetic model, the largest learning gain occurs at the start of training.\n\nFigure 10: Reward starts to plateau over training iterations for the \"reversing difficulty\" synthetic student.\nReported values are average reward over 100 random rollouts.\n\nIn practice, a teacher could also adjust the Ndrills parameter and repeat the entire teaching session by\nrepeating the expertise identification method in Alg. 2, which may return different skills as a student\nimproves over time. Planning the optimal overall curriculum is likely extremely task-dependent, and\nrequires a much stronger model of student learning that incorporates concepts such as forgetting,\nwhich we leave for future work.\n\nG Source Code\n\nWe provide all source code necessary to replicate our user study, for both PARKING and WRITING\nenvironments, as well as the trained CompILE models for both environments, at https://github.\ncom/Stanford-ILIAD/teaching.\n\nH Environment Pre-Processing\n\nAs described in Sec. 5 of the main paper, our PARKING environment is built off of the HighwayEnv\ngoal-based task whereas our WRITING environment is custom built based off of the Omniglot dataset.\nFor the purpose of simplifying our user study, we make the following modifications:\n\n1. For PARKING, while we train our expert agent and skill-discovery algorithms across all possible\nparking goals, we only pick goals in the bottom right quadrant for teaching students in our user\nstudy as a simplification. To expand to all goals in the task, we believe further teaching time would\nbe necessary due to the larger number and variety of skills required.\n\n2. Likewise, for WRITING, we limit our sequences to contain only up to 5 different Balinese\ncharacters (\u201cNa\u201d, \u201cMa\u201d, \u201cPa\u201d, \u201cBa\u201d, \u201cWa\u201d) to reduce the amount of skills required to learn the\noverall task.\n\n3. Because crowdworkers in the Omniglot dataset differ in terms of interfaces used at the time of\ndata collection, we \u201cinfill\u201d all action sequences as a method of standardization. Specifically, we\ninfill between any two consecutive states that differ by more than 1 pixel. Because these infilled\ntrajectories are used to train our CompILE module for SKILLEXTRACTOR, we likewise infill all\nuser trajectories collected in our user study.\n\nI Hyperparameters & Training Details\n\nHere, we describe all necessary hyperparameters to replicate training our (i) PARKING expert agent,\n(ii) skill-discovery CompILE modules for both PARKING and WRITING tasks, and (iii) synthetic\nstudents for PARKING. All models are trained on 1 NVIDIA TITAN RTX GPU, and the longest\ntraining time (for the expert PARKING agent) is roughly 5 hours.\n1. PARKING expert agent: We train a StableBaselines3 implementation of Soft Actor-Critic for 106\nepochs with a learning rate of 0.001, which achieves a roughly 100% parking success rate.\n\n2. PARKING CompILE module: We train a CompILE module (using the code from [28]) for 2000\niterations with a learning rate of 0.001, batch size of 100, latent dimension of 16 (i.e. 16 possible\n\n17\n\n\fskills), prior expected length of skill segments of 10, and prior number of segments per expert\ndemonstration of 4. As described in the main paper, for PARKING, we add a penalty to the original\nloss function that is the MSE loss between the state differences between two consecutive states in\nthe CompILE reconstruction and that in the training data.\n\n3. WRITING CompILE module: We train a CompILE module for 80 iterations with a learning rate\nof 0.005, batch size of 50, latent dimension of 24 (i.e. 24 possible skills), prior expected length\nof skill segments of 250, and prior number of segments per expert demonstration of 8. For both\nPARKING and WRITING, we find it necessary to set the latent dimension high as many latent\ncodes correspond to zero skill segments.\n\n4. PARKING synthetic \u201chalf-trained\u201d student: We train a 4-layer feed-forward neural network for\n50 epochs with a learning rate of 0.0005 and batch size 256 via behavior cloning on rollouts from\nthe expert agent, which receives an eval MSE loss of 0.049.\n\n5. PARKING synthetic \u201creversing difficulty\u201d student: We train a 4-layer feed-forward neural\nnetwork for 400 epochs with a learning rate of 0.0005 and batch size 256 via behavior cloning on\nrollouts from the expert agent with only 20% of the data containing reverse acceleration actions\n(negative y-value), which receives an eval MSE loss of 0.021.\n\nJ User Study\n\nWe recruited users on Prolific, a crowdsourcing platform to conduct research studies, as part of an\nIRB-approved study (Protocol No. 49406 reviewed by Stanford University). We recruited up to 25\nusers for each setting in our user study for both WRITING and PARKING tasks. Overall, participants\nwere paid an estimated wage of 15 dollars per hour, and took on average 20 minutes to complete\nthe entire study, including reading instructions, learning the motor control task, and completing a\npost-task survey.\n\nEach student was provided a link to an instructions page, where they provided a username to access\nthe interfaces we built for both tasks. Each interface included step-by-step instructions on the side.\nAs described in the main paper, students participated in a series of pre-test tries at the task, a sequence\nof practice sessions, and then an evaluation round. In the PARKING task, due to its difficulty, practice\nsessions consisted of both expert demonstrations (with joystick movement corresponding to expert\nactions) and student practice mode, while the WRITING task practice sessions consisted only of\npractice (of either full sequences, skills, or drills). Furthermore, to help guide students, we overlayed\nthe state sequence of the target skill/drill/full-trajectory during demo and practice sessions for all\nsettings in PARKING. Finally, for both motor control tasks we imposed a time limit on students\nfor pre-test, evaluation, and practice sessions, proportional to the length of the sequence. For fair\ncomparison, we ensure that the total number of allowed time-steps is roughly equivalent between\nsettings we directly compare with each other. We include images of the instructions and example\ninterfaces for both tasks below.\n\nFinally, each student participant completed a post-task survey where they provided ratings for how\nhelpful they found the practice sessions for learning the control task, information about whether\nstudents used a trackpad or computer mouse, as well as any feedback about the interface or task itself.\nWe found no significant impact on performance from whether a students used a trackpad or computer\nmouse. We provide the complete list of survey questions asked below.\n\nOverall, students found the PARKING task particularly challenging, often asking for more practice\nsessions, which many found helpful (e.g. \u201cIt was interesting to give it a go and see how I improved in\na short time.\u201d, \u201cI could see that I was improving as the experiment went on\u201d). Meanwhile, students\nparticipating in the WRITING task students enjoyed the educational experience of learning a new\nscript (e.g. \u201cI am interested in writing forms and would one day like to learn some unusual scripts.\u201d,\n\u201cinteresting learning to write another language\u201d), but wished to learn more about the characters\u2019\nmeaning, motivating further research in making automatically-discovered skills (which may not\nnecessarily be characters) more interpretable to students (e.g. \u201cI would like to know what Balinese\ncharacters I\u2019m tracing and their meaning\u201d).\n\n18\n\n\fFigure 11: User study interface for the PARKING task where student participants learn how to park a car with a\njoystick controller. A black circle marks the front of the car, and the blue square marks the goal parking spot.\n\nFigure 12: User study interface for the WRITING task where student participants learn how to trace Balinese\ncharacters. After the user lets go of their mouse, or when the timer is over, a score representing the reward would\nbe displayed.\n\n19\n\n\fFigure 13: Instructions for the PARKING task where student participants learn how to park a car with a joystick\ncontroller.\n\n20\n\n\fFigure 14: Instructions for the WRITING task where student participants learn how to trace Balinese characters.\n\n21\n\n\fUser Study Survey Questions:\n\n1. Did you find the demos and practice sessions useful in learning how to park the car? / Did you\nfind the practice sessions helpful in learning how to write the different characters? (Rating 1-7)\n2. How easy was it to learn how to park the car? / How easy was it to learn how to write the\n\ncharacters? (Rating 1-7)\n\n3. How easy was it to learn how to park the car? / How easy was it to learn how to write the\n\ncharacters? (Rating 1-7)\n\n4. What else would have been helpful to learn how to park the car? / What else would have been\n\nhelpful to learn how to write the characters?\n\n5. What did you like about the experiment?\n6. What would you wish to change about the experiment?\n7. Did you use a laptop trackpad or a mouse to complete this study?\n\n22\n\n\f", "system_prompts": [{"role": "system", "content": "You are a computer science researcher currently reviewing a paper titled \"Assistive Teaching of Motor Control Tasks to Humans\" for the NeurIPS computer science conference. Your goal is to try to be as objective and truthful as possible in your answers about the paper provided. Your reviews will be used for causal reasoning in determining the quality of the paper."}], "problems_dict": {"q1b": [[{"role": "user", "content": "The following is the conclusion section of the paper you are reviewing:\n\n\nOne future direction is to consider incentives in our paradigm like the literature of information\nelicitation without verification [13, 18, 5, 22, 9]. We have asked a class of students at Peking\nUniversity: why are bar chairs high? using our paradigm. We cluster the answers by hand. The\nplurality answer is \u201cthe bar counter is high\u201d and our top-ranking answer is \u201cbetter eye contact with\npeople who stand\u201d. Thus, another future diction is to extend our approach to the scenario where\npeople\u2019s answers are sentences, where we can apply NLP to cluster them automatically. In summary,\nwe propose the first empirically validated method to learn the thinking hierarchy without any prior\nin the general problem-solving scenarios. Potentially, our paradigm can be used to make a better\ndecision when we crowd-source opinions in a new field with little prior information. Moreover, when\nwe elicit the crowds\u2019 opinions for a policy, with the thinking hierarchy information, it\u2019s possible to\nunderstand the crowds\u2019 opinions better. However, regarding the negative impact, it may be easier\nto implement a social media manipulation of public opinion with the full thinking hierarchy of the\ncrowds. One interesting future direction is to explore the impact of the thinking hierarchy information.\n\n\nBased on this section, do the authors describe limitations of their work?"}]], "q1c": [[{"role": "user", "content": "The following is the conclusion section of the paper you are reviewing:\n\n\nOne future direction is to consider incentives in our paradigm like the literature of information\nelicitation without verification [13, 18, 5, 22, 9]. We have asked a class of students at Peking\nUniversity: why are bar chairs high? using our paradigm. We cluster the answers by hand. The\nplurality answer is \u201cthe bar counter is high\u201d and our top-ranking answer is \u201cbetter eye contact with\npeople who stand\u201d. Thus, another future diction is to extend our approach to the scenario where\npeople\u2019s answers are sentences, where we can apply NLP to cluster them automatically. In summary,\nwe propose the first empirically validated method to learn the thinking hierarchy without any prior\nin the general problem-solving scenarios. Potentially, our paradigm can be used to make a better\ndecision when we crowd-source opinions in a new field with little prior information. Moreover, when\nwe elicit the crowds\u2019 opinions for a policy, with the thinking hierarchy information, it\u2019s possible to\nunderstand the crowds\u2019 opinions better. However, regarding the negative impact, it may be easier\nto implement a social media manipulation of public opinion with the full thinking hierarchy of the\ncrowds. One interesting future direction is to explore the impact of the thinking hierarchy information.\n\n\nBased on this section, do the authors address potential negative societal impacts of their work?"}]], "q2a": [[{"role": "user", "content": "The following is the abstract of the paper you are reviewing:\n\n\nWhen we use the wisdom of the crowds, we usually rank the answers according to\ntheir popularity, especially when we cannot verify the answers. However, this can\nbe very dangerous when the majority make systematic mistakes. A fundamental\nquestion arises: can we build a hierarchy among the answers without any prior\nwhere the higher-ranking answers, which may not be supported by the majority,\nare from more sophisticated people? To address the question, we propose 1) a\nnovel model to describe people\u2019s thinking hierarchy; 2) two algorithms to learn\nthe thinking hierarchy without any prior; 3) a novel open-response based crowd-\nsourcing approach based on the above theoretic framework. In addition to theoretic\njustifications, we conduct four empirical crowdsourcing studies and show that a)\nthe accuracy of the top-ranking answers learned by our approach is much higher\nthan that of plurality voting (In one question, the plurality answer is supported by\n74 respondents but the correct answer is only supported by 3 respondents. Our\napproach ranks the correct answer the highest without any prior); b) our model\nhas a high goodness-of-fit, especially for the questions where our top-ranking\nanswer is correct. To the best of our knowledge, we are the first to propose a\nthinking hierarchy model with empirical validations in the general problem-solving\nscenarios; and the first to propose a practical open-response based crowdsourcing\napproach that beats plurality voting without any prior.\n\n1\n\n\nBased on this abstract, do the authors have theoretical results? Please start your answer with \"Yes\", \"No\", or \"Unsure\"."}, {"role": "user", "content": "The following is the methods section of the paper you are reviewing:\n\n\nWe first introduce our model for thinking hierarchy. Tversky and Kahneman [27] propose that people\nhave two systems, a fast and intuitive system, and a slow and logical system. For example, Alice\nstarts to solve the circle problem. When she reads the question, she can run her intuitive system 1 and\nobtain answer \u201c3\u201d. However, when she starts to think carefully, she runs her more careful system 2 to\nobtain answer \u201c4\u201d.\n\nWe propose a more general model where people can run multiple oracles to approach the question\nduring their thinking process. Conceptually similar to the cognitive hierarchy theory [3], we assume\nthe oracles have different levels. People usually run lower level oracles before the higher level oracles.\nIn the previous example, system 1 is the lower level oracle and system 2 is the higher one. We assume\nthat Alice runs system 2 after system 1. Thus, in our model, people who know the answer is \u201c4\u201d can\n\n4\n\n\fpredict that other people answer \u201c3\u201d, which is hypothesized in Kong and Schoenebeck [10]. It\u2019s also\npossible that some people run the most sophisticated oracle directly without running the lower ones.\nThus, we design the model such that it does not require the higher-type to be able to predict ALL\nlower types. We also allow the oracles\u2019 outputs to be random.\n\nSection 2.1 introduces the model of thinking hierarchy. Section 2.2 reduces the model inference\nproblem to a matrix decomposition problem. Section 2.3 and section 2.4 show how to learn the\nthinking hierarchy.\n\n2.1 Thinking hierarchy\n\nFixing a question q (e.g. the circle problem), T denotes the set of thinking types. A denotes the set of\npossible answers. Both T and A are finite sets. \u2206A denotes all possible distributions over A. We\nsometimes use \u201cprob\u201d as a shorthand for probability.\n\nGenerating answers We will describe how people of different thinking types generate answers.\nDefinition 2.1 (Oracles of thinking types W ). An answer generating oracle maps the question to\nan (random) answer in A. Each type t corresponds to an oracle Ot. The output Ot(q) is a random\nvariable whose distribution is wt \u2208 \u2206A. W denotes a |T | \u00d7 |A| matrix where each row t is wt.\nEach respondent is type t with prob pt and (cid:80)\nrunning the oracle Ot. For all a \u2208 A, the probability that a respondent answers a will be (cid:80)\nWe assume the probability is positive for all a \u2208 A.\nExample 2.2 (A running example). There are two types T = {0, 1}. The answer space is A =\n{3, 4, 6}. O0 will output \u20183\u2019 with probability 0.8 and \u20186\u2019 with probability 0.2. O1 will output \u20184\u2019\n0\n1\n\nt pt = 1. A type t respondent generates the answer by\nt ptwt(a).\n\ndeterministically. In this example, W =\n\nwhere the first row is the distribution of\n\n(cid:20)0.8\n0\n\n0.2\n0\n\n(cid:21)\n\nO0\u2019s output and the second row is the distribution of O1\u2019s output.\n\nGenerating predictions We then describe how people of different thinking types predict what other\npeople will answer. Here the prediction is not a distribution, but an answer other people may report.\nWhen a type t respondent makes a prediction, she will run an oracle, which is Ot\u2032 with probability\npt\u2192t\u2032 where (cid:80)\n\nt\u2032 pt\u2192t\u2032 = 1. She uses the output of Ot\u2032 as the prediction g \u2208 A.\n\nCombination: answer-prediction joint distribution M M denotes a |A| \u00d7 |A| matrix where\nMa,g is the probability an respondent answers a and predicts g. \u039b denotes a |T | \u00d7 |T | matrix where\n\u039bt,t\u2032 = ptpt\u2192t\u2032 is the probability a respondent is type t and predicts type t\u2032.\nExample 2.3. In this example, when type 0 respondent makes a prediction, with prob 1, she will run\nO0 again. When type 1 respondent makes a prediction, with prob 0.5, she will run O1 again, with\nprob 0.5, she will run O0. Moreover, a respondent is type 0 with prob 0.7, and type 1 with prob 0.3.\n\nHere \u039b =\n\n(cid:20)p0p0\u21920\np1p1\u21920\n\np0p0\u21921\np1p1\u21921\n\n(cid:21)\n\n=\n\n(cid:20) 0.7 \u2217 1\n0.3 \u2217 0.5\n\n0.7 \u2217 0\n0.3 \u2217 0.5\n\n(cid:21)\n\n=\n\n(cid:20) 0.7\n0.15\n\n(cid:21)\n\n.\n\n0\n0.15\n\nClaim 2.4. Based on the above generating processes, M = W\u22a4\u039bW.\n\nProof. For each respondent, the probability she answers a and predicts g will be\n(cid:88)\n\n(cid:88)\n\n(cid:88)\n\nptwt(a)\n\npt\u2192t\u2032wt\u2032(g) =\n\nwt(a)ptpt\u2192t\u2032wt\u2032(g).\n\nMa,g =\n\nt\n\nt\u2032\n\nt,t\u2032\n\nWe sum over all possible types the respondent will be. Given she is type t, she runs oracle Ot to\ngenerate the answer and wt(a) is the probability that the answer is a. We sum over all possible\noracles she runs to predict. Given that she runs Ot\u2032, wt\u2032(g) is the probability the prediction is g.\n\nKey assumption: upper-triangular \u039b we assume that people of a less sophisticated level can\nnever run the oracles of more sophisticated levels. A linear ordering of types \u03c0 : {1, 2, \u00b7 \u00b7 \u00b7 , |T |} (cid:55)\u2192 T\nmaps a ranking position to a type. For example, \u03c0(1) \u2208 T is the top-ranking type.\nAssumption 2.5. We assume that with a proper ordering \u03c0 of the types, \u039b is an upper-triangular\nmatrix. Formally, there exists \u03c0 such that \u2200i > j, \u039b\u03c0(i),\u03c0(j) = 0. Any \u03c0 that makes \u039b upper-\ntriangular is a valid thinking hierarchy of the types.\n\n5\n\n\fIn the running example, the valid thinking hierarchy is \u03c0(1) = type 1, \u03c0(2) = type 0. Note that the\nabove assumption does not require \u2200i \u2264 j, \u039b\u03c0(i),\u03c0(j) > 0. When \u039b is a diagonal matrix, types cannot\npredict each other and are equally sophisticated, thus any ordering is a valid thinking hierarchy.\n\nAn algorithm finds the thinking hierarchy when the algorithm is given M which is generated by\nlatent (unknown) W, \u039b, and the algorithm will output a matrix W\u2217 which equals a row-permuted\nW where the row order is a valid thinking hierarchy. Formally, there exists a valid thinking hierarchy\n\u03c0 such that the ith row of W\u2217 is the \u03c0(i)th row of W, i.e, w\u2217\n\ni = w\u03c0(i).\n\n2.2 Non-negative Congruence Triangularization (NCT)\n\nWith the above model, inferring thinking hierarchy leads to a novel matrix decomposition problem,\nwhich is similar to the symmetric non-negative matrix factorization problem (NMF)6. A non-negative\nmatrix is a matrix whose elements are non-negative.\nDefinition 2.6 (Non-negative Congruence7 Triangularization (NCT)). Given a non-negative matrix\nM, NCT aims to find non-negative matrices W and non-negative upper-triangular matrix \u039b such\nthat M = W\u22a4\u039bW. In a Frobenius norm based approximated version, given a set of matrices W,\nNCT aims to find non-negative matrices W and non-negative upper-triangular matrix \u039b to minimize\n\nmin\nW\u2208W,\u039b\n\n||M \u2212 W\u22a4\u039bW||2\nF\n\nand the minimum is defined as the lack-of-fit of M regarding W 8.\n\nLike NMF, it\u2019s impossible to ask for the strict uniqueness of the results. Let P\u039b be the set of\npermutation matrices such that \u03a0\u22a4\u039b\u03a0 is still upper-triangular.\nIf (W, \u039b) is a solution, then\n(\u03a0\u22121DW, \u03a0\u22a4D\u22121\u039bD\u22121\u03a0) is also a solution where D is a diagonal matrix with positive elements\nand \u03a0 \u2208 P\u039b. We state the uniqueness results as follows and the proof is deferred to Appendix C.\nProposition 2.7 (Uniqueness). If |T | \u2264 |A| and T columns of W consist of a permuted positive\ndiagonal matrix, NCT for M = W\u22a4\u039bW is unique in the sense that then for all W\u2032\u22a4\u039b\u2032W\u2032 =\nW\u22a4\u039bW, there exists a positive diagonal matrix D and a |T | \u00d7 |T | permutation matrix \u03a0 \u2208 P\u039b\nsuch that W\u2032 = \u03a0\u22121DW.\n\nWhen we restrict W to be \u201csemi-orthogonal\u201d, we obtain a clean format of NCT without searching\nfor optimal \u039b. I is the set of all \u201csemi-orthogonal\u201d matrices W where each column of W has\nand only has one non-zero element and WW\u22a4 = I. For example, the W in Example 2.2 can be\nnormalized to a semi-orthogonal matrix. The following lemma follows from the expansion of the\nFrobenius norm and we defer the proof to Appendix C.\n\nLemma 2.8 (Semi-orthogonal: minimizing F-norm = maximizing upper-triangular\u2019s sum of the\nsquare). For all set of matrices W \u2282 I, minW\u2208W,\u039b ||M \u2212 W\u22a4\u039bW||2\nF is equivalent to solv-\ni,j and setting \u039b as Up(WMW\u22a4), the upper-triangular area of\ning maxW\u2208W\nWMW\u22a4.\n\ni\u2264j(WMW\u22a4)2\n\n(cid:80)\n\n2.3\n\nInferring the thinking hierarchy with answer-prediction joint distribution M\n\nGiven M, inferring the thinking hierarchy is equivalent to solving NCT in general. Though we do not\nhave M, later we will show a proxy for M. For simplicity of practical use, we introduce two simple\nranking algorithms by employing Lemma 2.8. The ranking algorithm takes M as input and outputs a\nlinear ordering of answers \u03c0 : {1, 2, \u00b7 \u00b7 \u00b7 , |A|} (cid:55)\u2192 A that maps a ranking position to an answer.\n\nAnswer-Ranking Algorithm (Default) AR(M) The algorithm computes\n\n\u03a0\u2217 \u2190 arg max\n\u03a0\u2208P\n\n(cid:88)\n\n(\u03a0M\u03a0\u22a4)2\ni,j\n\ni\u2264j\n\n6Symmetric NMF: minW ||M \u2212 W\u22a4W||2\nF\n7We use congruence here though it is not matrix congruence since W may not be a square matrix.\n8M = W\u22a4\u039bW, W \u2208 W has zero lack-of-fit.\n\n6\n\n\fwhere P is the set of all |A| \u00d7 |A| permutation matrices. There is a one to one mapping between each\npermutation matrix \u03a0 and a linear ordering \u03c0: \u03a0i,\u03c0(i) = 1, \u2200i. Therefore, the optimal \u03a0\u2217 leads to an\noptimal rank over answers directly and the default algorithm can be also represented as\n\n\u03c0\u2217 \u2190 arg max\n\n\u03c0\n\nM 2\n\n\u03c0(i),\u03c0(j).\n\n(cid:88)\n\ni\u2264j\n\nTo find the optimal rank, we use a dynamic programming based algorithm (see the supplementary\nmaterials) which takes O(2|A||A|2). In practice, |A| is usually at most 7 or 8. In our empirical study,\nthe default algorithm takes 91 milliseconds to finish the computation of all 152 questions.\n\nThe default algorithm implicitly assumes |T | = |A| and all oracles are deterministic. To allow\n|T | < |A| and non-deterministic oracles, we introduce a variant that generalizes P to a subset of\nsemi-orthogonal matrices I. Every |T | \u00d7 |A| semi-orthogonal W indicates a hard clustering. Each\ncluster t \u2208 T contains all answers a such that Wt,a > 0. For example, the W in Example 2.2 can\nbe normalized to a semi-orthogonal matrix and indicates a hard clustering {4}, {6, 3}. Therefore,\nthe variant algorithm will partition the answers into multiple clusters and assign a hierarchy to the\nclusters.\n\nAnswer-Ranking Algorithm (Variant) AR+(M, W) The algorithm computes\n\nW\u2217 \u2190 arg max\nW\u2208I\n\n(cid:88)\n\n(WMW\u22a4)2\n\ni,j.\n\ni\u2264j\n\nwhere W \u2282 I. W\u2217 is normalized such that every row sums to 1. This algorithm does not restrict\n|T | = |A| and learns the optimal |T |.\n\nW\u2217 \u21d2 Answer rank The output W\u2217 indicates a hard clustering of all answers. We rank all answer\nas follows: for any i < j, the answers in cluster i has a higher rank 9 than the answers in cluster j.\nFor all i, for any two answers a, a\u2032 in the same cluster i, a is ranked higher than a\u2032 if W \u2217\ni,a\u2032.\n\ni,a > W \u2217\n\nTheoretical justification When M perfectly fits the model with the restriction that the latent W is\na permutation or a hard clustering, we show that our algorithm finds the thinking hierarchy. Otherwise,\nour algorithm finds the \u201cclosest\u201d solution measured by the Frobenius norm.\nTheorem 2.9. When there exists \u03a00 \u2208 P and non-negative upper-triangular matrix \u039b0 such that\n0 \u039b0\u03a00, AR(M) finds the thinking hierarchy 10. In general, AR(M) will output \u03a0\u2217 where\nM = \u03a0\u22a4\n\u03a0\u2217, \u039b\u2217 = Up(\u03a0\u2217M\u03a0\u2217\u22a4) is a solution to arg min\u03a0\u2208P,\u039b ||M \u2212 \u03a0\u22a4\u039b\u03a0||2\nF . The above statement\nis still valid by replacing P by W \u2282 I and AR(M) by AR+(M, W).\n\nProposition 2.7 and Lemma 2.8 almost directly imply the above theorem. We defer the formal proof\nto Appendix C.\n\n2.4 A proxy for answer-prediction joint distribution M\n\nIn practice, we do not have perfect M. We use the following open-response paradigm to obtain a\nproxy for M.\n\nAnswer-prediction paradigm the respondents are asked Q1: What\u2019s your answer? Answer:\nQ2: What do you think other people will answer:\n\n.\n\nIn the circle example, the possible feedback can be \u201canswer: 4; prediction: 3\u201d, \u201canswer: 3; prediction:\n6,9,1\u201d... We collect all answers provided by the respondents 11 and denote the set of them by A. In\nthe circle example, A = {1, 2, 3, 4, 6, 9}. We also allow respondents to provide no prediction or\nmultiple predictions.\n\n9Rank 1 answer is in the highest position.\n10See definition in the last paragraph in Section 2.1.\n11In practice, we set a threshold \u03b8 and collect answers which are provided by at least \u03b8 fraction of the\nrespondents. We allow multiple predictions and also allow people to answer \u201cI do not know\u201d. See Section 3 for\nmore details.\n\n7\n\n\f(a) Accuracy of algorithms\n\n(b) Empirical distribution of lack-of-fit\n\nFigure 3: The results of our experiment\n\nAnswer-prediction matrix We aggregate the feedback and visualize it by an Answer-Prediction\nmatrix. The Answer-Prediction matrix A is a |A| \u00d7 |A| square matrix where |A| is the number of\ndistinct answers provided by the respondents. Each entry Aa,g, a, g \u2208 A is the number of respondents\nthat answer \u201ca\u201d and predict \u201cg\u201d.\n\nWe will show that with proper assumptions, the answer-prediction matrix A\u2019s expectation is pro-\nportional to M. First, for ease of analysis, we assume that each respondent\u2019s predictions are i.i.d.\nsamples12. Second, since we allow people to optionally provide predictions, we need to additionally\nassume that the number of predictions each respondent is willing to provide is independent of her\ntype and answer. We state the formal result as follows and the proof is deferred to Appendix C.\n\nProposition 2.10. When each respondent\u2019s predictions are i.i.d. samples, and the number of\npredictions she gives is independent of her type and answer, the answer-prediction matrix A\u2019s\nexpectation is proportional to M.\n\n\nBased on this section, do the authors state the assumptions for their theoretical results?"}]], "q2b": [[{"role": "user", "content": "The following is the abstract of the paper you are reviewing:\n\n\nWhen we use the wisdom of the crowds, we usually rank the answers according to\ntheir popularity, especially when we cannot verify the answers. However, this can\nbe very dangerous when the majority make systematic mistakes. A fundamental\nquestion arises: can we build a hierarchy among the answers without any prior\nwhere the higher-ranking answers, which may not be supported by the majority,\nare from more sophisticated people? To address the question, we propose 1) a\nnovel model to describe people\u2019s thinking hierarchy; 2) two algorithms to learn\nthe thinking hierarchy without any prior; 3) a novel open-response based crowd-\nsourcing approach based on the above theoretic framework. In addition to theoretic\njustifications, we conduct four empirical crowdsourcing studies and show that a)\nthe accuracy of the top-ranking answers learned by our approach is much higher\nthan that of plurality voting (In one question, the plurality answer is supported by\n74 respondents but the correct answer is only supported by 3 respondents. Our\napproach ranks the correct answer the highest without any prior); b) our model\nhas a high goodness-of-fit, especially for the questions where our top-ranking\nanswer is correct. To the best of our knowledge, we are the first to propose a\nthinking hierarchy model with empirical validations in the general problem-solving\nscenarios; and the first to propose a practical open-response based crowdsourcing\napproach that beats plurality voting without any prior.\n\n1\n\n\nBased on this abstract, do the authors have theoretical results? Please start your answer with \"Yes\", \"No\", or \"Unsure\"."}, {"role": "user", "content": "The following is the methods section of the paper you are reviewing:\n\n\nWe first introduce our model for thinking hierarchy. Tversky and Kahneman [27] propose that people\nhave two systems, a fast and intuitive system, and a slow and logical system. For example, Alice\nstarts to solve the circle problem. When she reads the question, she can run her intuitive system 1 and\nobtain answer \u201c3\u201d. However, when she starts to think carefully, she runs her more careful system 2 to\nobtain answer \u201c4\u201d.\n\nWe propose a more general model where people can run multiple oracles to approach the question\nduring their thinking process. Conceptually similar to the cognitive hierarchy theory [3], we assume\nthe oracles have different levels. People usually run lower level oracles before the higher level oracles.\nIn the previous example, system 1 is the lower level oracle and system 2 is the higher one. We assume\nthat Alice runs system 2 after system 1. Thus, in our model, people who know the answer is \u201c4\u201d can\n\n4\n\n\fpredict that other people answer \u201c3\u201d, which is hypothesized in Kong and Schoenebeck [10]. It\u2019s also\npossible that some people run the most sophisticated oracle directly without running the lower ones.\nThus, we design the model such that it does not require the higher-type to be able to predict ALL\nlower types. We also allow the oracles\u2019 outputs to be random.\n\nSection 2.1 introduces the model of thinking hierarchy. Section 2.2 reduces the model inference\nproblem to a matrix decomposition problem. Section 2.3 and section 2.4 show how to learn the\nthinking hierarchy.\n\n2.1 Thinking hierarchy\n\nFixing a question q (e.g. the circle problem), T denotes the set of thinking types. A denotes the set of\npossible answers. Both T and A are finite sets. \u2206A denotes all possible distributions over A. We\nsometimes use \u201cprob\u201d as a shorthand for probability.\n\nGenerating answers We will describe how people of different thinking types generate answers.\nDefinition 2.1 (Oracles of thinking types W ). An answer generating oracle maps the question to\nan (random) answer in A. Each type t corresponds to an oracle Ot. The output Ot(q) is a random\nvariable whose distribution is wt \u2208 \u2206A. W denotes a |T | \u00d7 |A| matrix where each row t is wt.\nEach respondent is type t with prob pt and (cid:80)\nrunning the oracle Ot. For all a \u2208 A, the probability that a respondent answers a will be (cid:80)\nWe assume the probability is positive for all a \u2208 A.\nExample 2.2 (A running example). There are two types T = {0, 1}. The answer space is A =\n{3, 4, 6}. O0 will output \u20183\u2019 with probability 0.8 and \u20186\u2019 with probability 0.2. O1 will output \u20184\u2019\n0\n1\n\nt pt = 1. A type t respondent generates the answer by\nt ptwt(a).\n\ndeterministically. In this example, W =\n\nwhere the first row is the distribution of\n\n(cid:20)0.8\n0\n\n0.2\n0\n\n(cid:21)\n\nO0\u2019s output and the second row is the distribution of O1\u2019s output.\n\nGenerating predictions We then describe how people of different thinking types predict what other\npeople will answer. Here the prediction is not a distribution, but an answer other people may report.\nWhen a type t respondent makes a prediction, she will run an oracle, which is Ot\u2032 with probability\npt\u2192t\u2032 where (cid:80)\n\nt\u2032 pt\u2192t\u2032 = 1. She uses the output of Ot\u2032 as the prediction g \u2208 A.\n\nCombination: answer-prediction joint distribution M M denotes a |A| \u00d7 |A| matrix where\nMa,g is the probability an respondent answers a and predicts g. \u039b denotes a |T | \u00d7 |T | matrix where\n\u039bt,t\u2032 = ptpt\u2192t\u2032 is the probability a respondent is type t and predicts type t\u2032.\nExample 2.3. In this example, when type 0 respondent makes a prediction, with prob 1, she will run\nO0 again. When type 1 respondent makes a prediction, with prob 0.5, she will run O1 again, with\nprob 0.5, she will run O0. Moreover, a respondent is type 0 with prob 0.7, and type 1 with prob 0.3.\n\nHere \u039b =\n\n(cid:20)p0p0\u21920\np1p1\u21920\n\np0p0\u21921\np1p1\u21921\n\n(cid:21)\n\n=\n\n(cid:20) 0.7 \u2217 1\n0.3 \u2217 0.5\n\n0.7 \u2217 0\n0.3 \u2217 0.5\n\n(cid:21)\n\n=\n\n(cid:20) 0.7\n0.15\n\n(cid:21)\n\n.\n\n0\n0.15\n\nClaim 2.4. Based on the above generating processes, M = W\u22a4\u039bW.\n\nProof. For each respondent, the probability she answers a and predicts g will be\n(cid:88)\n\n(cid:88)\n\n(cid:88)\n\nptwt(a)\n\npt\u2192t\u2032wt\u2032(g) =\n\nwt(a)ptpt\u2192t\u2032wt\u2032(g).\n\nMa,g =\n\nt\n\nt\u2032\n\nt,t\u2032\n\nWe sum over all possible types the respondent will be. Given she is type t, she runs oracle Ot to\ngenerate the answer and wt(a) is the probability that the answer is a. We sum over all possible\noracles she runs to predict. Given that she runs Ot\u2032, wt\u2032(g) is the probability the prediction is g.\n\nKey assumption: upper-triangular \u039b we assume that people of a less sophisticated level can\nnever run the oracles of more sophisticated levels. A linear ordering of types \u03c0 : {1, 2, \u00b7 \u00b7 \u00b7 , |T |} (cid:55)\u2192 T\nmaps a ranking position to a type. For example, \u03c0(1) \u2208 T is the top-ranking type.\nAssumption 2.5. We assume that with a proper ordering \u03c0 of the types, \u039b is an upper-triangular\nmatrix. Formally, there exists \u03c0 such that \u2200i > j, \u039b\u03c0(i),\u03c0(j) = 0. Any \u03c0 that makes \u039b upper-\ntriangular is a valid thinking hierarchy of the types.\n\n5\n\n\fIn the running example, the valid thinking hierarchy is \u03c0(1) = type 1, \u03c0(2) = type 0. Note that the\nabove assumption does not require \u2200i \u2264 j, \u039b\u03c0(i),\u03c0(j) > 0. When \u039b is a diagonal matrix, types cannot\npredict each other and are equally sophisticated, thus any ordering is a valid thinking hierarchy.\n\nAn algorithm finds the thinking hierarchy when the algorithm is given M which is generated by\nlatent (unknown) W, \u039b, and the algorithm will output a matrix W\u2217 which equals a row-permuted\nW where the row order is a valid thinking hierarchy. Formally, there exists a valid thinking hierarchy\n\u03c0 such that the ith row of W\u2217 is the \u03c0(i)th row of W, i.e, w\u2217\n\ni = w\u03c0(i).\n\n2.2 Non-negative Congruence Triangularization (NCT)\n\nWith the above model, inferring thinking hierarchy leads to a novel matrix decomposition problem,\nwhich is similar to the symmetric non-negative matrix factorization problem (NMF)6. A non-negative\nmatrix is a matrix whose elements are non-negative.\nDefinition 2.6 (Non-negative Congruence7 Triangularization (NCT)). Given a non-negative matrix\nM, NCT aims to find non-negative matrices W and non-negative upper-triangular matrix \u039b such\nthat M = W\u22a4\u039bW. In a Frobenius norm based approximated version, given a set of matrices W,\nNCT aims to find non-negative matrices W and non-negative upper-triangular matrix \u039b to minimize\n\nmin\nW\u2208W,\u039b\n\n||M \u2212 W\u22a4\u039bW||2\nF\n\nand the minimum is defined as the lack-of-fit of M regarding W 8.\n\nLike NMF, it\u2019s impossible to ask for the strict uniqueness of the results. Let P\u039b be the set of\npermutation matrices such that \u03a0\u22a4\u039b\u03a0 is still upper-triangular.\nIf (W, \u039b) is a solution, then\n(\u03a0\u22121DW, \u03a0\u22a4D\u22121\u039bD\u22121\u03a0) is also a solution where D is a diagonal matrix with positive elements\nand \u03a0 \u2208 P\u039b. We state the uniqueness results as follows and the proof is deferred to Appendix C.\nProposition 2.7 (Uniqueness). If |T | \u2264 |A| and T columns of W consist of a permuted positive\ndiagonal matrix, NCT for M = W\u22a4\u039bW is unique in the sense that then for all W\u2032\u22a4\u039b\u2032W\u2032 =\nW\u22a4\u039bW, there exists a positive diagonal matrix D and a |T | \u00d7 |T | permutation matrix \u03a0 \u2208 P\u039b\nsuch that W\u2032 = \u03a0\u22121DW.\n\nWhen we restrict W to be \u201csemi-orthogonal\u201d, we obtain a clean format of NCT without searching\nfor optimal \u039b. I is the set of all \u201csemi-orthogonal\u201d matrices W where each column of W has\nand only has one non-zero element and WW\u22a4 = I. For example, the W in Example 2.2 can be\nnormalized to a semi-orthogonal matrix. The following lemma follows from the expansion of the\nFrobenius norm and we defer the proof to Appendix C.\n\nLemma 2.8 (Semi-orthogonal: minimizing F-norm = maximizing upper-triangular\u2019s sum of the\nsquare). For all set of matrices W \u2282 I, minW\u2208W,\u039b ||M \u2212 W\u22a4\u039bW||2\nF is equivalent to solv-\ni,j and setting \u039b as Up(WMW\u22a4), the upper-triangular area of\ning maxW\u2208W\nWMW\u22a4.\n\ni\u2264j(WMW\u22a4)2\n\n(cid:80)\n\n2.3\n\nInferring the thinking hierarchy with answer-prediction joint distribution M\n\nGiven M, inferring the thinking hierarchy is equivalent to solving NCT in general. Though we do not\nhave M, later we will show a proxy for M. For simplicity of practical use, we introduce two simple\nranking algorithms by employing Lemma 2.8. The ranking algorithm takes M as input and outputs a\nlinear ordering of answers \u03c0 : {1, 2, \u00b7 \u00b7 \u00b7 , |A|} (cid:55)\u2192 A that maps a ranking position to an answer.\n\nAnswer-Ranking Algorithm (Default) AR(M) The algorithm computes\n\n\u03a0\u2217 \u2190 arg max\n\u03a0\u2208P\n\n(cid:88)\n\n(\u03a0M\u03a0\u22a4)2\ni,j\n\ni\u2264j\n\n6Symmetric NMF: minW ||M \u2212 W\u22a4W||2\nF\n7We use congruence here though it is not matrix congruence since W may not be a square matrix.\n8M = W\u22a4\u039bW, W \u2208 W has zero lack-of-fit.\n\n6\n\n\fwhere P is the set of all |A| \u00d7 |A| permutation matrices. There is a one to one mapping between each\npermutation matrix \u03a0 and a linear ordering \u03c0: \u03a0i,\u03c0(i) = 1, \u2200i. Therefore, the optimal \u03a0\u2217 leads to an\noptimal rank over answers directly and the default algorithm can be also represented as\n\n\u03c0\u2217 \u2190 arg max\n\n\u03c0\n\nM 2\n\n\u03c0(i),\u03c0(j).\n\n(cid:88)\n\ni\u2264j\n\nTo find the optimal rank, we use a dynamic programming based algorithm (see the supplementary\nmaterials) which takes O(2|A||A|2). In practice, |A| is usually at most 7 or 8. In our empirical study,\nthe default algorithm takes 91 milliseconds to finish the computation of all 152 questions.\n\nThe default algorithm implicitly assumes |T | = |A| and all oracles are deterministic. To allow\n|T | < |A| and non-deterministic oracles, we introduce a variant that generalizes P to a subset of\nsemi-orthogonal matrices I. Every |T | \u00d7 |A| semi-orthogonal W indicates a hard clustering. Each\ncluster t \u2208 T contains all answers a such that Wt,a > 0. For example, the W in Example 2.2 can\nbe normalized to a semi-orthogonal matrix and indicates a hard clustering {4}, {6, 3}. Therefore,\nthe variant algorithm will partition the answers into multiple clusters and assign a hierarchy to the\nclusters.\n\nAnswer-Ranking Algorithm (Variant) AR+(M, W) The algorithm computes\n\nW\u2217 \u2190 arg max\nW\u2208I\n\n(cid:88)\n\n(WMW\u22a4)2\n\ni,j.\n\ni\u2264j\n\nwhere W \u2282 I. W\u2217 is normalized such that every row sums to 1. This algorithm does not restrict\n|T | = |A| and learns the optimal |T |.\n\nW\u2217 \u21d2 Answer rank The output W\u2217 indicates a hard clustering of all answers. We rank all answer\nas follows: for any i < j, the answers in cluster i has a higher rank 9 than the answers in cluster j.\nFor all i, for any two answers a, a\u2032 in the same cluster i, a is ranked higher than a\u2032 if W \u2217\ni,a\u2032.\n\ni,a > W \u2217\n\nTheoretical justification When M perfectly fits the model with the restriction that the latent W is\na permutation or a hard clustering, we show that our algorithm finds the thinking hierarchy. Otherwise,\nour algorithm finds the \u201cclosest\u201d solution measured by the Frobenius norm.\nTheorem 2.9. When there exists \u03a00 \u2208 P and non-negative upper-triangular matrix \u039b0 such that\n0 \u039b0\u03a00, AR(M) finds the thinking hierarchy 10. In general, AR(M) will output \u03a0\u2217 where\nM = \u03a0\u22a4\n\u03a0\u2217, \u039b\u2217 = Up(\u03a0\u2217M\u03a0\u2217\u22a4) is a solution to arg min\u03a0\u2208P,\u039b ||M \u2212 \u03a0\u22a4\u039b\u03a0||2\nF . The above statement\nis still valid by replacing P by W \u2282 I and AR(M) by AR+(M, W).\n\nProposition 2.7 and Lemma 2.8 almost directly imply the above theorem. We defer the formal proof\nto Appendix C.\n\n2.4 A proxy for answer-prediction joint distribution M\n\nIn practice, we do not have perfect M. We use the following open-response paradigm to obtain a\nproxy for M.\n\nAnswer-prediction paradigm the respondents are asked Q1: What\u2019s your answer? Answer:\nQ2: What do you think other people will answer:\n\n.\n\nIn the circle example, the possible feedback can be \u201canswer: 4; prediction: 3\u201d, \u201canswer: 3; prediction:\n6,9,1\u201d... We collect all answers provided by the respondents 11 and denote the set of them by A. In\nthe circle example, A = {1, 2, 3, 4, 6, 9}. We also allow respondents to provide no prediction or\nmultiple predictions.\n\n9Rank 1 answer is in the highest position.\n10See definition in the last paragraph in Section 2.1.\n11In practice, we set a threshold \u03b8 and collect answers which are provided by at least \u03b8 fraction of the\nrespondents. We allow multiple predictions and also allow people to answer \u201cI do not know\u201d. See Section 3 for\nmore details.\n\n7\n\n\f(a) Accuracy of algorithms\n\n(b) Empirical distribution of lack-of-fit\n\nFigure 3: The results of our experiment\n\nAnswer-prediction matrix We aggregate the feedback and visualize it by an Answer-Prediction\nmatrix. The Answer-Prediction matrix A is a |A| \u00d7 |A| square matrix where |A| is the number of\ndistinct answers provided by the respondents. Each entry Aa,g, a, g \u2208 A is the number of respondents\nthat answer \u201ca\u201d and predict \u201cg\u201d.\n\nWe will show that with proper assumptions, the answer-prediction matrix A\u2019s expectation is pro-\nportional to M. First, for ease of analysis, we assume that each respondent\u2019s predictions are i.i.d.\nsamples12. Second, since we allow people to optionally provide predictions, we need to additionally\nassume that the number of predictions each respondent is willing to provide is independent of her\ntype and answer. We state the formal result as follows and the proof is deferred to Appendix C.\n\nProposition 2.10. When each respondent\u2019s predictions are i.i.d. samples, and the number of\npredictions she gives is independent of her type and answer, the answer-prediction matrix A\u2019s\nexpectation is proportional to M.\n\n\nBased on this section, do the authors include complete proofs for all their theoretical results?"}]], "q3a": [[{"role": "user", "content": "The following is the experiments section of the paper you are reviewing:\n\n\nWe conduct four studies, study 1 (35 math problems), study 2 (30 Go problems), study 3 (44 general\nknowledge questions), and study 4 (43 Chinese character pronunciation questions).\n\nData collection All studies are performed by online questionnaires. We recruit the respondents by\nan online announcement13 or from an online platform that is similar to Amazon Mechanical Turk. We\nget respondents\u2019 consent for using and sharing their data for research. Respondents are asked not to\nsearch for the answers to the questions or communicate with other people. We allow the respondents\nto answer \u201cI do not know\u201d for all questions. Except for Go problems, all questionnaires use flat\npayment. We illustrate the data collection process in detail in Appendix A. We allow respondents\nto participate in more than one study because our algorithms analyze each question separately and\nindependently.\n\nData processing We merge answers which are the same, like \u20180.5\u2019 and \u201850%\u2019. We omit the answers\nthat are reported by less than (\u2264) 3% of respondents or one person. The remaining answers, excluding\n\u201cI do not know\u201d, form the answer set A whose size is |A|. We then construct the Answer-Prediction\nmatrix and perform our algorithms. Pseudo-codes are attached in Appendix B. Our algorithms do not\nrequire any prior or the respondents\u2019 expertise levels.\n\n3.1 Results\n\n12This may not be a very good assumption since i.i.d. samples can repeat but respondents usually do not\nrepeat their predictions. If we do not want this assumption, we can choose to only use the first prediction from\neach respondent (if there exists) to construct the answer-prediction matrix.\n\n13Many are students from top universities in China. See Appendix A for more details.\n\n8\n\n\fType\nMath\nGo\nGeneral knowledge\nChinese character\n\nTotal Our algorithm(Default) Our algorithm(Variant)\n\n35\n30\n44\n43\n\n29\n28\n41\n36\n\n29\n28\n41\n35\n\nPlurality voting\n24\n23\n35\n34\n\nTable 1: The number of questions our algorithms/baseline are correct.\n\n(a) the Monty Hall problem: you can select one\nclosed door of three. A prize, a car, is behind\none of the doors. The other two doors hide goats.\nAfter you have made your choice, Monty Hall\nwill open one of the remaining doors and show\nthat it does not contain the prize. He then asks\nyou if you would like to switch your choice to\nthe other unopened door. What is the probability\nto get the prize if you switch?\n\n(b) the Taxicab problem: 85% of taxis in this\ncity are green, the others are blue. A witness\nsees a blue taxi. She is usually correct with\nprobability 80%. What is the probability that\nthe taxi saw by the witness is blue?\n\n(c) Pick a move for black such that they can be\nalive.\n\n(d) Pick a move for black such that they can be\nalive by ko.\n\n(e) the boundary question: what river forms the\nboundary between North Korea and China?\n\n(f) the Middle Age New Year question: when\nwas the new year in middle age?\n\n(g) the pronunciation of \u7762\n\n(h) the pronunciation of \u6ec2\n\nFigure 4: The ranked answer-prediction matrices\n\n9\n\n2/31/21/32/31/21/3Answer34261413517146Prediction415080121520415080121520Answer160500008201200801619501104818000522130010218PredictionD3C1B2C2B1D3C1B2C2B1Answer62131091410042012115300002PredictionB8A8B6B7B8A8B6B7Answer112540630001230035PredictionYaluandDoomanYaluSonghuaYaluandDoomanYaluSonghuaAnswer33007410005PredictionApr1Dec25Jan1Apr1Dec25Jan1Answer15410028180633Predictionsui1ju1zhi4zhui1sui1ju1zhi4zhui1Answer115023161300210107Predictionpang1pang2pang1pang2Answer1511216Prediction\fWe compare our approach to the baseline, the plurality voting, regarding the accuracy of the top-\nranking answers. Both of the algorithms beat plurality voting for all studies and the default is slightly\nbetter. Among all 152 questions, in 138 questions, the variant algorithm outputs the same hierarchy as\nthe default algorithm. For other questions, the top-ranking type of the variant algorithm may contain\nmore than one answer. The top-ranking answer is the answer supported by more people among all\nanswers in the top-ranking type. In one question the variant is wrong but the default is correct, the\nvariant algorithm assigns both the correct answer and the incorrect plurality answer to the top-ranking\ntype, thus outputting the incorrect answer as the top-ranking answer.\n\nWe also compute the lack-of-fit index (Definition 2.6) of the algorithms and find that the questions\nthe algorithm outputs the correct answer have a smaller lack-of-fit thus fitting the thinking hierarchy\nmodel better. Therefore, we can use the lack-of-fit index as a reliability index of the algorithms.\n\nWe additionally pick several representative examples for each study (Figure 4) where the matrices\nare ranked by the default algorithm and the diagonal area modified like Example 1.1. In all of these\nexamples, the plurality is incorrect while our approach is correct. Results of other questions are\nillustrated at https://elicitation.info/classroom/1/. Detailed explanations are illustrated\nin Appendix D and here we provide some highlights. First, our approach elicits a rich hierarchy. For\nexample, the taxicab problem is borrowed from Kahneman [8] and previous studies show that people\nusually ignore the base rate and report \u201880%\u2019. The imagined levels can be 41%\u219280%. We elicit a\nmuch richer level \u201c41%\u219250%\u219280%\u219212%\u219215%\u219220%\u201d. Second, the most sophisticated level\nmay fail to predict the least one. In the Taxicab problem, the correct \u201c41%\u201d supporters successfully\npredict the common wrong answer \u201c80%\u201d. However, they fail to predict the surprisingly wrong\nanswers \u201c12%,20%\u201d, which are in contrast successfully predicted by \u201c80%\u201d supporters. Our model\nis sufficiently general to allow this situation. Third, even for problems (like Go) without famous\nmistakes, our approach still works. Moreover, in the boundary question, we identify the correct\nanswer without any prior when only 3 respondents are correct.\n\n\nBased on this section, do the authors include code, data, and instructions needed to reproduce the main experimental results, or provide them in a URL?"}]], "q3b": [[{"role": "user", "content": "The following is the experiments section of the paper you are reviewing:\n\n\nWe conduct four studies, study 1 (35 math problems), study 2 (30 Go problems), study 3 (44 general\nknowledge questions), and study 4 (43 Chinese character pronunciation questions).\n\nData collection All studies are performed by online questionnaires. We recruit the respondents by\nan online announcement13 or from an online platform that is similar to Amazon Mechanical Turk. We\nget respondents\u2019 consent for using and sharing their data for research. Respondents are asked not to\nsearch for the answers to the questions or communicate with other people. We allow the respondents\nto answer \u201cI do not know\u201d for all questions. Except for Go problems, all questionnaires use flat\npayment. We illustrate the data collection process in detail in Appendix A. We allow respondents\nto participate in more than one study because our algorithms analyze each question separately and\nindependently.\n\nData processing We merge answers which are the same, like \u20180.5\u2019 and \u201850%\u2019. We omit the answers\nthat are reported by less than (\u2264) 3% of respondents or one person. The remaining answers, excluding\n\u201cI do not know\u201d, form the answer set A whose size is |A|. We then construct the Answer-Prediction\nmatrix and perform our algorithms. Pseudo-codes are attached in Appendix B. Our algorithms do not\nrequire any prior or the respondents\u2019 expertise levels.\n\n3.1 Results\n\n12This may not be a very good assumption since i.i.d. samples can repeat but respondents usually do not\nrepeat their predictions. If we do not want this assumption, we can choose to only use the first prediction from\neach respondent (if there exists) to construct the answer-prediction matrix.\n\n13Many are students from top universities in China. See Appendix A for more details.\n\n8\n\n\fType\nMath\nGo\nGeneral knowledge\nChinese character\n\nTotal Our algorithm(Default) Our algorithm(Variant)\n\n35\n30\n44\n43\n\n29\n28\n41\n36\n\n29\n28\n41\n35\n\nPlurality voting\n24\n23\n35\n34\n\nTable 1: The number of questions our algorithms/baseline are correct.\n\n(a) the Monty Hall problem: you can select one\nclosed door of three. A prize, a car, is behind\none of the doors. The other two doors hide goats.\nAfter you have made your choice, Monty Hall\nwill open one of the remaining doors and show\nthat it does not contain the prize. He then asks\nyou if you would like to switch your choice to\nthe other unopened door. What is the probability\nto get the prize if you switch?\n\n(b) the Taxicab problem: 85% of taxis in this\ncity are green, the others are blue. A witness\nsees a blue taxi. She is usually correct with\nprobability 80%. What is the probability that\nthe taxi saw by the witness is blue?\n\n(c) Pick a move for black such that they can be\nalive.\n\n(d) Pick a move for black such that they can be\nalive by ko.\n\n(e) the boundary question: what river forms the\nboundary between North Korea and China?\n\n(f) the Middle Age New Year question: when\nwas the new year in middle age?\n\n(g) the pronunciation of \u7762\n\n(h) the pronunciation of \u6ec2\n\nFigure 4: The ranked answer-prediction matrices\n\n9\n\n2/31/21/32/31/21/3Answer34261413517146Prediction415080121520415080121520Answer160500008201200801619501104818000522130010218PredictionD3C1B2C2B1D3C1B2C2B1Answer62131091410042012115300002PredictionB8A8B6B7B8A8B6B7Answer112540630001230035PredictionYaluandDoomanYaluSonghuaYaluandDoomanYaluSonghuaAnswer33007410005PredictionApr1Dec25Jan1Apr1Dec25Jan1Answer15410028180633Predictionsui1ju1zhi4zhui1sui1ju1zhi4zhui1Answer115023161300210107Predictionpang1pang2pang1pang2Answer1511216Prediction\fWe compare our approach to the baseline, the plurality voting, regarding the accuracy of the top-\nranking answers. Both of the algorithms beat plurality voting for all studies and the default is slightly\nbetter. Among all 152 questions, in 138 questions, the variant algorithm outputs the same hierarchy as\nthe default algorithm. For other questions, the top-ranking type of the variant algorithm may contain\nmore than one answer. The top-ranking answer is the answer supported by more people among all\nanswers in the top-ranking type. In one question the variant is wrong but the default is correct, the\nvariant algorithm assigns both the correct answer and the incorrect plurality answer to the top-ranking\ntype, thus outputting the incorrect answer as the top-ranking answer.\n\nWe also compute the lack-of-fit index (Definition 2.6) of the algorithms and find that the questions\nthe algorithm outputs the correct answer have a smaller lack-of-fit thus fitting the thinking hierarchy\nmodel better. Therefore, we can use the lack-of-fit index as a reliability index of the algorithms.\n\nWe additionally pick several representative examples for each study (Figure 4) where the matrices\nare ranked by the default algorithm and the diagonal area modified like Example 1.1. In all of these\nexamples, the plurality is incorrect while our approach is correct. Results of other questions are\nillustrated at https://elicitation.info/classroom/1/. Detailed explanations are illustrated\nin Appendix D and here we provide some highlights. First, our approach elicits a rich hierarchy. For\nexample, the taxicab problem is borrowed from Kahneman [8] and previous studies show that people\nusually ignore the base rate and report \u201880%\u2019. The imagined levels can be 41%\u219280%. We elicit a\nmuch richer level \u201c41%\u219250%\u219280%\u219212%\u219215%\u219220%\u201d. Second, the most sophisticated level\nmay fail to predict the least one. In the Taxicab problem, the correct \u201c41%\u201d supporters successfully\npredict the common wrong answer \u201c80%\u201d. However, they fail to predict the surprisingly wrong\nanswers \u201c12%,20%\u201d, which are in contrast successfully predicted by \u201c80%\u201d supporters. Our model\nis sufficiently general to allow this situation. Third, even for problems (like Go) without famous\nmistakes, our approach still works. Moreover, in the boundary question, we identify the correct\nanswer without any prior when only 3 respondents are correct.\n\nBased on this section, do the authors specify all training details (e.g., data splits, hyperparameters, how they were chosen) for their results?"}]], "q3c": [[{"role": "user", "content": "The following is the experiments section of the paper you are reviewing:\n\n\nWe conduct four studies, study 1 (35 math problems), study 2 (30 Go problems), study 3 (44 general\nknowledge questions), and study 4 (43 Chinese character pronunciation questions).\n\nData collection All studies are performed by online questionnaires. We recruit the respondents by\nan online announcement13 or from an online platform that is similar to Amazon Mechanical Turk. We\nget respondents\u2019 consent for using and sharing their data for research. Respondents are asked not to\nsearch for the answers to the questions or communicate with other people. We allow the respondents\nto answer \u201cI do not know\u201d for all questions. Except for Go problems, all questionnaires use flat\npayment. We illustrate the data collection process in detail in Appendix A. We allow respondents\nto participate in more than one study because our algorithms analyze each question separately and\nindependently.\n\nData processing We merge answers which are the same, like \u20180.5\u2019 and \u201850%\u2019. We omit the answers\nthat are reported by less than (\u2264) 3% of respondents or one person. The remaining answers, excluding\n\u201cI do not know\u201d, form the answer set A whose size is |A|. We then construct the Answer-Prediction\nmatrix and perform our algorithms. Pseudo-codes are attached in Appendix B. Our algorithms do not\nrequire any prior or the respondents\u2019 expertise levels.\n\n3.1 Results\n\n12This may not be a very good assumption since i.i.d. samples can repeat but respondents usually do not\nrepeat their predictions. If we do not want this assumption, we can choose to only use the first prediction from\neach respondent (if there exists) to construct the answer-prediction matrix.\n\n13Many are students from top universities in China. See Appendix A for more details.\n\n8\n\n\fType\nMath\nGo\nGeneral knowledge\nChinese character\n\nTotal Our algorithm(Default) Our algorithm(Variant)\n\n35\n30\n44\n43\n\n29\n28\n41\n36\n\n29\n28\n41\n35\n\nPlurality voting\n24\n23\n35\n34\n\nTable 1: The number of questions our algorithms/baseline are correct.\n\n(a) the Monty Hall problem: you can select one\nclosed door of three. A prize, a car, is behind\none of the doors. The other two doors hide goats.\nAfter you have made your choice, Monty Hall\nwill open one of the remaining doors and show\nthat it does not contain the prize. He then asks\nyou if you would like to switch your choice to\nthe other unopened door. What is the probability\nto get the prize if you switch?\n\n(b) the Taxicab problem: 85% of taxis in this\ncity are green, the others are blue. A witness\nsees a blue taxi. She is usually correct with\nprobability 80%. What is the probability that\nthe taxi saw by the witness is blue?\n\n(c) Pick a move for black such that they can be\nalive.\n\n(d) Pick a move for black such that they can be\nalive by ko.\n\n(e) the boundary question: what river forms the\nboundary between North Korea and China?\n\n(f) the Middle Age New Year question: when\nwas the new year in middle age?\n\n(g) the pronunciation of \u7762\n\n(h) the pronunciation of \u6ec2\n\nFigure 4: The ranked answer-prediction matrices\n\n9\n\n2/31/21/32/31/21/3Answer34261413517146Prediction415080121520415080121520Answer160500008201200801619501104818000522130010218PredictionD3C1B2C2B1D3C1B2C2B1Answer62131091410042012115300002PredictionB8A8B6B7B8A8B6B7Answer112540630001230035PredictionYaluandDoomanYaluSonghuaYaluandDoomanYaluSonghuaAnswer33007410005PredictionApr1Dec25Jan1Apr1Dec25Jan1Answer15410028180633Predictionsui1ju1zhi4zhui1sui1ju1zhi4zhui1Answer115023161300210107Predictionpang1pang2pang1pang2Answer1511216Prediction\fWe compare our approach to the baseline, the plurality voting, regarding the accuracy of the top-\nranking answers. Both of the algorithms beat plurality voting for all studies and the default is slightly\nbetter. Among all 152 questions, in 138 questions, the variant algorithm outputs the same hierarchy as\nthe default algorithm. For other questions, the top-ranking type of the variant algorithm may contain\nmore than one answer. The top-ranking answer is the answer supported by more people among all\nanswers in the top-ranking type. In one question the variant is wrong but the default is correct, the\nvariant algorithm assigns both the correct answer and the incorrect plurality answer to the top-ranking\ntype, thus outputting the incorrect answer as the top-ranking answer.\n\nWe also compute the lack-of-fit index (Definition 2.6) of the algorithms and find that the questions\nthe algorithm outputs the correct answer have a smaller lack-of-fit thus fitting the thinking hierarchy\nmodel better. Therefore, we can use the lack-of-fit index as a reliability index of the algorithms.\n\nWe additionally pick several representative examples for each study (Figure 4) where the matrices\nare ranked by the default algorithm and the diagonal area modified like Example 1.1. In all of these\nexamples, the plurality is incorrect while our approach is correct. Results of other questions are\nillustrated at https://elicitation.info/classroom/1/. Detailed explanations are illustrated\nin Appendix D and here we provide some highlights. First, our approach elicits a rich hierarchy. For\nexample, the taxicab problem is borrowed from Kahneman [8] and previous studies show that people\nusually ignore the base rate and report \u201880%\u2019. The imagined levels can be 41%\u219280%. We elicit a\nmuch richer level \u201c41%\u219250%\u219280%\u219212%\u219215%\u219220%\u201d. Second, the most sophisticated level\nmay fail to predict the least one. In the Taxicab problem, the correct \u201c41%\u201d supporters successfully\npredict the common wrong answer \u201c80%\u201d. However, they fail to predict the surprisingly wrong\nanswers \u201c12%,20%\u201d, which are in contrast successfully predicted by \u201c80%\u201d supporters. Our model\nis sufficiently general to allow this situation. Third, even for problems (like Go) without famous\nmistakes, our approach still works. Moreover, in the boundary question, we identify the correct\nanswer without any prior when only 3 respondents are correct.\n\nBased on this section, do the authors report error bars (e.g., with respect to the random seed after running experiments multiple times)?"}]], "q3d": [[{"role": "user", "content": "The following is the experiments section of the paper you are reviewing:\n\n\nWe conduct four studies, study 1 (35 math problems), study 2 (30 Go problems), study 3 (44 general\nknowledge questions), and study 4 (43 Chinese character pronunciation questions).\n\nData collection All studies are performed by online questionnaires. We recruit the respondents by\nan online announcement13 or from an online platform that is similar to Amazon Mechanical Turk. We\nget respondents\u2019 consent for using and sharing their data for research. Respondents are asked not to\nsearch for the answers to the questions or communicate with other people. We allow the respondents\nto answer \u201cI do not know\u201d for all questions. Except for Go problems, all questionnaires use flat\npayment. We illustrate the data collection process in detail in Appendix A. We allow respondents\nto participate in more than one study because our algorithms analyze each question separately and\nindependently.\n\nData processing We merge answers which are the same, like \u20180.5\u2019 and \u201850%\u2019. We omit the answers\nthat are reported by less than (\u2264) 3% of respondents or one person. The remaining answers, excluding\n\u201cI do not know\u201d, form the answer set A whose size is |A|. We then construct the Answer-Prediction\nmatrix and perform our algorithms. Pseudo-codes are attached in Appendix B. Our algorithms do not\nrequire any prior or the respondents\u2019 expertise levels.\n\n3.1 Results\n\n12This may not be a very good assumption since i.i.d. samples can repeat but respondents usually do not\nrepeat their predictions. If we do not want this assumption, we can choose to only use the first prediction from\neach respondent (if there exists) to construct the answer-prediction matrix.\n\n13Many are students from top universities in China. See Appendix A for more details.\n\n8\n\n\fType\nMath\nGo\nGeneral knowledge\nChinese character\n\nTotal Our algorithm(Default) Our algorithm(Variant)\n\n35\n30\n44\n43\n\n29\n28\n41\n36\n\n29\n28\n41\n35\n\nPlurality voting\n24\n23\n35\n34\n\nTable 1: The number of questions our algorithms/baseline are correct.\n\n(a) the Monty Hall problem: you can select one\nclosed door of three. A prize, a car, is behind\none of the doors. The other two doors hide goats.\nAfter you have made your choice, Monty Hall\nwill open one of the remaining doors and show\nthat it does not contain the prize. He then asks\nyou if you would like to switch your choice to\nthe other unopened door. What is the probability\nto get the prize if you switch?\n\n(b) the Taxicab problem: 85% of taxis in this\ncity are green, the others are blue. A witness\nsees a blue taxi. She is usually correct with\nprobability 80%. What is the probability that\nthe taxi saw by the witness is blue?\n\n(c) Pick a move for black such that they can be\nalive.\n\n(d) Pick a move for black such that they can be\nalive by ko.\n\n(e) the boundary question: what river forms the\nboundary between North Korea and China?\n\n(f) the Middle Age New Year question: when\nwas the new year in middle age?\n\n(g) the pronunciation of \u7762\n\n(h) the pronunciation of \u6ec2\n\nFigure 4: The ranked answer-prediction matrices\n\n9\n\n2/31/21/32/31/21/3Answer34261413517146Prediction415080121520415080121520Answer160500008201200801619501104818000522130010218PredictionD3C1B2C2B1D3C1B2C2B1Answer62131091410042012115300002PredictionB8A8B6B7B8A8B6B7Answer112540630001230035PredictionYaluandDoomanYaluSonghuaYaluandDoomanYaluSonghuaAnswer33007410005PredictionApr1Dec25Jan1Apr1Dec25Jan1Answer15410028180633Predictionsui1ju1zhi4zhui1sui1ju1zhi4zhui1Answer115023161300210107Predictionpang1pang2pang1pang2Answer1511216Prediction\fWe compare our approach to the baseline, the plurality voting, regarding the accuracy of the top-\nranking answers. Both of the algorithms beat plurality voting for all studies and the default is slightly\nbetter. Among all 152 questions, in 138 questions, the variant algorithm outputs the same hierarchy as\nthe default algorithm. For other questions, the top-ranking type of the variant algorithm may contain\nmore than one answer. The top-ranking answer is the answer supported by more people among all\nanswers in the top-ranking type. In one question the variant is wrong but the default is correct, the\nvariant algorithm assigns both the correct answer and the incorrect plurality answer to the top-ranking\ntype, thus outputting the incorrect answer as the top-ranking answer.\n\nWe also compute the lack-of-fit index (Definition 2.6) of the algorithms and find that the questions\nthe algorithm outputs the correct answer have a smaller lack-of-fit thus fitting the thinking hierarchy\nmodel better. Therefore, we can use the lack-of-fit index as a reliability index of the algorithms.\n\nWe additionally pick several representative examples for each study (Figure 4) where the matrices\nare ranked by the default algorithm and the diagonal area modified like Example 1.1. In all of these\nexamples, the plurality is incorrect while our approach is correct. Results of other questions are\nillustrated at https://elicitation.info/classroom/1/. Detailed explanations are illustrated\nin Appendix D and here we provide some highlights. First, our approach elicits a rich hierarchy. For\nexample, the taxicab problem is borrowed from Kahneman [8] and previous studies show that people\nusually ignore the base rate and report \u201880%\u2019. The imagined levels can be 41%\u219280%. We elicit a\nmuch richer level \u201c41%\u219250%\u219280%\u219212%\u219215%\u219220%\u201d. Second, the most sophisticated level\nmay fail to predict the least one. In the Taxicab problem, the correct \u201c41%\u201d supporters successfully\npredict the common wrong answer \u201c80%\u201d. However, they fail to predict the surprisingly wrong\nanswers \u201c12%,20%\u201d, which are in contrast successfully predicted by \u201c80%\u201d supporters. Our model\nis sufficiently general to allow this situation. Third, even for problems (like Go) without famous\nmistakes, our approach still works. Moreover, in the boundary question, we identify the correct\nanswer without any prior when only 3 respondents are correct.\n\nBased on this section, do the authors include the amount of compute and type of resources used (e.g., type of GPUs, internal cluster, or cloud provider)?"}]], "q4a": [[{"role": "user", "content": "The following is the methods section of the paper you are reviewing:\n\n\nWe first introduce our model for thinking hierarchy. Tversky and Kahneman [27] propose that people\nhave two systems, a fast and intuitive system, and a slow and logical system. For example, Alice\nstarts to solve the circle problem. When she reads the question, she can run her intuitive system 1 and\nobtain answer \u201c3\u201d. However, when she starts to think carefully, she runs her more careful system 2 to\nobtain answer \u201c4\u201d.\n\nWe propose a more general model where people can run multiple oracles to approach the question\nduring their thinking process. Conceptually similar to the cognitive hierarchy theory [3], we assume\nthe oracles have different levels. People usually run lower level oracles before the higher level oracles.\nIn the previous example, system 1 is the lower level oracle and system 2 is the higher one. We assume\nthat Alice runs system 2 after system 1. Thus, in our model, people who know the answer is \u201c4\u201d can\n\n4\n\n\fpredict that other people answer \u201c3\u201d, which is hypothesized in Kong and Schoenebeck [10]. It\u2019s also\npossible that some people run the most sophisticated oracle directly without running the lower ones.\nThus, we design the model such that it does not require the higher-type to be able to predict ALL\nlower types. We also allow the oracles\u2019 outputs to be random.\n\nSection 2.1 introduces the model of thinking hierarchy. Section 2.2 reduces the model inference\nproblem to a matrix decomposition problem. Section 2.3 and section 2.4 show how to learn the\nthinking hierarchy.\n\n2.1 Thinking hierarchy\n\nFixing a question q (e.g. the circle problem), T denotes the set of thinking types. A denotes the set of\npossible answers. Both T and A are finite sets. \u2206A denotes all possible distributions over A. We\nsometimes use \u201cprob\u201d as a shorthand for probability.\n\nGenerating answers We will describe how people of different thinking types generate answers.\nDefinition 2.1 (Oracles of thinking types W ). An answer generating oracle maps the question to\nan (random) answer in A. Each type t corresponds to an oracle Ot. The output Ot(q) is a random\nvariable whose distribution is wt \u2208 \u2206A. W denotes a |T | \u00d7 |A| matrix where each row t is wt.\nEach respondent is type t with prob pt and (cid:80)\nrunning the oracle Ot. For all a \u2208 A, the probability that a respondent answers a will be (cid:80)\nWe assume the probability is positive for all a \u2208 A.\nExample 2.2 (A running example). There are two types T = {0, 1}. The answer space is A =\n{3, 4, 6}. O0 will output \u20183\u2019 with probability 0.8 and \u20186\u2019 with probability 0.2. O1 will output \u20184\u2019\n0\n1\n\nt pt = 1. A type t respondent generates the answer by\nt ptwt(a).\n\ndeterministically. In this example, W =\n\nwhere the first row is the distribution of\n\n(cid:20)0.8\n0\n\n0.2\n0\n\n(cid:21)\n\nO0\u2019s output and the second row is the distribution of O1\u2019s output.\n\nGenerating predictions We then describe how people of different thinking types predict what other\npeople will answer. Here the prediction is not a distribution, but an answer other people may report.\nWhen a type t respondent makes a prediction, she will run an oracle, which is Ot\u2032 with probability\npt\u2192t\u2032 where (cid:80)\n\nt\u2032 pt\u2192t\u2032 = 1. She uses the output of Ot\u2032 as the prediction g \u2208 A.\n\nCombination: answer-prediction joint distribution M M denotes a |A| \u00d7 |A| matrix where\nMa,g is the probability an respondent answers a and predicts g. \u039b denotes a |T | \u00d7 |T | matrix where\n\u039bt,t\u2032 = ptpt\u2192t\u2032 is the probability a respondent is type t and predicts type t\u2032.\nExample 2.3. In this example, when type 0 respondent makes a prediction, with prob 1, she will run\nO0 again. When type 1 respondent makes a prediction, with prob 0.5, she will run O1 again, with\nprob 0.5, she will run O0. Moreover, a respondent is type 0 with prob 0.7, and type 1 with prob 0.3.\n\nHere \u039b =\n\n(cid:20)p0p0\u21920\np1p1\u21920\n\np0p0\u21921\np1p1\u21921\n\n(cid:21)\n\n=\n\n(cid:20) 0.7 \u2217 1\n0.3 \u2217 0.5\n\n0.7 \u2217 0\n0.3 \u2217 0.5\n\n(cid:21)\n\n=\n\n(cid:20) 0.7\n0.15\n\n(cid:21)\n\n.\n\n0\n0.15\n\nClaim 2.4. Based on the above generating processes, M = W\u22a4\u039bW.\n\nProof. For each respondent, the probability she answers a and predicts g will be\n(cid:88)\n\n(cid:88)\n\n(cid:88)\n\nptwt(a)\n\npt\u2192t\u2032wt\u2032(g) =\n\nwt(a)ptpt\u2192t\u2032wt\u2032(g).\n\nMa,g =\n\nt\n\nt\u2032\n\nt,t\u2032\n\nWe sum over all possible types the respondent will be. Given she is type t, she runs oracle Ot to\ngenerate the answer and wt(a) is the probability that the answer is a. We sum over all possible\noracles she runs to predict. Given that she runs Ot\u2032, wt\u2032(g) is the probability the prediction is g.\n\nKey assumption: upper-triangular \u039b we assume that people of a less sophisticated level can\nnever run the oracles of more sophisticated levels. A linear ordering of types \u03c0 : {1, 2, \u00b7 \u00b7 \u00b7 , |T |} (cid:55)\u2192 T\nmaps a ranking position to a type. For example, \u03c0(1) \u2208 T is the top-ranking type.\nAssumption 2.5. We assume that with a proper ordering \u03c0 of the types, \u039b is an upper-triangular\nmatrix. Formally, there exists \u03c0 such that \u2200i > j, \u039b\u03c0(i),\u03c0(j) = 0. Any \u03c0 that makes \u039b upper-\ntriangular is a valid thinking hierarchy of the types.\n\n5\n\n\fIn the running example, the valid thinking hierarchy is \u03c0(1) = type 1, \u03c0(2) = type 0. Note that the\nabove assumption does not require \u2200i \u2264 j, \u039b\u03c0(i),\u03c0(j) > 0. When \u039b is a diagonal matrix, types cannot\npredict each other and are equally sophisticated, thus any ordering is a valid thinking hierarchy.\n\nAn algorithm finds the thinking hierarchy when the algorithm is given M which is generated by\nlatent (unknown) W, \u039b, and the algorithm will output a matrix W\u2217 which equals a row-permuted\nW where the row order is a valid thinking hierarchy. Formally, there exists a valid thinking hierarchy\n\u03c0 such that the ith row of W\u2217 is the \u03c0(i)th row of W, i.e, w\u2217\n\ni = w\u03c0(i).\n\n2.2 Non-negative Congruence Triangularization (NCT)\n\nWith the above model, inferring thinking hierarchy leads to a novel matrix decomposition problem,\nwhich is similar to the symmetric non-negative matrix factorization problem (NMF)6. A non-negative\nmatrix is a matrix whose elements are non-negative.\nDefinition 2.6 (Non-negative Congruence7 Triangularization (NCT)). Given a non-negative matrix\nM, NCT aims to find non-negative matrices W and non-negative upper-triangular matrix \u039b such\nthat M = W\u22a4\u039bW. In a Frobenius norm based approximated version, given a set of matrices W,\nNCT aims to find non-negative matrices W and non-negative upper-triangular matrix \u039b to minimize\n\nmin\nW\u2208W,\u039b\n\n||M \u2212 W\u22a4\u039bW||2\nF\n\nand the minimum is defined as the lack-of-fit of M regarding W 8.\n\nLike NMF, it\u2019s impossible to ask for the strict uniqueness of the results. Let P\u039b be the set of\npermutation matrices such that \u03a0\u22a4\u039b\u03a0 is still upper-triangular.\nIf (W, \u039b) is a solution, then\n(\u03a0\u22121DW, \u03a0\u22a4D\u22121\u039bD\u22121\u03a0) is also a solution where D is a diagonal matrix with positive elements\nand \u03a0 \u2208 P\u039b. We state the uniqueness results as follows and the proof is deferred to Appendix C.\nProposition 2.7 (Uniqueness). If |T | \u2264 |A| and T columns of W consist of a permuted positive\ndiagonal matrix, NCT for M = W\u22a4\u039bW is unique in the sense that then for all W\u2032\u22a4\u039b\u2032W\u2032 =\nW\u22a4\u039bW, there exists a positive diagonal matrix D and a |T | \u00d7 |T | permutation matrix \u03a0 \u2208 P\u039b\nsuch that W\u2032 = \u03a0\u22121DW.\n\nWhen we restrict W to be \u201csemi-orthogonal\u201d, we obtain a clean format of NCT without searching\nfor optimal \u039b. I is the set of all \u201csemi-orthogonal\u201d matrices W where each column of W has\nand only has one non-zero element and WW\u22a4 = I. For example, the W in Example 2.2 can be\nnormalized to a semi-orthogonal matrix. The following lemma follows from the expansion of the\nFrobenius norm and we defer the proof to Appendix C.\n\nLemma 2.8 (Semi-orthogonal: minimizing F-norm = maximizing upper-triangular\u2019s sum of the\nsquare). For all set of matrices W \u2282 I, minW\u2208W,\u039b ||M \u2212 W\u22a4\u039bW||2\nF is equivalent to solv-\ni,j and setting \u039b as Up(WMW\u22a4), the upper-triangular area of\ning maxW\u2208W\nWMW\u22a4.\n\ni\u2264j(WMW\u22a4)2\n\n(cid:80)\n\n2.3\n\nInferring the thinking hierarchy with answer-prediction joint distribution M\n\nGiven M, inferring the thinking hierarchy is equivalent to solving NCT in general. Though we do not\nhave M, later we will show a proxy for M. For simplicity of practical use, we introduce two simple\nranking algorithms by employing Lemma 2.8. The ranking algorithm takes M as input and outputs a\nlinear ordering of answers \u03c0 : {1, 2, \u00b7 \u00b7 \u00b7 , |A|} (cid:55)\u2192 A that maps a ranking position to an answer.\n\nAnswer-Ranking Algorithm (Default) AR(M) The algorithm computes\n\n\u03a0\u2217 \u2190 arg max\n\u03a0\u2208P\n\n(cid:88)\n\n(\u03a0M\u03a0\u22a4)2\ni,j\n\ni\u2264j\n\n6Symmetric NMF: minW ||M \u2212 W\u22a4W||2\nF\n7We use congruence here though it is not matrix congruence since W may not be a square matrix.\n8M = W\u22a4\u039bW, W \u2208 W has zero lack-of-fit.\n\n6\n\n\fwhere P is the set of all |A| \u00d7 |A| permutation matrices. There is a one to one mapping between each\npermutation matrix \u03a0 and a linear ordering \u03c0: \u03a0i,\u03c0(i) = 1, \u2200i. Therefore, the optimal \u03a0\u2217 leads to an\noptimal rank over answers directly and the default algorithm can be also represented as\n\n\u03c0\u2217 \u2190 arg max\n\n\u03c0\n\nM 2\n\n\u03c0(i),\u03c0(j).\n\n(cid:88)\n\ni\u2264j\n\nTo find the optimal rank, we use a dynamic programming based algorithm (see the supplementary\nmaterials) which takes O(2|A||A|2). In practice, |A| is usually at most 7 or 8. In our empirical study,\nthe default algorithm takes 91 milliseconds to finish the computation of all 152 questions.\n\nThe default algorithm implicitly assumes |T | = |A| and all oracles are deterministic. To allow\n|T | < |A| and non-deterministic oracles, we introduce a variant that generalizes P to a subset of\nsemi-orthogonal matrices I. Every |T | \u00d7 |A| semi-orthogonal W indicates a hard clustering. Each\ncluster t \u2208 T contains all answers a such that Wt,a > 0. For example, the W in Example 2.2 can\nbe normalized to a semi-orthogonal matrix and indicates a hard clustering {4}, {6, 3}. Therefore,\nthe variant algorithm will partition the answers into multiple clusters and assign a hierarchy to the\nclusters.\n\nAnswer-Ranking Algorithm (Variant) AR+(M, W) The algorithm computes\n\nW\u2217 \u2190 arg max\nW\u2208I\n\n(cid:88)\n\n(WMW\u22a4)2\n\ni,j.\n\ni\u2264j\n\nwhere W \u2282 I. W\u2217 is normalized such that every row sums to 1. This algorithm does not restrict\n|T | = |A| and learns the optimal |T |.\n\nW\u2217 \u21d2 Answer rank The output W\u2217 indicates a hard clustering of all answers. We rank all answer\nas follows: for any i < j, the answers in cluster i has a higher rank 9 than the answers in cluster j.\nFor all i, for any two answers a, a\u2032 in the same cluster i, a is ranked higher than a\u2032 if W \u2217\ni,a\u2032.\n\ni,a > W \u2217\n\nTheoretical justification When M perfectly fits the model with the restriction that the latent W is\na permutation or a hard clustering, we show that our algorithm finds the thinking hierarchy. Otherwise,\nour algorithm finds the \u201cclosest\u201d solution measured by the Frobenius norm.\nTheorem 2.9. When there exists \u03a00 \u2208 P and non-negative upper-triangular matrix \u039b0 such that\n0 \u039b0\u03a00, AR(M) finds the thinking hierarchy 10. In general, AR(M) will output \u03a0\u2217 where\nM = \u03a0\u22a4\n\u03a0\u2217, \u039b\u2217 = Up(\u03a0\u2217M\u03a0\u2217\u22a4) is a solution to arg min\u03a0\u2208P,\u039b ||M \u2212 \u03a0\u22a4\u039b\u03a0||2\nF . The above statement\nis still valid by replacing P by W \u2282 I and AR(M) by AR+(M, W).\n\nProposition 2.7 and Lemma 2.8 almost directly imply the above theorem. We defer the formal proof\nto Appendix C.\n\n2.4 A proxy for answer-prediction joint distribution M\n\nIn practice, we do not have perfect M. We use the following open-response paradigm to obtain a\nproxy for M.\n\nAnswer-prediction paradigm the respondents are asked Q1: What\u2019s your answer? Answer:\nQ2: What do you think other people will answer:\n\n.\n\nIn the circle example, the possible feedback can be \u201canswer: 4; prediction: 3\u201d, \u201canswer: 3; prediction:\n6,9,1\u201d... We collect all answers provided by the respondents 11 and denote the set of them by A. In\nthe circle example, A = {1, 2, 3, 4, 6, 9}. We also allow respondents to provide no prediction or\nmultiple predictions.\n\n9Rank 1 answer is in the highest position.\n10See definition in the last paragraph in Section 2.1.\n11In practice, we set a threshold \u03b8 and collect answers which are provided by at least \u03b8 fraction of the\nrespondents. We allow multiple predictions and also allow people to answer \u201cI do not know\u201d. See Section 3 for\nmore details.\n\n7\n\n\f(a) Accuracy of algorithms\n\n(b) Empirical distribution of lack-of-fit\n\nFigure 3: The results of our experiment\n\nAnswer-prediction matrix We aggregate the feedback and visualize it by an Answer-Prediction\nmatrix. The Answer-Prediction matrix A is a |A| \u00d7 |A| square matrix where |A| is the number of\ndistinct answers provided by the respondents. Each entry Aa,g, a, g \u2208 A is the number of respondents\nthat answer \u201ca\u201d and predict \u201cg\u201d.\n\nWe will show that with proper assumptions, the answer-prediction matrix A\u2019s expectation is pro-\nportional to M. First, for ease of analysis, we assume that each respondent\u2019s predictions are i.i.d.\nsamples12. Second, since we allow people to optionally provide predictions, we need to additionally\nassume that the number of predictions each respondent is willing to provide is independent of her\ntype and answer. We state the formal result as follows and the proof is deferred to Appendix C.\n\nProposition 2.10. When each respondent\u2019s predictions are i.i.d. samples, and the number of\npredictions she gives is independent of her type and answer, the answer-prediction matrix A\u2019s\nexpectation is proportional to M.\n\n\nBased on this section, do the authors use existing assets (e.g., code, data, models) that are not new contributions in their paper? Please start your answer with \"Yes\", \"No\", or \"Unsure\"."}, {"role": "user", "content": "The following is the experiments section of the paper you are reviewing:\nexperiments section: \n\nWe conduct four studies, study 1 (35 math problems), study 2 (30 Go problems), study 3 (44 general\nknowledge questions), and study 4 (43 Chinese character pronunciation questions).\n\nData collection All studies are performed by online questionnaires. We recruit the respondents by\nan online announcement13 or from an online platform that is similar to Amazon Mechanical Turk. We\nget respondents\u2019 consent for using and sharing their data for research. Respondents are asked not to\nsearch for the answers to the questions or communicate with other people. We allow the respondents\nto answer \u201cI do not know\u201d for all questions. Except for Go problems, all questionnaires use flat\npayment. We illustrate the data collection process in detail in Appendix A. We allow respondents\nto participate in more than one study because our algorithms analyze each question separately and\nindependently.\n\nData processing We merge answers which are the same, like \u20180.5\u2019 and \u201850%\u2019. We omit the answers\nthat are reported by less than (\u2264) 3% of respondents or one person. The remaining answers, excluding\n\u201cI do not know\u201d, form the answer set A whose size is |A|. We then construct the Answer-Prediction\nmatrix and perform our algorithms. Pseudo-codes are attached in Appendix B. Our algorithms do not\nrequire any prior or the respondents\u2019 expertise levels.\n\n3.1 Results\n\n12This may not be a very good assumption since i.i.d. samples can repeat but respondents usually do not\nrepeat their predictions. If we do not want this assumption, we can choose to only use the first prediction from\neach respondent (if there exists) to construct the answer-prediction matrix.\n\n13Many are students from top universities in China. See Appendix A for more details.\n\n8\n\n\fType\nMath\nGo\nGeneral knowledge\nChinese character\n\nTotal Our algorithm(Default) Our algorithm(Variant)\n\n35\n30\n44\n43\n\n29\n28\n41\n36\n\n29\n28\n41\n35\n\nPlurality voting\n24\n23\n35\n34\n\nTable 1: The number of questions our algorithms/baseline are correct.\n\n(a) the Monty Hall problem: you can select one\nclosed door of three. A prize, a car, is behind\none of the doors. The other two doors hide goats.\nAfter you have made your choice, Monty Hall\nwill open one of the remaining doors and show\nthat it does not contain the prize. He then asks\nyou if you would like to switch your choice to\nthe other unopened door. What is the probability\nto get the prize if you switch?\n\n(b) the Taxicab problem: 85% of taxis in this\ncity are green, the others are blue. A witness\nsees a blue taxi. She is usually correct with\nprobability 80%. What is the probability that\nthe taxi saw by the witness is blue?\n\n(c) Pick a move for black such that they can be\nalive.\n\n(d) Pick a move for black such that they can be\nalive by ko.\n\n(e) the boundary question: what river forms the\nboundary between North Korea and China?\n\n(f) the Middle Age New Year question: when\nwas the new year in middle age?\n\n(g) the pronunciation of \u7762\n\n(h) the pronunciation of \u6ec2\n\nFigure 4: The ranked answer-prediction matrices\n\n9\n\n2/31/21/32/31/21/3Answer34261413517146Prediction415080121520415080121520Answer160500008201200801619501104818000522130010218PredictionD3C1B2C2B1D3C1B2C2B1Answer62131091410042012115300002PredictionB8A8B6B7B8A8B6B7Answer112540630001230035PredictionYaluandDoomanYaluSonghuaYaluandDoomanYaluSonghuaAnswer33007410005PredictionApr1Dec25Jan1Apr1Dec25Jan1Answer15410028180633Predictionsui1ju1zhi4zhui1sui1ju1zhi4zhui1Answer115023161300210107Predictionpang1pang2pang1pang2Answer1511216Prediction\fWe compare our approach to the baseline, the plurality voting, regarding the accuracy of the top-\nranking answers. Both of the algorithms beat plurality voting for all studies and the default is slightly\nbetter. Among all 152 questions, in 138 questions, the variant algorithm outputs the same hierarchy as\nthe default algorithm. For other questions, the top-ranking type of the variant algorithm may contain\nmore than one answer. The top-ranking answer is the answer supported by more people among all\nanswers in the top-ranking type. In one question the variant is wrong but the default is correct, the\nvariant algorithm assigns both the correct answer and the incorrect plurality answer to the top-ranking\ntype, thus outputting the incorrect answer as the top-ranking answer.\n\nWe also compute the lack-of-fit index (Definition 2.6) of the algorithms and find that the questions\nthe algorithm outputs the correct answer have a smaller lack-of-fit thus fitting the thinking hierarchy\nmodel better. Therefore, we can use the lack-of-fit index as a reliability index of the algorithms.\n\nWe additionally pick several representative examples for each study (Figure 4) where the matrices\nare ranked by the default algorithm and the diagonal area modified like Example 1.1. In all of these\nexamples, the plurality is incorrect while our approach is correct. Results of other questions are\nillustrated at https://elicitation.info/classroom/1/. Detailed explanations are illustrated\nin Appendix D and here we provide some highlights. First, our approach elicits a rich hierarchy. For\nexample, the taxicab problem is borrowed from Kahneman [8] and previous studies show that people\nusually ignore the base rate and report \u201880%\u2019. The imagined levels can be 41%\u219280%. We elicit a\nmuch richer level \u201c41%\u219250%\u219280%\u219212%\u219215%\u219220%\u201d. Second, the most sophisticated level\nmay fail to predict the least one. In the Taxicab problem, the correct \u201c41%\u201d supporters successfully\npredict the common wrong answer \u201c80%\u201d. However, they fail to predict the surprisingly wrong\nanswers \u201c12%,20%\u201d, which are in contrast successfully predicted by \u201c80%\u201d supporters. Our model\nis sufficiently general to allow this situation. Third, even for problems (like Go) without famous\nmistakes, our approach still works. Moreover, in the boundary question, we identify the correct\nanswer without any prior when only 3 respondents are correct.\n\n\nBased on the section, do the authors cite the creators of the existing assets that they use?"}]], "q4b": [[{"role": "user", "content": "The following is the experiments section of the paper you are reviewing:\n\n\nWe conduct four studies, study 1 (35 math problems), study 2 (30 Go problems), study 3 (44 general\nknowledge questions), and study 4 (43 Chinese character pronunciation questions).\n\nData collection All studies are performed by online questionnaires. We recruit the respondents by\nan online announcement13 or from an online platform that is similar to Amazon Mechanical Turk. We\nget respondents\u2019 consent for using and sharing their data for research. Respondents are asked not to\nsearch for the answers to the questions or communicate with other people. We allow the respondents\nto answer \u201cI do not know\u201d for all questions. Except for Go problems, all questionnaires use flat\npayment. We illustrate the data collection process in detail in Appendix A. We allow respondents\nto participate in more than one study because our algorithms analyze each question separately and\nindependently.\n\nData processing We merge answers which are the same, like \u20180.5\u2019 and \u201850%\u2019. We omit the answers\nthat are reported by less than (\u2264) 3% of respondents or one person. The remaining answers, excluding\n\u201cI do not know\u201d, form the answer set A whose size is |A|. We then construct the Answer-Prediction\nmatrix and perform our algorithms. Pseudo-codes are attached in Appendix B. Our algorithms do not\nrequire any prior or the respondents\u2019 expertise levels.\n\n3.1 Results\n\n12This may not be a very good assumption since i.i.d. samples can repeat but respondents usually do not\nrepeat their predictions. If we do not want this assumption, we can choose to only use the first prediction from\neach respondent (if there exists) to construct the answer-prediction matrix.\n\n13Many are students from top universities in China. See Appendix A for more details.\n\n8\n\n\fType\nMath\nGo\nGeneral knowledge\nChinese character\n\nTotal Our algorithm(Default) Our algorithm(Variant)\n\n35\n30\n44\n43\n\n29\n28\n41\n36\n\n29\n28\n41\n35\n\nPlurality voting\n24\n23\n35\n34\n\nTable 1: The number of questions our algorithms/baseline are correct.\n\n(a) the Monty Hall problem: you can select one\nclosed door of three. A prize, a car, is behind\none of the doors. The other two doors hide goats.\nAfter you have made your choice, Monty Hall\nwill open one of the remaining doors and show\nthat it does not contain the prize. He then asks\nyou if you would like to switch your choice to\nthe other unopened door. What is the probability\nto get the prize if you switch?\n\n(b) the Taxicab problem: 85% of taxis in this\ncity are green, the others are blue. A witness\nsees a blue taxi. She is usually correct with\nprobability 80%. What is the probability that\nthe taxi saw by the witness is blue?\n\n(c) Pick a move for black such that they can be\nalive.\n\n(d) Pick a move for black such that they can be\nalive by ko.\n\n(e) the boundary question: what river forms the\nboundary between North Korea and China?\n\n(f) the Middle Age New Year question: when\nwas the new year in middle age?\n\n(g) the pronunciation of \u7762\n\n(h) the pronunciation of \u6ec2\n\nFigure 4: The ranked answer-prediction matrices\n\n9\n\n2/31/21/32/31/21/3Answer34261413517146Prediction415080121520415080121520Answer160500008201200801619501104818000522130010218PredictionD3C1B2C2B1D3C1B2C2B1Answer62131091410042012115300002PredictionB8A8B6B7B8A8B6B7Answer112540630001230035PredictionYaluandDoomanYaluSonghuaYaluandDoomanYaluSonghuaAnswer33007410005PredictionApr1Dec25Jan1Apr1Dec25Jan1Answer15410028180633Predictionsui1ju1zhi4zhui1sui1ju1zhi4zhui1Answer115023161300210107Predictionpang1pang2pang1pang2Answer1511216Prediction\fWe compare our approach to the baseline, the plurality voting, regarding the accuracy of the top-\nranking answers. Both of the algorithms beat plurality voting for all studies and the default is slightly\nbetter. Among all 152 questions, in 138 questions, the variant algorithm outputs the same hierarchy as\nthe default algorithm. For other questions, the top-ranking type of the variant algorithm may contain\nmore than one answer. The top-ranking answer is the answer supported by more people among all\nanswers in the top-ranking type. In one question the variant is wrong but the default is correct, the\nvariant algorithm assigns both the correct answer and the incorrect plurality answer to the top-ranking\ntype, thus outputting the incorrect answer as the top-ranking answer.\n\nWe also compute the lack-of-fit index (Definition 2.6) of the algorithms and find that the questions\nthe algorithm outputs the correct answer have a smaller lack-of-fit thus fitting the thinking hierarchy\nmodel better. Therefore, we can use the lack-of-fit index as a reliability index of the algorithms.\n\nWe additionally pick several representative examples for each study (Figure 4) where the matrices\nare ranked by the default algorithm and the diagonal area modified like Example 1.1. In all of these\nexamples, the plurality is incorrect while our approach is correct. Results of other questions are\nillustrated at https://elicitation.info/classroom/1/. Detailed explanations are illustrated\nin Appendix D and here we provide some highlights. First, our approach elicits a rich hierarchy. For\nexample, the taxicab problem is borrowed from Kahneman [8] and previous studies show that people\nusually ignore the base rate and report \u201880%\u2019. The imagined levels can be 41%\u219280%. We elicit a\nmuch richer level \u201c41%\u219250%\u219280%\u219212%\u219215%\u219220%\u201d. Second, the most sophisticated level\nmay fail to predict the least one. In the Taxicab problem, the correct \u201c41%\u201d supporters successfully\npredict the common wrong answer \u201c80%\u201d. However, they fail to predict the surprisingly wrong\nanswers \u201c12%,20%\u201d, which are in contrast successfully predicted by \u201c80%\u201d supporters. Our model\nis sufficiently general to allow this situation. Third, even for problems (like Go) without famous\nmistakes, our approach still works. Moreover, in the boundary question, we identify the correct\nanswer without any prior when only 3 respondents are correct.\n\n\nBased on this section, do the authors use existing assets (e.g., code, data, models) that are not new contributions in their paper? Please start your answer with \"Yes\", \"No\", or \"Unsure\"."}, {"role": "user", "content": "The following is the experiments section of the paper you are reviewing:\n\n\nWe conduct four studies, study 1 (35 math problems), study 2 (30 Go problems), study 3 (44 general\nknowledge questions), and study 4 (43 Chinese character pronunciation questions).\n\nData collection All studies are performed by online questionnaires. We recruit the respondents by\nan online announcement13 or from an online platform that is similar to Amazon Mechanical Turk. We\nget respondents\u2019 consent for using and sharing their data for research. Respondents are asked not to\nsearch for the answers to the questions or communicate with other people. We allow the respondents\nto answer \u201cI do not know\u201d for all questions. Except for Go problems, all questionnaires use flat\npayment. We illustrate the data collection process in detail in Appendix A. We allow respondents\nto participate in more than one study because our algorithms analyze each question separately and\nindependently.\n\nData processing We merge answers which are the same, like \u20180.5\u2019 and \u201850%\u2019. We omit the answers\nthat are reported by less than (\u2264) 3% of respondents or one person. The remaining answers, excluding\n\u201cI do not know\u201d, form the answer set A whose size is |A|. We then construct the Answer-Prediction\nmatrix and perform our algorithms. Pseudo-codes are attached in Appendix B. Our algorithms do not\nrequire any prior or the respondents\u2019 expertise levels.\n\n3.1 Results\n\n12This may not be a very good assumption since i.i.d. samples can repeat but respondents usually do not\nrepeat their predictions. If we do not want this assumption, we can choose to only use the first prediction from\neach respondent (if there exists) to construct the answer-prediction matrix.\n\n13Many are students from top universities in China. See Appendix A for more details.\n\n8\n\n\fType\nMath\nGo\nGeneral knowledge\nChinese character\n\nTotal Our algorithm(Default) Our algorithm(Variant)\n\n35\n30\n44\n43\n\n29\n28\n41\n36\n\n29\n28\n41\n35\n\nPlurality voting\n24\n23\n35\n34\n\nTable 1: The number of questions our algorithms/baseline are correct.\n\n(a) the Monty Hall problem: you can select one\nclosed door of three. A prize, a car, is behind\none of the doors. The other two doors hide goats.\nAfter you have made your choice, Monty Hall\nwill open one of the remaining doors and show\nthat it does not contain the prize. He then asks\nyou if you would like to switch your choice to\nthe other unopened door. What is the probability\nto get the prize if you switch?\n\n(b) the Taxicab problem: 85% of taxis in this\ncity are green, the others are blue. A witness\nsees a blue taxi. She is usually correct with\nprobability 80%. What is the probability that\nthe taxi saw by the witness is blue?\n\n(c) Pick a move for black such that they can be\nalive.\n\n(d) Pick a move for black such that they can be\nalive by ko.\n\n(e) the boundary question: what river forms the\nboundary between North Korea and China?\n\n(f) the Middle Age New Year question: when\nwas the new year in middle age?\n\n(g) the pronunciation of \u7762\n\n(h) the pronunciation of \u6ec2\n\nFigure 4: The ranked answer-prediction matrices\n\n9\n\n2/31/21/32/31/21/3Answer34261413517146Prediction415080121520415080121520Answer160500008201200801619501104818000522130010218PredictionD3C1B2C2B1D3C1B2C2B1Answer62131091410042012115300002PredictionB8A8B6B7B8A8B6B7Answer112540630001230035PredictionYaluandDoomanYaluSonghuaYaluandDoomanYaluSonghuaAnswer33007410005PredictionApr1Dec25Jan1Apr1Dec25Jan1Answer15410028180633Predictionsui1ju1zhi4zhui1sui1ju1zhi4zhui1Answer115023161300210107Predictionpang1pang2pang1pang2Answer1511216Prediction\fWe compare our approach to the baseline, the plurality voting, regarding the accuracy of the top-\nranking answers. Both of the algorithms beat plurality voting for all studies and the default is slightly\nbetter. Among all 152 questions, in 138 questions, the variant algorithm outputs the same hierarchy as\nthe default algorithm. For other questions, the top-ranking type of the variant algorithm may contain\nmore than one answer. The top-ranking answer is the answer supported by more people among all\nanswers in the top-ranking type. In one question the variant is wrong but the default is correct, the\nvariant algorithm assigns both the correct answer and the incorrect plurality answer to the top-ranking\ntype, thus outputting the incorrect answer as the top-ranking answer.\n\nWe also compute the lack-of-fit index (Definition 2.6) of the algorithms and find that the questions\nthe algorithm outputs the correct answer have a smaller lack-of-fit thus fitting the thinking hierarchy\nmodel better. Therefore, we can use the lack-of-fit index as a reliability index of the algorithms.\n\nWe additionally pick several representative examples for each study (Figure 4) where the matrices\nare ranked by the default algorithm and the diagonal area modified like Example 1.1. In all of these\nexamples, the plurality is incorrect while our approach is correct. Results of other questions are\nillustrated at https://elicitation.info/classroom/1/. Detailed explanations are illustrated\nin Appendix D and here we provide some highlights. First, our approach elicits a rich hierarchy. For\nexample, the taxicab problem is borrowed from Kahneman [8] and previous studies show that people\nusually ignore the base rate and report \u201880%\u2019. The imagined levels can be 41%\u219280%. We elicit a\nmuch richer level \u201c41%\u219250%\u219280%\u219212%\u219215%\u219220%\u201d. Second, the most sophisticated level\nmay fail to predict the least one. In the Taxicab problem, the correct \u201c41%\u201d supporters successfully\npredict the common wrong answer \u201c80%\u201d. However, they fail to predict the surprisingly wrong\nanswers \u201c12%,20%\u201d, which are in contrast successfully predicted by \u201c80%\u201d supporters. Our model\nis sufficiently general to allow this situation. Third, even for problems (like Go) without famous\nmistakes, our approach still works. Moreover, in the boundary question, we identify the correct\nanswer without any prior when only 3 respondents are correct.\n\n\nBased on the section, do the authors mention the licenses of the existing assets that they use?"}]], "q4c": [[{"role": "user", "content": "The following is the abstract section of the paper you are reviewing:\n\n\nWhen we use the wisdom of the crowds, we usually rank the answers according to\ntheir popularity, especially when we cannot verify the answers. However, this can\nbe very dangerous when the majority make systematic mistakes. A fundamental\nquestion arises: can we build a hierarchy among the answers without any prior\nwhere the higher-ranking answers, which may not be supported by the majority,\nare from more sophisticated people? To address the question, we propose 1) a\nnovel model to describe people\u2019s thinking hierarchy; 2) two algorithms to learn\nthe thinking hierarchy without any prior; 3) a novel open-response based crowd-\nsourcing approach based on the above theoretic framework. In addition to theoretic\njustifications, we conduct four empirical crowdsourcing studies and show that a)\nthe accuracy of the top-ranking answers learned by our approach is much higher\nthan that of plurality voting (In one question, the plurality answer is supported by\n74 respondents but the correct answer is only supported by 3 respondents. Our\napproach ranks the correct answer the highest without any prior); b) our model\nhas a high goodness-of-fit, especially for the questions where our top-ranking\nanswer is correct. To the best of our knowledge, we are the first to propose a\nthinking hierarchy model with empirical validations in the general problem-solving\nscenarios; and the first to propose a practical open-response based crowdsourcing\napproach that beats plurality voting without any prior.\n\n1\n\n\nBased on this abstract, do the authors curate/release new assets (e.g., code, data, models)? Please start your answer with \"Yes\", \"No\", or \"Unsure\"."}, {"role": "user", "content": "The following is the experiments section of the paper you are reviewing:\n\n\nWe conduct four studies, study 1 (35 math problems), study 2 (30 Go problems), study 3 (44 general\nknowledge questions), and study 4 (43 Chinese character pronunciation questions).\n\nData collection All studies are performed by online questionnaires. We recruit the respondents by\nan online announcement13 or from an online platform that is similar to Amazon Mechanical Turk. We\nget respondents\u2019 consent for using and sharing their data for research. Respondents are asked not to\nsearch for the answers to the questions or communicate with other people. We allow the respondents\nto answer \u201cI do not know\u201d for all questions. Except for Go problems, all questionnaires use flat\npayment. We illustrate the data collection process in detail in Appendix A. We allow respondents\nto participate in more than one study because our algorithms analyze each question separately and\nindependently.\n\nData processing We merge answers which are the same, like \u20180.5\u2019 and \u201850%\u2019. We omit the answers\nthat are reported by less than (\u2264) 3% of respondents or one person. The remaining answers, excluding\n\u201cI do not know\u201d, form the answer set A whose size is |A|. We then construct the Answer-Prediction\nmatrix and perform our algorithms. Pseudo-codes are attached in Appendix B. Our algorithms do not\nrequire any prior or the respondents\u2019 expertise levels.\n\n3.1 Results\n\n12This may not be a very good assumption since i.i.d. samples can repeat but respondents usually do not\nrepeat their predictions. If we do not want this assumption, we can choose to only use the first prediction from\neach respondent (if there exists) to construct the answer-prediction matrix.\n\n13Many are students from top universities in China. See Appendix A for more details.\n\n8\n\n\fType\nMath\nGo\nGeneral knowledge\nChinese character\n\nTotal Our algorithm(Default) Our algorithm(Variant)\n\n35\n30\n44\n43\n\n29\n28\n41\n36\n\n29\n28\n41\n35\n\nPlurality voting\n24\n23\n35\n34\n\nTable 1: The number of questions our algorithms/baseline are correct.\n\n(a) the Monty Hall problem: you can select one\nclosed door of three. A prize, a car, is behind\none of the doors. The other two doors hide goats.\nAfter you have made your choice, Monty Hall\nwill open one of the remaining doors and show\nthat it does not contain the prize. He then asks\nyou if you would like to switch your choice to\nthe other unopened door. What is the probability\nto get the prize if you switch?\n\n(b) the Taxicab problem: 85% of taxis in this\ncity are green, the others are blue. A witness\nsees a blue taxi. She is usually correct with\nprobability 80%. What is the probability that\nthe taxi saw by the witness is blue?\n\n(c) Pick a move for black such that they can be\nalive.\n\n(d) Pick a move for black such that they can be\nalive by ko.\n\n(e) the boundary question: what river forms the\nboundary between North Korea and China?\n\n(f) the Middle Age New Year question: when\nwas the new year in middle age?\n\n(g) the pronunciation of \u7762\n\n(h) the pronunciation of \u6ec2\n\nFigure 4: The ranked answer-prediction matrices\n\n9\n\n2/31/21/32/31/21/3Answer34261413517146Prediction415080121520415080121520Answer160500008201200801619501104818000522130010218PredictionD3C1B2C2B1D3C1B2C2B1Answer62131091410042012115300002PredictionB8A8B6B7B8A8B6B7Answer112540630001230035PredictionYaluandDoomanYaluSonghuaYaluandDoomanYaluSonghuaAnswer33007410005PredictionApr1Dec25Jan1Apr1Dec25Jan1Answer15410028180633Predictionsui1ju1zhi4zhui1sui1ju1zhi4zhui1Answer115023161300210107Predictionpang1pang2pang1pang2Answer1511216Prediction\fWe compare our approach to the baseline, the plurality voting, regarding the accuracy of the top-\nranking answers. Both of the algorithms beat plurality voting for all studies and the default is slightly\nbetter. Among all 152 questions, in 138 questions, the variant algorithm outputs the same hierarchy as\nthe default algorithm. For other questions, the top-ranking type of the variant algorithm may contain\nmore than one answer. The top-ranking answer is the answer supported by more people among all\nanswers in the top-ranking type. In one question the variant is wrong but the default is correct, the\nvariant algorithm assigns both the correct answer and the incorrect plurality answer to the top-ranking\ntype, thus outputting the incorrect answer as the top-ranking answer.\n\nWe also compute the lack-of-fit index (Definition 2.6) of the algorithms and find that the questions\nthe algorithm outputs the correct answer have a smaller lack-of-fit thus fitting the thinking hierarchy\nmodel better. Therefore, we can use the lack-of-fit index as a reliability index of the algorithms.\n\nWe additionally pick several representative examples for each study (Figure 4) where the matrices\nare ranked by the default algorithm and the diagonal area modified like Example 1.1. In all of these\nexamples, the plurality is incorrect while our approach is correct. Results of other questions are\nillustrated at https://elicitation.info/classroom/1/. Detailed explanations are illustrated\nin Appendix D and here we provide some highlights. First, our approach elicits a rich hierarchy. For\nexample, the taxicab problem is borrowed from Kahneman [8] and previous studies show that people\nusually ignore the base rate and report \u201880%\u2019. The imagined levels can be 41%\u219280%. We elicit a\nmuch richer level \u201c41%\u219250%\u219280%\u219212%\u219215%\u219220%\u201d. Second, the most sophisticated level\nmay fail to predict the least one. In the Taxicab problem, the correct \u201c41%\u201d supporters successfully\npredict the common wrong answer \u201c80%\u201d. However, they fail to predict the surprisingly wrong\nanswers \u201c12%,20%\u201d, which are in contrast successfully predicted by \u201c80%\u201d supporters. Our model\nis sufficiently general to allow this situation. Third, even for problems (like Go) without famous\nmistakes, our approach still works. Moreover, in the boundary question, we identify the correct\nanswer without any prior when only 3 respondents are correct.\n\n\nBased on this section, do the authors include any new assets either in the supplemental material or as a URL?"}]], "q4d": [[{"role": "user", "content": "The following is the abstract section of the paper you are reviewing:\n\n\nWhen we use the wisdom of the crowds, we usually rank the answers according to\ntheir popularity, especially when we cannot verify the answers. However, this can\nbe very dangerous when the majority make systematic mistakes. A fundamental\nquestion arises: can we build a hierarchy among the answers without any prior\nwhere the higher-ranking answers, which may not be supported by the majority,\nare from more sophisticated people? To address the question, we propose 1) a\nnovel model to describe people\u2019s thinking hierarchy; 2) two algorithms to learn\nthe thinking hierarchy without any prior; 3) a novel open-response based crowd-\nsourcing approach based on the above theoretic framework. In addition to theoretic\njustifications, we conduct four empirical crowdsourcing studies and show that a)\nthe accuracy of the top-ranking answers learned by our approach is much higher\nthan that of plurality voting (In one question, the plurality answer is supported by\n74 respondents but the correct answer is only supported by 3 respondents. Our\napproach ranks the correct answer the highest without any prior); b) our model\nhas a high goodness-of-fit, especially for the questions where our top-ranking\nanswer is correct. To the best of our knowledge, we are the first to propose a\nthinking hierarchy model with empirical validations in the general problem-solving\nscenarios; and the first to propose a practical open-response based crowdsourcing\napproach that beats plurality voting without any prior.\n\n1\n\n\nBased on this abstract, do the authors curate/release new assets (e.g., code, data, models)? Please start your answer with \"Yes\", \"No\", or \"Unsure\"."}, {"role": "user", "content": "The following is the experiments section of the paper you are reviewing:\n\n\nWe conduct four studies, study 1 (35 math problems), study 2 (30 Go problems), study 3 (44 general\nknowledge questions), and study 4 (43 Chinese character pronunciation questions).\n\nData collection All studies are performed by online questionnaires. We recruit the respondents by\nan online announcement13 or from an online platform that is similar to Amazon Mechanical Turk. We\nget respondents\u2019 consent for using and sharing their data for research. Respondents are asked not to\nsearch for the answers to the questions or communicate with other people. We allow the respondents\nto answer \u201cI do not know\u201d for all questions. Except for Go problems, all questionnaires use flat\npayment. We illustrate the data collection process in detail in Appendix A. We allow respondents\nto participate in more than one study because our algorithms analyze each question separately and\nindependently.\n\nData processing We merge answers which are the same, like \u20180.5\u2019 and \u201850%\u2019. We omit the answers\nthat are reported by less than (\u2264) 3% of respondents or one person. The remaining answers, excluding\n\u201cI do not know\u201d, form the answer set A whose size is |A|. We then construct the Answer-Prediction\nmatrix and perform our algorithms. Pseudo-codes are attached in Appendix B. Our algorithms do not\nrequire any prior or the respondents\u2019 expertise levels.\n\n3.1 Results\n\n12This may not be a very good assumption since i.i.d. samples can repeat but respondents usually do not\nrepeat their predictions. If we do not want this assumption, we can choose to only use the first prediction from\neach respondent (if there exists) to construct the answer-prediction matrix.\n\n13Many are students from top universities in China. See Appendix A for more details.\n\n8\n\n\fType\nMath\nGo\nGeneral knowledge\nChinese character\n\nTotal Our algorithm(Default) Our algorithm(Variant)\n\n35\n30\n44\n43\n\n29\n28\n41\n36\n\n29\n28\n41\n35\n\nPlurality voting\n24\n23\n35\n34\n\nTable 1: The number of questions our algorithms/baseline are correct.\n\n(a) the Monty Hall problem: you can select one\nclosed door of three. A prize, a car, is behind\none of the doors. The other two doors hide goats.\nAfter you have made your choice, Monty Hall\nwill open one of the remaining doors and show\nthat it does not contain the prize. He then asks\nyou if you would like to switch your choice to\nthe other unopened door. What is the probability\nto get the prize if you switch?\n\n(b) the Taxicab problem: 85% of taxis in this\ncity are green, the others are blue. A witness\nsees a blue taxi. She is usually correct with\nprobability 80%. What is the probability that\nthe taxi saw by the witness is blue?\n\n(c) Pick a move for black such that they can be\nalive.\n\n(d) Pick a move for black such that they can be\nalive by ko.\n\n(e) the boundary question: what river forms the\nboundary between North Korea and China?\n\n(f) the Middle Age New Year question: when\nwas the new year in middle age?\n\n(g) the pronunciation of \u7762\n\n(h) the pronunciation of \u6ec2\n\nFigure 4: The ranked answer-prediction matrices\n\n9\n\n2/31/21/32/31/21/3Answer34261413517146Prediction415080121520415080121520Answer160500008201200801619501104818000522130010218PredictionD3C1B2C2B1D3C1B2C2B1Answer62131091410042012115300002PredictionB8A8B6B7B8A8B6B7Answer112540630001230035PredictionYaluandDoomanYaluSonghuaYaluandDoomanYaluSonghuaAnswer33007410005PredictionApr1Dec25Jan1Apr1Dec25Jan1Answer15410028180633Predictionsui1ju1zhi4zhui1sui1ju1zhi4zhui1Answer115023161300210107Predictionpang1pang2pang1pang2Answer1511216Prediction\fWe compare our approach to the baseline, the plurality voting, regarding the accuracy of the top-\nranking answers. Both of the algorithms beat plurality voting for all studies and the default is slightly\nbetter. Among all 152 questions, in 138 questions, the variant algorithm outputs the same hierarchy as\nthe default algorithm. For other questions, the top-ranking type of the variant algorithm may contain\nmore than one answer. The top-ranking answer is the answer supported by more people among all\nanswers in the top-ranking type. In one question the variant is wrong but the default is correct, the\nvariant algorithm assigns both the correct answer and the incorrect plurality answer to the top-ranking\ntype, thus outputting the incorrect answer as the top-ranking answer.\n\nWe also compute the lack-of-fit index (Definition 2.6) of the algorithms and find that the questions\nthe algorithm outputs the correct answer have a smaller lack-of-fit thus fitting the thinking hierarchy\nmodel better. Therefore, we can use the lack-of-fit index as a reliability index of the algorithms.\n\nWe additionally pick several representative examples for each study (Figure 4) where the matrices\nare ranked by the default algorithm and the diagonal area modified like Example 1.1. In all of these\nexamples, the plurality is incorrect while our approach is correct. Results of other questions are\nillustrated at https://elicitation.info/classroom/1/. Detailed explanations are illustrated\nin Appendix D and here we provide some highlights. First, our approach elicits a rich hierarchy. For\nexample, the taxicab problem is borrowed from Kahneman [8] and previous studies show that people\nusually ignore the base rate and report \u201880%\u2019. The imagined levels can be 41%\u219280%. We elicit a\nmuch richer level \u201c41%\u219250%\u219280%\u219212%\u219215%\u219220%\u201d. Second, the most sophisticated level\nmay fail to predict the least one. In the Taxicab problem, the correct \u201c41%\u201d supporters successfully\npredict the common wrong answer \u201c80%\u201d. However, they fail to predict the surprisingly wrong\nanswers \u201c12%,20%\u201d, which are in contrast successfully predicted by \u201c80%\u201d supporters. Our model\nis sufficiently general to allow this situation. Third, even for problems (like Go) without famous\nmistakes, our approach still works. Moreover, in the boundary question, we identify the correct\nanswer without any prior when only 3 respondents are correct.\n\n\nBased on this section, do the authors discuss whether and how consent was obtained from people whose data the authors are using/curating?"}]], "q4e": [[{"role": "user", "content": "The following is the abstract section of the paper you are reviewing:\n\n\nWhen we use the wisdom of the crowds, we usually rank the answers according to\ntheir popularity, especially when we cannot verify the answers. However, this can\nbe very dangerous when the majority make systematic mistakes. A fundamental\nquestion arises: can we build a hierarchy among the answers without any prior\nwhere the higher-ranking answers, which may not be supported by the majority,\nare from more sophisticated people? To address the question, we propose 1) a\nnovel model to describe people\u2019s thinking hierarchy; 2) two algorithms to learn\nthe thinking hierarchy without any prior; 3) a novel open-response based crowd-\nsourcing approach based on the above theoretic framework. In addition to theoretic\njustifications, we conduct four empirical crowdsourcing studies and show that a)\nthe accuracy of the top-ranking answers learned by our approach is much higher\nthan that of plurality voting (In one question, the plurality answer is supported by\n74 respondents but the correct answer is only supported by 3 respondents. Our\napproach ranks the correct answer the highest without any prior); b) our model\nhas a high goodness-of-fit, especially for the questions where our top-ranking\nanswer is correct. To the best of our knowledge, we are the first to propose a\nthinking hierarchy model with empirical validations in the general problem-solving\nscenarios; and the first to propose a practical open-response based crowdsourcing\napproach that beats plurality voting without any prior.\n\n1\n\n\nBased on this abstract, do the authors curate/release new assets (e.g., code, data, models)? Please start your answer with \"Yes\", \"No\", or \"Unsure\"."}, {"role": "user", "content": "The following is the experiments section of the paper you are reviewing:\n\n\nWe conduct four studies, study 1 (35 math problems), study 2 (30 Go problems), study 3 (44 general\nknowledge questions), and study 4 (43 Chinese character pronunciation questions).\n\nData collection All studies are performed by online questionnaires. We recruit the respondents by\nan online announcement13 or from an online platform that is similar to Amazon Mechanical Turk. We\nget respondents\u2019 consent for using and sharing their data for research. Respondents are asked not to\nsearch for the answers to the questions or communicate with other people. We allow the respondents\nto answer \u201cI do not know\u201d for all questions. Except for Go problems, all questionnaires use flat\npayment. We illustrate the data collection process in detail in Appendix A. We allow respondents\nto participate in more than one study because our algorithms analyze each question separately and\nindependently.\n\nData processing We merge answers which are the same, like \u20180.5\u2019 and \u201850%\u2019. We omit the answers\nthat are reported by less than (\u2264) 3% of respondents or one person. The remaining answers, excluding\n\u201cI do not know\u201d, form the answer set A whose size is |A|. We then construct the Answer-Prediction\nmatrix and perform our algorithms. Pseudo-codes are attached in Appendix B. Our algorithms do not\nrequire any prior or the respondents\u2019 expertise levels.\n\n3.1 Results\n\n12This may not be a very good assumption since i.i.d. samples can repeat but respondents usually do not\nrepeat their predictions. If we do not want this assumption, we can choose to only use the first prediction from\neach respondent (if there exists) to construct the answer-prediction matrix.\n\n13Many are students from top universities in China. See Appendix A for more details.\n\n8\n\n\fType\nMath\nGo\nGeneral knowledge\nChinese character\n\nTotal Our algorithm(Default) Our algorithm(Variant)\n\n35\n30\n44\n43\n\n29\n28\n41\n36\n\n29\n28\n41\n35\n\nPlurality voting\n24\n23\n35\n34\n\nTable 1: The number of questions our algorithms/baseline are correct.\n\n(a) the Monty Hall problem: you can select one\nclosed door of three. A prize, a car, is behind\none of the doors. The other two doors hide goats.\nAfter you have made your choice, Monty Hall\nwill open one of the remaining doors and show\nthat it does not contain the prize. He then asks\nyou if you would like to switch your choice to\nthe other unopened door. What is the probability\nto get the prize if you switch?\n\n(b) the Taxicab problem: 85% of taxis in this\ncity are green, the others are blue. A witness\nsees a blue taxi. She is usually correct with\nprobability 80%. What is the probability that\nthe taxi saw by the witness is blue?\n\n(c) Pick a move for black such that they can be\nalive.\n\n(d) Pick a move for black such that they can be\nalive by ko.\n\n(e) the boundary question: what river forms the\nboundary between North Korea and China?\n\n(f) the Middle Age New Year question: when\nwas the new year in middle age?\n\n(g) the pronunciation of \u7762\n\n(h) the pronunciation of \u6ec2\n\nFigure 4: The ranked answer-prediction matrices\n\n9\n\n2/31/21/32/31/21/3Answer34261413517146Prediction415080121520415080121520Answer160500008201200801619501104818000522130010218PredictionD3C1B2C2B1D3C1B2C2B1Answer62131091410042012115300002PredictionB8A8B6B7B8A8B6B7Answer112540630001230035PredictionYaluandDoomanYaluSonghuaYaluandDoomanYaluSonghuaAnswer33007410005PredictionApr1Dec25Jan1Apr1Dec25Jan1Answer15410028180633Predictionsui1ju1zhi4zhui1sui1ju1zhi4zhui1Answer115023161300210107Predictionpang1pang2pang1pang2Answer1511216Prediction\fWe compare our approach to the baseline, the plurality voting, regarding the accuracy of the top-\nranking answers. Both of the algorithms beat plurality voting for all studies and the default is slightly\nbetter. Among all 152 questions, in 138 questions, the variant algorithm outputs the same hierarchy as\nthe default algorithm. For other questions, the top-ranking type of the variant algorithm may contain\nmore than one answer. The top-ranking answer is the answer supported by more people among all\nanswers in the top-ranking type. In one question the variant is wrong but the default is correct, the\nvariant algorithm assigns both the correct answer and the incorrect plurality answer to the top-ranking\ntype, thus outputting the incorrect answer as the top-ranking answer.\n\nWe also compute the lack-of-fit index (Definition 2.6) of the algorithms and find that the questions\nthe algorithm outputs the correct answer have a smaller lack-of-fit thus fitting the thinking hierarchy\nmodel better. Therefore, we can use the lack-of-fit index as a reliability index of the algorithms.\n\nWe additionally pick several representative examples for each study (Figure 4) where the matrices\nare ranked by the default algorithm and the diagonal area modified like Example 1.1. In all of these\nexamples, the plurality is incorrect while our approach is correct. Results of other questions are\nillustrated at https://elicitation.info/classroom/1/. Detailed explanations are illustrated\nin Appendix D and here we provide some highlights. First, our approach elicits a rich hierarchy. For\nexample, the taxicab problem is borrowed from Kahneman [8] and previous studies show that people\nusually ignore the base rate and report \u201880%\u2019. The imagined levels can be 41%\u219280%. We elicit a\nmuch richer level \u201c41%\u219250%\u219280%\u219212%\u219215%\u219220%\u201d. Second, the most sophisticated level\nmay fail to predict the least one. In the Taxicab problem, the correct \u201c41%\u201d supporters successfully\npredict the common wrong answer \u201c80%\u201d. However, they fail to predict the surprisingly wrong\nanswers \u201c12%,20%\u201d, which are in contrast successfully predicted by \u201c80%\u201d supporters. Our model\nis sufficiently general to allow this situation. Third, even for problems (like Go) without famous\nmistakes, our approach still works. Moreover, in the boundary question, we identify the correct\nanswer without any prior when only 3 respondents are correct.\n\n\nBased on this section, do the authors discuss whether the data used/curated contains personally identifiable information or offensive content?"}]], "q5a": [[{"role": "user", "content": "The following is the abstract of the paper you are reviewing:\n\n\nWhen we use the wisdom of the crowds, we usually rank the answers according to\ntheir popularity, especially when we cannot verify the answers. However, this can\nbe very dangerous when the majority make systematic mistakes. A fundamental\nquestion arises: can we build a hierarchy among the answers without any prior\nwhere the higher-ranking answers, which may not be supported by the majority,\nare from more sophisticated people? To address the question, we propose 1) a\nnovel model to describe people\u2019s thinking hierarchy; 2) two algorithms to learn\nthe thinking hierarchy without any prior; 3) a novel open-response based crowd-\nsourcing approach based on the above theoretic framework. In addition to theoretic\njustifications, we conduct four empirical crowdsourcing studies and show that a)\nthe accuracy of the top-ranking answers learned by our approach is much higher\nthan that of plurality voting (In one question, the plurality answer is supported by\n74 respondents but the correct answer is only supported by 3 respondents. Our\napproach ranks the correct answer the highest without any prior); b) our model\nhas a high goodness-of-fit, especially for the questions where our top-ranking\nanswer is correct. To the best of our knowledge, we are the first to propose a\nthinking hierarchy model with empirical validations in the general problem-solving\nscenarios; and the first to propose a practical open-response based crowdsourcing\napproach that beats plurality voting without any prior.\n\n1\n\n\nBased on this abstract, do the authors use crowdsourcing or conduct research with human subjects? Please start your answer with \"Yes\", \"No\", or \"Unsure\"."}, {"role": "user", "content": "The following is the experiments section of the paper you are reviewing:\n\n\nWe conduct four studies, study 1 (35 math problems), study 2 (30 Go problems), study 3 (44 general\nknowledge questions), and study 4 (43 Chinese character pronunciation questions).\n\nData collection All studies are performed by online questionnaires. We recruit the respondents by\nan online announcement13 or from an online platform that is similar to Amazon Mechanical Turk. We\nget respondents\u2019 consent for using and sharing their data for research. Respondents are asked not to\nsearch for the answers to the questions or communicate with other people. We allow the respondents\nto answer \u201cI do not know\u201d for all questions. Except for Go problems, all questionnaires use flat\npayment. We illustrate the data collection process in detail in Appendix A. We allow respondents\nto participate in more than one study because our algorithms analyze each question separately and\nindependently.\n\nData processing We merge answers which are the same, like \u20180.5\u2019 and \u201850%\u2019. We omit the answers\nthat are reported by less than (\u2264) 3% of respondents or one person. The remaining answers, excluding\n\u201cI do not know\u201d, form the answer set A whose size is |A|. We then construct the Answer-Prediction\nmatrix and perform our algorithms. Pseudo-codes are attached in Appendix B. Our algorithms do not\nrequire any prior or the respondents\u2019 expertise levels.\n\n3.1 Results\n\n12This may not be a very good assumption since i.i.d. samples can repeat but respondents usually do not\nrepeat their predictions. If we do not want this assumption, we can choose to only use the first prediction from\neach respondent (if there exists) to construct the answer-prediction matrix.\n\n13Many are students from top universities in China. See Appendix A for more details.\n\n8\n\n\fType\nMath\nGo\nGeneral knowledge\nChinese character\n\nTotal Our algorithm(Default) Our algorithm(Variant)\n\n35\n30\n44\n43\n\n29\n28\n41\n36\n\n29\n28\n41\n35\n\nPlurality voting\n24\n23\n35\n34\n\nTable 1: The number of questions our algorithms/baseline are correct.\n\n(a) the Monty Hall problem: you can select one\nclosed door of three. A prize, a car, is behind\none of the doors. The other two doors hide goats.\nAfter you have made your choice, Monty Hall\nwill open one of the remaining doors and show\nthat it does not contain the prize. He then asks\nyou if you would like to switch your choice to\nthe other unopened door. What is the probability\nto get the prize if you switch?\n\n(b) the Taxicab problem: 85% of taxis in this\ncity are green, the others are blue. A witness\nsees a blue taxi. She is usually correct with\nprobability 80%. What is the probability that\nthe taxi saw by the witness is blue?\n\n(c) Pick a move for black such that they can be\nalive.\n\n(d) Pick a move for black such that they can be\nalive by ko.\n\n(e) the boundary question: what river forms the\nboundary between North Korea and China?\n\n(f) the Middle Age New Year question: when\nwas the new year in middle age?\n\n(g) the pronunciation of \u7762\n\n(h) the pronunciation of \u6ec2\n\nFigure 4: The ranked answer-prediction matrices\n\n9\n\n2/31/21/32/31/21/3Answer34261413517146Prediction415080121520415080121520Answer160500008201200801619501104818000522130010218PredictionD3C1B2C2B1D3C1B2C2B1Answer62131091410042012115300002PredictionB8A8B6B7B8A8B6B7Answer112540630001230035PredictionYaluandDoomanYaluSonghuaYaluandDoomanYaluSonghuaAnswer33007410005PredictionApr1Dec25Jan1Apr1Dec25Jan1Answer15410028180633Predictionsui1ju1zhi4zhui1sui1ju1zhi4zhui1Answer115023161300210107Predictionpang1pang2pang1pang2Answer1511216Prediction\fWe compare our approach to the baseline, the plurality voting, regarding the accuracy of the top-\nranking answers. Both of the algorithms beat plurality voting for all studies and the default is slightly\nbetter. Among all 152 questions, in 138 questions, the variant algorithm outputs the same hierarchy as\nthe default algorithm. For other questions, the top-ranking type of the variant algorithm may contain\nmore than one answer. The top-ranking answer is the answer supported by more people among all\nanswers in the top-ranking type. In one question the variant is wrong but the default is correct, the\nvariant algorithm assigns both the correct answer and the incorrect plurality answer to the top-ranking\ntype, thus outputting the incorrect answer as the top-ranking answer.\n\nWe also compute the lack-of-fit index (Definition 2.6) of the algorithms and find that the questions\nthe algorithm outputs the correct answer have a smaller lack-of-fit thus fitting the thinking hierarchy\nmodel better. Therefore, we can use the lack-of-fit index as a reliability index of the algorithms.\n\nWe additionally pick several representative examples for each study (Figure 4) where the matrices\nare ranked by the default algorithm and the diagonal area modified like Example 1.1. In all of these\nexamples, the plurality is incorrect while our approach is correct. Results of other questions are\nillustrated at https://elicitation.info/classroom/1/. Detailed explanations are illustrated\nin Appendix D and here we provide some highlights. First, our approach elicits a rich hierarchy. For\nexample, the taxicab problem is borrowed from Kahneman [8] and previous studies show that people\nusually ignore the base rate and report \u201880%\u2019. The imagined levels can be 41%\u219280%. We elicit a\nmuch richer level \u201c41%\u219250%\u219280%\u219212%\u219215%\u219220%\u201d. Second, the most sophisticated level\nmay fail to predict the least one. In the Taxicab problem, the correct \u201c41%\u201d supporters successfully\npredict the common wrong answer \u201c80%\u201d. However, they fail to predict the surprisingly wrong\nanswers \u201c12%,20%\u201d, which are in contrast successfully predicted by \u201c80%\u201d supporters. Our model\nis sufficiently general to allow this situation. Third, even for problems (like Go) without famous\nmistakes, our approach still works. Moreover, in the boundary question, we identify the correct\nanswer without any prior when only 3 respondents are correct.\n\n\nBased on this section, do the authors include the full text of instructions given to participants and screenshots, if applicable?"}]], "q5b": [[{"role": "user", "content": "The following is the abstract of the paper you are reviewing:\n\n\nWhen we use the wisdom of the crowds, we usually rank the answers according to\ntheir popularity, especially when we cannot verify the answers. However, this can\nbe very dangerous when the majority make systematic mistakes. A fundamental\nquestion arises: can we build a hierarchy among the answers without any prior\nwhere the higher-ranking answers, which may not be supported by the majority,\nare from more sophisticated people? To address the question, we propose 1) a\nnovel model to describe people\u2019s thinking hierarchy; 2) two algorithms to learn\nthe thinking hierarchy without any prior; 3) a novel open-response based crowd-\nsourcing approach based on the above theoretic framework. In addition to theoretic\njustifications, we conduct four empirical crowdsourcing studies and show that a)\nthe accuracy of the top-ranking answers learned by our approach is much higher\nthan that of plurality voting (In one question, the plurality answer is supported by\n74 respondents but the correct answer is only supported by 3 respondents. Our\napproach ranks the correct answer the highest without any prior); b) our model\nhas a high goodness-of-fit, especially for the questions where our top-ranking\nanswer is correct. To the best of our knowledge, we are the first to propose a\nthinking hierarchy model with empirical validations in the general problem-solving\nscenarios; and the first to propose a practical open-response based crowdsourcing\napproach that beats plurality voting without any prior.\n\n1\n\n\nBased on this abstract, do the authors use crowdsourcing or conduct research with human subjects? Please start your answer with \"Yes\", \"No\", or \"Unsure\"."}, {"role": "user", "content": "The following is the experiments section of the paper you are reviewing:\n\n\nWe conduct four studies, study 1 (35 math problems), study 2 (30 Go problems), study 3 (44 general\nknowledge questions), and study 4 (43 Chinese character pronunciation questions).\n\nData collection All studies are performed by online questionnaires. We recruit the respondents by\nan online announcement13 or from an online platform that is similar to Amazon Mechanical Turk. We\nget respondents\u2019 consent for using and sharing their data for research. Respondents are asked not to\nsearch for the answers to the questions or communicate with other people. We allow the respondents\nto answer \u201cI do not know\u201d for all questions. Except for Go problems, all questionnaires use flat\npayment. We illustrate the data collection process in detail in Appendix A. We allow respondents\nto participate in more than one study because our algorithms analyze each question separately and\nindependently.\n\nData processing We merge answers which are the same, like \u20180.5\u2019 and \u201850%\u2019. We omit the answers\nthat are reported by less than (\u2264) 3% of respondents or one person. The remaining answers, excluding\n\u201cI do not know\u201d, form the answer set A whose size is |A|. We then construct the Answer-Prediction\nmatrix and perform our algorithms. Pseudo-codes are attached in Appendix B. Our algorithms do not\nrequire any prior or the respondents\u2019 expertise levels.\n\n3.1 Results\n\n12This may not be a very good assumption since i.i.d. samples can repeat but respondents usually do not\nrepeat their predictions. If we do not want this assumption, we can choose to only use the first prediction from\neach respondent (if there exists) to construct the answer-prediction matrix.\n\n13Many are students from top universities in China. See Appendix A for more details.\n\n8\n\n\fType\nMath\nGo\nGeneral knowledge\nChinese character\n\nTotal Our algorithm(Default) Our algorithm(Variant)\n\n35\n30\n44\n43\n\n29\n28\n41\n36\n\n29\n28\n41\n35\n\nPlurality voting\n24\n23\n35\n34\n\nTable 1: The number of questions our algorithms/baseline are correct.\n\n(a) the Monty Hall problem: you can select one\nclosed door of three. A prize, a car, is behind\none of the doors. The other two doors hide goats.\nAfter you have made your choice, Monty Hall\nwill open one of the remaining doors and show\nthat it does not contain the prize. He then asks\nyou if you would like to switch your choice to\nthe other unopened door. What is the probability\nto get the prize if you switch?\n\n(b) the Taxicab problem: 85% of taxis in this\ncity are green, the others are blue. A witness\nsees a blue taxi. She is usually correct with\nprobability 80%. What is the probability that\nthe taxi saw by the witness is blue?\n\n(c) Pick a move for black such that they can be\nalive.\n\n(d) Pick a move for black such that they can be\nalive by ko.\n\n(e) the boundary question: what river forms the\nboundary between North Korea and China?\n\n(f) the Middle Age New Year question: when\nwas the new year in middle age?\n\n(g) the pronunciation of \u7762\n\n(h) the pronunciation of \u6ec2\n\nFigure 4: The ranked answer-prediction matrices\n\n9\n\n2/31/21/32/31/21/3Answer34261413517146Prediction415080121520415080121520Answer160500008201200801619501104818000522130010218PredictionD3C1B2C2B1D3C1B2C2B1Answer62131091410042012115300002PredictionB8A8B6B7B8A8B6B7Answer112540630001230035PredictionYaluandDoomanYaluSonghuaYaluandDoomanYaluSonghuaAnswer33007410005PredictionApr1Dec25Jan1Apr1Dec25Jan1Answer15410028180633Predictionsui1ju1zhi4zhui1sui1ju1zhi4zhui1Answer115023161300210107Predictionpang1pang2pang1pang2Answer1511216Prediction\fWe compare our approach to the baseline, the plurality voting, regarding the accuracy of the top-\nranking answers. Both of the algorithms beat plurality voting for all studies and the default is slightly\nbetter. Among all 152 questions, in 138 questions, the variant algorithm outputs the same hierarchy as\nthe default algorithm. For other questions, the top-ranking type of the variant algorithm may contain\nmore than one answer. The top-ranking answer is the answer supported by more people among all\nanswers in the top-ranking type. In one question the variant is wrong but the default is correct, the\nvariant algorithm assigns both the correct answer and the incorrect plurality answer to the top-ranking\ntype, thus outputting the incorrect answer as the top-ranking answer.\n\nWe also compute the lack-of-fit index (Definition 2.6) of the algorithms and find that the questions\nthe algorithm outputs the correct answer have a smaller lack-of-fit thus fitting the thinking hierarchy\nmodel better. Therefore, we can use the lack-of-fit index as a reliability index of the algorithms.\n\nWe additionally pick several representative examples for each study (Figure 4) where the matrices\nare ranked by the default algorithm and the diagonal area modified like Example 1.1. In all of these\nexamples, the plurality is incorrect while our approach is correct. Results of other questions are\nillustrated at https://elicitation.info/classroom/1/. Detailed explanations are illustrated\nin Appendix D and here we provide some highlights. First, our approach elicits a rich hierarchy. For\nexample, the taxicab problem is borrowed from Kahneman [8] and previous studies show that people\nusually ignore the base rate and report \u201880%\u2019. The imagined levels can be 41%\u219280%. We elicit a\nmuch richer level \u201c41%\u219250%\u219280%\u219212%\u219215%\u219220%\u201d. Second, the most sophisticated level\nmay fail to predict the least one. In the Taxicab problem, the correct \u201c41%\u201d supporters successfully\npredict the common wrong answer \u201c80%\u201d. However, they fail to predict the surprisingly wrong\nanswers \u201c12%,20%\u201d, which are in contrast successfully predicted by \u201c80%\u201d supporters. Our model\nis sufficiently general to allow this situation. Third, even for problems (like Go) without famous\nmistakes, our approach still works. Moreover, in the boundary question, we identify the correct\nanswer without any prior when only 3 respondents are correct.\n\n\nBased on this section, do the authors describe any potential participant risks, with links to Institutional Review Board (IRB) approvals, if applicable?"}]], "q5c": [[{"role": "user", "content": "The following is the abstract of the paper you are reviewing:\n\n\nWhen we use the wisdom of the crowds, we usually rank the answers according to\ntheir popularity, especially when we cannot verify the answers. However, this can\nbe very dangerous when the majority make systematic mistakes. A fundamental\nquestion arises: can we build a hierarchy among the answers without any prior\nwhere the higher-ranking answers, which may not be supported by the majority,\nare from more sophisticated people? To address the question, we propose 1) a\nnovel model to describe people\u2019s thinking hierarchy; 2) two algorithms to learn\nthe thinking hierarchy without any prior; 3) a novel open-response based crowd-\nsourcing approach based on the above theoretic framework. In addition to theoretic\njustifications, we conduct four empirical crowdsourcing studies and show that a)\nthe accuracy of the top-ranking answers learned by our approach is much higher\nthan that of plurality voting (In one question, the plurality answer is supported by\n74 respondents but the correct answer is only supported by 3 respondents. Our\napproach ranks the correct answer the highest without any prior); b) our model\nhas a high goodness-of-fit, especially for the questions where our top-ranking\nanswer is correct. To the best of our knowledge, we are the first to propose a\nthinking hierarchy model with empirical validations in the general problem-solving\nscenarios; and the first to propose a practical open-response based crowdsourcing\napproach that beats plurality voting without any prior.\n\n1\n\n\nBased on this abstract, do the authors use crowdsourcing or conduct research with human subjects? Please start your answer with \"Yes\", \"No\", or \"Unsure\"."}, {"role": "user", "content": "The following is the experiments section of the paper you are reviewing:\n\n\nWe conduct four studies, study 1 (35 math problems), study 2 (30 Go problems), study 3 (44 general\nknowledge questions), and study 4 (43 Chinese character pronunciation questions).\n\nData collection All studies are performed by online questionnaires. We recruit the respondents by\nan online announcement13 or from an online platform that is similar to Amazon Mechanical Turk. We\nget respondents\u2019 consent for using and sharing their data for research. Respondents are asked not to\nsearch for the answers to the questions or communicate with other people. We allow the respondents\nto answer \u201cI do not know\u201d for all questions. Except for Go problems, all questionnaires use flat\npayment. We illustrate the data collection process in detail in Appendix A. We allow respondents\nto participate in more than one study because our algorithms analyze each question separately and\nindependently.\n\nData processing We merge answers which are the same, like \u20180.5\u2019 and \u201850%\u2019. We omit the answers\nthat are reported by less than (\u2264) 3% of respondents or one person. The remaining answers, excluding\n\u201cI do not know\u201d, form the answer set A whose size is |A|. We then construct the Answer-Prediction\nmatrix and perform our algorithms. Pseudo-codes are attached in Appendix B. Our algorithms do not\nrequire any prior or the respondents\u2019 expertise levels.\n\n3.1 Results\n\n12This may not be a very good assumption since i.i.d. samples can repeat but respondents usually do not\nrepeat their predictions. If we do not want this assumption, we can choose to only use the first prediction from\neach respondent (if there exists) to construct the answer-prediction matrix.\n\n13Many are students from top universities in China. See Appendix A for more details.\n\n8\n\n\fType\nMath\nGo\nGeneral knowledge\nChinese character\n\nTotal Our algorithm(Default) Our algorithm(Variant)\n\n35\n30\n44\n43\n\n29\n28\n41\n36\n\n29\n28\n41\n35\n\nPlurality voting\n24\n23\n35\n34\n\nTable 1: The number of questions our algorithms/baseline are correct.\n\n(a) the Monty Hall problem: you can select one\nclosed door of three. A prize, a car, is behind\none of the doors. The other two doors hide goats.\nAfter you have made your choice, Monty Hall\nwill open one of the remaining doors and show\nthat it does not contain the prize. He then asks\nyou if you would like to switch your choice to\nthe other unopened door. What is the probability\nto get the prize if you switch?\n\n(b) the Taxicab problem: 85% of taxis in this\ncity are green, the others are blue. A witness\nsees a blue taxi. She is usually correct with\nprobability 80%. What is the probability that\nthe taxi saw by the witness is blue?\n\n(c) Pick a move for black such that they can be\nalive.\n\n(d) Pick a move for black such that they can be\nalive by ko.\n\n(e) the boundary question: what river forms the\nboundary between North Korea and China?\n\n(f) the Middle Age New Year question: when\nwas the new year in middle age?\n\n(g) the pronunciation of \u7762\n\n(h) the pronunciation of \u6ec2\n\nFigure 4: The ranked answer-prediction matrices\n\n9\n\n2/31/21/32/31/21/3Answer34261413517146Prediction415080121520415080121520Answer160500008201200801619501104818000522130010218PredictionD3C1B2C2B1D3C1B2C2B1Answer62131091410042012115300002PredictionB8A8B6B7B8A8B6B7Answer112540630001230035PredictionYaluandDoomanYaluSonghuaYaluandDoomanYaluSonghuaAnswer33007410005PredictionApr1Dec25Jan1Apr1Dec25Jan1Answer15410028180633Predictionsui1ju1zhi4zhui1sui1ju1zhi4zhui1Answer115023161300210107Predictionpang1pang2pang1pang2Answer1511216Prediction\fWe compare our approach to the baseline, the plurality voting, regarding the accuracy of the top-\nranking answers. Both of the algorithms beat plurality voting for all studies and the default is slightly\nbetter. Among all 152 questions, in 138 questions, the variant algorithm outputs the same hierarchy as\nthe default algorithm. For other questions, the top-ranking type of the variant algorithm may contain\nmore than one answer. The top-ranking answer is the answer supported by more people among all\nanswers in the top-ranking type. In one question the variant is wrong but the default is correct, the\nvariant algorithm assigns both the correct answer and the incorrect plurality answer to the top-ranking\ntype, thus outputting the incorrect answer as the top-ranking answer.\n\nWe also compute the lack-of-fit index (Definition 2.6) of the algorithms and find that the questions\nthe algorithm outputs the correct answer have a smaller lack-of-fit thus fitting the thinking hierarchy\nmodel better. Therefore, we can use the lack-of-fit index as a reliability index of the algorithms.\n\nWe additionally pick several representative examples for each study (Figure 4) where the matrices\nare ranked by the default algorithm and the diagonal area modified like Example 1.1. In all of these\nexamples, the plurality is incorrect while our approach is correct. Results of other questions are\nillustrated at https://elicitation.info/classroom/1/. Detailed explanations are illustrated\nin Appendix D and here we provide some highlights. First, our approach elicits a rich hierarchy. For\nexample, the taxicab problem is borrowed from Kahneman [8] and previous studies show that people\nusually ignore the base rate and report \u201880%\u2019. The imagined levels can be 41%\u219280%. We elicit a\nmuch richer level \u201c41%\u219250%\u219280%\u219212%\u219215%\u219220%\u201d. Second, the most sophisticated level\nmay fail to predict the least one. In the Taxicab problem, the correct \u201c41%\u201d supporters successfully\npredict the common wrong answer \u201c80%\u201d. However, they fail to predict the surprisingly wrong\nanswers \u201c12%,20%\u201d, which are in contrast successfully predicted by \u201c80%\u201d supporters. Our model\nis sufficiently general to allow this situation. Third, even for problems (like Go) without famous\nmistakes, our approach still works. Moreover, in the boundary question, we identify the correct\nanswer without any prior when only 3 respondents are correct.\n\n\nBased on this section, do the authors include the estimated hourly wage paid to participants and the total amount spent on participant compensation?"}]]}}