{"paper_index": 19, "title": "Eliciting Thinking Hierarchy without a Prior", "abstract": "\n\nWhen we use the wisdom of the crowds, we usually rank the answers according to\ntheir popularity, especially when we cannot verify the answers. However, this can\nbe very dangerous when the majority make systematic mistakes. A fundamental\nquestion arises: can we build a hierarchy among the answers without any prior\nwhere the higher-ranking answers, which may not be supported by the majority,\nare from more sophisticated people? To address the question, we propose 1) a\nnovel model to describe people\u2019s thinking hierarchy; 2) two algorithms to learn\nthe thinking hierarchy without any prior; 3) a novel open-response based crowd-\nsourcing approach based on the above theoretic framework. In addition to theoretic\njustifications, we conduct four empirical crowdsourcing studies and show that a)\nthe accuracy of the top-ranking answers learned by our approach is much higher\nthan that of plurality voting (In one question, the plurality answer is supported by\n74 respondents but the correct answer is only supported by 3 respondents. Our\napproach ranks the correct answer the highest without any prior); b) our model\nhas a high goodness-of-fit, especially for the questions where our top-ranking\nanswer is correct. To the best of our knowledge, we are the first to propose a\nthinking hierarchy model with empirical validations in the general problem-solving\nscenarios; and the first to propose a practical open-response based crowdsourcing\napproach that beats plurality voting without any prior.\n\n1\n\n", "introduction": "\n\nThe wisdom of the crowds has been proved to lead to better decision-making and problem-solving\nthan that of an individual, especially when we do not have sufficient prior knowledge to identify\nindividual experts [26, 2, 28]. Plurality is one of the most popular ways to aggregate the crowd\u2019s\nopinions. The opinions are usually ranked according to their popularity. However, it can be very\n\n\u2217Corresponding Author\n\u2020This author is currently at Stanford University\n\u2021This author is currently at Yale University\n\n36th Conference on Neural Information Processing Systems (NeurIPS 2022).\n\n\fdangerous when the majority are systematically biased. Here is a real-world study we perform. We\nhave asked multiple top university students the following question.\n\nThe radius of Circle A is 1/3 the radius of Circle B. Circle A rolls around Circle B one trip back to its\nstarting point. How many times will Circle A revolve in total?\n\nFigure 1: Collected answers of the circle problem\n\nWe have collected answers \u201c1 (11 people), 2 (8 people), 3 (134 people), 4 (16 people), 6 (27 people),\n9 (21 people)\u201d. The plurality answer is \u201c3\u201d, the ratio between the big circle\u2019s circumference and the\nsmall circle\u2019s. However, the correct answer is \u201c4\u201d4 which is only supported by 16 people.\n\nWith prior knowledge like the expertise level of each individual respondent, we may be able to\nidentify the correct answer. However, sometimes it\u2019s quite expensive and difficult to obtain prior\nknowledge, especially in new fields.\n\nTo address the above issue, Prelec et al. [19] propose an innovative approach, surprisingly popular.\nThey prepare multiple choices, ask the respondents to pick one option, and more importantly, predict\nthe distribution over other people\u2019s choices. They use the predictions to construct a prior distribution\nover the choices, and then select the choice that is more popular than the prior such that the bias\nis corrected. Many other work [11, 6, 20] develop the idea of using the prior or the predictions to\ncorrect the bias.\n\nHowever, first, it\u2019s not applicable to employ the previous approaches into the running example, the\ncircle problem, because they require prior knowledge to design the choices. It\u2019s also effortful for\nrespondents to report a whole distribution over all choices.\n\nSecond, previous works focus on using the predictions to correct bias, while it\u2019s intrinsically inter-\nesting to build a thinking hierarchy using their predictions. This leads to a hierarchy among the\nanswers as well. The famous cognitive hierarchy theory (CHT) [24, 25, 3] builds a thinking theory\nin the scenarios when people play games such that we can learn the actions of players of different\nsophistication levels. Nevertheless, CHT is designed only for a game-theoretic setting.\n\nWe are curious about building a thinking theory in general problem-solving scenarios. The key insight\nis that people of a more sophisticated level know the mind of lower levels, but not vice versa [3, 10].\nWe want to apply the insight to learn the answers of people of different sophistication levels, called\nthe thinking hierarchy, without any prior.\n\nKey question We aim to build a practical approach to learn the thinking hierarchy without any\nprior. Based on the thinking hierarchy, we can rank the answers such that the higher-ranking answers,\nwhich may not be supported by the majority, are from more sophisticated people.\n\nIn addition to building a thinking theory in the general problem-solving scenarios, in practice, there\nare multiple reasons why we want the hierarchy rather than only the best. First, for some questions\nlike subjective questions (e.g. why are bar chairs high?), there may be more than one high-quality\nanswer and the full hierarchy provides a richer result. Second, the hierarchy among the answers helps\nto understand how people think better, which is important especially when we try to elicit people\u2019s\nopinions about a policy.\n\nOur approach We follow the framework of asking for answers and predictions simultaneously\nand extend it to a more practical open-response based paradigm. The paradigm asks a single open\n\n4Interested readers are referred to https://math.stackexchange.com/questions/1351058/\n\ncircle-revolutions-rolling-around-another-circle for explanations.\n\n2\n\n\f(a) Original\n\n(b) Ranked\n\nFigure 2: The empirical results of the circle problem.\n\nresponse question and asks for both each respondent\u2019s answer and prediction 5 for other people\u2019s\nanswers. For example, in the circle problem, a respondent can provide: answer: \u201c4\u201d, prediction: \u201c3\u201d.\nWe then construct an answer-prediction matrix A that records the number of people who report a\nspecific answer-prediction pair (e.g. Figure 2(a) shows that 28 people answer \u201c3\u201d and predict other\npeople answer \u201c6\u201d.).\n\nTo learn the thinking hierarchy, we propose a novel model. Our model describes how people of\ndifferent sophistication levels answer the question and more importantly, predict other people\u2019s\nanswers. The joint distribution over a respondent\u2019s answer and prediction depends on the latent\nparameters that describe people\u2019s thinking hierarchy. We show that given the joint distribution\nover a respondent\u2019s answer and prediction, we can infer the latent thinking hierarchy by solving\na new variant of the non-negative matrix factorization problem, called non-negative congruence\ntriangularization (NCT), which may be of an independent interest. Based on the analysis of NCT, we\nprovide two simple answer-ranking algorithms and show that with proper assumptions, the algorithms\nwill learn the latent thinking hierarchy given the joint distribution over a respondent\u2019s answer and\nprediction.\n\nFinally, we show that the answer-prediction matrix collected by the paradigm is a proxy for the joint\ndistribution over a respondent\u2019s answer and prediction. We implement the NCT based answer-ranking\nalgorithms on the answer-prediction matrix. The default algorithm ranks the answers to maximize the\nsum of the square of the elements in the upper triangular area of the matrix. In a variant version, to\nallow different answers to have the same sophistication level, the answers are partitioned to compress\nthe matrix. The algorithm maximizes the sum of the square of the upper triangular area of the\ncompressed matrix.\n\nIn addition to the above theoretic framework, we also run empirical studies by asking people questions\nin various areas including math, Go, general knowledge, and character pronunciation.\n\nExample 1.1 (Empirical results of the circle problem). In the circle problem, we have collected the\nempirical answer-prediction matrix (Figure 2(a)) and ranked it (Figure 2(b)) based on the default\nalgorithm. The default ranking algorithm does not use any diagonal element. Thus, for ease of\nillustration, the diagonal elements are modified (i.e., for all a, Aa,a is modified to the number of\nrespondents who answer a) such that we can compare our method to the plurality voting visually.\n\nMore empirical results will be illustrated in Section 3.1. We show the superiority of our algorithm\nby comparing our algorithm to plurality voting by the accuracy of the top-ranking answers. We also\ntest the goodness-of-fit of our model based on the collected data set. To summarize, we provide a\nnovel theoretic framework to study people\u2019s thinking hierarchy in the problem-solving scenarios and\na practical open-response based crowdsourcing approach that outputs a high-quality answer only\nsupported by 16 people when the wrong answer is supported by 134 people (another question is 3 vs.\n74) without any prior.\n\n5Unlike previous work, the prediction in our model is not a distribution but an answer the respondent thinks\n\nother people may answer.\n\n3\n\n369412369412Answer1342821452132720007121000110016012000112500008Prediction423691423691Answer1611100008500042134282150013272000712100220011Prediction\f1.1 Related work\n\nInformation aggregation with the second order information Prelec [18], Prelec et al. [19] start\nthe framework that asks the respondents both their answers and predictions for the distribution over\nother people\u2019s answers. However, to implement the framework, the requester needs to design a\nmulti-choice question whose choices may require prior knowledge. The effort for a distribution\nreport is non-minimal to many respondents and the quality can be an issue since most people are not\nperfect Bayesian. Kong and Schoenebeck [10] study how to elicit thinking hierarchy theoretically by\npeople\u2019s predictions and also assumes that more sophisticated people can reason about the mind of\nless sophisticated people. However, they focus on multi-choice questions, and in their framework,\nagents either need to perform multiple tasks or report non-minimal distributions. Thus, it\u2019s difficult to\nperform empirical studies using their framework and there is also no empirical validation. Moreover,\nthey assume more sophisticated people can reason about the mind of ALL less sophisticated people\nwhile our model allows more sophisticated people cannot reason about some less sophisticated people.\nTwo recent works Hosseini et al. [7], Schoenebeck and Tao [21] study how to aggregate people\u2019s\nvotes to rank a set of predetermined candidates (e.g. ranking paintings based on price [7], or two\npresidential candidates [21]) better by using people\u2019s predictions. Both of them treat the pairwise\ncomparisons of the candidates as the elicited signals and use people\u2019s predictions to improve the\nsignal quality. In contrast, rather than eliciting ranking based on price or preference, we elicit the\nthinking hierarchy among people by using people\u2019s predictions for other people to determine the\nhierarchy over the answers. Like Prelec et al. [19], Hosseini et al. [7], Schoenebeck and Tao [21],\nthere are other works that use people\u2019s predictions to reduce the bias of the collected feedback. For\nexample, Dasgupta et al. [6] use people\u2019s predictions to reduce the bias caused by interactions between\nusers on a social network. Rothschild and Wolfers [20] show that voters\u2019 expectations for other\npeople\u2019s votes are more informative than their intentions. In addition to the discrete setting, recently,\na growing literature, including Palley and Soll [16], Martinie et al. [12], Chen et al. [4], Wilkening\net al. [29], Palley and Satop\u00e4\u00e4 [15], Peker [17], aggregate forecasts with additional second order\ninformation like each forecaster\u2019s expectation for the average of other forecasters\u2019 forecasts.\n\nPeer prediction Starting from Miller et al. [13], a series of work (e.g.[13, 18, 5, 22, 11, 9]) focus\non eliciting information without a prior by designing incentive-compatible mechanisms. This field is\ncalled peer prediction, or information elicitation without prior. In contrast, this work focus on how to\naggregate information and identify high-quality information without a prior. Liu et al. [11], Prelec\net al. [19], Hosseini et al. [7], Schoenebeck and Tao [21] focus on information aggregation without\nprior. However, they do not focus on the problem of learning thinking hierarchy like this work.\n\nBounded rationality Starting from Simon [23], the term \u201cbounded rationality\u201d describes the\ndecision maker\u2019s cognitive limitations. Stahl [24] propose a behavioral model of bounded rationality\nto predict people\u2019s behaviors in strategic games, the level-k theory. Level-k theory assumes that\nplayers have different levels of sophistication. Level-0 players play non-strategically, level-1 players\nplay optimal response to level-0 players... A variant of level-k theory is cognitive hierarchy theory\n[25, 3] where level-k players believe that lower-level players\u2019 percentages follow a certain type\nof distribution. Our Thinking hierarchy model is conceptually similar to level-k but focuses on a\nnon-game setting. Moreover, the level-k theory focus on estimating the average levels of a population\nwhile we focus on identifying each respondent\u2019s level.\n\n", "methods": "\n\nWe first introduce our model for thinking hierarchy. Tversky and Kahneman [27] propose that people\nhave two systems, a fast and intuitive system, and a slow and logical system. For example, Alice\nstarts to solve the circle problem. When she reads the question, she can run her intuitive system 1 and\nobtain answer \u201c3\u201d. However, when she starts to think carefully, she runs her more careful system 2 to\nobtain answer \u201c4\u201d.\n\nWe propose a more general model where people can run multiple oracles to approach the question\nduring their thinking process. Conceptually similar to the cognitive hierarchy theory [3], we assume\nthe oracles have different levels. People usually run lower level oracles before the higher level oracles.\nIn the previous example, system 1 is the lower level oracle and system 2 is the higher one. We assume\nthat Alice runs system 2 after system 1. Thus, in our model, people who know the answer is \u201c4\u201d can\n\n4\n\n\fpredict that other people answer \u201c3\u201d, which is hypothesized in Kong and Schoenebeck [10]. It\u2019s also\npossible that some people run the most sophisticated oracle directly without running the lower ones.\nThus, we design the model such that it does not require the higher-type to be able to predict ALL\nlower types. We also allow the oracles\u2019 outputs to be random.\n\nSection 2.1 introduces the model of thinking hierarchy. Section 2.2 reduces the model inference\nproblem to a matrix decomposition problem. Section 2.3 and section 2.4 show how to learn the\nthinking hierarchy.\n\n2.1 Thinking hierarchy\n\nFixing a question q (e.g. the circle problem), T denotes the set of thinking types. A denotes the set of\npossible answers. Both T and A are finite sets. \u2206A denotes all possible distributions over A. We\nsometimes use \u201cprob\u201d as a shorthand for probability.\n\nGenerating answers We will describe how people of different thinking types generate answers.\nDefinition 2.1 (Oracles of thinking types W ). An answer generating oracle maps the question to\nan (random) answer in A. Each type t corresponds to an oracle Ot. The output Ot(q) is a random\nvariable whose distribution is wt \u2208 \u2206A. W denotes a |T | \u00d7 |A| matrix where each row t is wt.\nEach respondent is type t with prob pt and (cid:80)\nrunning the oracle Ot. For all a \u2208 A, the probability that a respondent answers a will be (cid:80)\nWe assume the probability is positive for all a \u2208 A.\nExample 2.2 (A running example). There are two types T = {0, 1}. The answer space is A =\n{3, 4, 6}. O0 will output \u20183\u2019 with probability 0.8 and \u20186\u2019 with probability 0.2. O1 will output \u20184\u2019\n0\n1\n\nt pt = 1. A type t respondent generates the answer by\nt ptwt(a).\n\ndeterministically. In this example, W =\n\nwhere the first row is the distribution of\n\n(cid:20)0.8\n0\n\n0.2\n0\n\n(cid:21)\n\nO0\u2019s output and the second row is the distribution of O1\u2019s output.\n\nGenerating predictions We then describe how people of different thinking types predict what other\npeople will answer. Here the prediction is not a distribution, but an answer other people may report.\nWhen a type t respondent makes a prediction, she will run an oracle, which is Ot\u2032 with probability\npt\u2192t\u2032 where (cid:80)\n\nt\u2032 pt\u2192t\u2032 = 1. She uses the output of Ot\u2032 as the prediction g \u2208 A.\n\nCombination: answer-prediction joint distribution M M denotes a |A| \u00d7 |A| matrix where\nMa,g is the probability an respondent answers a and predicts g. \u039b denotes a |T | \u00d7 |T | matrix where\n\u039bt,t\u2032 = ptpt\u2192t\u2032 is the probability a respondent is type t and predicts type t\u2032.\nExample 2.3. In this example, when type 0 respondent makes a prediction, with prob 1, she will run\nO0 again. When type 1 respondent makes a prediction, with prob 0.5, she will run O1 again, with\nprob 0.5, she will run O0. Moreover, a respondent is type 0 with prob 0.7, and type 1 with prob 0.3.\n\nHere \u039b =\n\n(cid:20)p0p0\u21920\np1p1\u21920\n\np0p0\u21921\np1p1\u21921\n\n(cid:21)\n\n=\n\n(cid:20) 0.7 \u2217 1\n0.3 \u2217 0.5\n\n0.7 \u2217 0\n0.3 \u2217 0.5\n\n(cid:21)\n\n=\n\n(cid:20) 0.7\n0.15\n\n(cid:21)\n\n.\n\n0\n0.15\n\nClaim 2.4. Based on the above generating processes, M = W\u22a4\u039bW.\n\nProof. For each respondent, the probability she answers a and predicts g will be\n(cid:88)\n\n(cid:88)\n\n(cid:88)\n\nptwt(a)\n\npt\u2192t\u2032wt\u2032(g) =\n\nwt(a)ptpt\u2192t\u2032wt\u2032(g).\n\nMa,g =\n\nt\n\nt\u2032\n\nt,t\u2032\n\nWe sum over all possible types the respondent will be. Given she is type t, she runs oracle Ot to\ngenerate the answer and wt(a) is the probability that the answer is a. We sum over all possible\noracles she runs to predict. Given that she runs Ot\u2032, wt\u2032(g) is the probability the prediction is g.\n\nKey assumption: upper-triangular \u039b we assume that people of a less sophisticated level can\nnever run the oracles of more sophisticated levels. A linear ordering of types \u03c0 : {1, 2, \u00b7 \u00b7 \u00b7 , |T |} (cid:55)\u2192 T\nmaps a ranking position to a type. For example, \u03c0(1) \u2208 T is the top-ranking type.\nAssumption 2.5. We assume that with a proper ordering \u03c0 of the types, \u039b is an upper-triangular\nmatrix. Formally, there exists \u03c0 such that \u2200i > j, \u039b\u03c0(i),\u03c0(j) = 0. Any \u03c0 that makes \u039b upper-\ntriangular is a valid thinking hierarchy of the types.\n\n5\n\n\fIn the running example, the valid thinking hierarchy is \u03c0(1) = type 1, \u03c0(2) = type 0. Note that the\nabove assumption does not require \u2200i \u2264 j, \u039b\u03c0(i),\u03c0(j) > 0. When \u039b is a diagonal matrix, types cannot\npredict each other and are equally sophisticated, thus any ordering is a valid thinking hierarchy.\n\nAn algorithm finds the thinking hierarchy when the algorithm is given M which is generated by\nlatent (unknown) W, \u039b, and the algorithm will output a matrix W\u2217 which equals a row-permuted\nW where the row order is a valid thinking hierarchy. Formally, there exists a valid thinking hierarchy\n\u03c0 such that the ith row of W\u2217 is the \u03c0(i)th row of W, i.e, w\u2217\n\ni = w\u03c0(i).\n\n2.2 Non-negative Congruence Triangularization (NCT)\n\nWith the above model, inferring thinking hierarchy leads to a novel matrix decomposition problem,\nwhich is similar to the symmetric non-negative matrix factorization problem (NMF)6. A non-negative\nmatrix is a matrix whose elements are non-negative.\nDefinition 2.6 (Non-negative Congruence7 Triangularization (NCT)). Given a non-negative matrix\nM, NCT aims to find non-negative matrices W and non-negative upper-triangular matrix \u039b such\nthat M = W\u22a4\u039bW. In a Frobenius norm based approximated version, given a set of matrices W,\nNCT aims to find non-negative matrices W and non-negative upper-triangular matrix \u039b to minimize\n\nmin\nW\u2208W,\u039b\n\n||M \u2212 W\u22a4\u039bW||2\nF\n\nand the minimum is defined as the lack-of-fit of M regarding W 8.\n\nLike NMF, it\u2019s impossible to ask for the strict uniqueness of the results. Let P\u039b be the set of\npermutation matrices such that \u03a0\u22a4\u039b\u03a0 is still upper-triangular.\nIf (W, \u039b) is a solution, then\n(\u03a0\u22121DW, \u03a0\u22a4D\u22121\u039bD\u22121\u03a0) is also a solution where D is a diagonal matrix with positive elements\nand \u03a0 \u2208 P\u039b. We state the uniqueness results as follows and the proof is deferred to Appendix C.\nProposition 2.7 (Uniqueness). If |T | \u2264 |A| and T columns of W consist of a permuted positive\ndiagonal matrix, NCT for M = W\u22a4\u039bW is unique in the sense that then for all W\u2032\u22a4\u039b\u2032W\u2032 =\nW\u22a4\u039bW, there exists a positive diagonal matrix D and a |T | \u00d7 |T | permutation matrix \u03a0 \u2208 P\u039b\nsuch that W\u2032 = \u03a0\u22121DW.\n\nWhen we restrict W to be \u201csemi-orthogonal\u201d, we obtain a clean format of NCT without searching\nfor optimal \u039b. I is the set of all \u201csemi-orthogonal\u201d matrices W where each column of W has\nand only has one non-zero element and WW\u22a4 = I. For example, the W in Example 2.2 can be\nnormalized to a semi-orthogonal matrix. The following lemma follows from the expansion of the\nFrobenius norm and we defer the proof to Appendix C.\n\nLemma 2.8 (Semi-orthogonal: minimizing F-norm = maximizing upper-triangular\u2019s sum of the\nsquare). For all set of matrices W \u2282 I, minW\u2208W,\u039b ||M \u2212 W\u22a4\u039bW||2\nF is equivalent to solv-\ni,j and setting \u039b as Up(WMW\u22a4), the upper-triangular area of\ning maxW\u2208W\nWMW\u22a4.\n\ni\u2264j(WMW\u22a4)2\n\n(cid:80)\n\n2.3\n\nInferring the thinking hierarchy with answer-prediction joint distribution M\n\nGiven M, inferring the thinking hierarchy is equivalent to solving NCT in general. Though we do not\nhave M, later we will show a proxy for M. For simplicity of practical use, we introduce two simple\nranking algorithms by employing Lemma 2.8. The ranking algorithm takes M as input and outputs a\nlinear ordering of answers \u03c0 : {1, 2, \u00b7 \u00b7 \u00b7 , |A|} (cid:55)\u2192 A that maps a ranking position to an answer.\n\nAnswer-Ranking Algorithm (Default) AR(M) The algorithm computes\n\n\u03a0\u2217 \u2190 arg max\n\u03a0\u2208P\n\n(cid:88)\n\n(\u03a0M\u03a0\u22a4)2\ni,j\n\ni\u2264j\n\n6Symmetric NMF: minW ||M \u2212 W\u22a4W||2\nF\n7We use congruence here though it is not matrix congruence since W may not be a square matrix.\n8M = W\u22a4\u039bW, W \u2208 W has zero lack-of-fit.\n\n6\n\n\fwhere P is the set of all |A| \u00d7 |A| permutation matrices. There is a one to one mapping between each\npermutation matrix \u03a0 and a linear ordering \u03c0: \u03a0i,\u03c0(i) = 1, \u2200i. Therefore, the optimal \u03a0\u2217 leads to an\noptimal rank over answers directly and the default algorithm can be also represented as\n\n\u03c0\u2217 \u2190 arg max\n\n\u03c0\n\nM 2\n\n\u03c0(i),\u03c0(j).\n\n(cid:88)\n\ni\u2264j\n\nTo find the optimal rank, we use a dynamic programming based algorithm (see the supplementary\nmaterials) which takes O(2|A||A|2). In practice, |A| is usually at most 7 or 8. In our empirical study,\nthe default algorithm takes 91 milliseconds to finish the computation of all 152 questions.\n\nThe default algorithm implicitly assumes |T | = |A| and all oracles are deterministic. To allow\n|T | < |A| and non-deterministic oracles, we introduce a variant that generalizes P to a subset of\nsemi-orthogonal matrices I. Every |T | \u00d7 |A| semi-orthogonal W indicates a hard clustering. Each\ncluster t \u2208 T contains all answers a such that Wt,a > 0. For example, the W in Example 2.2 can\nbe normalized to a semi-orthogonal matrix and indicates a hard clustering {4}, {6, 3}. Therefore,\nthe variant algorithm will partition the answers into multiple clusters and assign a hierarchy to the\nclusters.\n\nAnswer-Ranking Algorithm (Variant) AR+(M, W) The algorithm computes\n\nW\u2217 \u2190 arg max\nW\u2208I\n\n(cid:88)\n\n(WMW\u22a4)2\n\ni,j.\n\ni\u2264j\n\nwhere W \u2282 I. W\u2217 is normalized such that every row sums to 1. This algorithm does not restrict\n|T | = |A| and learns the optimal |T |.\n\nW\u2217 \u21d2 Answer rank The output W\u2217 indicates a hard clustering of all answers. We rank all answer\nas follows: for any i < j, the answers in cluster i has a higher rank 9 than the answers in cluster j.\nFor all i, for any two answers a, a\u2032 in the same cluster i, a is ranked higher than a\u2032 if W \u2217\ni,a\u2032.\n\ni,a > W \u2217\n\nTheoretical justification When M perfectly fits the model with the restriction that the latent W is\na permutation or a hard clustering, we show that our algorithm finds the thinking hierarchy. Otherwise,\nour algorithm finds the \u201cclosest\u201d solution measured by the Frobenius norm.\nTheorem 2.9. When there exists \u03a00 \u2208 P and non-negative upper-triangular matrix \u039b0 such that\n0 \u039b0\u03a00, AR(M) finds the thinking hierarchy 10. In general, AR(M) will output \u03a0\u2217 where\nM = \u03a0\u22a4\n\u03a0\u2217, \u039b\u2217 = Up(\u03a0\u2217M\u03a0\u2217\u22a4) is a solution to arg min\u03a0\u2208P,\u039b ||M \u2212 \u03a0\u22a4\u039b\u03a0||2\nF . The above statement\nis still valid by replacing P by W \u2282 I and AR(M) by AR+(M, W).\n\nProposition 2.7 and Lemma 2.8 almost directly imply the above theorem. We defer the formal proof\nto Appendix C.\n\n2.4 A proxy for answer-prediction joint distribution M\n\nIn practice, we do not have perfect M. We use the following open-response paradigm to obtain a\nproxy for M.\n\nAnswer-prediction paradigm the respondents are asked Q1: What\u2019s your answer? Answer:\nQ2: What do you think other people will answer:\n\n.\n\nIn the circle example, the possible feedback can be \u201canswer: 4; prediction: 3\u201d, \u201canswer: 3; prediction:\n6,9,1\u201d... We collect all answers provided by the respondents 11 and denote the set of them by A. In\nthe circle example, A = {1, 2, 3, 4, 6, 9}. We also allow respondents to provide no prediction or\nmultiple predictions.\n\n9Rank 1 answer is in the highest position.\n10See definition in the last paragraph in Section 2.1.\n11In practice, we set a threshold \u03b8 and collect answers which are provided by at least \u03b8 fraction of the\nrespondents. We allow multiple predictions and also allow people to answer \u201cI do not know\u201d. See Section 3 for\nmore details.\n\n7\n\n\f(a) Accuracy of algorithms\n\n(b) Empirical distribution of lack-of-fit\n\nFigure 3: The results of our experiment\n\nAnswer-prediction matrix We aggregate the feedback and visualize it by an Answer-Prediction\nmatrix. The Answer-Prediction matrix A is a |A| \u00d7 |A| square matrix where |A| is the number of\ndistinct answers provided by the respondents. Each entry Aa,g, a, g \u2208 A is the number of respondents\nthat answer \u201ca\u201d and predict \u201cg\u201d.\n\nWe will show that with proper assumptions, the answer-prediction matrix A\u2019s expectation is pro-\nportional to M. First, for ease of analysis, we assume that each respondent\u2019s predictions are i.i.d.\nsamples12. Second, since we allow people to optionally provide predictions, we need to additionally\nassume that the number of predictions each respondent is willing to provide is independent of her\ntype and answer. We state the formal result as follows and the proof is deferred to Appendix C.\n\nProposition 2.10. When each respondent\u2019s predictions are i.i.d. samples, and the number of\npredictions she gives is independent of her type and answer, the answer-prediction matrix A\u2019s\nexpectation is proportional to M.\n\n", "experiments": "\n\nWe conduct four studies, study 1 (35 math problems), study 2 (30 Go problems), study 3 (44 general\nknowledge questions), and study 4 (43 Chinese character pronunciation questions).\n\nData collection All studies are performed by online questionnaires. We recruit the respondents by\nan online announcement13 or from an online platform that is similar to Amazon Mechanical Turk. We\nget respondents\u2019 consent for using and sharing their data for research. Respondents are asked not to\nsearch for the answers to the questions or communicate with other people. We allow the respondents\nto answer \u201cI do not know\u201d for all questions. Except for Go problems, all questionnaires use flat\npayment. We illustrate the data collection process in detail in Appendix A. We allow respondents\nto participate in more than one study because our algorithms analyze each question separately and\nindependently.\n\nData processing We merge answers which are the same, like \u20180.5\u2019 and \u201850%\u2019. We omit the answers\nthat are reported by less than (\u2264) 3% of respondents or one person. The remaining answers, excluding\n\u201cI do not know\u201d, form the answer set A whose size is |A|. We then construct the Answer-Prediction\nmatrix and perform our algorithms. Pseudo-codes are attached in Appendix B. Our algorithms do not\nrequire any prior or the respondents\u2019 expertise levels.\n\n3.1 Results\n\n12This may not be a very good assumption since i.i.d. samples can repeat but respondents usually do not\nrepeat their predictions. If we do not want this assumption, we can choose to only use the first prediction from\neach respondent (if there exists) to construct the answer-prediction matrix.\n\n13Many are students from top universities in China. See Appendix A for more details.\n\n8\n\n\fType\nMath\nGo\nGeneral knowledge\nChinese character\n\nTotal Our algorithm(Default) Our algorithm(Variant)\n\n35\n30\n44\n43\n\n29\n28\n41\n36\n\n29\n28\n41\n35\n\nPlurality voting\n24\n23\n35\n34\n\nTable 1: The number of questions our algorithms/baseline are correct.\n\n(a) the Monty Hall problem: you can select one\nclosed door of three. A prize, a car, is behind\none of the doors. The other two doors hide goats.\nAfter you have made your choice, Monty Hall\nwill open one of the remaining doors and show\nthat it does not contain the prize. He then asks\nyou if you would like to switch your choice to\nthe other unopened door. What is the probability\nto get the prize if you switch?\n\n(b) the Taxicab problem: 85% of taxis in this\ncity are green, the others are blue. A witness\nsees a blue taxi. She is usually correct with\nprobability 80%. What is the probability that\nthe taxi saw by the witness is blue?\n\n(c) Pick a move for black such that they can be\nalive.\n\n(d) Pick a move for black such that they can be\nalive by ko.\n\n(e) the boundary question: what river forms the\nboundary between North Korea and China?\n\n(f) the Middle Age New Year question: when\nwas the new year in middle age?\n\n(g) the pronunciation of \u7762\n\n(h) the pronunciation of \u6ec2\n\nFigure 4: The ranked answer-prediction matrices\n\n9\n\n2/31/21/32/31/21/3Answer34261413517146Prediction415080121520415080121520Answer160500008201200801619501104818000522130010218PredictionD3C1B2C2B1D3C1B2C2B1Answer62131091410042012115300002PredictionB8A8B6B7B8A8B6B7Answer112540630001230035PredictionYaluandDoomanYaluSonghuaYaluandDoomanYaluSonghuaAnswer33007410005PredictionApr1Dec25Jan1Apr1Dec25Jan1Answer15410028180633Predictionsui1ju1zhi4zhui1sui1ju1zhi4zhui1Answer115023161300210107Predictionpang1pang2pang1pang2Answer1511216Prediction\fWe compare our approach to the baseline, the plurality voting, regarding the accuracy of the top-\nranking answers. Both of the algorithms beat plurality voting for all studies and the default is slightly\nbetter. Among all 152 questions, in 138 questions, the variant algorithm outputs the same hierarchy as\nthe default algorithm. For other questions, the top-ranking type of the variant algorithm may contain\nmore than one answer. The top-ranking answer is the answer supported by more people among all\nanswers in the top-ranking type. In one question the variant is wrong but the default is correct, the\nvariant algorithm assigns both the correct answer and the incorrect plurality answer to the top-ranking\ntype, thus outputting the incorrect answer as the top-ranking answer.\n\nWe also compute the lack-of-fit index (Definition 2.6) of the algorithms and find that the questions\nthe algorithm outputs the correct answer have a smaller lack-of-fit thus fitting the thinking hierarchy\nmodel better. Therefore, we can use the lack-of-fit index as a reliability index of the algorithms.\n\nWe additionally pick several representative examples for each study (Figure 4) where the matrices\nare ranked by the default algorithm and the diagonal area modified like Example 1.1. In all of these\nexamples, the plurality is incorrect while our approach is correct. Results of other questions are\nillustrated at https://elicitation.info/classroom/1/. Detailed explanations are illustrated\nin Appendix D and here we provide some highlights. First, our approach elicits a rich hierarchy. For\nexample, the taxicab problem is borrowed from Kahneman [8] and previous studies show that people\nusually ignore the base rate and report \u201880%\u2019. The imagined levels can be 41%\u219280%. We elicit a\nmuch richer level \u201c41%\u219250%\u219280%\u219212%\u219215%\u219220%\u201d. Second, the most sophisticated level\nmay fail to predict the least one. In the Taxicab problem, the correct \u201c41%\u201d supporters successfully\npredict the common wrong answer \u201c80%\u201d. However, they fail to predict the surprisingly wrong\nanswers \u201c12%,20%\u201d, which are in contrast successfully predicted by \u201c80%\u201d supporters. Our model\nis sufficiently general to allow this situation. Third, even for problems (like Go) without famous\nmistakes, our approach still works. Moreover, in the boundary question, we identify the correct\nanswer without any prior when only 3 respondents are correct.\n\n", "conclusion": "\n\nOne future direction is to consider incentives in our paradigm like the literature of information\nelicitation without verification [13, 18, 5, 22, 9]. We have asked a class of students at Peking\nUniversity: why are bar chairs high? using our paradigm. We cluster the answers by hand. The\nplurality answer is \u201cthe bar counter is high\u201d and our top-ranking answer is \u201cbetter eye contact with\npeople who stand\u201d. Thus, another future diction is to extend our approach to the scenario where\npeople\u2019s answers are sentences, where we can apply NLP to cluster them automatically. In summary,\nwe propose the first empirically validated method to learn the thinking hierarchy without any prior\nin the general problem-solving scenarios. Potentially, our paradigm can be used to make a better\ndecision when we crowd-source opinions in a new field with little prior information. Moreover, when\nwe elicit the crowds\u2019 opinions for a policy, with the thinking hierarchy information, it\u2019s possible to\nunderstand the crowds\u2019 opinions better. However, regarding the negative impact, it may be easier\nto implement a social media manipulation of public opinion with the full thinking hierarchy of the\ncrowds. One interesting future direction is to explore the impact of the thinking hierarchy information.\n\n", "appendix": "", "system_prompts": [{"role": "system", "content": "You are a computer science researcher currently reviewing a paper titled \"Eliciting Thinking Hierarchy without a Prior\" for the NeurIPS computer science conference. Your goal is to try to be as objective and truthful as possible in your answers about the paper provided. Your reviews will be used for causal reasoning in determining the quality of the paper."}], "problems_dict": {"q1b": [[{"role": "user", "content": "The following is the conclusion section of the paper you are reviewing:\n\n\nOne future direction is to consider incentives in our paradigm like the literature of information\nelicitation without verification [13, 18, 5, 22, 9]. We have asked a class of students at Peking\nUniversity: why are bar chairs high? using our paradigm. We cluster the answers by hand. The\nplurality answer is \u201cthe bar counter is high\u201d and our top-ranking answer is \u201cbetter eye contact with\npeople who stand\u201d. Thus, another future diction is to extend our approach to the scenario where\npeople\u2019s answers are sentences, where we can apply NLP to cluster them automatically. In summary,\nwe propose the first empirically validated method to learn the thinking hierarchy without any prior\nin the general problem-solving scenarios. Potentially, our paradigm can be used to make a better\ndecision when we crowd-source opinions in a new field with little prior information. Moreover, when\nwe elicit the crowds\u2019 opinions for a policy, with the thinking hierarchy information, it\u2019s possible to\nunderstand the crowds\u2019 opinions better. However, regarding the negative impact, it may be easier\nto implement a social media manipulation of public opinion with the full thinking hierarchy of the\ncrowds. One interesting future direction is to explore the impact of the thinking hierarchy information.\n\n\nBased on this section, do the authors describe limitations of their work?"}]], "q1c": [[{"role": "user", "content": "The following is the conclusion section of the paper you are reviewing:\n\n\nOne future direction is to consider incentives in our paradigm like the literature of information\nelicitation without verification [13, 18, 5, 22, 9]. We have asked a class of students at Peking\nUniversity: why are bar chairs high? using our paradigm. We cluster the answers by hand. The\nplurality answer is \u201cthe bar counter is high\u201d and our top-ranking answer is \u201cbetter eye contact with\npeople who stand\u201d. Thus, another future diction is to extend our approach to the scenario where\npeople\u2019s answers are sentences, where we can apply NLP to cluster them automatically. In summary,\nwe propose the first empirically validated method to learn the thinking hierarchy without any prior\nin the general problem-solving scenarios. Potentially, our paradigm can be used to make a better\ndecision when we crowd-source opinions in a new field with little prior information. Moreover, when\nwe elicit the crowds\u2019 opinions for a policy, with the thinking hierarchy information, it\u2019s possible to\nunderstand the crowds\u2019 opinions better. However, regarding the negative impact, it may be easier\nto implement a social media manipulation of public opinion with the full thinking hierarchy of the\ncrowds. One interesting future direction is to explore the impact of the thinking hierarchy information.\n\n\nBased on this section, do the authors address potential negative societal impacts of their work?"}]], "q2a": [[{"role": "user", "content": "The following is the abstract of the paper you are reviewing:\n\n\nWhen we use the wisdom of the crowds, we usually rank the answers according to\ntheir popularity, especially when we cannot verify the answers. However, this can\nbe very dangerous when the majority make systematic mistakes. A fundamental\nquestion arises: can we build a hierarchy among the answers without any prior\nwhere the higher-ranking answers, which may not be supported by the majority,\nare from more sophisticated people? To address the question, we propose 1) a\nnovel model to describe people\u2019s thinking hierarchy; 2) two algorithms to learn\nthe thinking hierarchy without any prior; 3) a novel open-response based crowd-\nsourcing approach based on the above theoretic framework. In addition to theoretic\njustifications, we conduct four empirical crowdsourcing studies and show that a)\nthe accuracy of the top-ranking answers learned by our approach is much higher\nthan that of plurality voting (In one question, the plurality answer is supported by\n74 respondents but the correct answer is only supported by 3 respondents. Our\napproach ranks the correct answer the highest without any prior); b) our model\nhas a high goodness-of-fit, especially for the questions where our top-ranking\nanswer is correct. To the best of our knowledge, we are the first to propose a\nthinking hierarchy model with empirical validations in the general problem-solving\nscenarios; and the first to propose a practical open-response based crowdsourcing\napproach that beats plurality voting without any prior.\n\n1\n\n\nBased on this abstract, do the authors have theoretical results? Please start your answer with \"Yes\", \"No\", or \"Unsure\"."}, {"role": "user", "content": "The following is the methods section of the paper you are reviewing:\n\n\nWe first introduce our model for thinking hierarchy. Tversky and Kahneman [27] propose that people\nhave two systems, a fast and intuitive system, and a slow and logical system. For example, Alice\nstarts to solve the circle problem. When she reads the question, she can run her intuitive system 1 and\nobtain answer \u201c3\u201d. However, when she starts to think carefully, she runs her more careful system 2 to\nobtain answer \u201c4\u201d.\n\nWe propose a more general model where people can run multiple oracles to approach the question\nduring their thinking process. Conceptually similar to the cognitive hierarchy theory [3], we assume\nthe oracles have different levels. People usually run lower level oracles before the higher level oracles.\nIn the previous example, system 1 is the lower level oracle and system 2 is the higher one. We assume\nthat Alice runs system 2 after system 1. Thus, in our model, people who know the answer is \u201c4\u201d can\n\n4\n\n\fpredict that other people answer \u201c3\u201d, which is hypothesized in Kong and Schoenebeck [10]. It\u2019s also\npossible that some people run the most sophisticated oracle directly without running the lower ones.\nThus, we design the model such that it does not require the higher-type to be able to predict ALL\nlower types. We also allow the oracles\u2019 outputs to be random.\n\nSection 2.1 introduces the model of thinking hierarchy. Section 2.2 reduces the model inference\nproblem to a matrix decomposition problem. Section 2.3 and section 2.4 show how to learn the\nthinking hierarchy.\n\n2.1 Thinking hierarchy\n\nFixing a question q (e.g. the circle problem), T denotes the set of thinking types. A denotes the set of\npossible answers. Both T and A are finite sets. \u2206A denotes all possible distributions over A. We\nsometimes use \u201cprob\u201d as a shorthand for probability.\n\nGenerating answers We will describe how people of different thinking types generate answers.\nDefinition 2.1 (Oracles of thinking types W ). An answer generating oracle maps the question to\nan (random) answer in A. Each type t corresponds to an oracle Ot. The output Ot(q) is a random\nvariable whose distribution is wt \u2208 \u2206A. W denotes a |T | \u00d7 |A| matrix where each row t is wt.\nEach respondent is type t with prob pt and (cid:80)\nrunning the oracle Ot. For all a \u2208 A, the probability that a respondent answers a will be (cid:80)\nWe assume the probability is positive for all a \u2208 A.\nExample 2.2 (A running example). There are two types T = {0, 1}. The answer space is A =\n{3, 4, 6}. O0 will output \u20183\u2019 with probability 0.8 and \u20186\u2019 with probability 0.2. O1 will output \u20184\u2019\n0\n1\n\nt pt = 1. A type t respondent generates the answer by\nt ptwt(a).\n\ndeterministically. In this example, W =\n\nwhere the first row is the distribution of\n\n(cid:20)0.8\n0\n\n0.2\n0\n\n(cid:21)\n\nO0\u2019s output and the second row is the distribution of O1\u2019s output.\n\nGenerating predictions We then describe how people of different thinking types predict what other\npeople will answer. Here the prediction is not a distribution, but an answer other people may report.\nWhen a type t respondent makes a prediction, she will run an oracle, which is Ot\u2032 with probability\npt\u2192t\u2032 where (cid:80)\n\nt\u2032 pt\u2192t\u2032 = 1. She uses the output of Ot\u2032 as the prediction g \u2208 A.\n\nCombination: answer-prediction joint distribution M M denotes a |A| \u00d7 |A| matrix where\nMa,g is the probability an respondent answers a and predicts g. \u039b denotes a |T | \u00d7 |T | matrix where\n\u039bt,t\u2032 = ptpt\u2192t\u2032 is the probability a respondent is type t and predicts type t\u2032.\nExample 2.3. In this example, when type 0 respondent makes a prediction, with prob 1, she will run\nO0 again. When type 1 respondent makes a prediction, with prob 0.5, she will run O1 again, with\nprob 0.5, she will run O0. Moreover, a respondent is type 0 with prob 0.7, and type 1 with prob 0.3.\n\nHere \u039b =\n\n(cid:20)p0p0\u21920\np1p1\u21920\n\np0p0\u21921\np1p1\u21921\n\n(cid:21)\n\n=\n\n(cid:20) 0.7 \u2217 1\n0.3 \u2217 0.5\n\n0.7 \u2217 0\n0.3 \u2217 0.5\n\n(cid:21)\n\n=\n\n(cid:20) 0.7\n0.15\n\n(cid:21)\n\n.\n\n0\n0.15\n\nClaim 2.4. Based on the above generating processes, M = W\u22a4\u039bW.\n\nProof. For each respondent, the probability she answers a and predicts g will be\n(cid:88)\n\n(cid:88)\n\n(cid:88)\n\nptwt(a)\n\npt\u2192t\u2032wt\u2032(g) =\n\nwt(a)ptpt\u2192t\u2032wt\u2032(g).\n\nMa,g =\n\nt\n\nt\u2032\n\nt,t\u2032\n\nWe sum over all possible types the respondent will be. Given she is type t, she runs oracle Ot to\ngenerate the answer and wt(a) is the probability that the answer is a. We sum over all possible\noracles she runs to predict. Given that she runs Ot\u2032, wt\u2032(g) is the probability the prediction is g.\n\nKey assumption: upper-triangular \u039b we assume that people of a less sophisticated level can\nnever run the oracles of more sophisticated levels. A linear ordering of types \u03c0 : {1, 2, \u00b7 \u00b7 \u00b7 , |T |} (cid:55)\u2192 T\nmaps a ranking position to a type. For example, \u03c0(1) \u2208 T is the top-ranking type.\nAssumption 2.5. We assume that with a proper ordering \u03c0 of the types, \u039b is an upper-triangular\nmatrix. Formally, there exists \u03c0 such that \u2200i > j, \u039b\u03c0(i),\u03c0(j) = 0. Any \u03c0 that makes \u039b upper-\ntriangular is a valid thinking hierarchy of the types.\n\n5\n\n\fIn the running example, the valid thinking hierarchy is \u03c0(1) = type 1, \u03c0(2) = type 0. Note that the\nabove assumption does not require \u2200i \u2264 j, \u039b\u03c0(i),\u03c0(j) > 0. When \u039b is a diagonal matrix, types cannot\npredict each other and are equally sophisticated, thus any ordering is a valid thinking hierarchy.\n\nAn algorithm finds the thinking hierarchy when the algorithm is given M which is generated by\nlatent (unknown) W, \u039b, and the algorithm will output a matrix W\u2217 which equals a row-permuted\nW where the row order is a valid thinking hierarchy. Formally, there exists a valid thinking hierarchy\n\u03c0 such that the ith row of W\u2217 is the \u03c0(i)th row of W, i.e, w\u2217\n\ni = w\u03c0(i).\n\n2.2 Non-negative Congruence Triangularization (NCT)\n\nWith the above model, inferring thinking hierarchy leads to a novel matrix decomposition problem,\nwhich is similar to the symmetric non-negative matrix factorization problem (NMF)6. A non-negative\nmatrix is a matrix whose elements are non-negative.\nDefinition 2.6 (Non-negative Congruence7 Triangularization (NCT)). Given a non-negative matrix\nM, NCT aims to find non-negative matrices W and non-negative upper-triangular matrix \u039b such\nthat M = W\u22a4\u039bW. In a Frobenius norm based approximated version, given a set of matrices W,\nNCT aims to find non-negative matrices W and non-negative upper-triangular matrix \u039b to minimize\n\nmin\nW\u2208W,\u039b\n\n||M \u2212 W\u22a4\u039bW||2\nF\n\nand the minimum is defined as the lack-of-fit of M regarding W 8.\n\nLike NMF, it\u2019s impossible to ask for the strict uniqueness of the results. Let P\u039b be the set of\npermutation matrices such that \u03a0\u22a4\u039b\u03a0 is still upper-triangular.\nIf (W, \u039b) is a solution, then\n(\u03a0\u22121DW, \u03a0\u22a4D\u22121\u039bD\u22121\u03a0) is also a solution where D is a diagonal matrix with positive elements\nand \u03a0 \u2208 P\u039b. We state the uniqueness results as follows and the proof is deferred to Appendix C.\nProposition 2.7 (Uniqueness). If |T | \u2264 |A| and T columns of W consist of a permuted positive\ndiagonal matrix, NCT for M = W\u22a4\u039bW is unique in the sense that then for all W\u2032\u22a4\u039b\u2032W\u2032 =\nW\u22a4\u039bW, there exists a positive diagonal matrix D and a |T | \u00d7 |T | permutation matrix \u03a0 \u2208 P\u039b\nsuch that W\u2032 = \u03a0\u22121DW.\n\nWhen we restrict W to be \u201csemi-orthogonal\u201d, we obtain a clean format of NCT without searching\nfor optimal \u039b. I is the set of all \u201csemi-orthogonal\u201d matrices W where each column of W has\nand only has one non-zero element and WW\u22a4 = I. For example, the W in Example 2.2 can be\nnormalized to a semi-orthogonal matrix. The following lemma follows from the expansion of the\nFrobenius norm and we defer the proof to Appendix C.\n\nLemma 2.8 (Semi-orthogonal: minimizing F-norm = maximizing upper-triangular\u2019s sum of the\nsquare). For all set of matrices W \u2282 I, minW\u2208W,\u039b ||M \u2212 W\u22a4\u039bW||2\nF is equivalent to solv-\ni,j and setting \u039b as Up(WMW\u22a4), the upper-triangular area of\ning maxW\u2208W\nWMW\u22a4.\n\ni\u2264j(WMW\u22a4)2\n\n(cid:80)\n\n2.3\n\nInferring the thinking hierarchy with answer-prediction joint distribution M\n\nGiven M, inferring the thinking hierarchy is equivalent to solving NCT in general. Though we do not\nhave M, later we will show a proxy for M. For simplicity of practical use, we introduce two simple\nranking algorithms by employing Lemma 2.8. The ranking algorithm takes M as input and outputs a\nlinear ordering of answers \u03c0 : {1, 2, \u00b7 \u00b7 \u00b7 , |A|} (cid:55)\u2192 A that maps a ranking position to an answer.\n\nAnswer-Ranking Algorithm (Default) AR(M) The algorithm computes\n\n\u03a0\u2217 \u2190 arg max\n\u03a0\u2208P\n\n(cid:88)\n\n(\u03a0M\u03a0\u22a4)2\ni,j\n\ni\u2264j\n\n6Symmetric NMF: minW ||M \u2212 W\u22a4W||2\nF\n7We use congruence here though it is not matrix congruence since W may not be a square matrix.\n8M = W\u22a4\u039bW, W \u2208 W has zero lack-of-fit.\n\n6\n\n\fwhere P is the set of all |A| \u00d7 |A| permutation matrices. There is a one to one mapping between each\npermutation matrix \u03a0 and a linear ordering \u03c0: \u03a0i,\u03c0(i) = 1, \u2200i. Therefore, the optimal \u03a0\u2217 leads to an\noptimal rank over answers directly and the default algorithm can be also represented as\n\n\u03c0\u2217 \u2190 arg max\n\n\u03c0\n\nM 2\n\n\u03c0(i),\u03c0(j).\n\n(cid:88)\n\ni\u2264j\n\nTo find the optimal rank, we use a dynamic programming based algorithm (see the supplementary\nmaterials) which takes O(2|A||A|2). In practice, |A| is usually at most 7 or 8. In our empirical study,\nthe default algorithm takes 91 milliseconds to finish the computation of all 152 questions.\n\nThe default algorithm implicitly assumes |T | = |A| and all oracles are deterministic. To allow\n|T | < |A| and non-deterministic oracles, we introduce a variant that generalizes P to a subset of\nsemi-orthogonal matrices I. Every |T | \u00d7 |A| semi-orthogonal W indicates a hard clustering. Each\ncluster t \u2208 T contains all answers a such that Wt,a > 0. For example, the W in Example 2.2 can\nbe normalized to a semi-orthogonal matrix and indicates a hard clustering {4}, {6, 3}. Therefore,\nthe variant algorithm will partition the answers into multiple clusters and assign a hierarchy to the\nclusters.\n\nAnswer-Ranking Algorithm (Variant) AR+(M, W) The algorithm computes\n\nW\u2217 \u2190 arg max\nW\u2208I\n\n(cid:88)\n\n(WMW\u22a4)2\n\ni,j.\n\ni\u2264j\n\nwhere W \u2282 I. W\u2217 is normalized such that every row sums to 1. This algorithm does not restrict\n|T | = |A| and learns the optimal |T |.\n\nW\u2217 \u21d2 Answer rank The output W\u2217 indicates a hard clustering of all answers. We rank all answer\nas follows: for any i < j, the answers in cluster i has a higher rank 9 than the answers in cluster j.\nFor all i, for any two answers a, a\u2032 in the same cluster i, a is ranked higher than a\u2032 if W \u2217\ni,a\u2032.\n\ni,a > W \u2217\n\nTheoretical justification When M perfectly fits the model with the restriction that the latent W is\na permutation or a hard clustering, we show that our algorithm finds the thinking hierarchy. Otherwise,\nour algorithm finds the \u201cclosest\u201d solution measured by the Frobenius norm.\nTheorem 2.9. When there exists \u03a00 \u2208 P and non-negative upper-triangular matrix \u039b0 such that\n0 \u039b0\u03a00, AR(M) finds the thinking hierarchy 10. In general, AR(M) will output \u03a0\u2217 where\nM = \u03a0\u22a4\n\u03a0\u2217, \u039b\u2217 = Up(\u03a0\u2217M\u03a0\u2217\u22a4) is a solution to arg min\u03a0\u2208P,\u039b ||M \u2212 \u03a0\u22a4\u039b\u03a0||2\nF . The above statement\nis still valid by replacing P by W \u2282 I and AR(M) by AR+(M, W).\n\nProposition 2.7 and Lemma 2.8 almost directly imply the above theorem. We defer the formal proof\nto Appendix C.\n\n2.4 A proxy for answer-prediction joint distribution M\n\nIn practice, we do not have perfect M. We use the following open-response paradigm to obtain a\nproxy for M.\n\nAnswer-prediction paradigm the respondents are asked Q1: What\u2019s your answer? Answer:\nQ2: What do you think other people will answer:\n\n.\n\nIn the circle example, the possible feedback can be \u201canswer: 4; prediction: 3\u201d, \u201canswer: 3; prediction:\n6,9,1\u201d... We collect all answers provided by the respondents 11 and denote the set of them by A. In\nthe circle example, A = {1, 2, 3, 4, 6, 9}. We also allow respondents to provide no prediction or\nmultiple predictions.\n\n9Rank 1 answer is in the highest position.\n10See definition in the last paragraph in Section 2.1.\n11In practice, we set a threshold \u03b8 and collect answers which are provided by at least \u03b8 fraction of the\nrespondents. We allow multiple predictions and also allow people to answer \u201cI do not know\u201d. See Section 3 for\nmore details.\n\n7\n\n\f(a) Accuracy of algorithms\n\n(b) Empirical distribution of lack-of-fit\n\nFigure 3: The results of our experiment\n\nAnswer-prediction matrix We aggregate the feedback and visualize it by an Answer-Prediction\nmatrix. The Answer-Prediction matrix A is a |A| \u00d7 |A| square matrix where |A| is the number of\ndistinct answers provided by the respondents. Each entry Aa,g, a, g \u2208 A is the number of respondents\nthat answer \u201ca\u201d and predict \u201cg\u201d.\n\nWe will show that with proper assumptions, the answer-prediction matrix A\u2019s expectation is pro-\nportional to M. First, for ease of analysis, we assume that each respondent\u2019s predictions are i.i.d.\nsamples12. Second, since we allow people to optionally provide predictions, we need to additionally\nassume that the number of predictions each respondent is willing to provide is independent of her\ntype and answer. We state the formal result as follows and the proof is deferred to Appendix C.\n\nProposition 2.10. When each respondent\u2019s predictions are i.i.d. samples, and the number of\npredictions she gives is independent of her type and answer, the answer-prediction matrix A\u2019s\nexpectation is proportional to M.\n\n\nBased on this section, do the authors state the assumptions for their theoretical results?"}]], "q2b": [[{"role": "user", "content": "The following is the abstract of the paper you are reviewing:\n\n\nWhen we use the wisdom of the crowds, we usually rank the answers according to\ntheir popularity, especially when we cannot verify the answers. However, this can\nbe very dangerous when the majority make systematic mistakes. A fundamental\nquestion arises: can we build a hierarchy among the answers without any prior\nwhere the higher-ranking answers, which may not be supported by the majority,\nare from more sophisticated people? To address the question, we propose 1) a\nnovel model to describe people\u2019s thinking hierarchy; 2) two algorithms to learn\nthe thinking hierarchy without any prior; 3) a novel open-response based crowd-\nsourcing approach based on the above theoretic framework. In addition to theoretic\njustifications, we conduct four empirical crowdsourcing studies and show that a)\nthe accuracy of the top-ranking answers learned by our approach is much higher\nthan that of plurality voting (In one question, the plurality answer is supported by\n74 respondents but the correct answer is only supported by 3 respondents. Our\napproach ranks the correct answer the highest without any prior); b) our model\nhas a high goodness-of-fit, especially for the questions where our top-ranking\nanswer is correct. To the best of our knowledge, we are the first to propose a\nthinking hierarchy model with empirical validations in the general problem-solving\nscenarios; and the first to propose a practical open-response based crowdsourcing\napproach that beats plurality voting without any prior.\n\n1\n\n\nBased on this abstract, do the authors have theoretical results? Please start your answer with \"Yes\", \"No\", or \"Unsure\"."}, {"role": "user", "content": "The following is the methods section of the paper you are reviewing:\n\n\nWe first introduce our model for thinking hierarchy. Tversky and Kahneman [27] propose that people\nhave two systems, a fast and intuitive system, and a slow and logical system. For example, Alice\nstarts to solve the circle problem. When she reads the question, she can run her intuitive system 1 and\nobtain answer \u201c3\u201d. However, when she starts to think carefully, she runs her more careful system 2 to\nobtain answer \u201c4\u201d.\n\nWe propose a more general model where people can run multiple oracles to approach the question\nduring their thinking process. Conceptually similar to the cognitive hierarchy theory [3], we assume\nthe oracles have different levels. People usually run lower level oracles before the higher level oracles.\nIn the previous example, system 1 is the lower level oracle and system 2 is the higher one. We assume\nthat Alice runs system 2 after system 1. Thus, in our model, people who know the answer is \u201c4\u201d can\n\n4\n\n\fpredict that other people answer \u201c3\u201d, which is hypothesized in Kong and Schoenebeck [10]. It\u2019s also\npossible that some people run the most sophisticated oracle directly without running the lower ones.\nThus, we design the model such that it does not require the higher-type to be able to predict ALL\nlower types. We also allow the oracles\u2019 outputs to be random.\n\nSection 2.1 introduces the model of thinking hierarchy. Section 2.2 reduces the model inference\nproblem to a matrix decomposition problem. Section 2.3 and section 2.4 show how to learn the\nthinking hierarchy.\n\n2.1 Thinking hierarchy\n\nFixing a question q (e.g. the circle problem), T denotes the set of thinking types. A denotes the set of\npossible answers. Both T and A are finite sets. \u2206A denotes all possible distributions over A. We\nsometimes use \u201cprob\u201d as a shorthand for probability.\n\nGenerating answers We will describe how people of different thinking types generate answers.\nDefinition 2.1 (Oracles of thinking types W ). An answer generating oracle maps the question to\nan (random) answer in A. Each type t corresponds to an oracle Ot. The output Ot(q) is a random\nvariable whose distribution is wt \u2208 \u2206A. W denotes a |T | \u00d7 |A| matrix where each row t is wt.\nEach respondent is type t with prob pt and (cid:80)\nrunning the oracle Ot. For all a \u2208 A, the probability that a respondent answers a will be (cid:80)\nWe assume the probability is positive for all a \u2208 A.\nExample 2.2 (A running example). There are two types T = {0, 1}. The answer space is A =\n{3, 4, 6}. O0 will output \u20183\u2019 with probability 0.8 and \u20186\u2019 with probability 0.2. O1 will output \u20184\u2019\n0\n1\n\nt pt = 1. A type t respondent generates the answer by\nt ptwt(a).\n\ndeterministically. In this example, W =\n\nwhere the first row is the distribution of\n\n(cid:20)0.8\n0\n\n0.2\n0\n\n(cid:21)\n\nO0\u2019s output and the second row is the distribution of O1\u2019s output.\n\nGenerating predictions We then describe how people of different thinking types predict what other\npeople will answer. Here the prediction is not a distribution, but an answer other people may report.\nWhen a type t respondent makes a prediction, she will run an oracle, which is Ot\u2032 with probability\npt\u2192t\u2032 where (cid:80)\n\nt\u2032 pt\u2192t\u2032 = 1. She uses the output of Ot\u2032 as the prediction g \u2208 A.\n\nCombination: answer-prediction joint distribution M M denotes a |A| \u00d7 |A| matrix where\nMa,g is the probability an respondent answers a and predicts g. \u039b denotes a |T | \u00d7 |T | matrix where\n\u039bt,t\u2032 = ptpt\u2192t\u2032 is the probability a respondent is type t and predicts type t\u2032.\nExample 2.3. In this example, when type 0 respondent makes a prediction, with prob 1, she will run\nO0 again. When type 1 respondent makes a prediction, with prob 0.5, she will run O1 again, with\nprob 0.5, she will run O0. Moreover, a respondent is type 0 with prob 0.7, and type 1 with prob 0.3.\n\nHere \u039b =\n\n(cid:20)p0p0\u21920\np1p1\u21920\n\np0p0\u21921\np1p1\u21921\n\n(cid:21)\n\n=\n\n(cid:20) 0.7 \u2217 1\n0.3 \u2217 0.5\n\n0.7 \u2217 0\n0.3 \u2217 0.5\n\n(cid:21)\n\n=\n\n(cid:20) 0.7\n0.15\n\n(cid:21)\n\n.\n\n0\n0.15\n\nClaim 2.4. Based on the above generating processes, M = W\u22a4\u039bW.\n\nProof. For each respondent, the probability she answers a and predicts g will be\n(cid:88)\n\n(cid:88)\n\n(cid:88)\n\nptwt(a)\n\npt\u2192t\u2032wt\u2032(g) =\n\nwt(a)ptpt\u2192t\u2032wt\u2032(g).\n\nMa,g =\n\nt\n\nt\u2032\n\nt,t\u2032\n\nWe sum over all possible types the respondent will be. Given she is type t, she runs oracle Ot to\ngenerate the answer and wt(a) is the probability that the answer is a. We sum over all possible\noracles she runs to predict. Given that she runs Ot\u2032, wt\u2032(g) is the probability the prediction is g.\n\nKey assumption: upper-triangular \u039b we assume that people of a less sophisticated level can\nnever run the oracles of more sophisticated levels. A linear ordering of types \u03c0 : {1, 2, \u00b7 \u00b7 \u00b7 , |T |} (cid:55)\u2192 T\nmaps a ranking position to a type. For example, \u03c0(1) \u2208 T is the top-ranking type.\nAssumption 2.5. We assume that with a proper ordering \u03c0 of the types, \u039b is an upper-triangular\nmatrix. Formally, there exists \u03c0 such that \u2200i > j, \u039b\u03c0(i),\u03c0(j) = 0. Any \u03c0 that makes \u039b upper-\ntriangular is a valid thinking hierarchy of the types.\n\n5\n\n\fIn the running example, the valid thinking hierarchy is \u03c0(1) = type 1, \u03c0(2) = type 0. Note that the\nabove assumption does not require \u2200i \u2264 j, \u039b\u03c0(i),\u03c0(j) > 0. When \u039b is a diagonal matrix, types cannot\npredict each other and are equally sophisticated, thus any ordering is a valid thinking hierarchy.\n\nAn algorithm finds the thinking hierarchy when the algorithm is given M which is generated by\nlatent (unknown) W, \u039b, and the algorithm will output a matrix W\u2217 which equals a row-permuted\nW where the row order is a valid thinking hierarchy. Formally, there exists a valid thinking hierarchy\n\u03c0 such that the ith row of W\u2217 is the \u03c0(i)th row of W, i.e, w\u2217\n\ni = w\u03c0(i).\n\n2.2 Non-negative Congruence Triangularization (NCT)\n\nWith the above model, inferring thinking hierarchy leads to a novel matrix decomposition problem,\nwhich is similar to the symmetric non-negative matrix factorization problem (NMF)6. A non-negative\nmatrix is a matrix whose elements are non-negative.\nDefinition 2.6 (Non-negative Congruence7 Triangularization (NCT)). Given a non-negative matrix\nM, NCT aims to find non-negative matrices W and non-negative upper-triangular matrix \u039b such\nthat M = W\u22a4\u039bW. In a Frobenius norm based approximated version, given a set of matrices W,\nNCT aims to find non-negative matrices W and non-negative upper-triangular matrix \u039b to minimize\n\nmin\nW\u2208W,\u039b\n\n||M \u2212 W\u22a4\u039bW||2\nF\n\nand the minimum is defined as the lack-of-fit of M regarding W 8.\n\nLike NMF, it\u2019s impossible to ask for the strict uniqueness of the results. Let P\u039b be the set of\npermutation matrices such that \u03a0\u22a4\u039b\u03a0 is still upper-triangular.\nIf (W, \u039b) is a solution, then\n(\u03a0\u22121DW, \u03a0\u22a4D\u22121\u039bD\u22121\u03a0) is also a solution where D is a diagonal matrix with positive elements\nand \u03a0 \u2208 P\u039b. We state the uniqueness results as follows and the proof is deferred to Appendix C.\nProposition 2.7 (Uniqueness). If |T | \u2264 |A| and T columns of W consist of a permuted positive\ndiagonal matrix, NCT for M = W\u22a4\u039bW is unique in the sense that then for all W\u2032\u22a4\u039b\u2032W\u2032 =\nW\u22a4\u039bW, there exists a positive diagonal matrix D and a |T | \u00d7 |T | permutation matrix \u03a0 \u2208 P\u039b\nsuch that W\u2032 = \u03a0\u22121DW.\n\nWhen we restrict W to be \u201csemi-orthogonal\u201d, we obtain a clean format of NCT without searching\nfor optimal \u039b. I is the set of all \u201csemi-orthogonal\u201d matrices W where each column of W has\nand only has one non-zero element and WW\u22a4 = I. For example, the W in Example 2.2 can be\nnormalized to a semi-orthogonal matrix. The following lemma follows from the expansion of the\nFrobenius norm and we defer the proof to Appendix C.\n\nLemma 2.8 (Semi-orthogonal: minimizing F-norm = maximizing upper-triangular\u2019s sum of the\nsquare). For all set of matrices W \u2282 I, minW\u2208W,\u039b ||M \u2212 W\u22a4\u039bW||2\nF is equivalent to solv-\ni,j and setting \u039b as Up(WMW\u22a4), the upper-triangular area of\ning maxW\u2208W\nWMW\u22a4.\n\ni\u2264j(WMW\u22a4)2\n\n(cid:80)\n\n2.3\n\nInferring the thinking hierarchy with answer-prediction joint distribution M\n\nGiven M, inferring the thinking hierarchy is equivalent to solving NCT in general. Though we do not\nhave M, later we will show a proxy for M. For simplicity of practical use, we introduce two simple\nranking algorithms by employing Lemma 2.8. The ranking algorithm takes M as input and outputs a\nlinear ordering of answers \u03c0 : {1, 2, \u00b7 \u00b7 \u00b7 , |A|} (cid:55)\u2192 A that maps a ranking position to an answer.\n\nAnswer-Ranking Algorithm (Default) AR(M) The algorithm computes\n\n\u03a0\u2217 \u2190 arg max\n\u03a0\u2208P\n\n(cid:88)\n\n(\u03a0M\u03a0\u22a4)2\ni,j\n\ni\u2264j\n\n6Symmetric NMF: minW ||M \u2212 W\u22a4W||2\nF\n7We use congruence here though it is not matrix congruence since W may not be a square matrix.\n8M = W\u22a4\u039bW, W \u2208 W has zero lack-of-fit.\n\n6\n\n\fwhere P is the set of all |A| \u00d7 |A| permutation matrices. There is a one to one mapping between each\npermutation matrix \u03a0 and a linear ordering \u03c0: \u03a0i,\u03c0(i) = 1, \u2200i. Therefore, the optimal \u03a0\u2217 leads to an\noptimal rank over answers directly and the default algorithm can be also represented as\n\n\u03c0\u2217 \u2190 arg max\n\n\u03c0\n\nM 2\n\n\u03c0(i),\u03c0(j).\n\n(cid:88)\n\ni\u2264j\n\nTo find the optimal rank, we use a dynamic programming based algorithm (see the supplementary\nmaterials) which takes O(2|A||A|2). In practice, |A| is usually at most 7 or 8. In our empirical study,\nthe default algorithm takes 91 milliseconds to finish the computation of all 152 questions.\n\nThe default algorithm implicitly assumes |T | = |A| and all oracles are deterministic. To allow\n|T | < |A| and non-deterministic oracles, we introduce a variant that generalizes P to a subset of\nsemi-orthogonal matrices I. Every |T | \u00d7 |A| semi-orthogonal W indicates a hard clustering. Each\ncluster t \u2208 T contains all answers a such that Wt,a > 0. For example, the W in Example 2.2 can\nbe normalized to a semi-orthogonal matrix and indicates a hard clustering {4}, {6, 3}. Therefore,\nthe variant algorithm will partition the answers into multiple clusters and assign a hierarchy to the\nclusters.\n\nAnswer-Ranking Algorithm (Variant) AR+(M, W) The algorithm computes\n\nW\u2217 \u2190 arg max\nW\u2208I\n\n(cid:88)\n\n(WMW\u22a4)2\n\ni,j.\n\ni\u2264j\n\nwhere W \u2282 I. W\u2217 is normalized such that every row sums to 1. This algorithm does not restrict\n|T | = |A| and learns the optimal |T |.\n\nW\u2217 \u21d2 Answer rank The output W\u2217 indicates a hard clustering of all answers. We rank all answer\nas follows: for any i < j, the answers in cluster i has a higher rank 9 than the answers in cluster j.\nFor all i, for any two answers a, a\u2032 in the same cluster i, a is ranked higher than a\u2032 if W \u2217\ni,a\u2032.\n\ni,a > W \u2217\n\nTheoretical justification When M perfectly fits the model with the restriction that the latent W is\na permutation or a hard clustering, we show that our algorithm finds the thinking hierarchy. Otherwise,\nour algorithm finds the \u201cclosest\u201d solution measured by the Frobenius norm.\nTheorem 2.9. When there exists \u03a00 \u2208 P and non-negative upper-triangular matrix \u039b0 such that\n0 \u039b0\u03a00, AR(M) finds the thinking hierarchy 10. In general, AR(M) will output \u03a0\u2217 where\nM = \u03a0\u22a4\n\u03a0\u2217, \u039b\u2217 = Up(\u03a0\u2217M\u03a0\u2217\u22a4) is a solution to arg min\u03a0\u2208P,\u039b ||M \u2212 \u03a0\u22a4\u039b\u03a0||2\nF . The above statement\nis still valid by replacing P by W \u2282 I and AR(M) by AR+(M, W).\n\nProposition 2.7 and Lemma 2.8 almost directly imply the above theorem. We defer the formal proof\nto Appendix C.\n\n2.4 A proxy for answer-prediction joint distribution M\n\nIn practice, we do not have perfect M. We use the following open-response paradigm to obtain a\nproxy for M.\n\nAnswer-prediction paradigm the respondents are asked Q1: What\u2019s your answer? Answer:\nQ2: What do you think other people will answer:\n\n.\n\nIn the circle example, the possible feedback can be \u201canswer: 4; prediction: 3\u201d, \u201canswer: 3; prediction:\n6,9,1\u201d... We collect all answers provided by the respondents 11 and denote the set of them by A. In\nthe circle example, A = {1, 2, 3, 4, 6, 9}. We also allow respondents to provide no prediction or\nmultiple predictions.\n\n9Rank 1 answer is in the highest position.\n10See definition in the last paragraph in Section 2.1.\n11In practice, we set a threshold \u03b8 and collect answers which are provided by at least \u03b8 fraction of the\nrespondents. We allow multiple predictions and also allow people to answer \u201cI do not know\u201d. See Section 3 for\nmore details.\n\n7\n\n\f(a) Accuracy of algorithms\n\n(b) Empirical distribution of lack-of-fit\n\nFigure 3: The results of our experiment\n\nAnswer-prediction matrix We aggregate the feedback and visualize it by an Answer-Prediction\nmatrix. The Answer-Prediction matrix A is a |A| \u00d7 |A| square matrix where |A| is the number of\ndistinct answers provided by the respondents. Each entry Aa,g, a, g \u2208 A is the number of respondents\nthat answer \u201ca\u201d and predict \u201cg\u201d.\n\nWe will show that with proper assumptions, the answer-prediction matrix A\u2019s expectation is pro-\nportional to M. First, for ease of analysis, we assume that each respondent\u2019s predictions are i.i.d.\nsamples12. Second, since we allow people to optionally provide predictions, we need to additionally\nassume that the number of predictions each respondent is willing to provide is independent of her\ntype and answer. We state the formal result as follows and the proof is deferred to Appendix C.\n\nProposition 2.10. When each respondent\u2019s predictions are i.i.d. samples, and the number of\npredictions she gives is independent of her type and answer, the answer-prediction matrix A\u2019s\nexpectation is proportional to M.\n\n\nBased on this section, do the authors include complete proofs for all their theoretical results?"}]], "q3a": [[{"role": "user", "content": "The following is the experiments section of the paper you are reviewing:\n\n\nWe conduct four studies, study 1 (35 math problems), study 2 (30 Go problems), study 3 (44 general\nknowledge questions), and study 4 (43 Chinese character pronunciation questions).\n\nData collection All studies are performed by online questionnaires. We recruit the respondents by\nan online announcement13 or from an online platform that is similar to Amazon Mechanical Turk. We\nget respondents\u2019 consent for using and sharing their data for research. Respondents are asked not to\nsearch for the answers to the questions or communicate with other people. We allow the respondents\nto answer \u201cI do not know\u201d for all questions. Except for Go problems, all questionnaires use flat\npayment. We illustrate the data collection process in detail in Appendix A. We allow respondents\nto participate in more than one study because our algorithms analyze each question separately and\nindependently.\n\nData processing We merge answers which are the same, like \u20180.5\u2019 and \u201850%\u2019. We omit the answers\nthat are reported by less than (\u2264) 3% of respondents or one person. The remaining answers, excluding\n\u201cI do not know\u201d, form the answer set A whose size is |A|. We then construct the Answer-Prediction\nmatrix and perform our algorithms. Pseudo-codes are attached in Appendix B. Our algorithms do not\nrequire any prior or the respondents\u2019 expertise levels.\n\n3.1 Results\n\n12This may not be a very good assumption since i.i.d. samples can repeat but respondents usually do not\nrepeat their predictions. If we do not want this assumption, we can choose to only use the first prediction from\neach respondent (if there exists) to construct the answer-prediction matrix.\n\n13Many are students from top universities in China. See Appendix A for more details.\n\n8\n\n\fType\nMath\nGo\nGeneral knowledge\nChinese character\n\nTotal Our algorithm(Default) Our algorithm(Variant)\n\n35\n30\n44\n43\n\n29\n28\n41\n36\n\n29\n28\n41\n35\n\nPlurality voting\n24\n23\n35\n34\n\nTable 1: The number of questions our algorithms/baseline are correct.\n\n(a) the Monty Hall problem: you can select one\nclosed door of three. A prize, a car, is behind\none of the doors. The other two doors hide goats.\nAfter you have made your choice, Monty Hall\nwill open one of the remaining doors and show\nthat it does not contain the prize. He then asks\nyou if you would like to switch your choice to\nthe other unopened door. What is the probability\nto get the prize if you switch?\n\n(b) the Taxicab problem: 85% of taxis in this\ncity are green, the others are blue. A witness\nsees a blue taxi. She is usually correct with\nprobability 80%. What is the probability that\nthe taxi saw by the witness is blue?\n\n(c) Pick a move for black such that they can be\nalive.\n\n(d) Pick a move for black such that they can be\nalive by ko.\n\n(e) the boundary question: what river forms the\nboundary between North Korea and China?\n\n(f) the Middle Age New Year question: when\nwas the new year in middle age?\n\n(g) the pronunciation of \u7762\n\n(h) the pronunciation of \u6ec2\n\nFigure 4: The ranked answer-prediction matrices\n\n9\n\n2/31/21/32/31/21/3Answer34261413517146Prediction415080121520415080121520Answer160500008201200801619501104818000522130010218PredictionD3C1B2C2B1D3C1B2C2B1Answer62131091410042012115300002PredictionB8A8B6B7B8A8B6B7Answer112540630001230035PredictionYaluandDoomanYaluSonghuaYaluandDoomanYaluSonghuaAnswer33007410005PredictionApr1Dec25Jan1Apr1Dec25Jan1Answer15410028180633Predictionsui1ju1zhi4zhui1sui1ju1zhi4zhui1Answer115023161300210107Predictionpang1pang2pang1pang2Answer1511216Prediction\fWe compare our approach to the baseline, the plurality voting, regarding the accuracy of the top-\nranking answers. Both of the algorithms beat plurality voting for all studies and the default is slightly\nbetter. Among all 152 questions, in 138 questions, the variant algorithm outputs the same hierarchy as\nthe default algorithm. For other questions, the top-ranking type of the variant algorithm may contain\nmore than one answer. The top-ranking answer is the answer supported by more people among all\nanswers in the top-ranking type. In one question the variant is wrong but the default is correct, the\nvariant algorithm assigns both the correct answer and the incorrect plurality answer to the top-ranking\ntype, thus outputting the incorrect answer as the top-ranking answer.\n\nWe also compute the lack-of-fit index (Definition 2.6) of the algorithms and find that the questions\nthe algorithm outputs the correct answer have a smaller lack-of-fit thus fitting the thinking hierarchy\nmodel better. Therefore, we can use the lack-of-fit index as a reliability index of the algorithms.\n\nWe additionally pick several representative examples for each study (Figure 4) where the matrices\nare ranked by the default algorithm and the diagonal area modified like Example 1.1. In all of these\nexamples, the plurality is incorrect while our approach is correct. Results of other questions are\nillustrated at https://elicitation.info/classroom/1/. Detailed explanations are illustrated\nin Appendix D and here we provide some highlights. First, our approach elicits a rich hierarchy. For\nexample, the taxicab problem is borrowed from Kahneman [8] and previous studies show that people\nusually ignore the base rate and report \u201880%\u2019. The imagined levels can be 41%\u219280%. We elicit a\nmuch richer level \u201c41%\u219250%\u219280%\u219212%\u219215%\u219220%\u201d. Second, the most sophisticated level\nmay fail to predict the least one. In the Taxicab problem, the correct \u201c41%\u201d supporters successfully\npredict the common wrong answer \u201c80%\u201d. However, they fail to predict the surprisingly wrong\nanswers \u201c12%,20%\u201d, which are in contrast successfully predicted by \u201c80%\u201d supporters. Our model\nis sufficiently general to allow this situation. Third, even for problems (like Go) without famous\nmistakes, our approach still works. Moreover, in the boundary question, we identify the correct\nanswer without any prior when only 3 respondents are correct.\n\n\nBased on this section, do the authors include code, data, and instructions needed to reproduce the main experimental results, or provide them in a URL?"}]], "q3b": [[{"role": "user", "content": "The following is the experiments section of the paper you are reviewing:\n\n\nWe conduct four studies, study 1 (35 math problems), study 2 (30 Go problems), study 3 (44 general\nknowledge questions), and study 4 (43 Chinese character pronunciation questions).\n\nData collection All studies are performed by online questionnaires. We recruit the respondents by\nan online announcement13 or from an online platform that is similar to Amazon Mechanical Turk. We\nget respondents\u2019 consent for using and sharing their data for research. Respondents are asked not to\nsearch for the answers to the questions or communicate with other people. We allow the respondents\nto answer \u201cI do not know\u201d for all questions. Except for Go problems, all questionnaires use flat\npayment. We illustrate the data collection process in detail in Appendix A. We allow respondents\nto participate in more than one study because our algorithms analyze each question separately and\nindependently.\n\nData processing We merge answers which are the same, like \u20180.5\u2019 and \u201850%\u2019. We omit the answers\nthat are reported by less than (\u2264) 3% of respondents or one person. The remaining answers, excluding\n\u201cI do not know\u201d, form the answer set A whose size is |A|. We then construct the Answer-Prediction\nmatrix and perform our algorithms. Pseudo-codes are attached in Appendix B. Our algorithms do not\nrequire any prior or the respondents\u2019 expertise levels.\n\n3.1 Results\n\n12This may not be a very good assumption since i.i.d. samples can repeat but respondents usually do not\nrepeat their predictions. If we do not want this assumption, we can choose to only use the first prediction from\neach respondent (if there exists) to construct the answer-prediction matrix.\n\n13Many are students from top universities in China. See Appendix A for more details.\n\n8\n\n\fType\nMath\nGo\nGeneral knowledge\nChinese character\n\nTotal Our algorithm(Default) Our algorithm(Variant)\n\n35\n30\n44\n43\n\n29\n28\n41\n36\n\n29\n28\n41\n35\n\nPlurality voting\n24\n23\n35\n34\n\nTable 1: The number of questions our algorithms/baseline are correct.\n\n(a) the Monty Hall problem: you can select one\nclosed door of three. A prize, a car, is behind\none of the doors. The other two doors hide goats.\nAfter you have made your choice, Monty Hall\nwill open one of the remaining doors and show\nthat it does not contain the prize. He then asks\nyou if you would like to switch your choice to\nthe other unopened door. What is the probability\nto get the prize if you switch?\n\n(b) the Taxicab problem: 85% of taxis in this\ncity are green, the others are blue. A witness\nsees a blue taxi. She is usually correct with\nprobability 80%. What is the probability that\nthe taxi saw by the witness is blue?\n\n(c) Pick a move for black such that they can be\nalive.\n\n(d) Pick a move for black such that they can be\nalive by ko.\n\n(e) the boundary question: what river forms the\nboundary between North Korea and China?\n\n(f) the Middle Age New Year question: when\nwas the new year in middle age?\n\n(g) the pronunciation of \u7762\n\n(h) the pronunciation of \u6ec2\n\nFigure 4: The ranked answer-prediction matrices\n\n9\n\n2/31/21/32/31/21/3Answer34261413517146Prediction415080121520415080121520Answer160500008201200801619501104818000522130010218PredictionD3C1B2C2B1D3C1B2C2B1Answer62131091410042012115300002PredictionB8A8B6B7B8A8B6B7Answer112540630001230035PredictionYaluandDoomanYaluSonghuaYaluandDoomanYaluSonghuaAnswer33007410005PredictionApr1Dec25Jan1Apr1Dec25Jan1Answer15410028180633Predictionsui1ju1zhi4zhui1sui1ju1zhi4zhui1Answer115023161300210107Predictionpang1pang2pang1pang2Answer1511216Prediction\fWe compare our approach to the baseline, the plurality voting, regarding the accuracy of the top-\nranking answers. Both of the algorithms beat plurality voting for all studies and the default is slightly\nbetter. Among all 152 questions, in 138 questions, the variant algorithm outputs the same hierarchy as\nthe default algorithm. For other questions, the top-ranking type of the variant algorithm may contain\nmore than one answer. The top-ranking answer is the answer supported by more people among all\nanswers in the top-ranking type. In one question the variant is wrong but the default is correct, the\nvariant algorithm assigns both the correct answer and the incorrect plurality answer to the top-ranking\ntype, thus outputting the incorrect answer as the top-ranking answer.\n\nWe also compute the lack-of-fit index (Definition 2.6) of the algorithms and find that the questions\nthe algorithm outputs the correct answer have a smaller lack-of-fit thus fitting the thinking hierarchy\nmodel better. Therefore, we can use the lack-of-fit index as a reliability index of the algorithms.\n\nWe additionally pick several representative examples for each study (Figure 4) where the matrices\nare ranked by the default algorithm and the diagonal area modified like Example 1.1. In all of these\nexamples, the plurality is incorrect while our approach is correct. Results of other questions are\nillustrated at https://elicitation.info/classroom/1/. Detailed explanations are illustrated\nin Appendix D and here we provide some highlights. First, our approach elicits a rich hierarchy. For\nexample, the taxicab problem is borrowed from Kahneman [8] and previous studies show that people\nusually ignore the base rate and report \u201880%\u2019. The imagined levels can be 41%\u219280%. We elicit a\nmuch richer level \u201c41%\u219250%\u219280%\u219212%\u219215%\u219220%\u201d. Second, the most sophisticated level\nmay fail to predict the least one. In the Taxicab problem, the correct \u201c41%\u201d supporters successfully\npredict the common wrong answer \u201c80%\u201d. However, they fail to predict the surprisingly wrong\nanswers \u201c12%,20%\u201d, which are in contrast successfully predicted by \u201c80%\u201d supporters. Our model\nis sufficiently general to allow this situation. Third, even for problems (like Go) without famous\nmistakes, our approach still works. Moreover, in the boundary question, we identify the correct\nanswer without any prior when only 3 respondents are correct.\n\nBased on this section, do the authors specify all training details (e.g., data splits, hyperparameters, how they were chosen) for their results?"}]], "q3c": [[{"role": "user", "content": "The following is the experiments section of the paper you are reviewing:\n\n\nWe conduct four studies, study 1 (35 math problems), study 2 (30 Go problems), study 3 (44 general\nknowledge questions), and study 4 (43 Chinese character pronunciation questions).\n\nData collection All studies are performed by online questionnaires. We recruit the respondents by\nan online announcement13 or from an online platform that is similar to Amazon Mechanical Turk. We\nget respondents\u2019 consent for using and sharing their data for research. Respondents are asked not to\nsearch for the answers to the questions or communicate with other people. We allow the respondents\nto answer \u201cI do not know\u201d for all questions. Except for Go problems, all questionnaires use flat\npayment. We illustrate the data collection process in detail in Appendix A. We allow respondents\nto participate in more than one study because our algorithms analyze each question separately and\nindependently.\n\nData processing We merge answers which are the same, like \u20180.5\u2019 and \u201850%\u2019. We omit the answers\nthat are reported by less than (\u2264) 3% of respondents or one person. The remaining answers, excluding\n\u201cI do not know\u201d, form the answer set A whose size is |A|. We then construct the Answer-Prediction\nmatrix and perform our algorithms. Pseudo-codes are attached in Appendix B. Our algorithms do not\nrequire any prior or the respondents\u2019 expertise levels.\n\n3.1 Results\n\n12This may not be a very good assumption since i.i.d. samples can repeat but respondents usually do not\nrepeat their predictions. If we do not want this assumption, we can choose to only use the first prediction from\neach respondent (if there exists) to construct the answer-prediction matrix.\n\n13Many are students from top universities in China. See Appendix A for more details.\n\n8\n\n\fType\nMath\nGo\nGeneral knowledge\nChinese character\n\nTotal Our algorithm(Default) Our algorithm(Variant)\n\n35\n30\n44\n43\n\n29\n28\n41\n36\n\n29\n28\n41\n35\n\nPlurality voting\n24\n23\n35\n34\n\nTable 1: The number of questions our algorithms/baseline are correct.\n\n(a) the Monty Hall problem: you can select one\nclosed door of three. A prize, a car, is behind\none of the doors. The other two doors hide goats.\nAfter you have made your choice, Monty Hall\nwill open one of the remaining doors and show\nthat it does not contain the prize. He then asks\nyou if you would like to switch your choice to\nthe other unopened door. What is the probability\nto get the prize if you switch?\n\n(b) the Taxicab problem: 85% of taxis in this\ncity are green, the others are blue. A witness\nsees a blue taxi. She is usually correct with\nprobability 80%. What is the probability that\nthe taxi saw by the witness is blue?\n\n(c) Pick a move for black such that they can be\nalive.\n\n(d) Pick a move for black such that they can be\nalive by ko.\n\n(e) the boundary question: what river forms the\nboundary between North Korea and China?\n\n(f) the Middle Age New Year question: when\nwas the new year in middle age?\n\n(g) the pronunciation of \u7762\n\n(h) the pronunciation of \u6ec2\n\nFigure 4: The ranked answer-prediction matrices\n\n9\n\n2/31/21/32/31/21/3Answer34261413517146Prediction415080121520415080121520Answer160500008201200801619501104818000522130010218PredictionD3C1B2C2B1D3C1B2C2B1Answer62131091410042012115300002PredictionB8A8B6B7B8A8B6B7Answer112540630001230035PredictionYaluandDoomanYaluSonghuaYaluandDoomanYaluSonghuaAnswer33007410005PredictionApr1Dec25Jan1Apr1Dec25Jan1Answer15410028180633Predictionsui1ju1zhi4zhui1sui1ju1zhi4zhui1Answer115023161300210107Predictionpang1pang2pang1pang2Answer1511216Prediction\fWe compare our approach to the baseline, the plurality voting, regarding the accuracy of the top-\nranking answers. Both of the algorithms beat plurality voting for all studies and the default is slightly\nbetter. Among all 152 questions, in 138 questions, the variant algorithm outputs the same hierarchy as\nthe default algorithm. For other questions, the top-ranking type of the variant algorithm may contain\nmore than one answer. The top-ranking answer is the answer supported by more people among all\nanswers in the top-ranking type. In one question the variant is wrong but the default is correct, the\nvariant algorithm assigns both the correct answer and the incorrect plurality answer to the top-ranking\ntype, thus outputting the incorrect answer as the top-ranking answer.\n\nWe also compute the lack-of-fit index (Definition 2.6) of the algorithms and find that the questions\nthe algorithm outputs the correct answer have a smaller lack-of-fit thus fitting the thinking hierarchy\nmodel better. Therefore, we can use the lack-of-fit index as a reliability index of the algorithms.\n\nWe additionally pick several representative examples for each study (Figure 4) where the matrices\nare ranked by the default algorithm and the diagonal area modified like Example 1.1. In all of these\nexamples, the plurality is incorrect while our approach is correct. Results of other questions are\nillustrated at https://elicitation.info/classroom/1/. Detailed explanations are illustrated\nin Appendix D and here we provide some highlights. First, our approach elicits a rich hierarchy. For\nexample, the taxicab problem is borrowed from Kahneman [8] and previous studies show that people\nusually ignore the base rate and report \u201880%\u2019. The imagined levels can be 41%\u219280%. We elicit a\nmuch richer level \u201c41%\u219250%\u219280%\u219212%\u219215%\u219220%\u201d. Second, the most sophisticated level\nmay fail to predict the least one. In the Taxicab problem, the correct \u201c41%\u201d supporters successfully\npredict the common wrong answer \u201c80%\u201d. However, they fail to predict the surprisingly wrong\nanswers \u201c12%,20%\u201d, which are in contrast successfully predicted by \u201c80%\u201d supporters. Our model\nis sufficiently general to allow this situation. Third, even for problems (like Go) without famous\nmistakes, our approach still works. Moreover, in the boundary question, we identify the correct\nanswer without any prior when only 3 respondents are correct.\n\nBased on this section, do the authors report error bars (e.g., with respect to the random seed after running experiments multiple times)?"}]], "q3d": [[{"role": "user", "content": "The following is the experiments section of the paper you are reviewing:\n\n\nWe conduct four studies, study 1 (35 math problems), study 2 (30 Go problems), study 3 (44 general\nknowledge questions), and study 4 (43 Chinese character pronunciation questions).\n\nData collection All studies are performed by online questionnaires. We recruit the respondents by\nan online announcement13 or from an online platform that is similar to Amazon Mechanical Turk. We\nget respondents\u2019 consent for using and sharing their data for research. Respondents are asked not to\nsearch for the answers to the questions or communicate with other people. We allow the respondents\nto answer \u201cI do not know\u201d for all questions. Except for Go problems, all questionnaires use flat\npayment. We illustrate the data collection process in detail in Appendix A. We allow respondents\nto participate in more than one study because our algorithms analyze each question separately and\nindependently.\n\nData processing We merge answers which are the same, like \u20180.5\u2019 and \u201850%\u2019. We omit the answers\nthat are reported by less than (\u2264) 3% of respondents or one person. The remaining answers, excluding\n\u201cI do not know\u201d, form the answer set A whose size is |A|. We then construct the Answer-Prediction\nmatrix and perform our algorithms. Pseudo-codes are attached in Appendix B. Our algorithms do not\nrequire any prior or the respondents\u2019 expertise levels.\n\n3.1 Results\n\n12This may not be a very good assumption since i.i.d. samples can repeat but respondents usually do not\nrepeat their predictions. If we do not want this assumption, we can choose to only use the first prediction from\neach respondent (if there exists) to construct the answer-prediction matrix.\n\n13Many are students from top universities in China. See Appendix A for more details.\n\n8\n\n\fType\nMath\nGo\nGeneral knowledge\nChinese character\n\nTotal Our algorithm(Default) Our algorithm(Variant)\n\n35\n30\n44\n43\n\n29\n28\n41\n36\n\n29\n28\n41\n35\n\nPlurality voting\n24\n23\n35\n34\n\nTable 1: The number of questions our algorithms/baseline are correct.\n\n(a) the Monty Hall problem: you can select one\nclosed door of three. A prize, a car, is behind\none of the doors. The other two doors hide goats.\nAfter you have made your choice, Monty Hall\nwill open one of the remaining doors and show\nthat it does not contain the prize. He then asks\nyou if you would like to switch your choice to\nthe other unopened door. What is the probability\nto get the prize if you switch?\n\n(b) the Taxicab problem: 85% of taxis in this\ncity are green, the others are blue. A witness\nsees a blue taxi. She is usually correct with\nprobability 80%. What is the probability that\nthe taxi saw by the witness is blue?\n\n(c) Pick a move for black such that they can be\nalive.\n\n(d) Pick a move for black such that they can be\nalive by ko.\n\n(e) the boundary question: what river forms the\nboundary between North Korea and China?\n\n(f) the Middle Age New Year question: when\nwas the new year in middle age?\n\n(g) the pronunciation of \u7762\n\n(h) the pronunciation of \u6ec2\n\nFigure 4: The ranked answer-prediction matrices\n\n9\n\n2/31/21/32/31/21/3Answer34261413517146Prediction415080121520415080121520Answer160500008201200801619501104818000522130010218PredictionD3C1B2C2B1D3C1B2C2B1Answer62131091410042012115300002PredictionB8A8B6B7B8A8B6B7Answer112540630001230035PredictionYaluandDoomanYaluSonghuaYaluandDoomanYaluSonghuaAnswer33007410005PredictionApr1Dec25Jan1Apr1Dec25Jan1Answer15410028180633Predictionsui1ju1zhi4zhui1sui1ju1zhi4zhui1Answer115023161300210107Predictionpang1pang2pang1pang2Answer1511216Prediction\fWe compare our approach to the baseline, the plurality voting, regarding the accuracy of the top-\nranking answers. Both of the algorithms beat plurality voting for all studies and the default is slightly\nbetter. Among all 152 questions, in 138 questions, the variant algorithm outputs the same hierarchy as\nthe default algorithm. For other questions, the top-ranking type of the variant algorithm may contain\nmore than one answer. The top-ranking answer is the answer supported by more people among all\nanswers in the top-ranking type. In one question the variant is wrong but the default is correct, the\nvariant algorithm assigns both the correct answer and the incorrect plurality answer to the top-ranking\ntype, thus outputting the incorrect answer as the top-ranking answer.\n\nWe also compute the lack-of-fit index (Definition 2.6) of the algorithms and find that the questions\nthe algorithm outputs the correct answer have a smaller lack-of-fit thus fitting the thinking hierarchy\nmodel better. Therefore, we can use the lack-of-fit index as a reliability index of the algorithms.\n\nWe additionally pick several representative examples for each study (Figure 4) where the matrices\nare ranked by the default algorithm and the diagonal area modified like Example 1.1. In all of these\nexamples, the plurality is incorrect while our approach is correct. Results of other questions are\nillustrated at https://elicitation.info/classroom/1/. Detailed explanations are illustrated\nin Appendix D and here we provide some highlights. First, our approach elicits a rich hierarchy. For\nexample, the taxicab problem is borrowed from Kahneman [8] and previous studies show that people\nusually ignore the base rate and report \u201880%\u2019. The imagined levels can be 41%\u219280%. We elicit a\nmuch richer level \u201c41%\u219250%\u219280%\u219212%\u219215%\u219220%\u201d. Second, the most sophisticated level\nmay fail to predict the least one. In the Taxicab problem, the correct \u201c41%\u201d supporters successfully\npredict the common wrong answer \u201c80%\u201d. However, they fail to predict the surprisingly wrong\nanswers \u201c12%,20%\u201d, which are in contrast successfully predicted by \u201c80%\u201d supporters. Our model\nis sufficiently general to allow this situation. Third, even for problems (like Go) without famous\nmistakes, our approach still works. Moreover, in the boundary question, we identify the correct\nanswer without any prior when only 3 respondents are correct.\n\nBased on this section, do the authors include the amount of compute and type of resources used (e.g., type of GPUs, internal cluster, or cloud provider)?"}]], "q4a": [[{"role": "user", "content": "The following is the methods section of the paper you are reviewing:\n\n\nWe first introduce our model for thinking hierarchy. Tversky and Kahneman [27] propose that people\nhave two systems, a fast and intuitive system, and a slow and logical system. For example, Alice\nstarts to solve the circle problem. When she reads the question, she can run her intuitive system 1 and\nobtain answer \u201c3\u201d. However, when she starts to think carefully, she runs her more careful system 2 to\nobtain answer \u201c4\u201d.\n\nWe propose a more general model where people can run multiple oracles to approach the question\nduring their thinking process. Conceptually similar to the cognitive hierarchy theory [3], we assume\nthe oracles have different levels. People usually run lower level oracles before the higher level oracles.\nIn the previous example, system 1 is the lower level oracle and system 2 is the higher one. We assume\nthat Alice runs system 2 after system 1. Thus, in our model, people who know the answer is \u201c4\u201d can\n\n4\n\n\fpredict that other people answer \u201c3\u201d, which is hypothesized in Kong and Schoenebeck [10]. It\u2019s also\npossible that some people run the most sophisticated oracle directly without running the lower ones.\nThus, we design the model such that it does not require the higher-type to be able to predict ALL\nlower types. We also allow the oracles\u2019 outputs to be random.\n\nSection 2.1 introduces the model of thinking hierarchy. Section 2.2 reduces the model inference\nproblem to a matrix decomposition problem. Section 2.3 and section 2.4 show how to learn the\nthinking hierarchy.\n\n2.1 Thinking hierarchy\n\nFixing a question q (e.g. the circle problem), T denotes the set of thinking types. A denotes the set of\npossible answers. Both T and A are finite sets. \u2206A denotes all possible distributions over A. We\nsometimes use \u201cprob\u201d as a shorthand for probability.\n\nGenerating answers We will describe how people of different thinking types generate answers.\nDefinition 2.1 (Oracles of thinking types W ). An answer generating oracle maps the question to\nan (random) answer in A. Each type t corresponds to an oracle Ot. The output Ot(q) is a random\nvariable whose distribution is wt \u2208 \u2206A. W denotes a |T | \u00d7 |A| matrix where each row t is wt.\nEach respondent is type t with prob pt and (cid:80)\nrunning the oracle Ot. For all a \u2208 A, the probability that a respondent answers a will be (cid:80)\nWe assume the probability is positive for all a \u2208 A.\nExample 2.2 (A running example). There are two types T = {0, 1}. The answer space is A =\n{3, 4, 6}. O0 will output \u20183\u2019 with probability 0.8 and \u20186\u2019 with probability 0.2. O1 will output \u20184\u2019\n0\n1\n\nt pt = 1. A type t respondent generates the answer by\nt ptwt(a).\n\ndeterministically. In this example, W =\n\nwhere the first row is the distribution of\n\n(cid:20)0.8\n0\n\n0.2\n0\n\n(cid:21)\n\nO0\u2019s output and the second row is the distribution of O1\u2019s output.\n\nGenerating predictions We then describe how people of different thinking types predict what other\npeople will answer. Here the prediction is not a distribution, but an answer other people may report.\nWhen a type t respondent makes a prediction, she will run an oracle, which is Ot\u2032 with probability\npt\u2192t\u2032 where (cid:80)\n\nt\u2032 pt\u2192t\u2032 = 1. She uses the output of Ot\u2032 as the prediction g \u2208 A.\n\nCombination: answer-prediction joint distribution M M denotes a |A| \u00d7 |A| matrix where\nMa,g is the probability an respondent answers a and predicts g. \u039b denotes a |T | \u00d7 |T | matrix where\n\u039bt,t\u2032 = ptpt\u2192t\u2032 is the probability a respondent is type t and predicts type t\u2032.\nExample 2.3. In this example, when type 0 respondent makes a prediction, with prob 1, she will run\nO0 again. When type 1 respondent makes a prediction, with prob 0.5, she will run O1 again, with\nprob 0.5, she will run O0. Moreover, a respondent is type 0 with prob 0.7, and type 1 with prob 0.3.\n\nHere \u039b =\n\n(cid:20)p0p0\u21920\np1p1\u21920\n\np0p0\u21921\np1p1\u21921\n\n(cid:21)\n\n=\n\n(cid:20) 0.7 \u2217 1\n0.3 \u2217 0.5\n\n0.7 \u2217 0\n0.3 \u2217 0.5\n\n(cid:21)\n\n=\n\n(cid:20) 0.7\n0.15\n\n(cid:21)\n\n.\n\n0\n0.15\n\nClaim 2.4. Based on the above generating processes, M = W\u22a4\u039bW.\n\nProof. For each respondent, the probability she answers a and predicts g will be\n(cid:88)\n\n(cid:88)\n\n(cid:88)\n\nptwt(a)\n\npt\u2192t\u2032wt\u2032(g) =\n\nwt(a)ptpt\u2192t\u2032wt\u2032(g).\n\nMa,g =\n\nt\n\nt\u2032\n\nt,t\u2032\n\nWe sum over all possible types the respondent will be. Given she is type t, she runs oracle Ot to\ngenerate the answer and wt(a) is the probability that the answer is a. We sum over all possible\noracles she runs to predict. Given that she runs Ot\u2032, wt\u2032(g) is the probability the prediction is g.\n\nKey assumption: upper-triangular \u039b we assume that people of a less sophisticated level can\nnever run the oracles of more sophisticated levels. A linear ordering of types \u03c0 : {1, 2, \u00b7 \u00b7 \u00b7 , |T |} (cid:55)\u2192 T\nmaps a ranking position to a type. For example, \u03c0(1) \u2208 T is the top-ranking type.\nAssumption 2.5. We assume that with a proper ordering \u03c0 of the types, \u039b is an upper-triangular\nmatrix. Formally, there exists \u03c0 such that \u2200i > j, \u039b\u03c0(i),\u03c0(j) = 0. Any \u03c0 that makes \u039b upper-\ntriangular is a valid thinking hierarchy of the types.\n\n5\n\n\fIn the running example, the valid thinking hierarchy is \u03c0(1) = type 1, \u03c0(2) = type 0. Note that the\nabove assumption does not require \u2200i \u2264 j, \u039b\u03c0(i),\u03c0(j) > 0. When \u039b is a diagonal matrix, types cannot\npredict each other and are equally sophisticated, thus any ordering is a valid thinking hierarchy.\n\nAn algorithm finds the thinking hierarchy when the algorithm is given M which is generated by\nlatent (unknown) W, \u039b, and the algorithm will output a matrix W\u2217 which equals a row-permuted\nW where the row order is a valid thinking hierarchy. Formally, there exists a valid thinking hierarchy\n\u03c0 such that the ith row of W\u2217 is the \u03c0(i)th row of W, i.e, w\u2217\n\ni = w\u03c0(i).\n\n2.2 Non-negative Congruence Triangularization (NCT)\n\nWith the above model, inferring thinking hierarchy leads to a novel matrix decomposition problem,\nwhich is similar to the symmetric non-negative matrix factorization problem (NMF)6. A non-negative\nmatrix is a matrix whose elements are non-negative.\nDefinition 2.6 (Non-negative Congruence7 Triangularization (NCT)). Given a non-negative matrix\nM, NCT aims to find non-negative matrices W and non-negative upper-triangular matrix \u039b such\nthat M = W\u22a4\u039bW. In a Frobenius norm based approximated version, given a set of matrices W,\nNCT aims to find non-negative matrices W and non-negative upper-triangular matrix \u039b to minimize\n\nmin\nW\u2208W,\u039b\n\n||M \u2212 W\u22a4\u039bW||2\nF\n\nand the minimum is defined as the lack-of-fit of M regarding W 8.\n\nLike NMF, it\u2019s impossible to ask for the strict uniqueness of the results. Let P\u039b be the set of\npermutation matrices such that \u03a0\u22a4\u039b\u03a0 is still upper-triangular.\nIf (W, \u039b) is a solution, then\n(\u03a0\u22121DW, \u03a0\u22a4D\u22121\u039bD\u22121\u03a0) is also a solution where D is a diagonal matrix with positive elements\nand \u03a0 \u2208 P\u039b. We state the uniqueness results as follows and the proof is deferred to Appendix C.\nProposition 2.7 (Uniqueness). If |T | \u2264 |A| and T columns of W consist of a permuted positive\ndiagonal matrix, NCT for M = W\u22a4\u039bW is unique in the sense that then for all W\u2032\u22a4\u039b\u2032W\u2032 =\nW\u22a4\u039bW, there exists a positive diagonal matrix D and a |T | \u00d7 |T | permutation matrix \u03a0 \u2208 P\u039b\nsuch that W\u2032 = \u03a0\u22121DW.\n\nWhen we restrict W to be \u201csemi-orthogonal\u201d, we obtain a clean format of NCT without searching\nfor optimal \u039b. I is the set of all \u201csemi-orthogonal\u201d matrices W where each column of W has\nand only has one non-zero element and WW\u22a4 = I. For example, the W in Example 2.2 can be\nnormalized to a semi-orthogonal matrix. The following lemma follows from the expansion of the\nFrobenius norm and we defer the proof to Appendix C.\n\nLemma 2.8 (Semi-orthogonal: minimizing F-norm = maximizing upper-triangular\u2019s sum of the\nsquare). For all set of matrices W \u2282 I, minW\u2208W,\u039b ||M \u2212 W\u22a4\u039bW||2\nF is equivalent to solv-\ni,j and setting \u039b as Up(WMW\u22a4), the upper-triangular area of\ning maxW\u2208W\nWMW\u22a4.\n\ni\u2264j(WMW\u22a4)2\n\n(cid:80)\n\n2.3\n\nInferring the thinking hierarchy with answer-prediction joint distribution M\n\nGiven M, inferring the thinking hierarchy is equivalent to solving NCT in general. Though we do not\nhave M, later we will show a proxy for M. For simplicity of practical use, we introduce two simple\nranking algorithms by employing Lemma 2.8. The ranking algorithm takes M as input and outputs a\nlinear ordering of answers \u03c0 : {1, 2, \u00b7 \u00b7 \u00b7 , |A|} (cid:55)\u2192 A that maps a ranking position to an answer.\n\nAnswer-Ranking Algorithm (Default) AR(M) The algorithm computes\n\n\u03a0\u2217 \u2190 arg max\n\u03a0\u2208P\n\n(cid:88)\n\n(\u03a0M\u03a0\u22a4)2\ni,j\n\ni\u2264j\n\n6Symmetric NMF: minW ||M \u2212 W\u22a4W||2\nF\n7We use congruence here though it is not matrix congruence since W may not be a square matrix.\n8M = W\u22a4\u039bW, W \u2208 W has zero lack-of-fit.\n\n6\n\n\fwhere P is the set of all |A| \u00d7 |A| permutation matrices. There is a one to one mapping between each\npermutation matrix \u03a0 and a linear ordering \u03c0: \u03a0i,\u03c0(i) = 1, \u2200i. Therefore, the optimal \u03a0\u2217 leads to an\noptimal rank over answers directly and the default algorithm can be also represented as\n\n\u03c0\u2217 \u2190 arg max\n\n\u03c0\n\nM 2\n\n\u03c0(i),\u03c0(j).\n\n(cid:88)\n\ni\u2264j\n\nTo find the optimal rank, we use a dynamic programming based algorithm (see the supplementary\nmaterials) which takes O(2|A||A|2). In practice, |A| is usually at most 7 or 8. In our empirical study,\nthe default algorithm takes 91 milliseconds to finish the computation of all 152 questions.\n\nThe default algorithm implicitly assumes |T | = |A| and all oracles are deterministic. To allow\n|T | < |A| and non-deterministic oracles, we introduce a variant that generalizes P to a subset of\nsemi-orthogonal matrices I. Every |T | \u00d7 |A| semi-orthogonal W indicates a hard clustering. Each\ncluster t \u2208 T contains all answers a such that Wt,a > 0. For example, the W in Example 2.2 can\nbe normalized to a semi-orthogonal matrix and indicates a hard clustering {4}, {6, 3}. Therefore,\nthe variant algorithm will partition the answers into multiple clusters and assign a hierarchy to the\nclusters.\n\nAnswer-Ranking Algorithm (Variant) AR+(M, W) The algorithm computes\n\nW\u2217 \u2190 arg max\nW\u2208I\n\n(cid:88)\n\n(WMW\u22a4)2\n\ni,j.\n\ni\u2264j\n\nwhere W \u2282 I. W\u2217 is normalized such that every row sums to 1. This algorithm does not restrict\n|T | = |A| and learns the optimal |T |.\n\nW\u2217 \u21d2 Answer rank The output W\u2217 indicates a hard clustering of all answers. We rank all answer\nas follows: for any i < j, the answers in cluster i has a higher rank 9 than the answers in cluster j.\nFor all i, for any two answers a, a\u2032 in the same cluster i, a is ranked higher than a\u2032 if W \u2217\ni,a\u2032.\n\ni,a > W \u2217\n\nTheoretical justification When M perfectly fits the model with the restriction that the latent W is\na permutation or a hard clustering, we show that our algorithm finds the thinking hierarchy. Otherwise,\nour algorithm finds the \u201cclosest\u201d solution measured by the Frobenius norm.\nTheorem 2.9. When there exists \u03a00 \u2208 P and non-negative upper-triangular matrix \u039b0 such that\n0 \u039b0\u03a00, AR(M) finds the thinking hierarchy 10. In general, AR(M) will output \u03a0\u2217 where\nM = \u03a0\u22a4\n\u03a0\u2217, \u039b\u2217 = Up(\u03a0\u2217M\u03a0\u2217\u22a4) is a solution to arg min\u03a0\u2208P,\u039b ||M \u2212 \u03a0\u22a4\u039b\u03a0||2\nF . The above statement\nis still valid by replacing P by W \u2282 I and AR(M) by AR+(M, W).\n\nProposition 2.7 and Lemma 2.8 almost directly imply the above theorem. We defer the formal proof\nto Appendix C.\n\n2.4 A proxy for answer-prediction joint distribution M\n\nIn practice, we do not have perfect M. We use the following open-response paradigm to obtain a\nproxy for M.\n\nAnswer-prediction paradigm the respondents are asked Q1: What\u2019s your answer? Answer:\nQ2: What do you think other people will answer:\n\n.\n\nIn the circle example, the possible feedback can be \u201canswer: 4; prediction: 3\u201d, \u201canswer: 3; prediction:\n6,9,1\u201d... We collect all answers provided by the respondents 11 and denote the set of them by A. In\nthe circle example, A = {1, 2, 3, 4, 6, 9}. We also allow respondents to provide no prediction or\nmultiple predictions.\n\n9Rank 1 answer is in the highest position.\n10See definition in the last paragraph in Section 2.1.\n11In practice, we set a threshold \u03b8 and collect answers which are provided by at least \u03b8 fraction of the\nrespondents. We allow multiple predictions and also allow people to answer \u201cI do not know\u201d. See Section 3 for\nmore details.\n\n7\n\n\f(a) Accuracy of algorithms\n\n(b) Empirical distribution of lack-of-fit\n\nFigure 3: The results of our experiment\n\nAnswer-prediction matrix We aggregate the feedback and visualize it by an Answer-Prediction\nmatrix. The Answer-Prediction matrix A is a |A| \u00d7 |A| square matrix where |A| is the number of\ndistinct answers provided by the respondents. Each entry Aa,g, a, g \u2208 A is the number of respondents\nthat answer \u201ca\u201d and predict \u201cg\u201d.\n\nWe will show that with proper assumptions, the answer-prediction matrix A\u2019s expectation is pro-\nportional to M. First, for ease of analysis, we assume that each respondent\u2019s predictions are i.i.d.\nsamples12. Second, since we allow people to optionally provide predictions, we need to additionally\nassume that the number of predictions each respondent is willing to provide is independent of her\ntype and answer. We state the formal result as follows and the proof is deferred to Appendix C.\n\nProposition 2.10. When each respondent\u2019s predictions are i.i.d. samples, and the number of\npredictions she gives is independent of her type and answer, the answer-prediction matrix A\u2019s\nexpectation is proportional to M.\n\n\nBased on this section, do the authors use existing assets (e.g., code, data, models) that are not new contributions in their paper? Please start your answer with \"Yes\", \"No\", or \"Unsure\"."}, {"role": "user", "content": "The following is the experiments section of the paper you are reviewing:\nexperiments section: \n\nWe conduct four studies, study 1 (35 math problems), study 2 (30 Go problems), study 3 (44 general\nknowledge questions), and study 4 (43 Chinese character pronunciation questions).\n\nData collection All studies are performed by online questionnaires. We recruit the respondents by\nan online announcement13 or from an online platform that is similar to Amazon Mechanical Turk. We\nget respondents\u2019 consent for using and sharing their data for research. Respondents are asked not to\nsearch for the answers to the questions or communicate with other people. We allow the respondents\nto answer \u201cI do not know\u201d for all questions. Except for Go problems, all questionnaires use flat\npayment. We illustrate the data collection process in detail in Appendix A. We allow respondents\nto participate in more than one study because our algorithms analyze each question separately and\nindependently.\n\nData processing We merge answers which are the same, like \u20180.5\u2019 and \u201850%\u2019. We omit the answers\nthat are reported by less than (\u2264) 3% of respondents or one person. The remaining answers, excluding\n\u201cI do not know\u201d, form the answer set A whose size is |A|. We then construct the Answer-Prediction\nmatrix and perform our algorithms. Pseudo-codes are attached in Appendix B. Our algorithms do not\nrequire any prior or the respondents\u2019 expertise levels.\n\n3.1 Results\n\n12This may not be a very good assumption since i.i.d. samples can repeat but respondents usually do not\nrepeat their predictions. If we do not want this assumption, we can choose to only use the first prediction from\neach respondent (if there exists) to construct the answer-prediction matrix.\n\n13Many are students from top universities in China. See Appendix A for more details.\n\n8\n\n\fType\nMath\nGo\nGeneral knowledge\nChinese character\n\nTotal Our algorithm(Default) Our algorithm(Variant)\n\n35\n30\n44\n43\n\n29\n28\n41\n36\n\n29\n28\n41\n35\n\nPlurality voting\n24\n23\n35\n34\n\nTable 1: The number of questions our algorithms/baseline are correct.\n\n(a) the Monty Hall problem: you can select one\nclosed door of three. A prize, a car, is behind\none of the doors. The other two doors hide goats.\nAfter you have made your choice, Monty Hall\nwill open one of the remaining doors and show\nthat it does not contain the prize. He then asks\nyou if you would like to switch your choice to\nthe other unopened door. What is the probability\nto get the prize if you switch?\n\n(b) the Taxicab problem: 85% of taxis in this\ncity are green, the others are blue. A witness\nsees a blue taxi. She is usually correct with\nprobability 80%. What is the probability that\nthe taxi saw by the witness is blue?\n\n(c) Pick a move for black such that they can be\nalive.\n\n(d) Pick a move for black such that they can be\nalive by ko.\n\n(e) the boundary question: what river forms the\nboundary between North Korea and China?\n\n(f) the Middle Age New Year question: when\nwas the new year in middle age?\n\n(g) the pronunciation of \u7762\n\n(h) the pronunciation of \u6ec2\n\nFigure 4: The ranked answer-prediction matrices\n\n9\n\n2/31/21/32/31/21/3Answer34261413517146Prediction415080121520415080121520Answer160500008201200801619501104818000522130010218PredictionD3C1B2C2B1D3C1B2C2B1Answer62131091410042012115300002PredictionB8A8B6B7B8A8B6B7Answer112540630001230035PredictionYaluandDoomanYaluSonghuaYaluandDoomanYaluSonghuaAnswer33007410005PredictionApr1Dec25Jan1Apr1Dec25Jan1Answer15410028180633Predictionsui1ju1zhi4zhui1sui1ju1zhi4zhui1Answer115023161300210107Predictionpang1pang2pang1pang2Answer1511216Prediction\fWe compare our approach to the baseline, the plurality voting, regarding the accuracy of the top-\nranking answers. Both of the algorithms beat plurality voting for all studies and the default is slightly\nbetter. Among all 152 questions, in 138 questions, the variant algorithm outputs the same hierarchy as\nthe default algorithm. For other questions, the top-ranking type of the variant algorithm may contain\nmore than one answer. The top-ranking answer is the answer supported by more people among all\nanswers in the top-ranking type. In one question the variant is wrong but the default is correct, the\nvariant algorithm assigns both the correct answer and the incorrect plurality answer to the top-ranking\ntype, thus outputting the incorrect answer as the top-ranking answer.\n\nWe also compute the lack-of-fit index (Definition 2.6) of the algorithms and find that the questions\nthe algorithm outputs the correct answer have a smaller lack-of-fit thus fitting the thinking hierarchy\nmodel better. Therefore, we can use the lack-of-fit index as a reliability index of the algorithms.\n\nWe additionally pick several representative examples for each study (Figure 4) where the matrices\nare ranked by the default algorithm and the diagonal area modified like Example 1.1. In all of these\nexamples, the plurality is incorrect while our approach is correct. Results of other questions are\nillustrated at https://elicitation.info/classroom/1/. Detailed explanations are illustrated\nin Appendix D and here we provide some highlights. First, our approach elicits a rich hierarchy. For\nexample, the taxicab problem is borrowed from Kahneman [8] and previous studies show that people\nusually ignore the base rate and report \u201880%\u2019. The imagined levels can be 41%\u219280%. We elicit a\nmuch richer level \u201c41%\u219250%\u219280%\u219212%\u219215%\u219220%\u201d. Second, the most sophisticated level\nmay fail to predict the least one. In the Taxicab problem, the correct \u201c41%\u201d supporters successfully\npredict the common wrong answer \u201c80%\u201d. However, they fail to predict the surprisingly wrong\nanswers \u201c12%,20%\u201d, which are in contrast successfully predicted by \u201c80%\u201d supporters. Our model\nis sufficiently general to allow this situation. Third, even for problems (like Go) without famous\nmistakes, our approach still works. Moreover, in the boundary question, we identify the correct\nanswer without any prior when only 3 respondents are correct.\n\n\nBased on the section, do the authors cite the creators of the existing assets that they use?"}]], "q4b": [[{"role": "user", "content": "The following is the experiments section of the paper you are reviewing:\n\n\nWe conduct four studies, study 1 (35 math problems), study 2 (30 Go problems), study 3 (44 general\nknowledge questions), and study 4 (43 Chinese character pronunciation questions).\n\nData collection All studies are performed by online questionnaires. We recruit the respondents by\nan online announcement13 or from an online platform that is similar to Amazon Mechanical Turk. We\nget respondents\u2019 consent for using and sharing their data for research. Respondents are asked not to\nsearch for the answers to the questions or communicate with other people. We allow the respondents\nto answer \u201cI do not know\u201d for all questions. Except for Go problems, all questionnaires use flat\npayment. We illustrate the data collection process in detail in Appendix A. We allow respondents\nto participate in more than one study because our algorithms analyze each question separately and\nindependently.\n\nData processing We merge answers which are the same, like \u20180.5\u2019 and \u201850%\u2019. We omit the answers\nthat are reported by less than (\u2264) 3% of respondents or one person. The remaining answers, excluding\n\u201cI do not know\u201d, form the answer set A whose size is |A|. We then construct the Answer-Prediction\nmatrix and perform our algorithms. Pseudo-codes are attached in Appendix B. Our algorithms do not\nrequire any prior or the respondents\u2019 expertise levels.\n\n3.1 Results\n\n12This may not be a very good assumption since i.i.d. samples can repeat but respondents usually do not\nrepeat their predictions. If we do not want this assumption, we can choose to only use the first prediction from\neach respondent (if there exists) to construct the answer-prediction matrix.\n\n13Many are students from top universities in China. See Appendix A for more details.\n\n8\n\n\fType\nMath\nGo\nGeneral knowledge\nChinese character\n\nTotal Our algorithm(Default) Our algorithm(Variant)\n\n35\n30\n44\n43\n\n29\n28\n41\n36\n\n29\n28\n41\n35\n\nPlurality voting\n24\n23\n35\n34\n\nTable 1: The number of questions our algorithms/baseline are correct.\n\n(a) the Monty Hall problem: you can select one\nclosed door of three. A prize, a car, is behind\none of the doors. The other two doors hide goats.\nAfter you have made your choice, Monty Hall\nwill open one of the remaining doors and show\nthat it does not contain the prize. He then asks\nyou if you would like to switch your choice to\nthe other unopened door. What is the probability\nto get the prize if you switch?\n\n(b) the Taxicab problem: 85% of taxis in this\ncity are green, the others are blue. A witness\nsees a blue taxi. She is usually correct with\nprobability 80%. What is the probability that\nthe taxi saw by the witness is blue?\n\n(c) Pick a move for black such that they can be\nalive.\n\n(d) Pick a move for black such that they can be\nalive by ko.\n\n(e) the boundary question: what river forms the\nboundary between North Korea and China?\n\n(f) the Middle Age New Year question: when\nwas the new year in middle age?\n\n(g) the pronunciation of \u7762\n\n(h) the pronunciation of \u6ec2\n\nFigure 4: The ranked answer-prediction matrices\n\n9\n\n2/31/21/32/31/21/3Answer34261413517146Prediction415080121520415080121520Answer160500008201200801619501104818000522130010218PredictionD3C1B2C2B1D3C1B2C2B1Answer62131091410042012115300002PredictionB8A8B6B7B8A8B6B7Answer112540630001230035PredictionYaluandDoomanYaluSonghuaYaluandDoomanYaluSonghuaAnswer33007410005PredictionApr1Dec25Jan1Apr1Dec25Jan1Answer15410028180633Predictionsui1ju1zhi4zhui1sui1ju1zhi4zhui1Answer115023161300210107Predictionpang1pang2pang1pang2Answer1511216Prediction\fWe compare our approach to the baseline, the plurality voting, regarding the accuracy of the top-\nranking answers. Both of the algorithms beat plurality voting for all studies and the default is slightly\nbetter. Among all 152 questions, in 138 questions, the variant algorithm outputs the same hierarchy as\nthe default algorithm. For other questions, the top-ranking type of the variant algorithm may contain\nmore than one answer. The top-ranking answer is the answer supported by more people among all\nanswers in the top-ranking type. In one question the variant is wrong but the default is correct, the\nvariant algorithm assigns both the correct answer and the incorrect plurality answer to the top-ranking\ntype, thus outputting the incorrect answer as the top-ranking answer.\n\nWe also compute the lack-of-fit index (Definition 2.6) of the algorithms and find that the questions\nthe algorithm outputs the correct answer have a smaller lack-of-fit thus fitting the thinking hierarchy\nmodel better. Therefore, we can use the lack-of-fit index as a reliability index of the algorithms.\n\nWe additionally pick several representative examples for each study (Figure 4) where the matrices\nare ranked by the default algorithm and the diagonal area modified like Example 1.1. In all of these\nexamples, the plurality is incorrect while our approach is correct. Results of other questions are\nillustrated at https://elicitation.info/classroom/1/. Detailed explanations are illustrated\nin Appendix D and here we provide some highlights. First, our approach elicits a rich hierarchy. For\nexample, the taxicab problem is borrowed from Kahneman [8] and previous studies show that people\nusually ignore the base rate and report \u201880%\u2019. The imagined levels can be 41%\u219280%. We elicit a\nmuch richer level \u201c41%\u219250%\u219280%\u219212%\u219215%\u219220%\u201d. Second, the most sophisticated level\nmay fail to predict the least one. In the Taxicab problem, the correct \u201c41%\u201d supporters successfully\npredict the common wrong answer \u201c80%\u201d. However, they fail to predict the surprisingly wrong\nanswers \u201c12%,20%\u201d, which are in contrast successfully predicted by \u201c80%\u201d supporters. Our model\nis sufficiently general to allow this situation. Third, even for problems (like Go) without famous\nmistakes, our approach still works. Moreover, in the boundary question, we identify the correct\nanswer without any prior when only 3 respondents are correct.\n\n\nBased on this section, do the authors use existing assets (e.g., code, data, models) that are not new contributions in their paper? Please start your answer with \"Yes\", \"No\", or \"Unsure\"."}, {"role": "user", "content": "The following is the experiments section of the paper you are reviewing:\n\n\nWe conduct four studies, study 1 (35 math problems), study 2 (30 Go problems), study 3 (44 general\nknowledge questions), and study 4 (43 Chinese character pronunciation questions).\n\nData collection All studies are performed by online questionnaires. We recruit the respondents by\nan online announcement13 or from an online platform that is similar to Amazon Mechanical Turk. We\nget respondents\u2019 consent for using and sharing their data for research. Respondents are asked not to\nsearch for the answers to the questions or communicate with other people. We allow the respondents\nto answer \u201cI do not know\u201d for all questions. Except for Go problems, all questionnaires use flat\npayment. We illustrate the data collection process in detail in Appendix A. We allow respondents\nto participate in more than one study because our algorithms analyze each question separately and\nindependently.\n\nData processing We merge answers which are the same, like \u20180.5\u2019 and \u201850%\u2019. We omit the answers\nthat are reported by less than (\u2264) 3% of respondents or one person. The remaining answers, excluding\n\u201cI do not know\u201d, form the answer set A whose size is |A|. We then construct the Answer-Prediction\nmatrix and perform our algorithms. Pseudo-codes are attached in Appendix B. Our algorithms do not\nrequire any prior or the respondents\u2019 expertise levels.\n\n3.1 Results\n\n12This may not be a very good assumption since i.i.d. samples can repeat but respondents usually do not\nrepeat their predictions. If we do not want this assumption, we can choose to only use the first prediction from\neach respondent (if there exists) to construct the answer-prediction matrix.\n\n13Many are students from top universities in China. See Appendix A for more details.\n\n8\n\n\fType\nMath\nGo\nGeneral knowledge\nChinese character\n\nTotal Our algorithm(Default) Our algorithm(Variant)\n\n35\n30\n44\n43\n\n29\n28\n41\n36\n\n29\n28\n41\n35\n\nPlurality voting\n24\n23\n35\n34\n\nTable 1: The number of questions our algorithms/baseline are correct.\n\n(a) the Monty Hall problem: you can select one\nclosed door of three. A prize, a car, is behind\none of the doors. The other two doors hide goats.\nAfter you have made your choice, Monty Hall\nwill open one of the remaining doors and show\nthat it does not contain the prize. He then asks\nyou if you would like to switch your choice to\nthe other unopened door. What is the probability\nto get the prize if you switch?\n\n(b) the Taxicab problem: 85% of taxis in this\ncity are green, the others are blue. A witness\nsees a blue taxi. She is usually correct with\nprobability 80%. What is the probability that\nthe taxi saw by the witness is blue?\n\n(c) Pick a move for black such that they can be\nalive.\n\n(d) Pick a move for black such that they can be\nalive by ko.\n\n(e) the boundary question: what river forms the\nboundary between North Korea and China?\n\n(f) the Middle Age New Year question: when\nwas the new year in middle age?\n\n(g) the pronunciation of \u7762\n\n(h) the pronunciation of \u6ec2\n\nFigure 4: The ranked answer-prediction matrices\n\n9\n\n2/31/21/32/31/21/3Answer34261413517146Prediction415080121520415080121520Answer160500008201200801619501104818000522130010218PredictionD3C1B2C2B1D3C1B2C2B1Answer62131091410042012115300002PredictionB8A8B6B7B8A8B6B7Answer112540630001230035PredictionYaluandDoomanYaluSonghuaYaluandDoomanYaluSonghuaAnswer33007410005PredictionApr1Dec25Jan1Apr1Dec25Jan1Answer15410028180633Predictionsui1ju1zhi4zhui1sui1ju1zhi4zhui1Answer115023161300210107Predictionpang1pang2pang1pang2Answer1511216Prediction\fWe compare our approach to the baseline, the plurality voting, regarding the accuracy of the top-\nranking answers. Both of the algorithms beat plurality voting for all studies and the default is slightly\nbetter. Among all 152 questions, in 138 questions, the variant algorithm outputs the same hierarchy as\nthe default algorithm. For other questions, the top-ranking type of the variant algorithm may contain\nmore than one answer. The top-ranking answer is the answer supported by more people among all\nanswers in the top-ranking type. In one question the variant is wrong but the default is correct, the\nvariant algorithm assigns both the correct answer and the incorrect plurality answer to the top-ranking\ntype, thus outputting the incorrect answer as the top-ranking answer.\n\nWe also compute the lack-of-fit index (Definition 2.6) of the algorithms and find that the questions\nthe algorithm outputs the correct answer have a smaller lack-of-fit thus fitting the thinking hierarchy\nmodel better. Therefore, we can use the lack-of-fit index as a reliability index of the algorithms.\n\nWe additionally pick several representative examples for each study (Figure 4) where the matrices\nare ranked by the default algorithm and the diagonal area modified like Example 1.1. In all of these\nexamples, the plurality is incorrect while our approach is correct. Results of other questions are\nillustrated at https://elicitation.info/classroom/1/. Detailed explanations are illustrated\nin Appendix D and here we provide some highlights. First, our approach elicits a rich hierarchy. For\nexample, the taxicab problem is borrowed from Kahneman [8] and previous studies show that people\nusually ignore the base rate and report \u201880%\u2019. The imagined levels can be 41%\u219280%. We elicit a\nmuch richer level \u201c41%\u219250%\u219280%\u219212%\u219215%\u219220%\u201d. Second, the most sophisticated level\nmay fail to predict the least one. In the Taxicab problem, the correct \u201c41%\u201d supporters successfully\npredict the common wrong answer \u201c80%\u201d. However, they fail to predict the surprisingly wrong\nanswers \u201c12%,20%\u201d, which are in contrast successfully predicted by \u201c80%\u201d supporters. Our model\nis sufficiently general to allow this situation. Third, even for problems (like Go) without famous\nmistakes, our approach still works. Moreover, in the boundary question, we identify the correct\nanswer without any prior when only 3 respondents are correct.\n\n\nBased on the section, do the authors mention the licenses of the existing assets that they use?"}]], "q4c": [[{"role": "user", "content": "The following is the abstract section of the paper you are reviewing:\n\n\nWhen we use the wisdom of the crowds, we usually rank the answers according to\ntheir popularity, especially when we cannot verify the answers. However, this can\nbe very dangerous when the majority make systematic mistakes. A fundamental\nquestion arises: can we build a hierarchy among the answers without any prior\nwhere the higher-ranking answers, which may not be supported by the majority,\nare from more sophisticated people? To address the question, we propose 1) a\nnovel model to describe people\u2019s thinking hierarchy; 2) two algorithms to learn\nthe thinking hierarchy without any prior; 3) a novel open-response based crowd-\nsourcing approach based on the above theoretic framework. In addition to theoretic\njustifications, we conduct four empirical crowdsourcing studies and show that a)\nthe accuracy of the top-ranking answers learned by our approach is much higher\nthan that of plurality voting (In one question, the plurality answer is supported by\n74 respondents but the correct answer is only supported by 3 respondents. Our\napproach ranks the correct answer the highest without any prior); b) our model\nhas a high goodness-of-fit, especially for the questions where our top-ranking\nanswer is correct. To the best of our knowledge, we are the first to propose a\nthinking hierarchy model with empirical validations in the general problem-solving\nscenarios; and the first to propose a practical open-response based crowdsourcing\napproach that beats plurality voting without any prior.\n\n1\n\n\nBased on this abstract, do the authors curate/release new assets (e.g., code, data, models)? Please start your answer with \"Yes\", \"No\", or \"Unsure\"."}, {"role": "user", "content": "The following is the experiments section of the paper you are reviewing:\n\n\nWe conduct four studies, study 1 (35 math problems), study 2 (30 Go problems), study 3 (44 general\nknowledge questions), and study 4 (43 Chinese character pronunciation questions).\n\nData collection All studies are performed by online questionnaires. We recruit the respondents by\nan online announcement13 or from an online platform that is similar to Amazon Mechanical Turk. We\nget respondents\u2019 consent for using and sharing their data for research. Respondents are asked not to\nsearch for the answers to the questions or communicate with other people. We allow the respondents\nto answer \u201cI do not know\u201d for all questions. Except for Go problems, all questionnaires use flat\npayment. We illustrate the data collection process in detail in Appendix A. We allow respondents\nto participate in more than one study because our algorithms analyze each question separately and\nindependently.\n\nData processing We merge answers which are the same, like \u20180.5\u2019 and \u201850%\u2019. We omit the answers\nthat are reported by less than (\u2264) 3% of respondents or one person. The remaining answers, excluding\n\u201cI do not know\u201d, form the answer set A whose size is |A|. We then construct the Answer-Prediction\nmatrix and perform our algorithms. Pseudo-codes are attached in Appendix B. Our algorithms do not\nrequire any prior or the respondents\u2019 expertise levels.\n\n3.1 Results\n\n12This may not be a very good assumption since i.i.d. samples can repeat but respondents usually do not\nrepeat their predictions. If we do not want this assumption, we can choose to only use the first prediction from\neach respondent (if there exists) to construct the answer-prediction matrix.\n\n13Many are students from top universities in China. See Appendix A for more details.\n\n8\n\n\fType\nMath\nGo\nGeneral knowledge\nChinese character\n\nTotal Our algorithm(Default) Our algorithm(Variant)\n\n35\n30\n44\n43\n\n29\n28\n41\n36\n\n29\n28\n41\n35\n\nPlurality voting\n24\n23\n35\n34\n\nTable 1: The number of questions our algorithms/baseline are correct.\n\n(a) the Monty Hall problem: you can select one\nclosed door of three. A prize, a car, is behind\none of the doors. The other two doors hide goats.\nAfter you have made your choice, Monty Hall\nwill open one of the remaining doors and show\nthat it does not contain the prize. He then asks\nyou if you would like to switch your choice to\nthe other unopened door. What is the probability\nto get the prize if you switch?\n\n(b) the Taxicab problem: 85% of taxis in this\ncity are green, the others are blue. A witness\nsees a blue taxi. She is usually correct with\nprobability 80%. What is the probability that\nthe taxi saw by the witness is blue?\n\n(c) Pick a move for black such that they can be\nalive.\n\n(d) Pick a move for black such that they can be\nalive by ko.\n\n(e) the boundary question: what river forms the\nboundary between North Korea and China?\n\n(f) the Middle Age New Year question: when\nwas the new year in middle age?\n\n(g) the pronunciation of \u7762\n\n(h) the pronunciation of \u6ec2\n\nFigure 4: The ranked answer-prediction matrices\n\n9\n\n2/31/21/32/31/21/3Answer34261413517146Prediction415080121520415080121520Answer160500008201200801619501104818000522130010218PredictionD3C1B2C2B1D3C1B2C2B1Answer62131091410042012115300002PredictionB8A8B6B7B8A8B6B7Answer112540630001230035PredictionYaluandDoomanYaluSonghuaYaluandDoomanYaluSonghuaAnswer33007410005PredictionApr1Dec25Jan1Apr1Dec25Jan1Answer15410028180633Predictionsui1ju1zhi4zhui1sui1ju1zhi4zhui1Answer115023161300210107Predictionpang1pang2pang1pang2Answer1511216Prediction\fWe compare our approach to the baseline, the plurality voting, regarding the accuracy of the top-\nranking answers. Both of the algorithms beat plurality voting for all studies and the default is slightly\nbetter. Among all 152 questions, in 138 questions, the variant algorithm outputs the same hierarchy as\nthe default algorithm. For other questions, the top-ranking type of the variant algorithm may contain\nmore than one answer. The top-ranking answer is the answer supported by more people among all\nanswers in the top-ranking type. In one question the variant is wrong but the default is correct, the\nvariant algorithm assigns both the correct answer and the incorrect plurality answer to the top-ranking\ntype, thus outputting the incorrect answer as the top-ranking answer.\n\nWe also compute the lack-of-fit index (Definition 2.6) of the algorithms and find that the questions\nthe algorithm outputs the correct answer have a smaller lack-of-fit thus fitting the thinking hierarchy\nmodel better. Therefore, we can use the lack-of-fit index as a reliability index of the algorithms.\n\nWe additionally pick several representative examples for each study (Figure 4) where the matrices\nare ranked by the default algorithm and the diagonal area modified like Example 1.1. In all of these\nexamples, the plurality is incorrect while our approach is correct. Results of other questions are\nillustrated at https://elicitation.info/classroom/1/. Detailed explanations are illustrated\nin Appendix D and here we provide some highlights. First, our approach elicits a rich hierarchy. For\nexample, the taxicab problem is borrowed from Kahneman [8] and previous studies show that people\nusually ignore the base rate and report \u201880%\u2019. The imagined levels can be 41%\u219280%. We elicit a\nmuch richer level \u201c41%\u219250%\u219280%\u219212%\u219215%\u219220%\u201d. Second, the most sophisticated level\nmay fail to predict the least one. In the Taxicab problem, the correct \u201c41%\u201d supporters successfully\npredict the common wrong answer \u201c80%\u201d. However, they fail to predict the surprisingly wrong\nanswers \u201c12%,20%\u201d, which are in contrast successfully predicted by \u201c80%\u201d supporters. Our model\nis sufficiently general to allow this situation. Third, even for problems (like Go) without famous\nmistakes, our approach still works. Moreover, in the boundary question, we identify the correct\nanswer without any prior when only 3 respondents are correct.\n\n\nBased on this section, do the authors include any new assets either in the supplemental material or as a URL?"}]], "q4d": [[{"role": "user", "content": "The following is the abstract section of the paper you are reviewing:\n\n\nWhen we use the wisdom of the crowds, we usually rank the answers according to\ntheir popularity, especially when we cannot verify the answers. However, this can\nbe very dangerous when the majority make systematic mistakes. A fundamental\nquestion arises: can we build a hierarchy among the answers without any prior\nwhere the higher-ranking answers, which may not be supported by the majority,\nare from more sophisticated people? To address the question, we propose 1) a\nnovel model to describe people\u2019s thinking hierarchy; 2) two algorithms to learn\nthe thinking hierarchy without any prior; 3) a novel open-response based crowd-\nsourcing approach based on the above theoretic framework. In addition to theoretic\njustifications, we conduct four empirical crowdsourcing studies and show that a)\nthe accuracy of the top-ranking answers learned by our approach is much higher\nthan that of plurality voting (In one question, the plurality answer is supported by\n74 respondents but the correct answer is only supported by 3 respondents. Our\napproach ranks the correct answer the highest without any prior); b) our model\nhas a high goodness-of-fit, especially for the questions where our top-ranking\nanswer is correct. To the best of our knowledge, we are the first to propose a\nthinking hierarchy model with empirical validations in the general problem-solving\nscenarios; and the first to propose a practical open-response based crowdsourcing\napproach that beats plurality voting without any prior.\n\n1\n\n\nBased on this abstract, do the authors curate/release new assets (e.g., code, data, models)? Please start your answer with \"Yes\", \"No\", or \"Unsure\"."}, {"role": "user", "content": "The following is the experiments section of the paper you are reviewing:\n\n\nWe conduct four studies, study 1 (35 math problems), study 2 (30 Go problems), study 3 (44 general\nknowledge questions), and study 4 (43 Chinese character pronunciation questions).\n\nData collection All studies are performed by online questionnaires. We recruit the respondents by\nan online announcement13 or from an online platform that is similar to Amazon Mechanical Turk. We\nget respondents\u2019 consent for using and sharing their data for research. Respondents are asked not to\nsearch for the answers to the questions or communicate with other people. We allow the respondents\nto answer \u201cI do not know\u201d for all questions. Except for Go problems, all questionnaires use flat\npayment. We illustrate the data collection process in detail in Appendix A. We allow respondents\nto participate in more than one study because our algorithms analyze each question separately and\nindependently.\n\nData processing We merge answers which are the same, like \u20180.5\u2019 and \u201850%\u2019. We omit the answers\nthat are reported by less than (\u2264) 3% of respondents or one person. The remaining answers, excluding\n\u201cI do not know\u201d, form the answer set A whose size is |A|. We then construct the Answer-Prediction\nmatrix and perform our algorithms. Pseudo-codes are attached in Appendix B. Our algorithms do not\nrequire any prior or the respondents\u2019 expertise levels.\n\n3.1 Results\n\n12This may not be a very good assumption since i.i.d. samples can repeat but respondents usually do not\nrepeat their predictions. If we do not want this assumption, we can choose to only use the first prediction from\neach respondent (if there exists) to construct the answer-prediction matrix.\n\n13Many are students from top universities in China. See Appendix A for more details.\n\n8\n\n\fType\nMath\nGo\nGeneral knowledge\nChinese character\n\nTotal Our algorithm(Default) Our algorithm(Variant)\n\n35\n30\n44\n43\n\n29\n28\n41\n36\n\n29\n28\n41\n35\n\nPlurality voting\n24\n23\n35\n34\n\nTable 1: The number of questions our algorithms/baseline are correct.\n\n(a) the Monty Hall problem: you can select one\nclosed door of three. A prize, a car, is behind\none of the doors. The other two doors hide goats.\nAfter you have made your choice, Monty Hall\nwill open one of the remaining doors and show\nthat it does not contain the prize. He then asks\nyou if you would like to switch your choice to\nthe other unopened door. What is the probability\nto get the prize if you switch?\n\n(b) the Taxicab problem: 85% of taxis in this\ncity are green, the others are blue. A witness\nsees a blue taxi. She is usually correct with\nprobability 80%. What is the probability that\nthe taxi saw by the witness is blue?\n\n(c) Pick a move for black such that they can be\nalive.\n\n(d) Pick a move for black such that they can be\nalive by ko.\n\n(e) the boundary question: what river forms the\nboundary between North Korea and China?\n\n(f) the Middle Age New Year question: when\nwas the new year in middle age?\n\n(g) the pronunciation of \u7762\n\n(h) the pronunciation of \u6ec2\n\nFigure 4: The ranked answer-prediction matrices\n\n9\n\n2/31/21/32/31/21/3Answer34261413517146Prediction415080121520415080121520Answer160500008201200801619501104818000522130010218PredictionD3C1B2C2B1D3C1B2C2B1Answer62131091410042012115300002PredictionB8A8B6B7B8A8B6B7Answer112540630001230035PredictionYaluandDoomanYaluSonghuaYaluandDoomanYaluSonghuaAnswer33007410005PredictionApr1Dec25Jan1Apr1Dec25Jan1Answer15410028180633Predictionsui1ju1zhi4zhui1sui1ju1zhi4zhui1Answer115023161300210107Predictionpang1pang2pang1pang2Answer1511216Prediction\fWe compare our approach to the baseline, the plurality voting, regarding the accuracy of the top-\nranking answers. Both of the algorithms beat plurality voting for all studies and the default is slightly\nbetter. Among all 152 questions, in 138 questions, the variant algorithm outputs the same hierarchy as\nthe default algorithm. For other questions, the top-ranking type of the variant algorithm may contain\nmore than one answer. The top-ranking answer is the answer supported by more people among all\nanswers in the top-ranking type. In one question the variant is wrong but the default is correct, the\nvariant algorithm assigns both the correct answer and the incorrect plurality answer to the top-ranking\ntype, thus outputting the incorrect answer as the top-ranking answer.\n\nWe also compute the lack-of-fit index (Definition 2.6) of the algorithms and find that the questions\nthe algorithm outputs the correct answer have a smaller lack-of-fit thus fitting the thinking hierarchy\nmodel better. Therefore, we can use the lack-of-fit index as a reliability index of the algorithms.\n\nWe additionally pick several representative examples for each study (Figure 4) where the matrices\nare ranked by the default algorithm and the diagonal area modified like Example 1.1. In all of these\nexamples, the plurality is incorrect while our approach is correct. Results of other questions are\nillustrated at https://elicitation.info/classroom/1/. Detailed explanations are illustrated\nin Appendix D and here we provide some highlights. First, our approach elicits a rich hierarchy. For\nexample, the taxicab problem is borrowed from Kahneman [8] and previous studies show that people\nusually ignore the base rate and report \u201880%\u2019. The imagined levels can be 41%\u219280%. We elicit a\nmuch richer level \u201c41%\u219250%\u219280%\u219212%\u219215%\u219220%\u201d. Second, the most sophisticated level\nmay fail to predict the least one. In the Taxicab problem, the correct \u201c41%\u201d supporters successfully\npredict the common wrong answer \u201c80%\u201d. However, they fail to predict the surprisingly wrong\nanswers \u201c12%,20%\u201d, which are in contrast successfully predicted by \u201c80%\u201d supporters. Our model\nis sufficiently general to allow this situation. Third, even for problems (like Go) without famous\nmistakes, our approach still works. Moreover, in the boundary question, we identify the correct\nanswer without any prior when only 3 respondents are correct.\n\n\nBased on this section, do the authors discuss whether and how consent was obtained from people whose data the authors are using/curating?"}]], "q4e": [[{"role": "user", "content": "The following is the abstract section of the paper you are reviewing:\n\n\nWhen we use the wisdom of the crowds, we usually rank the answers according to\ntheir popularity, especially when we cannot verify the answers. However, this can\nbe very dangerous when the majority make systematic mistakes. A fundamental\nquestion arises: can we build a hierarchy among the answers without any prior\nwhere the higher-ranking answers, which may not be supported by the majority,\nare from more sophisticated people? To address the question, we propose 1) a\nnovel model to describe people\u2019s thinking hierarchy; 2) two algorithms to learn\nthe thinking hierarchy without any prior; 3) a novel open-response based crowd-\nsourcing approach based on the above theoretic framework. In addition to theoretic\njustifications, we conduct four empirical crowdsourcing studies and show that a)\nthe accuracy of the top-ranking answers learned by our approach is much higher\nthan that of plurality voting (In one question, the plurality answer is supported by\n74 respondents but the correct answer is only supported by 3 respondents. Our\napproach ranks the correct answer the highest without any prior); b) our model\nhas a high goodness-of-fit, especially for the questions where our top-ranking\nanswer is correct. To the best of our knowledge, we are the first to propose a\nthinking hierarchy model with empirical validations in the general problem-solving\nscenarios; and the first to propose a practical open-response based crowdsourcing\napproach that beats plurality voting without any prior.\n\n1\n\n\nBased on this abstract, do the authors curate/release new assets (e.g., code, data, models)? Please start your answer with \"Yes\", \"No\", or \"Unsure\"."}, {"role": "user", "content": "The following is the experiments section of the paper you are reviewing:\n\n\nWe conduct four studies, study 1 (35 math problems), study 2 (30 Go problems), study 3 (44 general\nknowledge questions), and study 4 (43 Chinese character pronunciation questions).\n\nData collection All studies are performed by online questionnaires. We recruit the respondents by\nan online announcement13 or from an online platform that is similar to Amazon Mechanical Turk. We\nget respondents\u2019 consent for using and sharing their data for research. Respondents are asked not to\nsearch for the answers to the questions or communicate with other people. We allow the respondents\nto answer \u201cI do not know\u201d for all questions. Except for Go problems, all questionnaires use flat\npayment. We illustrate the data collection process in detail in Appendix A. We allow respondents\nto participate in more than one study because our algorithms analyze each question separately and\nindependently.\n\nData processing We merge answers which are the same, like \u20180.5\u2019 and \u201850%\u2019. We omit the answers\nthat are reported by less than (\u2264) 3% of respondents or one person. The remaining answers, excluding\n\u201cI do not know\u201d, form the answer set A whose size is |A|. We then construct the Answer-Prediction\nmatrix and perform our algorithms. Pseudo-codes are attached in Appendix B. Our algorithms do not\nrequire any prior or the respondents\u2019 expertise levels.\n\n3.1 Results\n\n12This may not be a very good assumption since i.i.d. samples can repeat but respondents usually do not\nrepeat their predictions. If we do not want this assumption, we can choose to only use the first prediction from\neach respondent (if there exists) to construct the answer-prediction matrix.\n\n13Many are students from top universities in China. See Appendix A for more details.\n\n8\n\n\fType\nMath\nGo\nGeneral knowledge\nChinese character\n\nTotal Our algorithm(Default) Our algorithm(Variant)\n\n35\n30\n44\n43\n\n29\n28\n41\n36\n\n29\n28\n41\n35\n\nPlurality voting\n24\n23\n35\n34\n\nTable 1: The number of questions our algorithms/baseline are correct.\n\n(a) the Monty Hall problem: you can select one\nclosed door of three. A prize, a car, is behind\none of the doors. The other two doors hide goats.\nAfter you have made your choice, Monty Hall\nwill open one of the remaining doors and show\nthat it does not contain the prize. He then asks\nyou if you would like to switch your choice to\nthe other unopened door. What is the probability\nto get the prize if you switch?\n\n(b) the Taxicab problem: 85% of taxis in this\ncity are green, the others are blue. A witness\nsees a blue taxi. She is usually correct with\nprobability 80%. What is the probability that\nthe taxi saw by the witness is blue?\n\n(c) Pick a move for black such that they can be\nalive.\n\n(d) Pick a move for black such that they can be\nalive by ko.\n\n(e) the boundary question: what river forms the\nboundary between North Korea and China?\n\n(f) the Middle Age New Year question: when\nwas the new year in middle age?\n\n(g) the pronunciation of \u7762\n\n(h) the pronunciation of \u6ec2\n\nFigure 4: The ranked answer-prediction matrices\n\n9\n\n2/31/21/32/31/21/3Answer34261413517146Prediction415080121520415080121520Answer160500008201200801619501104818000522130010218PredictionD3C1B2C2B1D3C1B2C2B1Answer62131091410042012115300002PredictionB8A8B6B7B8A8B6B7Answer112540630001230035PredictionYaluandDoomanYaluSonghuaYaluandDoomanYaluSonghuaAnswer33007410005PredictionApr1Dec25Jan1Apr1Dec25Jan1Answer15410028180633Predictionsui1ju1zhi4zhui1sui1ju1zhi4zhui1Answer115023161300210107Predictionpang1pang2pang1pang2Answer1511216Prediction\fWe compare our approach to the baseline, the plurality voting, regarding the accuracy of the top-\nranking answers. Both of the algorithms beat plurality voting for all studies and the default is slightly\nbetter. Among all 152 questions, in 138 questions, the variant algorithm outputs the same hierarchy as\nthe default algorithm. For other questions, the top-ranking type of the variant algorithm may contain\nmore than one answer. The top-ranking answer is the answer supported by more people among all\nanswers in the top-ranking type. In one question the variant is wrong but the default is correct, the\nvariant algorithm assigns both the correct answer and the incorrect plurality answer to the top-ranking\ntype, thus outputting the incorrect answer as the top-ranking answer.\n\nWe also compute the lack-of-fit index (Definition 2.6) of the algorithms and find that the questions\nthe algorithm outputs the correct answer have a smaller lack-of-fit thus fitting the thinking hierarchy\nmodel better. Therefore, we can use the lack-of-fit index as a reliability index of the algorithms.\n\nWe additionally pick several representative examples for each study (Figure 4) where the matrices\nare ranked by the default algorithm and the diagonal area modified like Example 1.1. In all of these\nexamples, the plurality is incorrect while our approach is correct. Results of other questions are\nillustrated at https://elicitation.info/classroom/1/. Detailed explanations are illustrated\nin Appendix D and here we provide some highlights. First, our approach elicits a rich hierarchy. For\nexample, the taxicab problem is borrowed from Kahneman [8] and previous studies show that people\nusually ignore the base rate and report \u201880%\u2019. The imagined levels can be 41%\u219280%. We elicit a\nmuch richer level \u201c41%\u219250%\u219280%\u219212%\u219215%\u219220%\u201d. Second, the most sophisticated level\nmay fail to predict the least one. In the Taxicab problem, the correct \u201c41%\u201d supporters successfully\npredict the common wrong answer \u201c80%\u201d. However, they fail to predict the surprisingly wrong\nanswers \u201c12%,20%\u201d, which are in contrast successfully predicted by \u201c80%\u201d supporters. Our model\nis sufficiently general to allow this situation. Third, even for problems (like Go) without famous\nmistakes, our approach still works. Moreover, in the boundary question, we identify the correct\nanswer without any prior when only 3 respondents are correct.\n\n\nBased on this section, do the authors discuss whether the data used/curated contains personally identifiable information or offensive content?"}]], "q5a": [[{"role": "user", "content": "The following is the abstract of the paper you are reviewing:\n\n\nWhen we use the wisdom of the crowds, we usually rank the answers according to\ntheir popularity, especially when we cannot verify the answers. However, this can\nbe very dangerous when the majority make systematic mistakes. A fundamental\nquestion arises: can we build a hierarchy among the answers without any prior\nwhere the higher-ranking answers, which may not be supported by the majority,\nare from more sophisticated people? To address the question, we propose 1) a\nnovel model to describe people\u2019s thinking hierarchy; 2) two algorithms to learn\nthe thinking hierarchy without any prior; 3) a novel open-response based crowd-\nsourcing approach based on the above theoretic framework. In addition to theoretic\njustifications, we conduct four empirical crowdsourcing studies and show that a)\nthe accuracy of the top-ranking answers learned by our approach is much higher\nthan that of plurality voting (In one question, the plurality answer is supported by\n74 respondents but the correct answer is only supported by 3 respondents. Our\napproach ranks the correct answer the highest without any prior); b) our model\nhas a high goodness-of-fit, especially for the questions where our top-ranking\nanswer is correct. To the best of our knowledge, we are the first to propose a\nthinking hierarchy model with empirical validations in the general problem-solving\nscenarios; and the first to propose a practical open-response based crowdsourcing\napproach that beats plurality voting without any prior.\n\n1\n\n\nBased on this abstract, do the authors use crowdsourcing or conduct research with human subjects? Please start your answer with \"Yes\", \"No\", or \"Unsure\"."}, {"role": "user", "content": "The following is the experiments section of the paper you are reviewing:\n\n\nWe conduct four studies, study 1 (35 math problems), study 2 (30 Go problems), study 3 (44 general\nknowledge questions), and study 4 (43 Chinese character pronunciation questions).\n\nData collection All studies are performed by online questionnaires. We recruit the respondents by\nan online announcement13 or from an online platform that is similar to Amazon Mechanical Turk. We\nget respondents\u2019 consent for using and sharing their data for research. Respondents are asked not to\nsearch for the answers to the questions or communicate with other people. We allow the respondents\nto answer \u201cI do not know\u201d for all questions. Except for Go problems, all questionnaires use flat\npayment. We illustrate the data collection process in detail in Appendix A. We allow respondents\nto participate in more than one study because our algorithms analyze each question separately and\nindependently.\n\nData processing We merge answers which are the same, like \u20180.5\u2019 and \u201850%\u2019. We omit the answers\nthat are reported by less than (\u2264) 3% of respondents or one person. The remaining answers, excluding\n\u201cI do not know\u201d, form the answer set A whose size is |A|. We then construct the Answer-Prediction\nmatrix and perform our algorithms. Pseudo-codes are attached in Appendix B. Our algorithms do not\nrequire any prior or the respondents\u2019 expertise levels.\n\n3.1 Results\n\n12This may not be a very good assumption since i.i.d. samples can repeat but respondents usually do not\nrepeat their predictions. If we do not want this assumption, we can choose to only use the first prediction from\neach respondent (if there exists) to construct the answer-prediction matrix.\n\n13Many are students from top universities in China. See Appendix A for more details.\n\n8\n\n\fType\nMath\nGo\nGeneral knowledge\nChinese character\n\nTotal Our algorithm(Default) Our algorithm(Variant)\n\n35\n30\n44\n43\n\n29\n28\n41\n36\n\n29\n28\n41\n35\n\nPlurality voting\n24\n23\n35\n34\n\nTable 1: The number of questions our algorithms/baseline are correct.\n\n(a) the Monty Hall problem: you can select one\nclosed door of three. A prize, a car, is behind\none of the doors. The other two doors hide goats.\nAfter you have made your choice, Monty Hall\nwill open one of the remaining doors and show\nthat it does not contain the prize. He then asks\nyou if you would like to switch your choice to\nthe other unopened door. What is the probability\nto get the prize if you switch?\n\n(b) the Taxicab problem: 85% of taxis in this\ncity are green, the others are blue. A witness\nsees a blue taxi. She is usually correct with\nprobability 80%. What is the probability that\nthe taxi saw by the witness is blue?\n\n(c) Pick a move for black such that they can be\nalive.\n\n(d) Pick a move for black such that they can be\nalive by ko.\n\n(e) the boundary question: what river forms the\nboundary between North Korea and China?\n\n(f) the Middle Age New Year question: when\nwas the new year in middle age?\n\n(g) the pronunciation of \u7762\n\n(h) the pronunciation of \u6ec2\n\nFigure 4: The ranked answer-prediction matrices\n\n9\n\n2/31/21/32/31/21/3Answer34261413517146Prediction415080121520415080121520Answer160500008201200801619501104818000522130010218PredictionD3C1B2C2B1D3C1B2C2B1Answer62131091410042012115300002PredictionB8A8B6B7B8A8B6B7Answer112540630001230035PredictionYaluandDoomanYaluSonghuaYaluandDoomanYaluSonghuaAnswer33007410005PredictionApr1Dec25Jan1Apr1Dec25Jan1Answer15410028180633Predictionsui1ju1zhi4zhui1sui1ju1zhi4zhui1Answer115023161300210107Predictionpang1pang2pang1pang2Answer1511216Prediction\fWe compare our approach to the baseline, the plurality voting, regarding the accuracy of the top-\nranking answers. Both of the algorithms beat plurality voting for all studies and the default is slightly\nbetter. Among all 152 questions, in 138 questions, the variant algorithm outputs the same hierarchy as\nthe default algorithm. For other questions, the top-ranking type of the variant algorithm may contain\nmore than one answer. The top-ranking answer is the answer supported by more people among all\nanswers in the top-ranking type. In one question the variant is wrong but the default is correct, the\nvariant algorithm assigns both the correct answer and the incorrect plurality answer to the top-ranking\ntype, thus outputting the incorrect answer as the top-ranking answer.\n\nWe also compute the lack-of-fit index (Definition 2.6) of the algorithms and find that the questions\nthe algorithm outputs the correct answer have a smaller lack-of-fit thus fitting the thinking hierarchy\nmodel better. Therefore, we can use the lack-of-fit index as a reliability index of the algorithms.\n\nWe additionally pick several representative examples for each study (Figure 4) where the matrices\nare ranked by the default algorithm and the diagonal area modified like Example 1.1. In all of these\nexamples, the plurality is incorrect while our approach is correct. Results of other questions are\nillustrated at https://elicitation.info/classroom/1/. Detailed explanations are illustrated\nin Appendix D and here we provide some highlights. First, our approach elicits a rich hierarchy. For\nexample, the taxicab problem is borrowed from Kahneman [8] and previous studies show that people\nusually ignore the base rate and report \u201880%\u2019. The imagined levels can be 41%\u219280%. We elicit a\nmuch richer level \u201c41%\u219250%\u219280%\u219212%\u219215%\u219220%\u201d. Second, the most sophisticated level\nmay fail to predict the least one. In the Taxicab problem, the correct \u201c41%\u201d supporters successfully\npredict the common wrong answer \u201c80%\u201d. However, they fail to predict the surprisingly wrong\nanswers \u201c12%,20%\u201d, which are in contrast successfully predicted by \u201c80%\u201d supporters. Our model\nis sufficiently general to allow this situation. Third, even for problems (like Go) without famous\nmistakes, our approach still works. Moreover, in the boundary question, we identify the correct\nanswer without any prior when only 3 respondents are correct.\n\n\nBased on this section, do the authors include the full text of instructions given to participants and screenshots, if applicable?"}]], "q5b": [[{"role": "user", "content": "The following is the abstract of the paper you are reviewing:\n\n\nWhen we use the wisdom of the crowds, we usually rank the answers according to\ntheir popularity, especially when we cannot verify the answers. However, this can\nbe very dangerous when the majority make systematic mistakes. A fundamental\nquestion arises: can we build a hierarchy among the answers without any prior\nwhere the higher-ranking answers, which may not be supported by the majority,\nare from more sophisticated people? To address the question, we propose 1) a\nnovel model to describe people\u2019s thinking hierarchy; 2) two algorithms to learn\nthe thinking hierarchy without any prior; 3) a novel open-response based crowd-\nsourcing approach based on the above theoretic framework. In addition to theoretic\njustifications, we conduct four empirical crowdsourcing studies and show that a)\nthe accuracy of the top-ranking answers learned by our approach is much higher\nthan that of plurality voting (In one question, the plurality answer is supported by\n74 respondents but the correct answer is only supported by 3 respondents. Our\napproach ranks the correct answer the highest without any prior); b) our model\nhas a high goodness-of-fit, especially for the questions where our top-ranking\nanswer is correct. To the best of our knowledge, we are the first to propose a\nthinking hierarchy model with empirical validations in the general problem-solving\nscenarios; and the first to propose a practical open-response based crowdsourcing\napproach that beats plurality voting without any prior.\n\n1\n\n\nBased on this abstract, do the authors use crowdsourcing or conduct research with human subjects? Please start your answer with \"Yes\", \"No\", or \"Unsure\"."}, {"role": "user", "content": "The following is the experiments section of the paper you are reviewing:\n\n\nWe conduct four studies, study 1 (35 math problems), study 2 (30 Go problems), study 3 (44 general\nknowledge questions), and study 4 (43 Chinese character pronunciation questions).\n\nData collection All studies are performed by online questionnaires. We recruit the respondents by\nan online announcement13 or from an online platform that is similar to Amazon Mechanical Turk. We\nget respondents\u2019 consent for using and sharing their data for research. Respondents are asked not to\nsearch for the answers to the questions or communicate with other people. We allow the respondents\nto answer \u201cI do not know\u201d for all questions. Except for Go problems, all questionnaires use flat\npayment. We illustrate the data collection process in detail in Appendix A. We allow respondents\nto participate in more than one study because our algorithms analyze each question separately and\nindependently.\n\nData processing We merge answers which are the same, like \u20180.5\u2019 and \u201850%\u2019. We omit the answers\nthat are reported by less than (\u2264) 3% of respondents or one person. The remaining answers, excluding\n\u201cI do not know\u201d, form the answer set A whose size is |A|. We then construct the Answer-Prediction\nmatrix and perform our algorithms. Pseudo-codes are attached in Appendix B. Our algorithms do not\nrequire any prior or the respondents\u2019 expertise levels.\n\n3.1 Results\n\n12This may not be a very good assumption since i.i.d. samples can repeat but respondents usually do not\nrepeat their predictions. If we do not want this assumption, we can choose to only use the first prediction from\neach respondent (if there exists) to construct the answer-prediction matrix.\n\n13Many are students from top universities in China. See Appendix A for more details.\n\n8\n\n\fType\nMath\nGo\nGeneral knowledge\nChinese character\n\nTotal Our algorithm(Default) Our algorithm(Variant)\n\n35\n30\n44\n43\n\n29\n28\n41\n36\n\n29\n28\n41\n35\n\nPlurality voting\n24\n23\n35\n34\n\nTable 1: The number of questions our algorithms/baseline are correct.\n\n(a) the Monty Hall problem: you can select one\nclosed door of three. A prize, a car, is behind\none of the doors. The other two doors hide goats.\nAfter you have made your choice, Monty Hall\nwill open one of the remaining doors and show\nthat it does not contain the prize. He then asks\nyou if you would like to switch your choice to\nthe other unopened door. What is the probability\nto get the prize if you switch?\n\n(b) the Taxicab problem: 85% of taxis in this\ncity are green, the others are blue. A witness\nsees a blue taxi. She is usually correct with\nprobability 80%. What is the probability that\nthe taxi saw by the witness is blue?\n\n(c) Pick a move for black such that they can be\nalive.\n\n(d) Pick a move for black such that they can be\nalive by ko.\n\n(e) the boundary question: what river forms the\nboundary between North Korea and China?\n\n(f) the Middle Age New Year question: when\nwas the new year in middle age?\n\n(g) the pronunciation of \u7762\n\n(h) the pronunciation of \u6ec2\n\nFigure 4: The ranked answer-prediction matrices\n\n9\n\n2/31/21/32/31/21/3Answer34261413517146Prediction415080121520415080121520Answer160500008201200801619501104818000522130010218PredictionD3C1B2C2B1D3C1B2C2B1Answer62131091410042012115300002PredictionB8A8B6B7B8A8B6B7Answer112540630001230035PredictionYaluandDoomanYaluSonghuaYaluandDoomanYaluSonghuaAnswer33007410005PredictionApr1Dec25Jan1Apr1Dec25Jan1Answer15410028180633Predictionsui1ju1zhi4zhui1sui1ju1zhi4zhui1Answer115023161300210107Predictionpang1pang2pang1pang2Answer1511216Prediction\fWe compare our approach to the baseline, the plurality voting, regarding the accuracy of the top-\nranking answers. Both of the algorithms beat plurality voting for all studies and the default is slightly\nbetter. Among all 152 questions, in 138 questions, the variant algorithm outputs the same hierarchy as\nthe default algorithm. For other questions, the top-ranking type of the variant algorithm may contain\nmore than one answer. The top-ranking answer is the answer supported by more people among all\nanswers in the top-ranking type. In one question the variant is wrong but the default is correct, the\nvariant algorithm assigns both the correct answer and the incorrect plurality answer to the top-ranking\ntype, thus outputting the incorrect answer as the top-ranking answer.\n\nWe also compute the lack-of-fit index (Definition 2.6) of the algorithms and find that the questions\nthe algorithm outputs the correct answer have a smaller lack-of-fit thus fitting the thinking hierarchy\nmodel better. Therefore, we can use the lack-of-fit index as a reliability index of the algorithms.\n\nWe additionally pick several representative examples for each study (Figure 4) where the matrices\nare ranked by the default algorithm and the diagonal area modified like Example 1.1. In all of these\nexamples, the plurality is incorrect while our approach is correct. Results of other questions are\nillustrated at https://elicitation.info/classroom/1/. Detailed explanations are illustrated\nin Appendix D and here we provide some highlights. First, our approach elicits a rich hierarchy. For\nexample, the taxicab problem is borrowed from Kahneman [8] and previous studies show that people\nusually ignore the base rate and report \u201880%\u2019. The imagined levels can be 41%\u219280%. We elicit a\nmuch richer level \u201c41%\u219250%\u219280%\u219212%\u219215%\u219220%\u201d. Second, the most sophisticated level\nmay fail to predict the least one. In the Taxicab problem, the correct \u201c41%\u201d supporters successfully\npredict the common wrong answer \u201c80%\u201d. However, they fail to predict the surprisingly wrong\nanswers \u201c12%,20%\u201d, which are in contrast successfully predicted by \u201c80%\u201d supporters. Our model\nis sufficiently general to allow this situation. Third, even for problems (like Go) without famous\nmistakes, our approach still works. Moreover, in the boundary question, we identify the correct\nanswer without any prior when only 3 respondents are correct.\n\n\nBased on this section, do the authors describe any potential participant risks, with links to Institutional Review Board (IRB) approvals, if applicable?"}]], "q5c": [[{"role": "user", "content": "The following is the abstract of the paper you are reviewing:\n\n\nWhen we use the wisdom of the crowds, we usually rank the answers according to\ntheir popularity, especially when we cannot verify the answers. However, this can\nbe very dangerous when the majority make systematic mistakes. A fundamental\nquestion arises: can we build a hierarchy among the answers without any prior\nwhere the higher-ranking answers, which may not be supported by the majority,\nare from more sophisticated people? To address the question, we propose 1) a\nnovel model to describe people\u2019s thinking hierarchy; 2) two algorithms to learn\nthe thinking hierarchy without any prior; 3) a novel open-response based crowd-\nsourcing approach based on the above theoretic framework. In addition to theoretic\njustifications, we conduct four empirical crowdsourcing studies and show that a)\nthe accuracy of the top-ranking answers learned by our approach is much higher\nthan that of plurality voting (In one question, the plurality answer is supported by\n74 respondents but the correct answer is only supported by 3 respondents. Our\napproach ranks the correct answer the highest without any prior); b) our model\nhas a high goodness-of-fit, especially for the questions where our top-ranking\nanswer is correct. To the best of our knowledge, we are the first to propose a\nthinking hierarchy model with empirical validations in the general problem-solving\nscenarios; and the first to propose a practical open-response based crowdsourcing\napproach that beats plurality voting without any prior.\n\n1\n\n\nBased on this abstract, do the authors use crowdsourcing or conduct research with human subjects? Please start your answer with \"Yes\", \"No\", or \"Unsure\"."}, {"role": "user", "content": "The following is the experiments section of the paper you are reviewing:\n\n\nWe conduct four studies, study 1 (35 math problems), study 2 (30 Go problems), study 3 (44 general\nknowledge questions), and study 4 (43 Chinese character pronunciation questions).\n\nData collection All studies are performed by online questionnaires. We recruit the respondents by\nan online announcement13 or from an online platform that is similar to Amazon Mechanical Turk. We\nget respondents\u2019 consent for using and sharing their data for research. Respondents are asked not to\nsearch for the answers to the questions or communicate with other people. We allow the respondents\nto answer \u201cI do not know\u201d for all questions. Except for Go problems, all questionnaires use flat\npayment. We illustrate the data collection process in detail in Appendix A. We allow respondents\nto participate in more than one study because our algorithms analyze each question separately and\nindependently.\n\nData processing We merge answers which are the same, like \u20180.5\u2019 and \u201850%\u2019. We omit the answers\nthat are reported by less than (\u2264) 3% of respondents or one person. The remaining answers, excluding\n\u201cI do not know\u201d, form the answer set A whose size is |A|. We then construct the Answer-Prediction\nmatrix and perform our algorithms. Pseudo-codes are attached in Appendix B. Our algorithms do not\nrequire any prior or the respondents\u2019 expertise levels.\n\n3.1 Results\n\n12This may not be a very good assumption since i.i.d. samples can repeat but respondents usually do not\nrepeat their predictions. If we do not want this assumption, we can choose to only use the first prediction from\neach respondent (if there exists) to construct the answer-prediction matrix.\n\n13Many are students from top universities in China. See Appendix A for more details.\n\n8\n\n\fType\nMath\nGo\nGeneral knowledge\nChinese character\n\nTotal Our algorithm(Default) Our algorithm(Variant)\n\n35\n30\n44\n43\n\n29\n28\n41\n36\n\n29\n28\n41\n35\n\nPlurality voting\n24\n23\n35\n34\n\nTable 1: The number of questions our algorithms/baseline are correct.\n\n(a) the Monty Hall problem: you can select one\nclosed door of three. A prize, a car, is behind\none of the doors. The other two doors hide goats.\nAfter you have made your choice, Monty Hall\nwill open one of the remaining doors and show\nthat it does not contain the prize. He then asks\nyou if you would like to switch your choice to\nthe other unopened door. What is the probability\nto get the prize if you switch?\n\n(b) the Taxicab problem: 85% of taxis in this\ncity are green, the others are blue. A witness\nsees a blue taxi. She is usually correct with\nprobability 80%. What is the probability that\nthe taxi saw by the witness is blue?\n\n(c) Pick a move for black such that they can be\nalive.\n\n(d) Pick a move for black such that they can be\nalive by ko.\n\n(e) the boundary question: what river forms the\nboundary between North Korea and China?\n\n(f) the Middle Age New Year question: when\nwas the new year in middle age?\n\n(g) the pronunciation of \u7762\n\n(h) the pronunciation of \u6ec2\n\nFigure 4: The ranked answer-prediction matrices\n\n9\n\n2/31/21/32/31/21/3Answer34261413517146Prediction415080121520415080121520Answer160500008201200801619501104818000522130010218PredictionD3C1B2C2B1D3C1B2C2B1Answer62131091410042012115300002PredictionB8A8B6B7B8A8B6B7Answer112540630001230035PredictionYaluandDoomanYaluSonghuaYaluandDoomanYaluSonghuaAnswer33007410005PredictionApr1Dec25Jan1Apr1Dec25Jan1Answer15410028180633Predictionsui1ju1zhi4zhui1sui1ju1zhi4zhui1Answer115023161300210107Predictionpang1pang2pang1pang2Answer1511216Prediction\fWe compare our approach to the baseline, the plurality voting, regarding the accuracy of the top-\nranking answers. Both of the algorithms beat plurality voting for all studies and the default is slightly\nbetter. Among all 152 questions, in 138 questions, the variant algorithm outputs the same hierarchy as\nthe default algorithm. For other questions, the top-ranking type of the variant algorithm may contain\nmore than one answer. The top-ranking answer is the answer supported by more people among all\nanswers in the top-ranking type. In one question the variant is wrong but the default is correct, the\nvariant algorithm assigns both the correct answer and the incorrect plurality answer to the top-ranking\ntype, thus outputting the incorrect answer as the top-ranking answer.\n\nWe also compute the lack-of-fit index (Definition 2.6) of the algorithms and find that the questions\nthe algorithm outputs the correct answer have a smaller lack-of-fit thus fitting the thinking hierarchy\nmodel better. Therefore, we can use the lack-of-fit index as a reliability index of the algorithms.\n\nWe additionally pick several representative examples for each study (Figure 4) where the matrices\nare ranked by the default algorithm and the diagonal area modified like Example 1.1. In all of these\nexamples, the plurality is incorrect while our approach is correct. Results of other questions are\nillustrated at https://elicitation.info/classroom/1/. Detailed explanations are illustrated\nin Appendix D and here we provide some highlights. First, our approach elicits a rich hierarchy. For\nexample, the taxicab problem is borrowed from Kahneman [8] and previous studies show that people\nusually ignore the base rate and report \u201880%\u2019. The imagined levels can be 41%\u219280%. We elicit a\nmuch richer level \u201c41%\u219250%\u219280%\u219212%\u219215%\u219220%\u201d. Second, the most sophisticated level\nmay fail to predict the least one. In the Taxicab problem, the correct \u201c41%\u201d supporters successfully\npredict the common wrong answer \u201c80%\u201d. However, they fail to predict the surprisingly wrong\nanswers \u201c12%,20%\u201d, which are in contrast successfully predicted by \u201c80%\u201d supporters. Our model\nis sufficiently general to allow this situation. Third, even for problems (like Go) without famous\nmistakes, our approach still works. Moreover, in the boundary question, we identify the correct\nanswer without any prior when only 3 respondents are correct.\n\n\nBased on this section, do the authors include the estimated hourly wage paid to participants and the total amount spent on participant compensation?"}]]}}