{"paper_index": 16, "title": "Harmonizing the object recognition strategies of deep neural networks with humans", "abstract": "\n\nThe many successes of deep neural networks (DNNs) over the past decade have\nlargely been driven by computational scale rather than insights from biological\nintelligence. Here, we explore if these trends have also carried concomitant im-\nprovements in explaining the visual strategies humans rely on for object recognition.\nWe do this by comparing two related but distinct properties of visual strategies in\nhumans and DNNs: where they believe important visual features are in images and\nhow they use those features to categorize objects. Across 84 different DNNs trained\non ImageNet and three independent datasets measuring the where and the how of\nhuman visual strategies for object recognition on those images, we find a systematic\ntrade-off between DNN categorization accuracy and alignment with human visual\nstrategies for object recognition. State-of-the-art DNNs are progressively becoming\nless aligned with humans as their accuracy improves. We rectify this growing issue\nwith our neural harmonizer: a general-purpose training routine that both aligns\nDNN and human visual strategies and improves categorization accuracy. Our work\nrepresents the first demonstration that the scaling laws [1\u20133] that are guiding the\ndesign of DNNs today have also produced worse models of human vision. We\nrelease our code and data at https://serre-lab.github.io/Harmonization\nto help the field build more human-like DNNs.\n\n1\n\n", "introduction": "\n\nRich Sutton stated [4] that the bitter lesson \u201cfrom 70 years of AI research is that general methods\nthat leverage computation are ultimately the most effective, and by a large margin.\u201d Deep learning\nhas been the standard approach to object categorization problems ever since the paradigm shifting\nsuccess of AlexNet [5] on the ImageNet [6] benchmark a decade ago. As deep neural network (DNN)\nperformance has continued to improve in the intervening years, Sutton\u2019s lesson has become more\nfitting than ever, with recent networks rivaling and likely outperforming humans on the benchmark [7]\nthrough brute-force computational scale: increasing the number of network parameters and number of\nimages used for training orders-of-magnitude beyond AlexNet [1\u20133]. While the successes of so-called\n\u201cscaling laws\u201d are undeniable, this singular focus on performance in the field has side-stepped an\nequally important question that will govern the utility of object recognition models for the brain\nsciences and industry applications alike: are the visual strategies learned by DNNs aligned with those\nused by humans?\n\nThe visual strategies that mediate object recognition in humans can be decomposed into two related\nbut distinct processes: identifying where the important features for object recognition are in a scene,\nand determining how to integrate the selected features into a categorical decision [8, 9]. It has been\n\n*These authors contributed equally.\n1Department of Cognitive, Linguistic, & Psychological Sciences, Brown University, Providence, RI\n2Artificial and Natural Intelligence Toulouse Institute (ANITI), Toulouse, France\n3Carney Institute for Brain Science, Brown University, Providence, RI\n\n36th Conference on Neural Information Processing Systems (NeurIPS 2022).\n\n\fknown for nearly a century [10\u201313] that different humans attend to similar locations when asked\nto find and recognize objects. After selecting these important features, human observers are also\nconsistent in how they use those features to categorize objects \u2013 the inclusion of a few pixels in an\nimage can be the difference between recognizing an object or not [9, 14].\n\nHas the past decade of DNN development produced any models that are aligned with these human\nvisual strategies for object recognition? Such a model could transform cognitive science by support-\ning a better mechanistic understanding of how vision works. More human-like models of object\nrecognition would also resolve the problems with predictablity and interpretablity of DNNs [15\u201318],\nand control their alarming tendency to rely on \u201cshortcuts\u201d and dataset biases to perform well on\ntasks [19]. In this work, we perform the first large-scale and systematic comparison of the visual\nstrategies of DNNs and humans for object recognition on ImageNet.\n\nContributions.\nIn order to compare human and DNN visual strategies, we first turn to the human\nfeature importance maps collected by Linsley et al. [20,21]. Their datasets, ClickMe and Clicktionary,\ncontain maps of nearly 200,000 unique images in ImageNet that highlight the visual features humans\nbelieve are important for recognizing them. These datasets amount to a reverse inference on where\nimportant visual features are in ImageNet images (Fig. 1). We complement these datasets with\nnew psychophysics experiments that directly test how important visual features are used for object\nrecognition (Fig. 1). As DNN performance has increased on ImageNet, their alignment with\nhuman visual strategies captured in these datasets has worsened. This trade-off is found over 84\ndifferent DNNs representing all popular model classes \u2013 from those trained for adversarial robustness\nto those pushing the scaling laws in network capacity and training data. To summarize our findings:\n\n\u2022 The trade-off between DNN object recognition accuracy and alignment with human visual strategies\nreplicates across three unique datasets: ClickMe [20], Clicktionary [21], and our psychophysics\nexperiments.\n\n\u2022 We shift this trade-off with our neural harmonizer, a novel drop-in module for co-training any DNN\nto align with human visual strategies while also achieving high task accuracy. Harmonized DNNs\nlearn visual strategies that are significantly more aligned with humans than any other DNN we\ntested.\n\n\u2022 We release our data and code at https://serre-lab.github.io/Harmonization/ to help the\n\nfield tackle the growing misalignment between DNNs and humans.\n\n2 Related work\n\n", "methods": "\n\nHuman feature importance datasets. We focused on the ImageNet dataset to compare the visual\nstrategies of humans and DNNs for object recognition at scale. We relied on the two significant\nefforts for gathering feature importance data from humans on ImageNet: the Clicktionary [22] and\nClickMe [20] games, which use slightly different methods to collect their data. Both games begin with\nthe same basic setup: two players work together to locate features in an object image that they believe\nare important for categorizing it. As one of the players selects important image regions, those regions\nare filled into a blank canvas for the other observer to see and categorize the image as quickly as\npossible. In Clicktionary [22], both players are humans, whereas in ClickMe [20], the player selecting\nfeatures is a human and the player recognizing images is a DNN (VGG16 [49]). For both games,\nfeature importance maps depicting the average object category diagnosticity of every pixel was\ncomputed as the probability of it being clicked by a participant. In total, Clicktionary [22] contained\nfeature importance maps for 200 images from the ImageNet validation set, whereas ClickMe [20]\ncontained feature importance maps for a non-overlapping set of 196,499 images from ImageNet\ntraining and validation sets. Thus, ClickMe has far more data than Clicktionary, but Clicktionary\ndata has more reliable human feature importance data than ClickMe. Our experiments measure\nthe alignment between human and DNN visual strategies using ClickMe and Clicktionary feature\nimportance maps captured on the ImageNet validation set. As we describe in \u00a74, ClickMe feature\nimportance maps from the ImageNet training set are used to implement our neural harmonizer.\n\n3\n\n\fPsychophysics participants and dataset. We complemented the feature importance maps from\nClicktionary and ClickMe with psychophysics experiments on rapid visual categorization. We\nrecruited 199 participants from Amazon Mechanical Turk (mturk.com) to complete the experiments.\nParticipants viewed a psychophysics dataset consisting of the 100 animal and 100 non-animal images\nin the Clicktionary game taken from the ImageNet validation set [22]. We used the feature importance\nmaps for each image as masks for the object images, allowing us to control the proportion of important\nfeatures observers were shown when asked to recognize objects (Fig. 5a). We generated versions of\neach image that reveal anywhere between 1% to 100% (at log-scale spaced intervals) of the important\nobject pixels against a phase scrambled noise background (see Appendix \u00a71 for details on mask\ngeneration). The total number of revealed pixels was equal for every image at a given level of image\nmasking, and the revealed pixels were centered against the noise background. Each participant saw\nonly one masked version of each object image.\n\nPsychophysics experiment. Participants were instructed to categorize images in the psychophysics\ndataset as animals or non-animals as quickly and accurately as possible. Each experimental trial\nconsisted of the following sequence of events overlaid onto a white background (SI Fig. 1): (i)\na fixation cross displayed for a variable time (1,100\u20131,600ms); (ii) an image for 400ms; (iii) an\nadditional 150ms of response time. In other words, the experiment forced participants to perform\nrapid object categorization. They were given a total of 550ms to view an image and press a button\nto indicate its category (feedback was provided on trials in which responses were not provided\nwithin this time limit). Images were sized at 256 x 256 pixel resolution, which is equivalent to a\nstimulus size approximately between 5 \u2013 11 degrees of visual angle across a likely range of possible\ndisplay and seating setups we expect participants used for the experiment. Similar paradigms and\ntiming parameters have been shown to capture pre-attentive visual system processing [31, 50\u201352].\nParticipants provided informed consent electronically and were compensated $3.00 for their time (\u223c\n10\u201315 min; approximately $15.00/hr).\n\nModels. We compared humans with 84 different DNNs representing the variety of approaches\nused in the field today: 50 CNNs trained on ImageNet [1, 49, 53\u201364, 64\u201373], 6 CNNs trained on\nother datasets in addition to ImageNet (which we refer to as \u201cCNN extra data\u201d) [1, 66, 74], 10 vision\ntransformers [75\u201379], 6 CNNs trained with self-supervision [80, 81], and 13 models trained for\nrobustness to noise or adversarial examples [82, 83]. We used pretrained weights for each of these\nmodels supplied by their authors, with a variety of licenses (detailed in SI \u00a72), implemented in\nTensorflow 2.0, Keras, or PyTorch.\n\n", "experiments": "\n\n4.1 Where are diagnostic object features for humans and DNNs?\n\nTo systematically compare the visual strategies of object recognition for humans and DNNs on\nImageNet, we first turned to the ClickMe dataset of feature importance maps [20]. In order to derive\ncomparable feature importance maps for DNNs, we needed a method that could be efficiently and con-\nsistently applied to each of the 84 DNNs we tested without any idiosyncratic hyperparameters. This\nled us to choose a classic method for explainable artificial intelligence, image feature saliency [84].\nWe prepared human feature importance maps from ClickMe by taking the average importance map\nproduced by humans for every image that also appeared in ImageNet validation. We then used\nSpearman\u2019s rank-correlation to measure the similarity between human feature maps and DNN feature\nmaps for each image [50]. We also computed the inter-rater alignment of human feature importance\nmaps as the mean split-half correlation across 1000 random splits of the participant pool (\u03c1 = 0.66).\nWe then normalized each human-DNN correlation by this score [20].\n\nThere were dramatic qualitative differences between the features selected by humans and DNNs on\nImageNet. In general, humans selected less context and focused more on object parts: for animals,\nparts of their faces; for non-animals, parts that enable their usage, like the spade of a shovel (see\nFig. 2 and SI Fig. 5. The DNN that was most aligned with humans, the DenseNet121, was still only\n38% aligned with humans (Fig. 3).\n\nPlotting the relationship between DNNs\u2019 top-1 accuracy on ImageNet with their human alignment\nrevealed a striking trade-off: as the accuracy of DNNs has improved beyond DenseNet121, their\n\n4\n\n\fFigure 2: Human and DNNs rely on different features to recognize objects. In contrast, our neural\nharmonizer aligns DNN feature importance with humans. We smooth feature importance maps from\nhumans (ClickMe) and DNNs with a Gaussian kernel for visualization.\n\nalignment with humans has worsened (Fig. 3). For example, consider the ConvNext [1], which\nachieved the best top-1 accuracy in our experiments (85.8%), was only 22% aligned with humans \u2013\nequivalent to the alignment of the BagNet33 [69] (63% top-1 accuracy). As an additional control, we\ncomputed the similarity between the average ClickMe map, which exhibits a center bias [85, 86] (SI\nFig. 5), and each individual ClickMe map. This center-bias control was only outperformed by 42/84\nCNNs we tested (\u2020 in Fig. 3). Overall, we observe that human and DNN alignment has considerably\nworsened since the introduction of these two models.\n\nThe neural harmonizer. While scaling DNNs has immensely helped performance on popular\nbenchmark tasks, there are still fundamental differences in the architectures of DNNs and the\nhuman visual system [37] which could part of the reason to blame for poor alignment. While\nintroducing biological constraints into DNNs could help this problem, there is plenty of evidence that\ndoing so would hurt benchmark performance and require bespoke development for every different\narchitecture [87\u201389]. Is it possible to align a DNN\u2019s visual strategies with humans without hurting its\nperformance?\n\nSuch a general-purpose method for aligning human and DNN visual strategies should satisfy the\nfollowing criteria: (i) The method should work with any fully-differentiable network architecture.\n(ii) It should not present optimization issues that interfere with learning to solve a task, and the\ntask-accuracy of a model trained with the method should not be worse than a model trained without\nthe method. We created the neural harmonizer to satisfy these criteria.\nLet us consider a supervised categorization problem with an input space, X an output space Y \u2286 Rc\nand a predictor function f\u03b8 : X \u2192 Y parameterized by \u03b8, which maps an input vector x \u2208 X to an\noutput f\u03b8(x). We denote g : F \u00d7 X \u2192 X an explanation functional that, given a predictor f\u03b8 \u2208 F\nand an input, returns a feature importance map \u03d5 = g(f\u03b8, x). Here, we focus on DNN saliency\ng(f\u03b8, x) \u225c \u2207xf\u03b8(x) as our method for computing feature importance in DNNs, but the method can\nin principle work with any differentiable network explanation method.\n\nTo satisfy criterion (i), the neural harmonizer introduces a differentiable loss that will enforce\nalignment across feature importance map scales from any neural network. Let Pi(.) be the function\nmapping a feature importance map \u03d5 to it is representation in the N levels of a Gaussian pyramid,\nwith i \u2208 {1, ..., N }. The function Pi(\u03d5) is computed by downsampling Pi\u22121(\u03d5) using a Gaussian\n\n5\n\nHarmonized ViTHuman (ClickMe)MLP-MixerConvNextViTResNet50Robust ResNet50SnakeBearHareLeopardBallSimCLR\fFigure 3: The trade-off between DNN performance and alignment with human feature impor-\ntance from ClickMe [20]. Human feature alignment is the mean Spearman correlation between\nhuman and DNN feature importance maps, normalized by the average inter-rater alignment of hu-\nmans. The shaded region denotes the pareto frontier of the trade-offs between ImageNet accuracy\nand human feature alignment for unharmonized models. Harmonized models (VGG16, ResNet50,\nViT, and EfficientNetB0) are more accurate and aligned than versions of those models trained only\nfor categorization. Error bars are bootstrapped standard deviations over feature alignment. Arrows\nshow a shift in performance after training with the neural harmonizer. The feature alignment of an\naverage of ClickMe maps with held-out maps is denoted by \u2020.\n\nkernel, with P1(\u03d5) = \u03d5. We then seek to minimize (cid:80)N\nfeature importance maps between humans and DNNs at every scale of the pyramid.\n\n||Pi(g(f\u03b8, x)) \u2212 Pi(\u03d5)||2, which will align\n\ni\n\nTo satisfy criterion (ii), the neural harmonizer should work well with training routines designed\nfor large-scale computer vision challenges like ImageNet. This means that the neural harmonizer\nloss must avoid optimization issues at scale. To do this, we need a way of comparing feature\nimportance maps between humans and DNNs that is invariant to the norm of either map. We therefore\nstandardize feature importance maps from humans and DNNs before comparing them, and only\nmeasure alignment on the most important areas of the image for each observer. Formally, let z(.) be\na standardization function over feature importance maps that takes the mean and standard deviation\ncomputed for each map \u03d5 such that z(\u03d5) has 0 activation on average and unit standard deviation.\nTo focus alignment on important regions, let z(\u03d5)+ denote the positive part of the standardized\nexplanation z(\u03d5). Finally, we include a task loss, the familiar cross entropy objective, to yield the\ncomplete neural harmonization loss and train models that are at least as accurate as those trained\nwithout harmonization:\n\nLHarmonization =\u03bb1\n\nN\n(cid:88)\n\ni\n\n||(z \u25e6 Pi \u25e6 g(f\u03b8, x))+ \u2212 (z \u25e6 Pi(\u03d5))+||2\n\n+ LCCE(f\u03b8, x, y) + \u03bb2\n\n(cid:88)\n\n\u03b82\ni\n\ni\n\n(1)\n\n(2)\n\n6\n\nTop-1 ImageNet Accuracy (%)607080556575858040200CNNTransformerSelf-supervisedRobustCNN extra dataNeural harmonizer60100Human feature alignment (%)\u2020\fFigure 4: The trade-off between DNN performance and alignment with human feature impor-\ntance from Clicktionary [22]. Human feature alignment is the mean Spearman correlation between\nhuman and DNN feature importance maps, normalized by the average inter-rater alignment of humans.\nThe shaded region denotes the pareto frontier of the trade-offs between ImageNet accuracy and human\nfeature alignment for unharmonized models. Harmonized models (VGG16, ResNet50, MobileNetV1,\nand EfficientNetB0) are more accurate and aligned than versions of those models trained only for\ncategorization. Error bars are bootstrapped standard deviations over feature alignment. Arrows denote\na shift in performance after training with the neural harmonizer.\n\nTraining. We trained four different DNNs with the neural harmonizer: VGG16, ViT, ResNet50, and\nEfficientNetB0. These models were selected because they are popular convolutional and transformer\nnetworks with open-source architectures that are straightforward to train and also sit near the boundary\nof the trade-off between DNN performance and alignment with humans. Models were trained using\nthe neural harmonizer to optimize categorization performance on ImageNet and feature importance\nmap alignment with human data from ClickMe. We trained models on all images in the ImageNet\ntraining set, but because ClickMe only contains human feature importance maps for a portion of those\nimages, we computed the categorization loss but not the neural harmonizer loss for images without\nimportance maps. Models were trained using 8 cores V4 TPUs on the Google Cloud Platform, and\ntraining lasted approximately one day. Models were trained with an augmented ResNet training\nrecipe (built from https://github.com/tensorflow/tpu/). Models were optimized with SGD\nand momentum over batches of 512 images, a learning rate of 0.3, and label smoothing [90]. Images\nwere augmented with random left-right flips and mixup [91]. The learning rate was adjusted over\nthe course of training with a schedule that began with an initial warm-up period of 5 epochs and\nthen decaying according to a cosine function over 90 epochs, with decay at step 30, 50 and 80.\nWe validated that a ResNet50 and VGG16 trained with these hyperparameters and schedule using\nstandard cross-entropy (but not the neural harmonizer) matched published performance.\n\nThe neural harmonizer aligns human and DNN visual strategies. We found that harmonized\nmodels broke the trade-off between ImageNet accuracy and model alignment with ClickMe human\nfeature importance maps (Fig. 3). Harmonized models were significantly more aligned with feature\nimportance maps and also performed better on ImageNet. The changes in where harmonized models\nfind important features in images were dramatic: a harmonized ViT had feature importance maps that\nare far less reliant on context (Fig. 2) and approximately 150% more aligned with humans (Fig. 3;\n\n7\n\nTop-1 ImageNet Accuracy (%)60708055657585Human feature alignment (%)8040200CNNTransformerSelf-supervisedRobustCNN extra dataNeural harmonizer60100\fFigure 5: Comparing how humans and DNNs use visual features during object recognition.\n(a) Humans and DNNs categorized ImageNet validation images as animals or non-animals. The\nimages revealed only a portion of the most important visual features according to the Clicktionary\ngame [92]. (b) There was a trade-off between DNN top-1 accuracy on ImageNet and alignment with\nhuman visual decision making. The shaded region denotes the pareto frontier of the trade-off between\nImageNet accuracy and human feature alignment for unharmonized models. Arrows denote a shift in\nperformance after training with the neural harmonizer. Error bars are bootstrapped standard deviations\nover decision-making alignment. (c) A state-of-the-art DNN like the ViT learned a different strategy\nfor integrating visual features into decisions than humans or a harmonized ViT.\n\nViT goes from 28.7% to 72.6% alignment after harmonization). The same model also performed 4%\nbetter in top-1 accuracy without any changes to its architecture. Similar improvements were found for\n\n8\n\nTop-1 ImageNet Accuracy (%)60708055657585843657-log(mean squared error)AnimalNon-animal120100Important human features (%)Human features revealed (%)1Normalized performance (%)01008060402020406080100(a)(c)(b)HumanCNNTransformerSelf-supervisedRobustCNN extra dataNeural harmonizer\fthe harmonized VGG16 and ResNet50. While the EfficientNetB0 had only a minimal improvement\nin accuracy, it too exhibited a large boost in human feature alignment.\n\nClicktionary. To test if the trade-off between DNN ImageNet accuracy and alignment with humans\nis a general phenomenon we next turned to Clicktionary [22]. Indeed, we observed a similar trade-off\non this dataset as we found for ClickMe: alignment with human feature importance from Clicktionary\nhas worsened as DNN accuracy has improved on ImageNet (Fig. 4). As with ClickMe, harmonized\nDNNs shift the accuracy-alignment trade-off on this dataset.\n\n4.2 How do humans and DNNs integrate diagnostic object features into decisions?\n\nThe trade-off we discovered between DNN accuracy on ImageNet and alignment with human visual\nfeature importance suggests that the two use different visual strategies for object classification.\nHowever, there is potential for an even deeper problem. Even if two observers deem the same regions\nof an image as important for recognizing it, there is no guarantee that they use the selected features in\nthe same way to render their decisions. We posit that if two observers have aligned visual strategies,\nthe will agree on both where important features are in an image and how they use those features for\ndecisions.\n\nWe developed a psychophysics experiment to measure how different humans use features in ImageNet\nimages to recognize objects. Participants viewed versions of these images where only a proportion\nof the features that were deemed most important in the Clicktionary game were visible (Fig. 5a).\nParticipants had to accurately detect whether or not the image contained an animal within 550ms,\nwhich forced them to rely on feedforward processing as much as possible [33]. Each of the 200\nimages we used were shown to a single participant only once. We accumulated responses from all\nparticipants to construct decision curves that showed how accurately the average human converted\nany given proportion of image features into an object decision. We performed the same experiment\non DNNs as we did on humans, recording animal vs. non-animal decisions according to whether\nor not the most probable category in the model\u2019s 1000-category output was an animal. Because the\nexperiment was speeded, humans did not achieve perfect accuracy. Thus, we normalized performance\nfor humans and DNNs to compare the rate at which each integrated features into accurate decisions.\n\nWe discovered a similar trade-off between ImageNet accuracy and alignment with human visual\ndecision making in this experiment as we did in ClickMe and Clicktionary (Fig. 5b). Indeed, the\nmodel that was most aligned with human decision-making \u2013 the BagNet33 [69] \u2013 only achieved 63.0%\naccuracy on ImageNet. Surprisingly, harmonized models broke this trend, particularly the harmonized\nViT (Fig. 5b, top-right), despite no explicit constraints in that procedure which forced consistent\ndecision-making with humans. In contrast, an unharmonized ViT integrates visual information into\naccurate decisions less efficiently than humans or harmonized models (Fig. 5c).\n\n higher diameter and value norm.", "conclusion": "\n\nModels that reliably categorize objects like humans do would shift the paradigms of the cognitive\nsciences and artificial intelligence. But despite continuous progress over the past decade on the\nImageNet benchmark, DNNs are becoming worse models of human vision. Our finding resembles\na growing number of concurrent works showing similar trade-offs between DNN performance and\npredictions of human perception on different tasks [93, 94]. Our solution to this problem, the neural\nharmonizer, can be applied to any DNN to align their visual strategies with humans and even improve\nperformance.\n\nWe observed the greatest benefit of harmonization on the visual transformer, the ViT. This finding\nis particularly surprising given that transformers eschew the locality bias of convolutional neural\nnetworks that has helped them become the new standard for modeling human vision and cognition [37].\nThus, we suspect that the neural harmonizer is especially well-suited for large-scale training on low-\ninductive bias models, like transformers. We also hypothesize that the improvements in human\nalignment provided by the neural harmonizer will yield a variety of downstream benefits for a model\nlike the ViT, including better predictions of perceptual similarity, stimulus-evoked neural responses,\nand even performance on visual reasoning tasks. We leave these analyses for future work.\n\n9\n\n\fThe field of computer vision today is following Sutton\u2019s prescient lesson: benchmark tasks can be\nscaling architectural capacity and the size of training data. However, as we have demonstrated here,\nthese scaling laws are exchanging performance for alignment with human perception. We encourage\nthe field to re-analyze the costs and benefits of this exchange, particularly in light of the growing\nconcerns about DNNs leveraging shortcuts and dataset biases to achieve high performance [19].\nAlignment with human vision need not be exchanged with performance if DNNs are harmonized.\nOur codebase (https://serre-lab.github.io/Harmonization/) can be used to incorporate\nthe neural harmonizer into any DNN created and measure its alignment with humans on the datasets\nwe describe in this paper.\n\nLimitations. One possible explanation for the misalignment between DNNs and humans that\nwe observe is that recent DNNs have achieved superhuman accuracy on ImageNet. Superhuman\nDNNs have been described in biomedical applications [95, 96] where there is definitive biological\nground-truth labels, but ImageNet labels are noisy, making it unclear if such an achievement is\nlaudable (https://labelerrors.com/). Thus, an equally likely explanation is that the continued\nimprovements of DNNs at least partially reflect their exploitation of shortcuts in ImageNet [19].\n\nThe scope of our work is also limited in that it focuses on object recognition in ImageNet. It is\npossible that models trained on other tasks, such as segmentation, may be more aligned with humans.\n\nFinally, our modeling efforts were hamstrung for the largest-scale models in existence. Our work\ndoes not answer how much harmonization would help a model like CLIP because of the massive\ninvestment needed to train it. The neural harmonizer can be applied to CLIP but it is possible that\nmore ClickMe human feature importance maps are needed for successful harmonization.\n\nBroader impacts. A persistent issue in the field of artificial intelligence is the tendency of models to\nexploit dataset biases. A central theme of our work is that there are facets of human perception that are\nnot captured by DNNs, particularly those which follow the scaling laws which have been so embraced\nby industry leaders. Forcing DNNs to rely on similar visual strategies as humans could represent a\nscalable path forward to correcting the insidious biases which have assailed under-constrained models\nof artificial intelligence.\n\n", "appendix": "\n\nThe psychophysics experiments of \u00a74.2 were implemented with the psiTurk framework [1] and\ncustom javascript functions. Each trial sequence was converted to a HTML5-compatible video for\nthe fastest reliable presentation time possible in a web browser. Videos were cached before each\ntrial to optimize reliability of experiment timing within the web browser. A photo-diode verified\nthe reliability of stimulus timing in our experiment was consistently accurate within \u223c 10ms across\ndifferent operating system, web browser, and display type configurations.\n\nParticipants: We recruited 199 participants from Amazon Mechanical Turk (mturk.com) for the\nexperiments. Participants were based in the United States, used either the Firefox or Chrome browser\non a non-mobile device, and had a minimal average approval rating of 95% on past Mechanical Turk\ntasks.\n\nStimuli: Experiment images were taken from the Clicktionary dataset [2]. Images were sampled\nfrom 5 target and 5 distractor categories: border collie, sorrel (horse), great white shark, bald eagle,\nand panther; trailer truck, sports car, speedboat, airliner, and school bus. Images were presented to\nhuman participants (and DNNs) either intact or with a perceptual phase scrambled mask that exposed\na proportion of their most important visual features, as described in the main text. Images were cast\nto greyscale to control for trivial color-based cues for classification and blend the scrambled mask\nbackground into the foreground. Responses to intact images were used to normalize the performance\nof each observer on masked images relative to their maximum performance on these images.\n\nImage masks were created for each image to reveal only a proportion of the most important visual\nfeatures. For each image, we created masks that revealed between 1% and 100% (at log-scale spaced\nintervals) of the object pixels in the corresponding image\u2019s Clicktionary feature importance map. We\ngenerated these masks in two steps. First, we computed a phase-scrambled version of the image [3, 4].\nNext, we used a novel \u201cstochastic flood-fill\u201d algorithm to reveal a contiguous region of the most\nimportant visual features in the image according to humans. Our flood-fill algorithm was seeded on\nthe pixel deemed most important by humans in the image, then grew outwards anisotropically and\nbiased towards pixels with higher feature importance scores (Figure 1). The revealed region was\nalways centered on the image. Each participant saw every category exemplar only once, with its\namount of image revelation randomly selected from all possible configurations.\n\nAfter providing online consent, participants were instructed to complete a rapid visual categorization\ntask in which they had to classify stimuli revealing a portion of the most diagnostic object features\n(Fig. 3). Each experimental trial began with a cross for participants to fixate for a variable time\n(1,100\u20131,600ms), then a stimulus for 400ms, then another cross and additional time for participants\nto render a decision. Participants were instructed to provide a decision after the first fixation cross,\n\n*These authors contributed equally.\n1Department of Cognitive, Linguistic, & Psychological Sciences, Brown University, Providence, RI\n2Artificial and Natural Intelligence Toulouse Institute (ANITI), Toulouse, France\n3Carney Institute for Brain Science, Brown University, Providence, RI\n\n36th Conference on Neural Information Processing Systems (NeurIPS 2022).\n\n\fFigure 1: Overview of the psychophysics paradigm. Participants performed a rapid animals vs.\nvehicles categorization paradigm (top). Stimuli were created using feature importance maps derived\nfrom humans or DNNs via a \u201cstochastic flood-fill\u201d algorithm that revealed image regions of different\nsizes centered on important features. Sample stimuli are shown (bottom) for different percentages of\nimage revelation. Note that 100% revelation corresponds to all non-zero pixels in a feature importance\nmap.\n\nbut that they only had 650ms to answer. If they were too slow to respond they were told to respond\nfaster and the trial was discarded.\n\n2 Harmonization loss\n\nThe neural harmonizer loss Fig. 2 uses several components crucial to its performance: a pyramidal\nrepresentation of decision explanation maps and normalizing those maps.\n\n2\n\nFigure 1TargetDistractorPercent image revelation1%25%63%100%FullProcessed featureimportance map++Please respondfaster!Pre-stimulus \ufb01xation(1,100 \u2013 1,600ms)Stimulus400msExtra response150ms\u2026if windowIs reachedExperiment trial \fFigure 2: Computing the neural harmonizer loss..\n\nFigure 3: Psychophysics experiment instructions.\n\nWhen computing the difference between model explanations for an image and the human feature\nimportance map for that image, we rely on a pyramid representation of each to compute these\ndifferences Fig. 2). This pyramid allows for a model to align its feature representations with humans\nat multiple scales and corrects for an important problem in datasets like ClickMe: the human data is\nan approximation and not precise at the pixel level. This lack of precision can present optimization\nissues, and computing a pyramid representation alleviates those issues because it allows a model to\nlearn to focus on regions that are important for humans without pixel-level precision.\n\nStandardization tackles a similar problem: because of the imprecision of human data, we choose\nto focus harmonization on only the most important areas selected by humans in ClickMe. By\nstandardizing then rectifying before comparing human and model explanations, we reduce noise in\nthe harmonization procedure.\n\n3\n\n+ReLUReLUdistanceGaussianPyramidClickMemap(    )StandardizationaStandardizationa\fFigure 4: Example ClickMe feature importance maps on ImageNet images.\n\n3 Additional Results\n\n3.1 ClickMe\n\nThe ClickMe game by [5] was used to identify category diagnostic features in ImageNet images.\nThese feature importance maps largely focus on object regions rather than context, and in contrast to\nsegmentation maps select features on the \u201cfront\u201d or \u201cface\u201d of objects (Fig. 4).\n\nAs discussed in the main text, we found a trade-off between DNN top-1 ImageNet accuracy and\nthe alignment of their feature importance maps with humans importance maps from ClickMe. This\ntrade-off persists across multiple scales of feature importance maps, including 4\u00d7 (Fig. 6) and 16\u00d7\n(Fig. 7) sub-sampled maps, meaning that simple smoothing is not sufficient to fix the trade-off.\n\n3.2 ViT attention\n\nWhile in the main text we investigate alignment between humans and models using gradient feature\nimportance visualizations, the attention maps in transformer models like the ViT provide another\navenue for investigation. To understand whether or not attention maps from ViT are more aligned\nwith humans than their gradient-based decision explanation maps, we computed attention rollouts\nfor harmonized and unharmonized ViTs [6]. We found that both versions of the ViT had similar\ncorrelations between their attention rollouts and human ClickMe maps: 0.38 for the harmonized ViT\nand 0.393 for the unharmonized model. This surprising result suggests that the harmonizer affects the\nprocess by which ViTs integrate visual information into their decisions rather than how they allocate\nattention. Through manipulating ViT decision making processes, the harmonizer can induce the large\nchanges in gradient-based visualizations and psychophysics that we describe in the main text.\n\n3.3 Correlations between measurements of human visual strategies\n\nOur results rely on three independent datasets measuring different features of human visual strategies:\nClickMe, Clicktionary, and the psychophysics experiments we introduce in this manuscript. The fact\nthat all three evoke similar trade-offs between top-1 accuracy and human alignment is a surprising\nresult that deserves further attention. We investigated these trade-offs by measuring the correlation be-\ntween human alignment on each dataset, with and without models trained with the neural harmonizer.\nWe found that correlations between datasets were lower across the board when neural harmonizer\nmodels were not included. The association between model alignments with Clicktionary versus\npsychophysics results were not significant (\u03c1 = 0.21, n.s.; Fig. 9), but the associations between\n\n4\n\nHuman feature preferenceLessMore\fFigure 5: Feature importance maps of humans, harmonized, and unharmonized models on\nImageNet.\n\nmodel alignments with ClickMe versus psychophysics (\u03c1 = 0.51, p < 0.001; Fig. 8) and ClickMe\nversus Clicktionary (\u03c1 = 0.77, p < 0.001; Fig. 10) were both significant. Each correlation improved\nwhen the neural harmonizer models were included in the calculation. This finding indicates that the\nneural harmonizer successfully aligned visual strategies between humans and DNNs, and was not\nmerely benefiting from either where humans versus DNNs considered important visual features to be\nor how humans versus DNNs incorporated those features into their decisions.\n\n5\n\nSnakeShovelVanBrown bearAssaultrifleImageClickMeResNet50SimCLRRobustResNet50ConvNextHarmonizedViTHareSoccer ballLeopardMLP-MixerViTHarmonized ViTHuman (ClickMe)MLP-MixerConvNextViTResNet50Robust ResNet50SnakeBearHareLeopardBallSimCLRRi\ufb02eVanShovel\fFigure 6: The neural harmonizer\u2019s effect is robust across image scales. Here, we show that the\ntrade-off between ImageNet accuracy and alignment with humans holds across downsizing by a\nfactor of 4. The Neural harmonizer once again yields the model with the best alignment with humans.\nGrey-shaded area captures the trade-off between accuracy and alignment in standard DNNs. Error\nbars are bootstrapped standard deviations over feature alignment.\n\n6\n\nTop-1 ImageNet Accuracy (%)607080556575858040200CNNTransformerSelf-supervisedRobustCNN extra dataNeural harmonizer60100Human feature alignment (%)\u2020CNNTransformerSelf-supervisedRobustCNN extra dataNeural harmonizer\fFigure 7: The neural harmonizer\u2019s effect is robust across image scales. Here, we show that the\ntrade-off between ImageNet accuracy and alignment with humans holds across downsizing by a factor\nof 16. The Neural harmonizer once again yields the model with the best alignment with humans.\nGrey-shaded area captures the trade-off between accuracy and alignment in standard DNNs. Error\nbars are bootstrapped standard deviations over feature alignment.\n\nFigure 8: The association between ClickMe alignment versus psychophysics alignment. These\nscores are significantly correlated, \u03c1 = 0.68, p < 0.001. Error bars are bootstrapped standard\ndeviations over feature alignment.\n\n7\n\nTop-1 ImageNet Accuracy (%)607080556575858040200CNNTransformerSelf-supervisedRobustCNN extra dataNeural harmonizer60100Human feature alignment (%)-log(mean squared error)843CNNTransformerSelf-supervisedRobustCNN extra dataNeural harmonizer657Human feature alignment (%)10204060705030\fFigure 9: The association between Clicktionary alignment versus psychophysics alignment.\nThese scores are significantly correlated, \u03c1 = 0.53, p < 0.001.\n\nFigure 10: The association between ClickMe alignment versus Clicktionary alignment. These\nscores are significantly correlated, \u03c1 = 0.85, p < 0.001. Error bars are bootstrapped standard\ndeviations over feature alignment.\n\n8\n\n843CNNTransformerSelf-supervisedRobustCNN extra dataNeural harmonizer657Human feature alignment (%)1020304050-log(mean squared error)Clicktionary feature alignment (%)10020CNNTransformerSelf-supervisedRobustCNN extra dataNeural harmonizer604080Clickme feature alignment (%)20408010060\fFigure 11: The mean of ClickMe feature importance maps exhibits a center bias, likely due\nto the positioning of objects in ImageNet images rather than a purely spatial bias of human\nparticipants (compare to individual maps shown in Fig. 4).\n\n9\n\n\f", "system_prompts": [{"role": "system", "content": "You are a computer science researcher currently reviewing a paper titled \"Harmonizing the object recognition strategies of deep neural networks with humans\" for the NeurIPS computer science conference. Your goal is to try to be as objective and truthful as possible in your answers about the paper provided. Your reviews will be used for causal reasoning in determining the quality of the paper."}], "prompts": {"1b": {"role": "user", "content": "The following is the conclusion section of the paper you are reviewing:\n\n\nModels that reliably categorize objects like humans do would shift the paradigms of the cognitive\nsciences and artificial intelligence. But despite continuous progress over the past decade on the\nImageNet benchmark, DNNs are becoming worse models of human vision. Our finding resembles\na growing number of concurrent works showing similar trade-offs between DNN performance and\npredictions of human perception on different tasks [93, 94]. Our solution to this problem, the neural\nharmonizer, can be applied to any DNN to align their visual strategies with humans and even improve\nperformance.\n\nWe observed the greatest benefit of harmonization on the visual transformer, the ViT. This finding\nis particularly surprising given that transformers eschew the locality bias of convolutional neural\nnetworks that has helped them become the new standard for modeling human vision and cognition [37].\nThus, we suspect that the neural harmonizer is especially well-suited for large-scale training on low-\ninductive bias models, like transformers. We also hypothesize that the improvements in human\nalignment provided by the neural harmonizer will yield a variety of downstream benefits for a model\nlike the ViT, including better predictions of perceptual similarity, stimulus-evoked neural responses,\nand even performance on visual reasoning tasks. We leave these analyses for future work.\n\n9\n\n\fThe field of computer vision today is following Sutton\u2019s prescient lesson: benchmark tasks can be\nscaling architectural capacity and the size of training data. However, as we have demonstrated here,\nthese scaling laws are exchanging performance for alignment with human perception. We encourage\nthe field to re-analyze the costs and benefits of this exchange, particularly in light of the growing\nconcerns about DNNs leveraging shortcuts and dataset biases to achieve high performance [19].\nAlignment with human vision need not be exchanged with performance if DNNs are harmonized.\nOur codebase (https://serre-lab.github.io/Harmonization/) can be used to incorporate\nthe neural harmonizer into any DNN created and measure its alignment with humans on the datasets\nwe describe in this paper.\n\nLimitations. One possible explanation for the misalignment between DNNs and humans that\nwe observe is that recent DNNs have achieved superhuman accuracy on ImageNet. Superhuman\nDNNs have been described in biomedical applications [95, 96] where there is definitive biological\nground-truth labels, but ImageNet labels are noisy, making it unclear if such an achievement is\nlaudable (https://labelerrors.com/). Thus, an equally likely explanation is that the continued\nimprovements of DNNs at least partially reflect their exploitation of shortcuts in ImageNet [19].\n\nThe scope of our work is also limited in that it focuses on object recognition in ImageNet. It is\npossible that models trained on other tasks, such as segmentation, may be more aligned with humans.\n\nFinally, our modeling efforts were hamstrung for the largest-scale models in existence. Our work\ndoes not answer how much harmonization would help a model like CLIP because of the massive\ninvestment needed to train it. The neural harmonizer can be applied to CLIP but it is possible that\nmore ClickMe human feature importance maps are needed for successful harmonization.\n\nBroader impacts. A persistent issue in the field of artificial intelligence is the tendency of models to\nexploit dataset biases. A central theme of our work is that there are facets of human perception that are\nnot captured by DNNs, particularly those which follow the scaling laws which have been so embraced\nby industry leaders. Forcing DNNs to rely on similar visual strategies as humans could represent a\nscalable path forward to correcting the insidious biases which have assailed under-constrained models\nof artificial intelligence.\n\n\n\nBased on the section(s), please answer the following question with yes, no, or n/a and provide a brief justification for your answer.\nQuestion: Do the authors describe the limitations of their work?"}, "1c": {"role": "user", "content": "The following is the conclusion section of the paper you are reviewing:\n\n\nModels that reliably categorize objects like humans do would shift the paradigms of the cognitive\nsciences and artificial intelligence. But despite continuous progress over the past decade on the\nImageNet benchmark, DNNs are becoming worse models of human vision. Our finding resembles\na growing number of concurrent works showing similar trade-offs between DNN performance and\npredictions of human perception on different tasks [93, 94]. Our solution to this problem, the neural\nharmonizer, can be applied to any DNN to align their visual strategies with humans and even improve\nperformance.\n\nWe observed the greatest benefit of harmonization on the visual transformer, the ViT. This finding\nis particularly surprising given that transformers eschew the locality bias of convolutional neural\nnetworks that has helped them become the new standard for modeling human vision and cognition [37].\nThus, we suspect that the neural harmonizer is especially well-suited for large-scale training on low-\ninductive bias models, like transformers. We also hypothesize that the improvements in human\nalignment provided by the neural harmonizer will yield a variety of downstream benefits for a model\nlike the ViT, including better predictions of perceptual similarity, stimulus-evoked neural responses,\nand even performance on visual reasoning tasks. We leave these analyses for future work.\n\n9\n\n\fThe field of computer vision today is following Sutton\u2019s prescient lesson: benchmark tasks can be\nscaling architectural capacity and the size of training data. However, as we have demonstrated here,\nthese scaling laws are exchanging performance for alignment with human perception. We encourage\nthe field to re-analyze the costs and benefits of this exchange, particularly in light of the growing\nconcerns about DNNs leveraging shortcuts and dataset biases to achieve high performance [19].\nAlignment with human vision need not be exchanged with performance if DNNs are harmonized.\nOur codebase (https://serre-lab.github.io/Harmonization/) can be used to incorporate\nthe neural harmonizer into any DNN created and measure its alignment with humans on the datasets\nwe describe in this paper.\n\nLimitations. One possible explanation for the misalignment between DNNs and humans that\nwe observe is that recent DNNs have achieved superhuman accuracy on ImageNet. Superhuman\nDNNs have been described in biomedical applications [95, 96] where there is definitive biological\nground-truth labels, but ImageNet labels are noisy, making it unclear if such an achievement is\nlaudable (https://labelerrors.com/). Thus, an equally likely explanation is that the continued\nimprovements of DNNs at least partially reflect their exploitation of shortcuts in ImageNet [19].\n\nThe scope of our work is also limited in that it focuses on object recognition in ImageNet. It is\npossible that models trained on other tasks, such as segmentation, may be more aligned with humans.\n\nFinally, our modeling efforts were hamstrung for the largest-scale models in existence. Our work\ndoes not answer how much harmonization would help a model like CLIP because of the massive\ninvestment needed to train it. The neural harmonizer can be applied to CLIP but it is possible that\nmore ClickMe human feature importance maps are needed for successful harmonization.\n\nBroader impacts. A persistent issue in the field of artificial intelligence is the tendency of models to\nexploit dataset biases. A central theme of our work is that there are facets of human perception that are\nnot captured by DNNs, particularly those which follow the scaling laws which have been so embraced\nby industry leaders. Forcing DNNs to rely on similar visual strategies as humans could represent a\nscalable path forward to correcting the insidious biases which have assailed under-constrained models\nof artificial intelligence.\n\n\n\nBased on the section(s), please answer the following question with yes, no, or n/a and provide a brief justification for your answer.\nQuestion: Do the authors discuss any potential negative societal impacts of their work?"}, "2a": {"role": "user", "content": "The following is the methods section of the paper you are reviewing:\n\n\nHuman feature importance datasets. We focused on the ImageNet dataset to compare the visual\nstrategies of humans and DNNs for object recognition at scale. We relied on the two significant\nefforts for gathering feature importance data from humans on ImageNet: the Clicktionary [22] and\nClickMe [20] games, which use slightly different methods to collect their data. Both games begin with\nthe same basic setup: two players work together to locate features in an object image that they believe\nare important for categorizing it. As one of the players selects important image regions, those regions\nare filled into a blank canvas for the other observer to see and categorize the image as quickly as\npossible. In Clicktionary [22], both players are humans, whereas in ClickMe [20], the player selecting\nfeatures is a human and the player recognizing images is a DNN (VGG16 [49]). For both games,\nfeature importance maps depicting the average object category diagnosticity of every pixel was\ncomputed as the probability of it being clicked by a participant. In total, Clicktionary [22] contained\nfeature importance maps for 200 images from the ImageNet validation set, whereas ClickMe [20]\ncontained feature importance maps for a non-overlapping set of 196,499 images from ImageNet\ntraining and validation sets. Thus, ClickMe has far more data than Clicktionary, but Clicktionary\ndata has more reliable human feature importance data than ClickMe. Our experiments measure\nthe alignment between human and DNN visual strategies using ClickMe and Clicktionary feature\nimportance maps captured on the ImageNet validation set. As we describe in \u00a74, ClickMe feature\nimportance maps from the ImageNet training set are used to implement our neural harmonizer.\n\n3\n\n\fPsychophysics participants and dataset. We complemented the feature importance maps from\nClicktionary and ClickMe with psychophysics experiments on rapid visual categorization. We\nrecruited 199 participants from Amazon Mechanical Turk (mturk.com) to complete the experiments.\nParticipants viewed a psychophysics dataset consisting of the 100 animal and 100 non-animal images\nin the Clicktionary game taken from the ImageNet validation set [22]. We used the feature importance\nmaps for each image as masks for the object images, allowing us to control the proportion of important\nfeatures observers were shown when asked to recognize objects (Fig. 5a). We generated versions of\neach image that reveal anywhere between 1% to 100% (at log-scale spaced intervals) of the important\nobject pixels against a phase scrambled noise background (see Appendix \u00a71 for details on mask\ngeneration). The total number of revealed pixels was equal for every image at a given level of image\nmasking, and the revealed pixels were centered against the noise background. Each participant saw\nonly one masked version of each object image.\n\nPsychophysics experiment. Participants were instructed to categorize images in the psychophysics\ndataset as animals or non-animals as quickly and accurately as possible. Each experimental trial\nconsisted of the following sequence of events overlaid onto a white background (SI Fig. 1): (i)\na fixation cross displayed for a variable time (1,100\u20131,600ms); (ii) an image for 400ms; (iii) an\nadditional 150ms of response time. In other words, the experiment forced participants to perform\nrapid object categorization. They were given a total of 550ms to view an image and press a button\nto indicate its category (feedback was provided on trials in which responses were not provided\nwithin this time limit). Images were sized at 256 x 256 pixel resolution, which is equivalent to a\nstimulus size approximately between 5 \u2013 11 degrees of visual angle across a likely range of possible\ndisplay and seating setups we expect participants used for the experiment. Similar paradigms and\ntiming parameters have been shown to capture pre-attentive visual system processing [31, 50\u201352].\nParticipants provided informed consent electronically and were compensated $3.00 for their time (\u223c\n10\u201315 min; approximately $15.00/hr).\n\nModels. We compared humans with 84 different DNNs representing the variety of approaches\nused in the field today: 50 CNNs trained on ImageNet [1, 49, 53\u201364, 64\u201373], 6 CNNs trained on\nother datasets in addition to ImageNet (which we refer to as \u201cCNN extra data\u201d) [1, 66, 74], 10 vision\ntransformers [75\u201379], 6 CNNs trained with self-supervision [80, 81], and 13 models trained for\nrobustness to noise or adversarial examples [82, 83]. We used pretrained weights for each of these\nmodels supplied by their authors, with a variety of licenses (detailed in SI \u00a72), implemented in\nTensorflow 2.0, Keras, or PyTorch.\n\n\n\nBased on the section(s), please answer the following question with yes, no, or n/a and provide a brief justification for your answer.\nQuestion: If the authors include theoretical results, do the authors state the full set of assumptions of all theoretical results?"}, "2b": {"role": "user", "content": "The following is the methods section of the paper you are reviewing:\n\n\nHuman feature importance datasets. We focused on the ImageNet dataset to compare the visual\nstrategies of humans and DNNs for object recognition at scale. We relied on the two significant\nefforts for gathering feature importance data from humans on ImageNet: the Clicktionary [22] and\nClickMe [20] games, which use slightly different methods to collect their data. Both games begin with\nthe same basic setup: two players work together to locate features in an object image that they believe\nare important for categorizing it. As one of the players selects important image regions, those regions\nare filled into a blank canvas for the other observer to see and categorize the image as quickly as\npossible. In Clicktionary [22], both players are humans, whereas in ClickMe [20], the player selecting\nfeatures is a human and the player recognizing images is a DNN (VGG16 [49]). For both games,\nfeature importance maps depicting the average object category diagnosticity of every pixel was\ncomputed as the probability of it being clicked by a participant. In total, Clicktionary [22] contained\nfeature importance maps for 200 images from the ImageNet validation set, whereas ClickMe [20]\ncontained feature importance maps for a non-overlapping set of 196,499 images from ImageNet\ntraining and validation sets. Thus, ClickMe has far more data than Clicktionary, but Clicktionary\ndata has more reliable human feature importance data than ClickMe. Our experiments measure\nthe alignment between human and DNN visual strategies using ClickMe and Clicktionary feature\nimportance maps captured on the ImageNet validation set. As we describe in \u00a74, ClickMe feature\nimportance maps from the ImageNet training set are used to implement our neural harmonizer.\n\n3\n\n\fPsychophysics participants and dataset. We complemented the feature importance maps from\nClicktionary and ClickMe with psychophysics experiments on rapid visual categorization. We\nrecruited 199 participants from Amazon Mechanical Turk (mturk.com) to complete the experiments.\nParticipants viewed a psychophysics dataset consisting of the 100 animal and 100 non-animal images\nin the Clicktionary game taken from the ImageNet validation set [22]. We used the feature importance\nmaps for each image as masks for the object images, allowing us to control the proportion of important\nfeatures observers were shown when asked to recognize objects (Fig. 5a). We generated versions of\neach image that reveal anywhere between 1% to 100% (at log-scale spaced intervals) of the important\nobject pixels against a phase scrambled noise background (see Appendix \u00a71 for details on mask\ngeneration). The total number of revealed pixels was equal for every image at a given level of image\nmasking, and the revealed pixels were centered against the noise background. Each participant saw\nonly one masked version of each object image.\n\nPsychophysics experiment. Participants were instructed to categorize images in the psychophysics\ndataset as animals or non-animals as quickly and accurately as possible. Each experimental trial\nconsisted of the following sequence of events overlaid onto a white background (SI Fig. 1): (i)\na fixation cross displayed for a variable time (1,100\u20131,600ms); (ii) an image for 400ms; (iii) an\nadditional 150ms of response time. In other words, the experiment forced participants to perform\nrapid object categorization. They were given a total of 550ms to view an image and press a button\nto indicate its category (feedback was provided on trials in which responses were not provided\nwithin this time limit). Images were sized at 256 x 256 pixel resolution, which is equivalent to a\nstimulus size approximately between 5 \u2013 11 degrees of visual angle across a likely range of possible\ndisplay and seating setups we expect participants used for the experiment. Similar paradigms and\ntiming parameters have been shown to capture pre-attentive visual system processing [31, 50\u201352].\nParticipants provided informed consent electronically and were compensated $3.00 for their time (\u223c\n10\u201315 min; approximately $15.00/hr).\n\nModels. We compared humans with 84 different DNNs representing the variety of approaches\nused in the field today: 50 CNNs trained on ImageNet [1, 49, 53\u201364, 64\u201373], 6 CNNs trained on\nother datasets in addition to ImageNet (which we refer to as \u201cCNN extra data\u201d) [1, 66, 74], 10 vision\ntransformers [75\u201379], 6 CNNs trained with self-supervision [80, 81], and 13 models trained for\nrobustness to noise or adversarial examples [82, 83]. We used pretrained weights for each of these\nmodels supplied by their authors, with a variety of licenses (detailed in SI \u00a72), implemented in\nTensorflow 2.0, Keras, or PyTorch.\n\n\n\nBased on the section(s), please answer the following question with yes, no, or n/a and provide a brief justification for your answer.\nQuestion: If the authors include theoretical results, do the authors include complete proofs of all theoretical results?"}, "3a": {"role": "user", "content": "The following is the experiments section of the paper you are reviewing:\n\n\n4.1 Where are diagnostic object features for humans and DNNs?\n\nTo systematically compare the visual strategies of object recognition for humans and DNNs on\nImageNet, we first turned to the ClickMe dataset of feature importance maps [20]. In order to derive\ncomparable feature importance maps for DNNs, we needed a method that could be efficiently and con-\nsistently applied to each of the 84 DNNs we tested without any idiosyncratic hyperparameters. This\nled us to choose a classic method for explainable artificial intelligence, image feature saliency [84].\nWe prepared human feature importance maps from ClickMe by taking the average importance map\nproduced by humans for every image that also appeared in ImageNet validation. We then used\nSpearman\u2019s rank-correlation to measure the similarity between human feature maps and DNN feature\nmaps for each image [50]. We also computed the inter-rater alignment of human feature importance\nmaps as the mean split-half correlation across 1000 random splits of the participant pool (\u03c1 = 0.66).\nWe then normalized each human-DNN correlation by this score [20].\n\nThere were dramatic qualitative differences between the features selected by humans and DNNs on\nImageNet. In general, humans selected less context and focused more on object parts: for animals,\nparts of their faces; for non-animals, parts that enable their usage, like the spade of a shovel (see\nFig. 2 and SI Fig. 5. The DNN that was most aligned with humans, the DenseNet121, was still only\n38% aligned with humans (Fig. 3).\n\nPlotting the relationship between DNNs\u2019 top-1 accuracy on ImageNet with their human alignment\nrevealed a striking trade-off: as the accuracy of DNNs has improved beyond DenseNet121, their\n\n4\n\n\fFigure 2: Human and DNNs rely on different features to recognize objects. In contrast, our neural\nharmonizer aligns DNN feature importance with humans. We smooth feature importance maps from\nhumans (ClickMe) and DNNs with a Gaussian kernel for visualization.\n\nalignment with humans has worsened (Fig. 3). For example, consider the ConvNext [1], which\nachieved the best top-1 accuracy in our experiments (85.8%), was only 22% aligned with humans \u2013\nequivalent to the alignment of the BagNet33 [69] (63% top-1 accuracy). As an additional control, we\ncomputed the similarity between the average ClickMe map, which exhibits a center bias [85, 86] (SI\nFig. 5), and each individual ClickMe map. This center-bias control was only outperformed by 42/84\nCNNs we tested (\u2020 in Fig. 3). Overall, we observe that human and DNN alignment has considerably\nworsened since the introduction of these two models.\n\nThe neural harmonizer. While scaling DNNs has immensely helped performance on popular\nbenchmark tasks, there are still fundamental differences in the architectures of DNNs and the\nhuman visual system [37] which could part of the reason to blame for poor alignment. While\nintroducing biological constraints into DNNs could help this problem, there is plenty of evidence that\ndoing so would hurt benchmark performance and require bespoke development for every different\narchitecture [87\u201389]. Is it possible to align a DNN\u2019s visual strategies with humans without hurting its\nperformance?\n\nSuch a general-purpose method for aligning human and DNN visual strategies should satisfy the\nfollowing criteria: (i) The method should work with any fully-differentiable network architecture.\n(ii) It should not present optimization issues that interfere with learning to solve a task, and the\ntask-accuracy of a model trained with the method should not be worse than a model trained without\nthe method. We created the neural harmonizer to satisfy these criteria.\nLet us consider a supervised categorization problem with an input space, X an output space Y \u2286 Rc\nand a predictor function f\u03b8 : X \u2192 Y parameterized by \u03b8, which maps an input vector x \u2208 X to an\noutput f\u03b8(x). We denote g : F \u00d7 X \u2192 X an explanation functional that, given a predictor f\u03b8 \u2208 F\nand an input, returns a feature importance map \u03d5 = g(f\u03b8, x). Here, we focus on DNN saliency\ng(f\u03b8, x) \u225c \u2207xf\u03b8(x) as our method for computing feature importance in DNNs, but the method can\nin principle work with any differentiable network explanation method.\n\nTo satisfy criterion (i), the neural harmonizer introduces a differentiable loss that will enforce\nalignment across feature importance map scales from any neural network. Let Pi(.) be the function\nmapping a feature importance map \u03d5 to it is representation in the N levels of a Gaussian pyramid,\nwith i \u2208 {1, ..., N }. The function Pi(\u03d5) is computed by downsampling Pi\u22121(\u03d5) using a Gaussian\n\n5\n\nHarmonized ViTHuman (ClickMe)MLP-MixerConvNextViTResNet50Robust ResNet50SnakeBearHareLeopardBallSimCLR\fFigure 3: The trade-off between DNN performance and alignment with human feature impor-\ntance from ClickMe [20]. Human feature alignment is the mean Spearman correlation between\nhuman and DNN feature importance maps, normalized by the average inter-rater alignment of hu-\nmans. The shaded region denotes the pareto frontier of the trade-offs between ImageNet accuracy\nand human feature alignment for unharmonized models. Harmonized models (VGG16, ResNet50,\nViT, and EfficientNetB0) are more accurate and aligned than versions of those models trained only\nfor categorization. Error bars are bootstrapped standard deviations over feature alignment. Arrows\nshow a shift in performance after training with the neural harmonizer. The feature alignment of an\naverage of ClickMe maps with held-out maps is denoted by \u2020.\n\nkernel, with P1(\u03d5) = \u03d5. We then seek to minimize (cid:80)N\nfeature importance maps between humans and DNNs at every scale of the pyramid.\n\n||Pi(g(f\u03b8, x)) \u2212 Pi(\u03d5)||2, which will align\n\ni\n\nTo satisfy criterion (ii), the neural harmonizer should work well with training routines designed\nfor large-scale computer vision challenges like ImageNet. This means that the neural harmonizer\nloss must avoid optimization issues at scale. To do this, we need a way of comparing feature\nimportance maps between humans and DNNs that is invariant to the norm of either map. We therefore\nstandardize feature importance maps from humans and DNNs before comparing them, and only\nmeasure alignment on the most important areas of the image for each observer. Formally, let z(.) be\na standardization function over feature importance maps that takes the mean and standard deviation\ncomputed for each map \u03d5 such that z(\u03d5) has 0 activation on average and unit standard deviation.\nTo focus alignment on important regions, let z(\u03d5)+ denote the positive part of the standardized\nexplanation z(\u03d5). Finally, we include a task loss, the familiar cross entropy objective, to yield the\ncomplete neural harmonization loss and train models that are at least as accurate as those trained\nwithout harmonization:\n\nLHarmonization =\u03bb1\n\nN\n(cid:88)\n\ni\n\n||(z \u25e6 Pi \u25e6 g(f\u03b8, x))+ \u2212 (z \u25e6 Pi(\u03d5))+||2\n\n+ LCCE(f\u03b8, x, y) + \u03bb2\n\n(cid:88)\n\n\u03b82\ni\n\ni\n\n(1)\n\n(2)\n\n6\n\nTop-1 ImageNet Accuracy (%)607080556575858040200CNNTransformerSelf-supervisedRobustCNN extra dataNeural harmonizer60100Human feature alignment (%)\u2020\fFigure 4: The trade-off between DNN performance and alignment with human feature impor-\ntance from Clicktionary [22]. Human feature alignment is the mean Spearman correlation between\nhuman and DNN feature importance maps, normalized by the average inter-rater alignment of humans.\nThe shaded region denotes the pareto frontier of the trade-offs between ImageNet accuracy and human\nfeature alignment for unharmonized models. Harmonized models (VGG16, ResNet50, MobileNetV1,\nand EfficientNetB0) are more accurate and aligned than versions of those models trained only for\ncategorization. Error bars are bootstrapped standard deviations over feature alignment. Arrows denote\na shift in performance after training with the neural harmonizer.\n\nTraining. We trained four different DNNs with the neural harmonizer: VGG16, ViT, ResNet50, and\nEfficientNetB0. These models were selected because they are popular convolutional and transformer\nnetworks with open-source architectures that are straightforward to train and also sit near the boundary\nof the trade-off between DNN performance and alignment with humans. Models were trained using\nthe neural harmonizer to optimize categorization performance on ImageNet and feature importance\nmap alignment with human data from ClickMe. We trained models on all images in the ImageNet\ntraining set, but because ClickMe only contains human feature importance maps for a portion of those\nimages, we computed the categorization loss but not the neural harmonizer loss for images without\nimportance maps. Models were trained using 8 cores V4 TPUs on the Google Cloud Platform, and\ntraining lasted approximately one day. Models were trained with an augmented ResNet training\nrecipe (built from https://github.com/tensorflow/tpu/). Models were optimized with SGD\nand momentum over batches of 512 images, a learning rate of 0.3, and label smoothing [90]. Images\nwere augmented with random left-right flips and mixup [91]. The learning rate was adjusted over\nthe course of training with a schedule that began with an initial warm-up period of 5 epochs and\nthen decaying according to a cosine function over 90 epochs, with decay at step 30, 50 and 80.\nWe validated that a ResNet50 and VGG16 trained with these hyperparameters and schedule using\nstandard cross-entropy (but not the neural harmonizer) matched published performance.\n\nThe neural harmonizer aligns human and DNN visual strategies. We found that harmonized\nmodels broke the trade-off between ImageNet accuracy and model alignment with ClickMe human\nfeature importance maps (Fig. 3). Harmonized models were significantly more aligned with feature\nimportance maps and also performed better on ImageNet. The changes in where harmonized models\nfind important features in images were dramatic: a harmonized ViT had feature importance maps that\nare far less reliant on context (Fig. 2) and approximately 150% more aligned with humans (Fig. 3;\n\n7\n\nTop-1 ImageNet Accuracy (%)60708055657585Human feature alignment (%)8040200CNNTransformerSelf-supervisedRobustCNN extra dataNeural harmonizer60100\fFigure 5: Comparing how humans and DNNs use visual features during object recognition.\n(a) Humans and DNNs categorized ImageNet validation images as animals or non-animals. The\nimages revealed only a portion of the most important visual features according to the Clicktionary\ngame [92]. (b) There was a trade-off between DNN top-1 accuracy on ImageNet and alignment with\nhuman visual decision making. The shaded region denotes the pareto frontier of the trade-off between\nImageNet accuracy and human feature alignment for unharmonized models. Arrows denote a shift in\nperformance after training with the neural harmonizer. Error bars are bootstrapped standard deviations\nover decision-making alignment. (c) A state-of-the-art DNN like the ViT learned a different strategy\nfor integrating visual features into decisions than humans or a harmonized ViT.\n\nViT goes from 28.7% to 72.6% alignment after harmonization). The same model also performed 4%\nbetter in top-1 accuracy without any changes to its architecture. Similar improvements were found for\n\n8\n\nTop-1 ImageNet Accuracy (%)60708055657585843657-log(mean squared error)AnimalNon-animal120100Important human features (%)Human features revealed (%)1Normalized performance (%)01008060402020406080100(a)(c)(b)HumanCNNTransformerSelf-supervisedRobustCNN extra dataNeural harmonizer\fthe harmonized VGG16 and ResNet50. While the EfficientNetB0 had only a minimal improvement\nin accuracy, it too exhibited a large boost in human feature alignment.\n\nClicktionary. To test if the trade-off between DNN ImageNet accuracy and alignment with humans\nis a general phenomenon we next turned to Clicktionary [22]. Indeed, we observed a similar trade-off\non this dataset as we found for ClickMe: alignment with human feature importance from Clicktionary\nhas worsened as DNN accuracy has improved on ImageNet (Fig. 4). As with ClickMe, harmonized\nDNNs shift the accuracy-alignment trade-off on this dataset.\n\n4.2 How do humans and DNNs integrate diagnostic object features into decisions?\n\nThe trade-off we discovered between DNN accuracy on ImageNet and alignment with human visual\nfeature importance suggests that the two use different visual strategies for object classification.\nHowever, there is potential for an even deeper problem. Even if two observers deem the same regions\nof an image as important for recognizing it, there is no guarantee that they use the selected features in\nthe same way to render their decisions. We posit that if two observers have aligned visual strategies,\nthe will agree on both where important features are in an image and how they use those features for\ndecisions.\n\nWe developed a psychophysics experiment to measure how different humans use features in ImageNet\nimages to recognize objects. Participants viewed versions of these images where only a proportion\nof the features that were deemed most important in the Clicktionary game were visible (Fig. 5a).\nParticipants had to accurately detect whether or not the image contained an animal within 550ms,\nwhich forced them to rely on feedforward processing as much as possible [33]. Each of the 200\nimages we used were shown to a single participant only once. We accumulated responses from all\nparticipants to construct decision curves that showed how accurately the average human converted\nany given proportion of image features into an object decision. We performed the same experiment\non DNNs as we did on humans, recording animal vs. non-animal decisions according to whether\nor not the most probable category in the model\u2019s 1000-category output was an animal. Because the\nexperiment was speeded, humans did not achieve perfect accuracy. Thus, we normalized performance\nfor humans and DNNs to compare the rate at which each integrated features into accurate decisions.\n\nWe discovered a similar trade-off between ImageNet accuracy and alignment with human visual\ndecision making in this experiment as we did in ClickMe and Clicktionary (Fig. 5b). Indeed, the\nmodel that was most aligned with human decision-making \u2013 the BagNet33 [69] \u2013 only achieved 63.0%\naccuracy on ImageNet. Surprisingly, harmonized models broke this trend, particularly the harmonized\nViT (Fig. 5b, top-right), despite no explicit constraints in that procedure which forced consistent\ndecision-making with humans. In contrast, an unharmonized ViT integrates visual information into\naccurate decisions less efficiently than humans or harmonized models (Fig. 5c).\n\n higher diameter and value norm.\n\nBased on the section(s), please answer the following question with yes, no, or n/a and provide a brief justification for your answer.\nQuestion: If the authors ran experiments, do the authors include the code, data, and instructions needed to reproduce the main experimental results (either in the supplemental material or as a URL)?"}, "3b": {"role": "user", "content": "The following is the experiments section of the paper you are reviewing:\n\n\n4.1 Where are diagnostic object features for humans and DNNs?\n\nTo systematically compare the visual strategies of object recognition for humans and DNNs on\nImageNet, we first turned to the ClickMe dataset of feature importance maps [20]. In order to derive\ncomparable feature importance maps for DNNs, we needed a method that could be efficiently and con-\nsistently applied to each of the 84 DNNs we tested without any idiosyncratic hyperparameters. This\nled us to choose a classic method for explainable artificial intelligence, image feature saliency [84].\nWe prepared human feature importance maps from ClickMe by taking the average importance map\nproduced by humans for every image that also appeared in ImageNet validation. We then used\nSpearman\u2019s rank-correlation to measure the similarity between human feature maps and DNN feature\nmaps for each image [50]. We also computed the inter-rater alignment of human feature importance\nmaps as the mean split-half correlation across 1000 random splits of the participant pool (\u03c1 = 0.66).\nWe then normalized each human-DNN correlation by this score [20].\n\nThere were dramatic qualitative differences between the features selected by humans and DNNs on\nImageNet. In general, humans selected less context and focused more on object parts: for animals,\nparts of their faces; for non-animals, parts that enable their usage, like the spade of a shovel (see\nFig. 2 and SI Fig. 5. The DNN that was most aligned with humans, the DenseNet121, was still only\n38% aligned with humans (Fig. 3).\n\nPlotting the relationship between DNNs\u2019 top-1 accuracy on ImageNet with their human alignment\nrevealed a striking trade-off: as the accuracy of DNNs has improved beyond DenseNet121, their\n\n4\n\n\fFigure 2: Human and DNNs rely on different features to recognize objects. In contrast, our neural\nharmonizer aligns DNN feature importance with humans. We smooth feature importance maps from\nhumans (ClickMe) and DNNs with a Gaussian kernel for visualization.\n\nalignment with humans has worsened (Fig. 3). For example, consider the ConvNext [1], which\nachieved the best top-1 accuracy in our experiments (85.8%), was only 22% aligned with humans \u2013\nequivalent to the alignment of the BagNet33 [69] (63% top-1 accuracy). As an additional control, we\ncomputed the similarity between the average ClickMe map, which exhibits a center bias [85, 86] (SI\nFig. 5), and each individual ClickMe map. This center-bias control was only outperformed by 42/84\nCNNs we tested (\u2020 in Fig. 3). Overall, we observe that human and DNN alignment has considerably\nworsened since the introduction of these two models.\n\nThe neural harmonizer. While scaling DNNs has immensely helped performance on popular\nbenchmark tasks, there are still fundamental differences in the architectures of DNNs and the\nhuman visual system [37] which could part of the reason to blame for poor alignment. While\nintroducing biological constraints into DNNs could help this problem, there is plenty of evidence that\ndoing so would hurt benchmark performance and require bespoke development for every different\narchitecture [87\u201389]. Is it possible to align a DNN\u2019s visual strategies with humans without hurting its\nperformance?\n\nSuch a general-purpose method for aligning human and DNN visual strategies should satisfy the\nfollowing criteria: (i) The method should work with any fully-differentiable network architecture.\n(ii) It should not present optimization issues that interfere with learning to solve a task, and the\ntask-accuracy of a model trained with the method should not be worse than a model trained without\nthe method. We created the neural harmonizer to satisfy these criteria.\nLet us consider a supervised categorization problem with an input space, X an output space Y \u2286 Rc\nand a predictor function f\u03b8 : X \u2192 Y parameterized by \u03b8, which maps an input vector x \u2208 X to an\noutput f\u03b8(x). We denote g : F \u00d7 X \u2192 X an explanation functional that, given a predictor f\u03b8 \u2208 F\nand an input, returns a feature importance map \u03d5 = g(f\u03b8, x). Here, we focus on DNN saliency\ng(f\u03b8, x) \u225c \u2207xf\u03b8(x) as our method for computing feature importance in DNNs, but the method can\nin principle work with any differentiable network explanation method.\n\nTo satisfy criterion (i), the neural harmonizer introduces a differentiable loss that will enforce\nalignment across feature importance map scales from any neural network. Let Pi(.) be the function\nmapping a feature importance map \u03d5 to it is representation in the N levels of a Gaussian pyramid,\nwith i \u2208 {1, ..., N }. The function Pi(\u03d5) is computed by downsampling Pi\u22121(\u03d5) using a Gaussian\n\n5\n\nHarmonized ViTHuman (ClickMe)MLP-MixerConvNextViTResNet50Robust ResNet50SnakeBearHareLeopardBallSimCLR\fFigure 3: The trade-off between DNN performance and alignment with human feature impor-\ntance from ClickMe [20]. Human feature alignment is the mean Spearman correlation between\nhuman and DNN feature importance maps, normalized by the average inter-rater alignment of hu-\nmans. The shaded region denotes the pareto frontier of the trade-offs between ImageNet accuracy\nand human feature alignment for unharmonized models. Harmonized models (VGG16, ResNet50,\nViT, and EfficientNetB0) are more accurate and aligned than versions of those models trained only\nfor categorization. Error bars are bootstrapped standard deviations over feature alignment. Arrows\nshow a shift in performance after training with the neural harmonizer. The feature alignment of an\naverage of ClickMe maps with held-out maps is denoted by \u2020.\n\nkernel, with P1(\u03d5) = \u03d5. We then seek to minimize (cid:80)N\nfeature importance maps between humans and DNNs at every scale of the pyramid.\n\n||Pi(g(f\u03b8, x)) \u2212 Pi(\u03d5)||2, which will align\n\ni\n\nTo satisfy criterion (ii), the neural harmonizer should work well with training routines designed\nfor large-scale computer vision challenges like ImageNet. This means that the neural harmonizer\nloss must avoid optimization issues at scale. To do this, we need a way of comparing feature\nimportance maps between humans and DNNs that is invariant to the norm of either map. We therefore\nstandardize feature importance maps from humans and DNNs before comparing them, and only\nmeasure alignment on the most important areas of the image for each observer. Formally, let z(.) be\na standardization function over feature importance maps that takes the mean and standard deviation\ncomputed for each map \u03d5 such that z(\u03d5) has 0 activation on average and unit standard deviation.\nTo focus alignment on important regions, let z(\u03d5)+ denote the positive part of the standardized\nexplanation z(\u03d5). Finally, we include a task loss, the familiar cross entropy objective, to yield the\ncomplete neural harmonization loss and train models that are at least as accurate as those trained\nwithout harmonization:\n\nLHarmonization =\u03bb1\n\nN\n(cid:88)\n\ni\n\n||(z \u25e6 Pi \u25e6 g(f\u03b8, x))+ \u2212 (z \u25e6 Pi(\u03d5))+||2\n\n+ LCCE(f\u03b8, x, y) + \u03bb2\n\n(cid:88)\n\n\u03b82\ni\n\ni\n\n(1)\n\n(2)\n\n6\n\nTop-1 ImageNet Accuracy (%)607080556575858040200CNNTransformerSelf-supervisedRobustCNN extra dataNeural harmonizer60100Human feature alignment (%)\u2020\fFigure 4: The trade-off between DNN performance and alignment with human feature impor-\ntance from Clicktionary [22]. Human feature alignment is the mean Spearman correlation between\nhuman and DNN feature importance maps, normalized by the average inter-rater alignment of humans.\nThe shaded region denotes the pareto frontier of the trade-offs between ImageNet accuracy and human\nfeature alignment for unharmonized models. Harmonized models (VGG16, ResNet50, MobileNetV1,\nand EfficientNetB0) are more accurate and aligned than versions of those models trained only for\ncategorization. Error bars are bootstrapped standard deviations over feature alignment. Arrows denote\na shift in performance after training with the neural harmonizer.\n\nTraining. We trained four different DNNs with the neural harmonizer: VGG16, ViT, ResNet50, and\nEfficientNetB0. These models were selected because they are popular convolutional and transformer\nnetworks with open-source architectures that are straightforward to train and also sit near the boundary\nof the trade-off between DNN performance and alignment with humans. Models were trained using\nthe neural harmonizer to optimize categorization performance on ImageNet and feature importance\nmap alignment with human data from ClickMe. We trained models on all images in the ImageNet\ntraining set, but because ClickMe only contains human feature importance maps for a portion of those\nimages, we computed the categorization loss but not the neural harmonizer loss for images without\nimportance maps. Models were trained using 8 cores V4 TPUs on the Google Cloud Platform, and\ntraining lasted approximately one day. Models were trained with an augmented ResNet training\nrecipe (built from https://github.com/tensorflow/tpu/). Models were optimized with SGD\nand momentum over batches of 512 images, a learning rate of 0.3, and label smoothing [90]. Images\nwere augmented with random left-right flips and mixup [91]. The learning rate was adjusted over\nthe course of training with a schedule that began with an initial warm-up period of 5 epochs and\nthen decaying according to a cosine function over 90 epochs, with decay at step 30, 50 and 80.\nWe validated that a ResNet50 and VGG16 trained with these hyperparameters and schedule using\nstandard cross-entropy (but not the neural harmonizer) matched published performance.\n\nThe neural harmonizer aligns human and DNN visual strategies. We found that harmonized\nmodels broke the trade-off between ImageNet accuracy and model alignment with ClickMe human\nfeature importance maps (Fig. 3). Harmonized models were significantly more aligned with feature\nimportance maps and also performed better on ImageNet. The changes in where harmonized models\nfind important features in images were dramatic: a harmonized ViT had feature importance maps that\nare far less reliant on context (Fig. 2) and approximately 150% more aligned with humans (Fig. 3;\n\n7\n\nTop-1 ImageNet Accuracy (%)60708055657585Human feature alignment (%)8040200CNNTransformerSelf-supervisedRobustCNN extra dataNeural harmonizer60100\fFigure 5: Comparing how humans and DNNs use visual features during object recognition.\n(a) Humans and DNNs categorized ImageNet validation images as animals or non-animals. The\nimages revealed only a portion of the most important visual features according to the Clicktionary\ngame [92]. (b) There was a trade-off between DNN top-1 accuracy on ImageNet and alignment with\nhuman visual decision making. The shaded region denotes the pareto frontier of the trade-off between\nImageNet accuracy and human feature alignment for unharmonized models. Arrows denote a shift in\nperformance after training with the neural harmonizer. Error bars are bootstrapped standard deviations\nover decision-making alignment. (c) A state-of-the-art DNN like the ViT learned a different strategy\nfor integrating visual features into decisions than humans or a harmonized ViT.\n\nViT goes from 28.7% to 72.6% alignment after harmonization). The same model also performed 4%\nbetter in top-1 accuracy without any changes to its architecture. Similar improvements were found for\n\n8\n\nTop-1 ImageNet Accuracy (%)60708055657585843657-log(mean squared error)AnimalNon-animal120100Important human features (%)Human features revealed (%)1Normalized performance (%)01008060402020406080100(a)(c)(b)HumanCNNTransformerSelf-supervisedRobustCNN extra dataNeural harmonizer\fthe harmonized VGG16 and ResNet50. While the EfficientNetB0 had only a minimal improvement\nin accuracy, it too exhibited a large boost in human feature alignment.\n\nClicktionary. To test if the trade-off between DNN ImageNet accuracy and alignment with humans\nis a general phenomenon we next turned to Clicktionary [22]. Indeed, we observed a similar trade-off\non this dataset as we found for ClickMe: alignment with human feature importance from Clicktionary\nhas worsened as DNN accuracy has improved on ImageNet (Fig. 4). As with ClickMe, harmonized\nDNNs shift the accuracy-alignment trade-off on this dataset.\n\n4.2 How do humans and DNNs integrate diagnostic object features into decisions?\n\nThe trade-off we discovered between DNN accuracy on ImageNet and alignment with human visual\nfeature importance suggests that the two use different visual strategies for object classification.\nHowever, there is potential for an even deeper problem. Even if two observers deem the same regions\nof an image as important for recognizing it, there is no guarantee that they use the selected features in\nthe same way to render their decisions. We posit that if two observers have aligned visual strategies,\nthe will agree on both where important features are in an image and how they use those features for\ndecisions.\n\nWe developed a psychophysics experiment to measure how different humans use features in ImageNet\nimages to recognize objects. Participants viewed versions of these images where only a proportion\nof the features that were deemed most important in the Clicktionary game were visible (Fig. 5a).\nParticipants had to accurately detect whether or not the image contained an animal within 550ms,\nwhich forced them to rely on feedforward processing as much as possible [33]. Each of the 200\nimages we used were shown to a single participant only once. We accumulated responses from all\nparticipants to construct decision curves that showed how accurately the average human converted\nany given proportion of image features into an object decision. We performed the same experiment\non DNNs as we did on humans, recording animal vs. non-animal decisions according to whether\nor not the most probable category in the model\u2019s 1000-category output was an animal. Because the\nexperiment was speeded, humans did not achieve perfect accuracy. Thus, we normalized performance\nfor humans and DNNs to compare the rate at which each integrated features into accurate decisions.\n\nWe discovered a similar trade-off between ImageNet accuracy and alignment with human visual\ndecision making in this experiment as we did in ClickMe and Clicktionary (Fig. 5b). Indeed, the\nmodel that was most aligned with human decision-making \u2013 the BagNet33 [69] \u2013 only achieved 63.0%\naccuracy on ImageNet. Surprisingly, harmonized models broke this trend, particularly the harmonized\nViT (Fig. 5b, top-right), despite no explicit constraints in that procedure which forced consistent\ndecision-making with humans. In contrast, an unharmonized ViT integrates visual information into\naccurate decisions less efficiently than humans or harmonized models (Fig. 5c).\n\n higher diameter and value norm.\n\nBased on the section(s), please answer the following question with yes, no, or n/a and provide a brief justification for your answer.\nQuestion: If the authors ran experiments, do the authors specify all the training details (e.g., data splits, hyperparameters, how they were chosen)?"}, "3c": {"role": "user", "content": "The following is the experiments section of the paper you are reviewing:\n\n\n4.1 Where are diagnostic object features for humans and DNNs?\n\nTo systematically compare the visual strategies of object recognition for humans and DNNs on\nImageNet, we first turned to the ClickMe dataset of feature importance maps [20]. In order to derive\ncomparable feature importance maps for DNNs, we needed a method that could be efficiently and con-\nsistently applied to each of the 84 DNNs we tested without any idiosyncratic hyperparameters. This\nled us to choose a classic method for explainable artificial intelligence, image feature saliency [84].\nWe prepared human feature importance maps from ClickMe by taking the average importance map\nproduced by humans for every image that also appeared in ImageNet validation. We then used\nSpearman\u2019s rank-correlation to measure the similarity between human feature maps and DNN feature\nmaps for each image [50]. We also computed the inter-rater alignment of human feature importance\nmaps as the mean split-half correlation across 1000 random splits of the participant pool (\u03c1 = 0.66).\nWe then normalized each human-DNN correlation by this score [20].\n\nThere were dramatic qualitative differences between the features selected by humans and DNNs on\nImageNet. In general, humans selected less context and focused more on object parts: for animals,\nparts of their faces; for non-animals, parts that enable their usage, like the spade of a shovel (see\nFig. 2 and SI Fig. 5. The DNN that was most aligned with humans, the DenseNet121, was still only\n38% aligned with humans (Fig. 3).\n\nPlotting the relationship between DNNs\u2019 top-1 accuracy on ImageNet with their human alignment\nrevealed a striking trade-off: as the accuracy of DNNs has improved beyond DenseNet121, their\n\n4\n\n\fFigure 2: Human and DNNs rely on different features to recognize objects. In contrast, our neural\nharmonizer aligns DNN feature importance with humans. We smooth feature importance maps from\nhumans (ClickMe) and DNNs with a Gaussian kernel for visualization.\n\nalignment with humans has worsened (Fig. 3). For example, consider the ConvNext [1], which\nachieved the best top-1 accuracy in our experiments (85.8%), was only 22% aligned with humans \u2013\nequivalent to the alignment of the BagNet33 [69] (63% top-1 accuracy). As an additional control, we\ncomputed the similarity between the average ClickMe map, which exhibits a center bias [85, 86] (SI\nFig. 5), and each individual ClickMe map. This center-bias control was only outperformed by 42/84\nCNNs we tested (\u2020 in Fig. 3). Overall, we observe that human and DNN alignment has considerably\nworsened since the introduction of these two models.\n\nThe neural harmonizer. While scaling DNNs has immensely helped performance on popular\nbenchmark tasks, there are still fundamental differences in the architectures of DNNs and the\nhuman visual system [37] which could part of the reason to blame for poor alignment. While\nintroducing biological constraints into DNNs could help this problem, there is plenty of evidence that\ndoing so would hurt benchmark performance and require bespoke development for every different\narchitecture [87\u201389]. Is it possible to align a DNN\u2019s visual strategies with humans without hurting its\nperformance?\n\nSuch a general-purpose method for aligning human and DNN visual strategies should satisfy the\nfollowing criteria: (i) The method should work with any fully-differentiable network architecture.\n(ii) It should not present optimization issues that interfere with learning to solve a task, and the\ntask-accuracy of a model trained with the method should not be worse than a model trained without\nthe method. We created the neural harmonizer to satisfy these criteria.\nLet us consider a supervised categorization problem with an input space, X an output space Y \u2286 Rc\nand a predictor function f\u03b8 : X \u2192 Y parameterized by \u03b8, which maps an input vector x \u2208 X to an\noutput f\u03b8(x). We denote g : F \u00d7 X \u2192 X an explanation functional that, given a predictor f\u03b8 \u2208 F\nand an input, returns a feature importance map \u03d5 = g(f\u03b8, x). Here, we focus on DNN saliency\ng(f\u03b8, x) \u225c \u2207xf\u03b8(x) as our method for computing feature importance in DNNs, but the method can\nin principle work with any differentiable network explanation method.\n\nTo satisfy criterion (i), the neural harmonizer introduces a differentiable loss that will enforce\nalignment across feature importance map scales from any neural network. Let Pi(.) be the function\nmapping a feature importance map \u03d5 to it is representation in the N levels of a Gaussian pyramid,\nwith i \u2208 {1, ..., N }. The function Pi(\u03d5) is computed by downsampling Pi\u22121(\u03d5) using a Gaussian\n\n5\n\nHarmonized ViTHuman (ClickMe)MLP-MixerConvNextViTResNet50Robust ResNet50SnakeBearHareLeopardBallSimCLR\fFigure 3: The trade-off between DNN performance and alignment with human feature impor-\ntance from ClickMe [20]. Human feature alignment is the mean Spearman correlation between\nhuman and DNN feature importance maps, normalized by the average inter-rater alignment of hu-\nmans. The shaded region denotes the pareto frontier of the trade-offs between ImageNet accuracy\nand human feature alignment for unharmonized models. Harmonized models (VGG16, ResNet50,\nViT, and EfficientNetB0) are more accurate and aligned than versions of those models trained only\nfor categorization. Error bars are bootstrapped standard deviations over feature alignment. Arrows\nshow a shift in performance after training with the neural harmonizer. The feature alignment of an\naverage of ClickMe maps with held-out maps is denoted by \u2020.\n\nkernel, with P1(\u03d5) = \u03d5. We then seek to minimize (cid:80)N\nfeature importance maps between humans and DNNs at every scale of the pyramid.\n\n||Pi(g(f\u03b8, x)) \u2212 Pi(\u03d5)||2, which will align\n\ni\n\nTo satisfy criterion (ii), the neural harmonizer should work well with training routines designed\nfor large-scale computer vision challenges like ImageNet. This means that the neural harmonizer\nloss must avoid optimization issues at scale. To do this, we need a way of comparing feature\nimportance maps between humans and DNNs that is invariant to the norm of either map. We therefore\nstandardize feature importance maps from humans and DNNs before comparing them, and only\nmeasure alignment on the most important areas of the image for each observer. Formally, let z(.) be\na standardization function over feature importance maps that takes the mean and standard deviation\ncomputed for each map \u03d5 such that z(\u03d5) has 0 activation on average and unit standard deviation.\nTo focus alignment on important regions, let z(\u03d5)+ denote the positive part of the standardized\nexplanation z(\u03d5). Finally, we include a task loss, the familiar cross entropy objective, to yield the\ncomplete neural harmonization loss and train models that are at least as accurate as those trained\nwithout harmonization:\n\nLHarmonization =\u03bb1\n\nN\n(cid:88)\n\ni\n\n||(z \u25e6 Pi \u25e6 g(f\u03b8, x))+ \u2212 (z \u25e6 Pi(\u03d5))+||2\n\n+ LCCE(f\u03b8, x, y) + \u03bb2\n\n(cid:88)\n\n\u03b82\ni\n\ni\n\n(1)\n\n(2)\n\n6\n\nTop-1 ImageNet Accuracy (%)607080556575858040200CNNTransformerSelf-supervisedRobustCNN extra dataNeural harmonizer60100Human feature alignment (%)\u2020\fFigure 4: The trade-off between DNN performance and alignment with human feature impor-\ntance from Clicktionary [22]. Human feature alignment is the mean Spearman correlation between\nhuman and DNN feature importance maps, normalized by the average inter-rater alignment of humans.\nThe shaded region denotes the pareto frontier of the trade-offs between ImageNet accuracy and human\nfeature alignment for unharmonized models. Harmonized models (VGG16, ResNet50, MobileNetV1,\nand EfficientNetB0) are more accurate and aligned than versions of those models trained only for\ncategorization. Error bars are bootstrapped standard deviations over feature alignment. Arrows denote\na shift in performance after training with the neural harmonizer.\n\nTraining. We trained four different DNNs with the neural harmonizer: VGG16, ViT, ResNet50, and\nEfficientNetB0. These models were selected because they are popular convolutional and transformer\nnetworks with open-source architectures that are straightforward to train and also sit near the boundary\nof the trade-off between DNN performance and alignment with humans. Models were trained using\nthe neural harmonizer to optimize categorization performance on ImageNet and feature importance\nmap alignment with human data from ClickMe. We trained models on all images in the ImageNet\ntraining set, but because ClickMe only contains human feature importance maps for a portion of those\nimages, we computed the categorization loss but not the neural harmonizer loss for images without\nimportance maps. Models were trained using 8 cores V4 TPUs on the Google Cloud Platform, and\ntraining lasted approximately one day. Models were trained with an augmented ResNet training\nrecipe (built from https://github.com/tensorflow/tpu/). Models were optimized with SGD\nand momentum over batches of 512 images, a learning rate of 0.3, and label smoothing [90]. Images\nwere augmented with random left-right flips and mixup [91]. The learning rate was adjusted over\nthe course of training with a schedule that began with an initial warm-up period of 5 epochs and\nthen decaying according to a cosine function over 90 epochs, with decay at step 30, 50 and 80.\nWe validated that a ResNet50 and VGG16 trained with these hyperparameters and schedule using\nstandard cross-entropy (but not the neural harmonizer) matched published performance.\n\nThe neural harmonizer aligns human and DNN visual strategies. We found that harmonized\nmodels broke the trade-off between ImageNet accuracy and model alignment with ClickMe human\nfeature importance maps (Fig. 3). Harmonized models were significantly more aligned with feature\nimportance maps and also performed better on ImageNet. The changes in where harmonized models\nfind important features in images were dramatic: a harmonized ViT had feature importance maps that\nare far less reliant on context (Fig. 2) and approximately 150% more aligned with humans (Fig. 3;\n\n7\n\nTop-1 ImageNet Accuracy (%)60708055657585Human feature alignment (%)8040200CNNTransformerSelf-supervisedRobustCNN extra dataNeural harmonizer60100\fFigure 5: Comparing how humans and DNNs use visual features during object recognition.\n(a) Humans and DNNs categorized ImageNet validation images as animals or non-animals. The\nimages revealed only a portion of the most important visual features according to the Clicktionary\ngame [92]. (b) There was a trade-off between DNN top-1 accuracy on ImageNet and alignment with\nhuman visual decision making. The shaded region denotes the pareto frontier of the trade-off between\nImageNet accuracy and human feature alignment for unharmonized models. Arrows denote a shift in\nperformance after training with the neural harmonizer. Error bars are bootstrapped standard deviations\nover decision-making alignment. (c) A state-of-the-art DNN like the ViT learned a different strategy\nfor integrating visual features into decisions than humans or a harmonized ViT.\n\nViT goes from 28.7% to 72.6% alignment after harmonization). The same model also performed 4%\nbetter in top-1 accuracy without any changes to its architecture. Similar improvements were found for\n\n8\n\nTop-1 ImageNet Accuracy (%)60708055657585843657-log(mean squared error)AnimalNon-animal120100Important human features (%)Human features revealed (%)1Normalized performance (%)01008060402020406080100(a)(c)(b)HumanCNNTransformerSelf-supervisedRobustCNN extra dataNeural harmonizer\fthe harmonized VGG16 and ResNet50. While the EfficientNetB0 had only a minimal improvement\nin accuracy, it too exhibited a large boost in human feature alignment.\n\nClicktionary. To test if the trade-off between DNN ImageNet accuracy and alignment with humans\nis a general phenomenon we next turned to Clicktionary [22]. Indeed, we observed a similar trade-off\non this dataset as we found for ClickMe: alignment with human feature importance from Clicktionary\nhas worsened as DNN accuracy has improved on ImageNet (Fig. 4). As with ClickMe, harmonized\nDNNs shift the accuracy-alignment trade-off on this dataset.\n\n4.2 How do humans and DNNs integrate diagnostic object features into decisions?\n\nThe trade-off we discovered between DNN accuracy on ImageNet and alignment with human visual\nfeature importance suggests that the two use different visual strategies for object classification.\nHowever, there is potential for an even deeper problem. Even if two observers deem the same regions\nof an image as important for recognizing it, there is no guarantee that they use the selected features in\nthe same way to render their decisions. We posit that if two observers have aligned visual strategies,\nthe will agree on both where important features are in an image and how they use those features for\ndecisions.\n\nWe developed a psychophysics experiment to measure how different humans use features in ImageNet\nimages to recognize objects. Participants viewed versions of these images where only a proportion\nof the features that were deemed most important in the Clicktionary game were visible (Fig. 5a).\nParticipants had to accurately detect whether or not the image contained an animal within 550ms,\nwhich forced them to rely on feedforward processing as much as possible [33]. Each of the 200\nimages we used were shown to a single participant only once. We accumulated responses from all\nparticipants to construct decision curves that showed how accurately the average human converted\nany given proportion of image features into an object decision. We performed the same experiment\non DNNs as we did on humans, recording animal vs. non-animal decisions according to whether\nor not the most probable category in the model\u2019s 1000-category output was an animal. Because the\nexperiment was speeded, humans did not achieve perfect accuracy. Thus, we normalized performance\nfor humans and DNNs to compare the rate at which each integrated features into accurate decisions.\n\nWe discovered a similar trade-off between ImageNet accuracy and alignment with human visual\ndecision making in this experiment as we did in ClickMe and Clicktionary (Fig. 5b). Indeed, the\nmodel that was most aligned with human decision-making \u2013 the BagNet33 [69] \u2013 only achieved 63.0%\naccuracy on ImageNet. Surprisingly, harmonized models broke this trend, particularly the harmonized\nViT (Fig. 5b, top-right), despite no explicit constraints in that procedure which forced consistent\ndecision-making with humans. In contrast, an unharmonized ViT integrates visual information into\naccurate decisions less efficiently than humans or harmonized models (Fig. 5c).\n\n higher diameter and value norm.\n\nThe following is the appendix section of the paper you are reviewing:\n\n\nThe psychophysics experiments of \u00a74.2 were implemented with the psiTurk framework [1] and\ncustom javascript functions. Each trial sequence was converted to a HTML5-compatible video for\nthe fastest reliable presentation time possible in a web browser. Videos were cached before each\ntrial to optimize reliability of experiment timing within the web browser. A photo-diode verified\nthe reliability of stimulus timing in our experiment was consistently accurate within \u223c 10ms across\ndifferent operating system, web browser, and display type configurations.\n\nParticipants: We recruited 199 participants from Amazon Mechanical Turk (mturk.com) for the\nexperiments. Participants were based in the United States, used either the Firefox or Chrome browser\non a non-mobile device, and had a minimal average approval rating of 95% on past Mechanical Turk\ntasks.\n\nStimuli: Experiment images were taken from the Clicktionary dataset [2]. Images were sampled\nfrom 5 target and 5 distractor categories: border collie, sorrel (horse), great white shark, bald eagle,\nand panther; trailer truck, sports car, speedboat, airliner, and school bus. Images were presented to\nhuman participants (and DNNs) either intact or with a perceptual phase scrambled mask that exposed\na proportion of their most important visual features, as described in the main text. Images were cast\nto greyscale to control for trivial color-based cues for classification and blend the scrambled mask\nbackground into the foreground. Responses to intact images were used to normalize the performance\nof each observer on masked images relative to their maximum performance on these images.\n\nImage masks were created for each image to reveal only a proportion of the most important visual\nfeatures. For each image, we created masks that revealed between 1% and 100% (at log-scale spaced\nintervals) of the object pixels in the corresponding image\u2019s Clicktionary feature importance map. We\ngenerated these masks in two steps. First, we computed a phase-scrambled version of the image [3, 4].\nNext, we used a novel \u201cstochastic flood-fill\u201d algorithm to reveal a contiguous region of the most\nimportant visual features in the image according to humans. Our flood-fill algorithm was seeded on\nthe pixel deemed most important by humans in the image, then grew outwards anisotropically and\nbiased towards pixels with higher feature importance scores (Figure 1). The revealed region was\nalways centered on the image. Each participant saw every category exemplar only once, with its\namount of image revelation randomly selected from all possible configurations.\n\nAfter providing online consent, participants were instructed to complete a rapid visual categorization\ntask in which they had to classify stimuli revealing a portion of the most diagnostic object features\n(Fig. 3). Each experimental trial began with a cross for participants to fixate for a variable time\n(1,100\u20131,600ms), then a stimulus for 400ms, then another cross and additional time for participants\nto render a decision. Participants were instructed to provide a decision after the first fixation cross,\n\n*These authors contributed equally.\n1Department of Cognitive, Linguistic, & Psychological Sciences, Brown University, Providence, RI\n2Artificial and Natural Intelligence Toulouse Institute (ANITI), Toulouse, France\n3Carney Institute for Brain Science, Brown University, Providence, RI\n\n36th Conference on Neural Information Processing Systems (NeurIPS 2022).\n\n\fFigure 1: Overview of the psychophysics paradigm. Participants performed a rapid animals vs.\nvehicles categorization paradigm (top). Stimuli were created using feature importance maps derived\nfrom humans or DNNs via a \u201cstochastic flood-fill\u201d algorithm that revealed image regions of different\nsizes centered on important features. Sample stimuli are shown (bottom) for different percentages of\nimage revelation. Note that 100% revelation corresponds to all non-zero pixels in a feature importance\nmap.\n\nbut that they only had 650ms to answer. If they were too slow to respond they were told to respond\nfaster and the trial was discarded.\n\n2 Harmonization loss\n\nThe neural harmonizer loss Fig. 2 uses several components crucial to its performance: a pyramidal\nrepresentation of decision explanation maps and normalizing those maps.\n\n2\n\nFigure 1TargetDistractorPercent image revelation1%25%63%100%FullProcessed featureimportance map++Please respondfaster!Pre-stimulus \ufb01xation(1,100 \u2013 1,600ms)Stimulus400msExtra response150ms\u2026if windowIs reachedExperiment trial \fFigure 2: Computing the neural harmonizer loss..\n\nFigure 3: Psychophysics experiment instructions.\n\nWhen computing the difference between model explanations for an image and the human feature\nimportance map for that image, we rely on a pyramid representation of each to compute these\ndifferences Fig. 2). This pyramid allows for a model to align its feature representations with humans\nat multiple scales and corrects for an important problem in datasets like ClickMe: the human data is\nan approximation and not precise at the pixel level. This lack of precision can present optimization\nissues, and computing a pyramid representation alleviates those issues because it allows a model to\nlearn to focus on regions that are important for humans without pixel-level precision.\n\nStandardization tackles a similar problem: because of the imprecision of human data, we choose\nto focus harmonization on only the most important areas selected by humans in ClickMe. By\nstandardizing then rectifying before comparing human and model explanations, we reduce noise in\nthe harmonization procedure.\n\n3\n\n+ReLUReLUdistanceGaussianPyramidClickMemap(    )StandardizationaStandardizationa\fFigure 4: Example ClickMe feature importance maps on ImageNet images.\n\n3 Additional Results\n\n3.1 ClickMe\n\nThe ClickMe game by [5] was used to identify category diagnostic features in ImageNet images.\nThese feature importance maps largely focus on object regions rather than context, and in contrast to\nsegmentation maps select features on the \u201cfront\u201d or \u201cface\u201d of objects (Fig. 4).\n\nAs discussed in the main text, we found a trade-off between DNN top-1 ImageNet accuracy and\nthe alignment of their feature importance maps with humans importance maps from ClickMe. This\ntrade-off persists across multiple scales of feature importance maps, including 4\u00d7 (Fig. 6) and 16\u00d7\n(Fig. 7) sub-sampled maps, meaning that simple smoothing is not sufficient to fix the trade-off.\n\n3.2 ViT attention\n\nWhile in the main text we investigate alignment between humans and models using gradient feature\nimportance visualizations, the attention maps in transformer models like the ViT provide another\navenue for investigation. To understand whether or not attention maps from ViT are more aligned\nwith humans than their gradient-based decision explanation maps, we computed attention rollouts\nfor harmonized and unharmonized ViTs [6]. We found that both versions of the ViT had similar\ncorrelations between their attention rollouts and human ClickMe maps: 0.38 for the harmonized ViT\nand 0.393 for the unharmonized model. This surprising result suggests that the harmonizer affects the\nprocess by which ViTs integrate visual information into their decisions rather than how they allocate\nattention. Through manipulating ViT decision making processes, the harmonizer can induce the large\nchanges in gradient-based visualizations and psychophysics that we describe in the main text.\n\n3.3 Correlations between measurements of human visual strategies\n\nOur results rely on three independent datasets measuring different features of human visual strategies:\nClickMe, Clicktionary, and the psychophysics experiments we introduce in this manuscript. The fact\nthat all three evoke similar trade-offs between top-1 accuracy and human alignment is a surprising\nresult that deserves further attention. We investigated these trade-offs by measuring the correlation be-\ntween human alignment on each dataset, with and without models trained with the neural harmonizer.\nWe found that correlations between datasets were lower across the board when neural harmonizer\nmodels were not included. The association between model alignments with Clicktionary versus\npsychophysics results were not significant (\u03c1 = 0.21, n.s.; Fig. 9), but the associations between\n\n4\n\nHuman feature preferenceLessMore\fFigure 5: Feature importance maps of humans, harmonized, and unharmonized models on\nImageNet.\n\nmodel alignments with ClickMe versus psychophysics (\u03c1 = 0.51, p < 0.001; Fig. 8) and ClickMe\nversus Clicktionary (\u03c1 = 0.77, p < 0.001; Fig. 10) were both significant. Each correlation improved\nwhen the neural harmonizer models were included in the calculation. This finding indicates that the\nneural harmonizer successfully aligned visual strategies between humans and DNNs, and was not\nmerely benefiting from either where humans versus DNNs considered important visual features to be\nor how humans versus DNNs incorporated those features into their decisions.\n\n5\n\nSnakeShovelVanBrown bearAssaultrifleImageClickMeResNet50SimCLRRobustResNet50ConvNextHarmonizedViTHareSoccer ballLeopardMLP-MixerViTHarmonized ViTHuman (ClickMe)MLP-MixerConvNextViTResNet50Robust ResNet50SnakeBearHareLeopardBallSimCLRRi\ufb02eVanShovel\fFigure 6: The neural harmonizer\u2019s effect is robust across image scales. Here, we show that the\ntrade-off between ImageNet accuracy and alignment with humans holds across downsizing by a\nfactor of 4. The Neural harmonizer once again yields the model with the best alignment with humans.\nGrey-shaded area captures the trade-off between accuracy and alignment in standard DNNs. Error\nbars are bootstrapped standard deviations over feature alignment.\n\n6\n\nTop-1 ImageNet Accuracy (%)607080556575858040200CNNTransformerSelf-supervisedRobustCNN extra dataNeural harmonizer60100Human feature alignment (%)\u2020CNNTransformerSelf-supervisedRobustCNN extra dataNeural harmonizer\fFigure 7: The neural harmonizer\u2019s effect is robust across image scales. Here, we show that the\ntrade-off between ImageNet accuracy and alignment with humans holds across downsizing by a factor\nof 16. The Neural harmonizer once again yields the model with the best alignment with humans.\nGrey-shaded area captures the trade-off between accuracy and alignment in standard DNNs. Error\nbars are bootstrapped standard deviations over feature alignment.\n\nFigure 8: The association between ClickMe alignment versus psychophysics alignment. These\nscores are significantly correlated, \u03c1 = 0.68, p < 0.001. Error bars are bootstrapped standard\ndeviations over feature alignment.\n\n7\n\nTop-1 ImageNet Accuracy (%)607080556575858040200CNNTransformerSelf-supervisedRobustCNN extra dataNeural harmonizer60100Human feature alignment (%)-log(mean squared error)843CNNTransformerSelf-supervisedRobustCNN extra dataNeural harmonizer657Human feature alignment (%)10204060705030\fFigure 9: The association between Clicktionary alignment versus psychophysics alignment.\nThese scores are significantly correlated, \u03c1 = 0.53, p < 0.001.\n\nFigure 10: The association between ClickMe alignment versus Clicktionary alignment. These\nscores are significantly correlated, \u03c1 = 0.85, p < 0.001. Error bars are bootstrapped standard\ndeviations over feature alignment.\n\n8\n\n843CNNTransformerSelf-supervisedRobustCNN extra dataNeural harmonizer657Human feature alignment (%)1020304050-log(mean squared error)Clicktionary feature alignment (%)10020CNNTransformerSelf-supervisedRobustCNN extra dataNeural harmonizer604080Clickme feature alignment (%)20408010060\fFigure 11: The mean of ClickMe feature importance maps exhibits a center bias, likely due\nto the positioning of objects in ImageNet images rather than a purely spatial bias of human\nparticipants (compare to individual maps shown in Fig. 4).\n\n9\n\n\f\n\nBased on the section(s), please answer the following question with yes, no, or n/a and provide a brief justification for your answer.\nQuestion: If the authors ran experiments, do the authors report error bars (e.g., with respect to the random seed after running experiments multiple times)?"}, "3d": {"role": "user", "content": "The following is the experiments section of the paper you are reviewing:\n\n\n4.1 Where are diagnostic object features for humans and DNNs?\n\nTo systematically compare the visual strategies of object recognition for humans and DNNs on\nImageNet, we first turned to the ClickMe dataset of feature importance maps [20]. In order to derive\ncomparable feature importance maps for DNNs, we needed a method that could be efficiently and con-\nsistently applied to each of the 84 DNNs we tested without any idiosyncratic hyperparameters. This\nled us to choose a classic method for explainable artificial intelligence, image feature saliency [84].\nWe prepared human feature importance maps from ClickMe by taking the average importance map\nproduced by humans for every image that also appeared in ImageNet validation. We then used\nSpearman\u2019s rank-correlation to measure the similarity between human feature maps and DNN feature\nmaps for each image [50]. We also computed the inter-rater alignment of human feature importance\nmaps as the mean split-half correlation across 1000 random splits of the participant pool (\u03c1 = 0.66).\nWe then normalized each human-DNN correlation by this score [20].\n\nThere were dramatic qualitative differences between the features selected by humans and DNNs on\nImageNet. In general, humans selected less context and focused more on object parts: for animals,\nparts of their faces; for non-animals, parts that enable their usage, like the spade of a shovel (see\nFig. 2 and SI Fig. 5. The DNN that was most aligned with humans, the DenseNet121, was still only\n38% aligned with humans (Fig. 3).\n\nPlotting the relationship between DNNs\u2019 top-1 accuracy on ImageNet with their human alignment\nrevealed a striking trade-off: as the accuracy of DNNs has improved beyond DenseNet121, their\n\n4\n\n\fFigure 2: Human and DNNs rely on different features to recognize objects. In contrast, our neural\nharmonizer aligns DNN feature importance with humans. We smooth feature importance maps from\nhumans (ClickMe) and DNNs with a Gaussian kernel for visualization.\n\nalignment with humans has worsened (Fig. 3). For example, consider the ConvNext [1], which\nachieved the best top-1 accuracy in our experiments (85.8%), was only 22% aligned with humans \u2013\nequivalent to the alignment of the BagNet33 [69] (63% top-1 accuracy). As an additional control, we\ncomputed the similarity between the average ClickMe map, which exhibits a center bias [85, 86] (SI\nFig. 5), and each individual ClickMe map. This center-bias control was only outperformed by 42/84\nCNNs we tested (\u2020 in Fig. 3). Overall, we observe that human and DNN alignment has considerably\nworsened since the introduction of these two models.\n\nThe neural harmonizer. While scaling DNNs has immensely helped performance on popular\nbenchmark tasks, there are still fundamental differences in the architectures of DNNs and the\nhuman visual system [37] which could part of the reason to blame for poor alignment. While\nintroducing biological constraints into DNNs could help this problem, there is plenty of evidence that\ndoing so would hurt benchmark performance and require bespoke development for every different\narchitecture [87\u201389]. Is it possible to align a DNN\u2019s visual strategies with humans without hurting its\nperformance?\n\nSuch a general-purpose method for aligning human and DNN visual strategies should satisfy the\nfollowing criteria: (i) The method should work with any fully-differentiable network architecture.\n(ii) It should not present optimization issues that interfere with learning to solve a task, and the\ntask-accuracy of a model trained with the method should not be worse than a model trained without\nthe method. We created the neural harmonizer to satisfy these criteria.\nLet us consider a supervised categorization problem with an input space, X an output space Y \u2286 Rc\nand a predictor function f\u03b8 : X \u2192 Y parameterized by \u03b8, which maps an input vector x \u2208 X to an\noutput f\u03b8(x). We denote g : F \u00d7 X \u2192 X an explanation functional that, given a predictor f\u03b8 \u2208 F\nand an input, returns a feature importance map \u03d5 = g(f\u03b8, x). Here, we focus on DNN saliency\ng(f\u03b8, x) \u225c \u2207xf\u03b8(x) as our method for computing feature importance in DNNs, but the method can\nin principle work with any differentiable network explanation method.\n\nTo satisfy criterion (i), the neural harmonizer introduces a differentiable loss that will enforce\nalignment across feature importance map scales from any neural network. Let Pi(.) be the function\nmapping a feature importance map \u03d5 to it is representation in the N levels of a Gaussian pyramid,\nwith i \u2208 {1, ..., N }. The function Pi(\u03d5) is computed by downsampling Pi\u22121(\u03d5) using a Gaussian\n\n5\n\nHarmonized ViTHuman (ClickMe)MLP-MixerConvNextViTResNet50Robust ResNet50SnakeBearHareLeopardBallSimCLR\fFigure 3: The trade-off between DNN performance and alignment with human feature impor-\ntance from ClickMe [20]. Human feature alignment is the mean Spearman correlation between\nhuman and DNN feature importance maps, normalized by the average inter-rater alignment of hu-\nmans. The shaded region denotes the pareto frontier of the trade-offs between ImageNet accuracy\nand human feature alignment for unharmonized models. Harmonized models (VGG16, ResNet50,\nViT, and EfficientNetB0) are more accurate and aligned than versions of those models trained only\nfor categorization. Error bars are bootstrapped standard deviations over feature alignment. Arrows\nshow a shift in performance after training with the neural harmonizer. The feature alignment of an\naverage of ClickMe maps with held-out maps is denoted by \u2020.\n\nkernel, with P1(\u03d5) = \u03d5. We then seek to minimize (cid:80)N\nfeature importance maps between humans and DNNs at every scale of the pyramid.\n\n||Pi(g(f\u03b8, x)) \u2212 Pi(\u03d5)||2, which will align\n\ni\n\nTo satisfy criterion (ii), the neural harmonizer should work well with training routines designed\nfor large-scale computer vision challenges like ImageNet. This means that the neural harmonizer\nloss must avoid optimization issues at scale. To do this, we need a way of comparing feature\nimportance maps between humans and DNNs that is invariant to the norm of either map. We therefore\nstandardize feature importance maps from humans and DNNs before comparing them, and only\nmeasure alignment on the most important areas of the image for each observer. Formally, let z(.) be\na standardization function over feature importance maps that takes the mean and standard deviation\ncomputed for each map \u03d5 such that z(\u03d5) has 0 activation on average and unit standard deviation.\nTo focus alignment on important regions, let z(\u03d5)+ denote the positive part of the standardized\nexplanation z(\u03d5). Finally, we include a task loss, the familiar cross entropy objective, to yield the\ncomplete neural harmonization loss and train models that are at least as accurate as those trained\nwithout harmonization:\n\nLHarmonization =\u03bb1\n\nN\n(cid:88)\n\ni\n\n||(z \u25e6 Pi \u25e6 g(f\u03b8, x))+ \u2212 (z \u25e6 Pi(\u03d5))+||2\n\n+ LCCE(f\u03b8, x, y) + \u03bb2\n\n(cid:88)\n\n\u03b82\ni\n\ni\n\n(1)\n\n(2)\n\n6\n\nTop-1 ImageNet Accuracy (%)607080556575858040200CNNTransformerSelf-supervisedRobustCNN extra dataNeural harmonizer60100Human feature alignment (%)\u2020\fFigure 4: The trade-off between DNN performance and alignment with human feature impor-\ntance from Clicktionary [22]. Human feature alignment is the mean Spearman correlation between\nhuman and DNN feature importance maps, normalized by the average inter-rater alignment of humans.\nThe shaded region denotes the pareto frontier of the trade-offs between ImageNet accuracy and human\nfeature alignment for unharmonized models. Harmonized models (VGG16, ResNet50, MobileNetV1,\nand EfficientNetB0) are more accurate and aligned than versions of those models trained only for\ncategorization. Error bars are bootstrapped standard deviations over feature alignment. Arrows denote\na shift in performance after training with the neural harmonizer.\n\nTraining. We trained four different DNNs with the neural harmonizer: VGG16, ViT, ResNet50, and\nEfficientNetB0. These models were selected because they are popular convolutional and transformer\nnetworks with open-source architectures that are straightforward to train and also sit near the boundary\nof the trade-off between DNN performance and alignment with humans. Models were trained using\nthe neural harmonizer to optimize categorization performance on ImageNet and feature importance\nmap alignment with human data from ClickMe. We trained models on all images in the ImageNet\ntraining set, but because ClickMe only contains human feature importance maps for a portion of those\nimages, we computed the categorization loss but not the neural harmonizer loss for images without\nimportance maps. Models were trained using 8 cores V4 TPUs on the Google Cloud Platform, and\ntraining lasted approximately one day. Models were trained with an augmented ResNet training\nrecipe (built from https://github.com/tensorflow/tpu/). Models were optimized with SGD\nand momentum over batches of 512 images, a learning rate of 0.3, and label smoothing [90]. Images\nwere augmented with random left-right flips and mixup [91]. The learning rate was adjusted over\nthe course of training with a schedule that began with an initial warm-up period of 5 epochs and\nthen decaying according to a cosine function over 90 epochs, with decay at step 30, 50 and 80.\nWe validated that a ResNet50 and VGG16 trained with these hyperparameters and schedule using\nstandard cross-entropy (but not the neural harmonizer) matched published performance.\n\nThe neural harmonizer aligns human and DNN visual strategies. We found that harmonized\nmodels broke the trade-off between ImageNet accuracy and model alignment with ClickMe human\nfeature importance maps (Fig. 3). Harmonized models were significantly more aligned with feature\nimportance maps and also performed better on ImageNet. The changes in where harmonized models\nfind important features in images were dramatic: a harmonized ViT had feature importance maps that\nare far less reliant on context (Fig. 2) and approximately 150% more aligned with humans (Fig. 3;\n\n7\n\nTop-1 ImageNet Accuracy (%)60708055657585Human feature alignment (%)8040200CNNTransformerSelf-supervisedRobustCNN extra dataNeural harmonizer60100\fFigure 5: Comparing how humans and DNNs use visual features during object recognition.\n(a) Humans and DNNs categorized ImageNet validation images as animals or non-animals. The\nimages revealed only a portion of the most important visual features according to the Clicktionary\ngame [92]. (b) There was a trade-off between DNN top-1 accuracy on ImageNet and alignment with\nhuman visual decision making. The shaded region denotes the pareto frontier of the trade-off between\nImageNet accuracy and human feature alignment for unharmonized models. Arrows denote a shift in\nperformance after training with the neural harmonizer. Error bars are bootstrapped standard deviations\nover decision-making alignment. (c) A state-of-the-art DNN like the ViT learned a different strategy\nfor integrating visual features into decisions than humans or a harmonized ViT.\n\nViT goes from 28.7% to 72.6% alignment after harmonization). The same model also performed 4%\nbetter in top-1 accuracy without any changes to its architecture. Similar improvements were found for\n\n8\n\nTop-1 ImageNet Accuracy (%)60708055657585843657-log(mean squared error)AnimalNon-animal120100Important human features (%)Human features revealed (%)1Normalized performance (%)01008060402020406080100(a)(c)(b)HumanCNNTransformerSelf-supervisedRobustCNN extra dataNeural harmonizer\fthe harmonized VGG16 and ResNet50. While the EfficientNetB0 had only a minimal improvement\nin accuracy, it too exhibited a large boost in human feature alignment.\n\nClicktionary. To test if the trade-off between DNN ImageNet accuracy and alignment with humans\nis a general phenomenon we next turned to Clicktionary [22]. Indeed, we observed a similar trade-off\non this dataset as we found for ClickMe: alignment with human feature importance from Clicktionary\nhas worsened as DNN accuracy has improved on ImageNet (Fig. 4). As with ClickMe, harmonized\nDNNs shift the accuracy-alignment trade-off on this dataset.\n\n4.2 How do humans and DNNs integrate diagnostic object features into decisions?\n\nThe trade-off we discovered between DNN accuracy on ImageNet and alignment with human visual\nfeature importance suggests that the two use different visual strategies for object classification.\nHowever, there is potential for an even deeper problem. Even if two observers deem the same regions\nof an image as important for recognizing it, there is no guarantee that they use the selected features in\nthe same way to render their decisions. We posit that if two observers have aligned visual strategies,\nthe will agree on both where important features are in an image and how they use those features for\ndecisions.\n\nWe developed a psychophysics experiment to measure how different humans use features in ImageNet\nimages to recognize objects. Participants viewed versions of these images where only a proportion\nof the features that were deemed most important in the Clicktionary game were visible (Fig. 5a).\nParticipants had to accurately detect whether or not the image contained an animal within 550ms,\nwhich forced them to rely on feedforward processing as much as possible [33]. Each of the 200\nimages we used were shown to a single participant only once. We accumulated responses from all\nparticipants to construct decision curves that showed how accurately the average human converted\nany given proportion of image features into an object decision. We performed the same experiment\non DNNs as we did on humans, recording animal vs. non-animal decisions according to whether\nor not the most probable category in the model\u2019s 1000-category output was an animal. Because the\nexperiment was speeded, humans did not achieve perfect accuracy. Thus, we normalized performance\nfor humans and DNNs to compare the rate at which each integrated features into accurate decisions.\n\nWe discovered a similar trade-off between ImageNet accuracy and alignment with human visual\ndecision making in this experiment as we did in ClickMe and Clicktionary (Fig. 5b). Indeed, the\nmodel that was most aligned with human decision-making \u2013 the BagNet33 [69] \u2013 only achieved 63.0%\naccuracy on ImageNet. Surprisingly, harmonized models broke this trend, particularly the harmonized\nViT (Fig. 5b, top-right), despite no explicit constraints in that procedure which forced consistent\ndecision-making with humans. In contrast, an unharmonized ViT integrates visual information into\naccurate decisions less efficiently than humans or harmonized models (Fig. 5c).\n\n higher diameter and value norm.\n\nBased on the section(s), please answer the following question with yes, no, or n/a and provide a brief justification for your answer.\nQuestion: If the authors ran experiments, do the authors include the total amount of compute and the type of resources used (e.g., type of GPUs, internal cluster, or cloud provider)?"}, "4a": {"role": "user", "content": "The following is the methods section of the paper you are reviewing:\n\n\nHuman feature importance datasets. We focused on the ImageNet dataset to compare the visual\nstrategies of humans and DNNs for object recognition at scale. We relied on the two significant\nefforts for gathering feature importance data from humans on ImageNet: the Clicktionary [22] and\nClickMe [20] games, which use slightly different methods to collect their data. Both games begin with\nthe same basic setup: two players work together to locate features in an object image that they believe\nare important for categorizing it. As one of the players selects important image regions, those regions\nare filled into a blank canvas for the other observer to see and categorize the image as quickly as\npossible. In Clicktionary [22], both players are humans, whereas in ClickMe [20], the player selecting\nfeatures is a human and the player recognizing images is a DNN (VGG16 [49]). For both games,\nfeature importance maps depicting the average object category diagnosticity of every pixel was\ncomputed as the probability of it being clicked by a participant. In total, Clicktionary [22] contained\nfeature importance maps for 200 images from the ImageNet validation set, whereas ClickMe [20]\ncontained feature importance maps for a non-overlapping set of 196,499 images from ImageNet\ntraining and validation sets. Thus, ClickMe has far more data than Clicktionary, but Clicktionary\ndata has more reliable human feature importance data than ClickMe. Our experiments measure\nthe alignment between human and DNN visual strategies using ClickMe and Clicktionary feature\nimportance maps captured on the ImageNet validation set. As we describe in \u00a74, ClickMe feature\nimportance maps from the ImageNet training set are used to implement our neural harmonizer.\n\n3\n\n\fPsychophysics participants and dataset. We complemented the feature importance maps from\nClicktionary and ClickMe with psychophysics experiments on rapid visual categorization. We\nrecruited 199 participants from Amazon Mechanical Turk (mturk.com) to complete the experiments.\nParticipants viewed a psychophysics dataset consisting of the 100 animal and 100 non-animal images\nin the Clicktionary game taken from the ImageNet validation set [22]. We used the feature importance\nmaps for each image as masks for the object images, allowing us to control the proportion of important\nfeatures observers were shown when asked to recognize objects (Fig. 5a). We generated versions of\neach image that reveal anywhere between 1% to 100% (at log-scale spaced intervals) of the important\nobject pixels against a phase scrambled noise background (see Appendix \u00a71 for details on mask\ngeneration). The total number of revealed pixels was equal for every image at a given level of image\nmasking, and the revealed pixels were centered against the noise background. Each participant saw\nonly one masked version of each object image.\n\nPsychophysics experiment. Participants were instructed to categorize images in the psychophysics\ndataset as animals or non-animals as quickly and accurately as possible. Each experimental trial\nconsisted of the following sequence of events overlaid onto a white background (SI Fig. 1): (i)\na fixation cross displayed for a variable time (1,100\u20131,600ms); (ii) an image for 400ms; (iii) an\nadditional 150ms of response time. In other words, the experiment forced participants to perform\nrapid object categorization. They were given a total of 550ms to view an image and press a button\nto indicate its category (feedback was provided on trials in which responses were not provided\nwithin this time limit). Images were sized at 256 x 256 pixel resolution, which is equivalent to a\nstimulus size approximately between 5 \u2013 11 degrees of visual angle across a likely range of possible\ndisplay and seating setups we expect participants used for the experiment. Similar paradigms and\ntiming parameters have been shown to capture pre-attentive visual system processing [31, 50\u201352].\nParticipants provided informed consent electronically and were compensated $3.00 for their time (\u223c\n10\u201315 min; approximately $15.00/hr).\n\nModels. We compared humans with 84 different DNNs representing the variety of approaches\nused in the field today: 50 CNNs trained on ImageNet [1, 49, 53\u201364, 64\u201373], 6 CNNs trained on\nother datasets in addition to ImageNet (which we refer to as \u201cCNN extra data\u201d) [1, 66, 74], 10 vision\ntransformers [75\u201379], 6 CNNs trained with self-supervision [80, 81], and 13 models trained for\nrobustness to noise or adversarial examples [82, 83]. We used pretrained weights for each of these\nmodels supplied by their authors, with a variety of licenses (detailed in SI \u00a72), implemented in\nTensorflow 2.0, Keras, or PyTorch.\n\n\n\nBased on the section(s), please answer the following question with yes, no, or n/a and provide a brief justification for your answer.\nQuestion: If the authors use existing assets (e.g., code, data, models), do the authors cite the creators?"}, "4b": {"role": "user", "content": "The following is the methods section of the paper you are reviewing:\n\n\nHuman feature importance datasets. We focused on the ImageNet dataset to compare the visual\nstrategies of humans and DNNs for object recognition at scale. We relied on the two significant\nefforts for gathering feature importance data from humans on ImageNet: the Clicktionary [22] and\nClickMe [20] games, which use slightly different methods to collect their data. Both games begin with\nthe same basic setup: two players work together to locate features in an object image that they believe\nare important for categorizing it. As one of the players selects important image regions, those regions\nare filled into a blank canvas for the other observer to see and categorize the image as quickly as\npossible. In Clicktionary [22], both players are humans, whereas in ClickMe [20], the player selecting\nfeatures is a human and the player recognizing images is a DNN (VGG16 [49]). For both games,\nfeature importance maps depicting the average object category diagnosticity of every pixel was\ncomputed as the probability of it being clicked by a participant. In total, Clicktionary [22] contained\nfeature importance maps for 200 images from the ImageNet validation set, whereas ClickMe [20]\ncontained feature importance maps for a non-overlapping set of 196,499 images from ImageNet\ntraining and validation sets. Thus, ClickMe has far more data than Clicktionary, but Clicktionary\ndata has more reliable human feature importance data than ClickMe. Our experiments measure\nthe alignment between human and DNN visual strategies using ClickMe and Clicktionary feature\nimportance maps captured on the ImageNet validation set. As we describe in \u00a74, ClickMe feature\nimportance maps from the ImageNet training set are used to implement our neural harmonizer.\n\n3\n\n\fPsychophysics participants and dataset. We complemented the feature importance maps from\nClicktionary and ClickMe with psychophysics experiments on rapid visual categorization. We\nrecruited 199 participants from Amazon Mechanical Turk (mturk.com) to complete the experiments.\nParticipants viewed a psychophysics dataset consisting of the 100 animal and 100 non-animal images\nin the Clicktionary game taken from the ImageNet validation set [22]. We used the feature importance\nmaps for each image as masks for the object images, allowing us to control the proportion of important\nfeatures observers were shown when asked to recognize objects (Fig. 5a). We generated versions of\neach image that reveal anywhere between 1% to 100% (at log-scale spaced intervals) of the important\nobject pixels against a phase scrambled noise background (see Appendix \u00a71 for details on mask\ngeneration). The total number of revealed pixels was equal for every image at a given level of image\nmasking, and the revealed pixels were centered against the noise background. Each participant saw\nonly one masked version of each object image.\n\nPsychophysics experiment. Participants were instructed to categorize images in the psychophysics\ndataset as animals or non-animals as quickly and accurately as possible. Each experimental trial\nconsisted of the following sequence of events overlaid onto a white background (SI Fig. 1): (i)\na fixation cross displayed for a variable time (1,100\u20131,600ms); (ii) an image for 400ms; (iii) an\nadditional 150ms of response time. In other words, the experiment forced participants to perform\nrapid object categorization. They were given a total of 550ms to view an image and press a button\nto indicate its category (feedback was provided on trials in which responses were not provided\nwithin this time limit). Images were sized at 256 x 256 pixel resolution, which is equivalent to a\nstimulus size approximately between 5 \u2013 11 degrees of visual angle across a likely range of possible\ndisplay and seating setups we expect participants used for the experiment. Similar paradigms and\ntiming parameters have been shown to capture pre-attentive visual system processing [31, 50\u201352].\nParticipants provided informed consent electronically and were compensated $3.00 for their time (\u223c\n10\u201315 min; approximately $15.00/hr).\n\nModels. We compared humans with 84 different DNNs representing the variety of approaches\nused in the field today: 50 CNNs trained on ImageNet [1, 49, 53\u201364, 64\u201373], 6 CNNs trained on\nother datasets in addition to ImageNet (which we refer to as \u201cCNN extra data\u201d) [1, 66, 74], 10 vision\ntransformers [75\u201379], 6 CNNs trained with self-supervision [80, 81], and 13 models trained for\nrobustness to noise or adversarial examples [82, 83]. We used pretrained weights for each of these\nmodels supplied by their authors, with a variety of licenses (detailed in SI \u00a72), implemented in\nTensorflow 2.0, Keras, or PyTorch.\n\n\n\nBased on the section(s), please answer the following question with yes, no, or n/a and provide a brief justification for your answer.\nQuestion: If the authors use existing assets (e.g., code, data, models) or curate/release new assets, do the authors mention the license of the assets?"}, "4c": {"role": "user", "content": "The following is the introduction section of the paper you are reviewing:\n\n\nRich Sutton stated [4] that the bitter lesson \u201cfrom 70 years of AI research is that general methods\nthat leverage computation are ultimately the most effective, and by a large margin.\u201d Deep learning\nhas been the standard approach to object categorization problems ever since the paradigm shifting\nsuccess of AlexNet [5] on the ImageNet [6] benchmark a decade ago. As deep neural network (DNN)\nperformance has continued to improve in the intervening years, Sutton\u2019s lesson has become more\nfitting than ever, with recent networks rivaling and likely outperforming humans on the benchmark [7]\nthrough brute-force computational scale: increasing the number of network parameters and number of\nimages used for training orders-of-magnitude beyond AlexNet [1\u20133]. While the successes of so-called\n\u201cscaling laws\u201d are undeniable, this singular focus on performance in the field has side-stepped an\nequally important question that will govern the utility of object recognition models for the brain\nsciences and industry applications alike: are the visual strategies learned by DNNs aligned with those\nused by humans?\n\nThe visual strategies that mediate object recognition in humans can be decomposed into two related\nbut distinct processes: identifying where the important features for object recognition are in a scene,\nand determining how to integrate the selected features into a categorical decision [8, 9]. It has been\n\n*These authors contributed equally.\n1Department of Cognitive, Linguistic, & Psychological Sciences, Brown University, Providence, RI\n2Artificial and Natural Intelligence Toulouse Institute (ANITI), Toulouse, France\n3Carney Institute for Brain Science, Brown University, Providence, RI\n\n36th Conference on Neural Information Processing Systems (NeurIPS 2022).\n\n\fknown for nearly a century [10\u201313] that different humans attend to similar locations when asked\nto find and recognize objects. After selecting these important features, human observers are also\nconsistent in how they use those features to categorize objects \u2013 the inclusion of a few pixels in an\nimage can be the difference between recognizing an object or not [9, 14].\n\nHas the past decade of DNN development produced any models that are aligned with these human\nvisual strategies for object recognition? Such a model could transform cognitive science by support-\ning a better mechanistic understanding of how vision works. More human-like models of object\nrecognition would also resolve the problems with predictablity and interpretablity of DNNs [15\u201318],\nand control their alarming tendency to rely on \u201cshortcuts\u201d and dataset biases to perform well on\ntasks [19]. In this work, we perform the first large-scale and systematic comparison of the visual\nstrategies of DNNs and humans for object recognition on ImageNet.\n\nContributions.\nIn order to compare human and DNN visual strategies, we first turn to the human\nfeature importance maps collected by Linsley et al. [20,21]. Their datasets, ClickMe and Clicktionary,\ncontain maps of nearly 200,000 unique images in ImageNet that highlight the visual features humans\nbelieve are important for recognizing them. These datasets amount to a reverse inference on where\nimportant visual features are in ImageNet images (Fig. 1). We complement these datasets with\nnew psychophysics experiments that directly test how important visual features are used for object\nrecognition (Fig. 1). As DNN performance has increased on ImageNet, their alignment with\nhuman visual strategies captured in these datasets has worsened. This trade-off is found over 84\ndifferent DNNs representing all popular model classes \u2013 from those trained for adversarial robustness\nto those pushing the scaling laws in network capacity and training data. To summarize our findings:\n\n\u2022 The trade-off between DNN object recognition accuracy and alignment with human visual strategies\nreplicates across three unique datasets: ClickMe [20], Clicktionary [21], and our psychophysics\nexperiments.\n\n\u2022 We shift this trade-off with our neural harmonizer, a novel drop-in module for co-training any DNN\nto align with human visual strategies while also achieving high task accuracy. Harmonized DNNs\nlearn visual strategies that are significantly more aligned with humans than any other DNN we\ntested.\n\n\u2022 We release our data and code at https://serre-lab.github.io/Harmonization/ to help the\n\nfield tackle the growing misalignment between DNNs and humans.\n\n2 Related work\n\n\n\nThe following is the methods section of the paper you are reviewing:\n\n\nHuman feature importance datasets. We focused on the ImageNet dataset to compare the visual\nstrategies of humans and DNNs for object recognition at scale. We relied on the two significant\nefforts for gathering feature importance data from humans on ImageNet: the Clicktionary [22] and\nClickMe [20] games, which use slightly different methods to collect their data. Both games begin with\nthe same basic setup: two players work together to locate features in an object image that they believe\nare important for categorizing it. As one of the players selects important image regions, those regions\nare filled into a blank canvas for the other observer to see and categorize the image as quickly as\npossible. In Clicktionary [22], both players are humans, whereas in ClickMe [20], the player selecting\nfeatures is a human and the player recognizing images is a DNN (VGG16 [49]). For both games,\nfeature importance maps depicting the average object category diagnosticity of every pixel was\ncomputed as the probability of it being clicked by a participant. In total, Clicktionary [22] contained\nfeature importance maps for 200 images from the ImageNet validation set, whereas ClickMe [20]\ncontained feature importance maps for a non-overlapping set of 196,499 images from ImageNet\ntraining and validation sets. Thus, ClickMe has far more data than Clicktionary, but Clicktionary\ndata has more reliable human feature importance data than ClickMe. Our experiments measure\nthe alignment between human and DNN visual strategies using ClickMe and Clicktionary feature\nimportance maps captured on the ImageNet validation set. As we describe in \u00a74, ClickMe feature\nimportance maps from the ImageNet training set are used to implement our neural harmonizer.\n\n3\n\n\fPsychophysics participants and dataset. We complemented the feature importance maps from\nClicktionary and ClickMe with psychophysics experiments on rapid visual categorization. We\nrecruited 199 participants from Amazon Mechanical Turk (mturk.com) to complete the experiments.\nParticipants viewed a psychophysics dataset consisting of the 100 animal and 100 non-animal images\nin the Clicktionary game taken from the ImageNet validation set [22]. We used the feature importance\nmaps for each image as masks for the object images, allowing us to control the proportion of important\nfeatures observers were shown when asked to recognize objects (Fig. 5a). We generated versions of\neach image that reveal anywhere between 1% to 100% (at log-scale spaced intervals) of the important\nobject pixels against a phase scrambled noise background (see Appendix \u00a71 for details on mask\ngeneration). The total number of revealed pixels was equal for every image at a given level of image\nmasking, and the revealed pixels were centered against the noise background. Each participant saw\nonly one masked version of each object image.\n\nPsychophysics experiment. Participants were instructed to categorize images in the psychophysics\ndataset as animals or non-animals as quickly and accurately as possible. Each experimental trial\nconsisted of the following sequence of events overlaid onto a white background (SI Fig. 1): (i)\na fixation cross displayed for a variable time (1,100\u20131,600ms); (ii) an image for 400ms; (iii) an\nadditional 150ms of response time. In other words, the experiment forced participants to perform\nrapid object categorization. They were given a total of 550ms to view an image and press a button\nto indicate its category (feedback was provided on trials in which responses were not provided\nwithin this time limit). Images were sized at 256 x 256 pixel resolution, which is equivalent to a\nstimulus size approximately between 5 \u2013 11 degrees of visual angle across a likely range of possible\ndisplay and seating setups we expect participants used for the experiment. Similar paradigms and\ntiming parameters have been shown to capture pre-attentive visual system processing [31, 50\u201352].\nParticipants provided informed consent electronically and were compensated $3.00 for their time (\u223c\n10\u201315 min; approximately $15.00/hr).\n\nModels. We compared humans with 84 different DNNs representing the variety of approaches\nused in the field today: 50 CNNs trained on ImageNet [1, 49, 53\u201364, 64\u201373], 6 CNNs trained on\nother datasets in addition to ImageNet (which we refer to as \u201cCNN extra data\u201d) [1, 66, 74], 10 vision\ntransformers [75\u201379], 6 CNNs trained with self-supervision [80, 81], and 13 models trained for\nrobustness to noise or adversarial examples [82, 83]. We used pretrained weights for each of these\nmodels supplied by their authors, with a variety of licenses (detailed in SI \u00a72), implemented in\nTensorflow 2.0, Keras, or PyTorch.\n\n\n\nBased on the section(s), please answer the following question with yes, no, or n/a and provide a brief justification for your answer.\nQuestion: If the authors curate/release new assets (e.g., code, data, models), do the authors include any new assets either in the supplemental material or as a URL?"}, "4d": {"role": "user", "content": "The following is the methods section of the paper you are reviewing:\n\n\nHuman feature importance datasets. We focused on the ImageNet dataset to compare the visual\nstrategies of humans and DNNs for object recognition at scale. We relied on the two significant\nefforts for gathering feature importance data from humans on ImageNet: the Clicktionary [22] and\nClickMe [20] games, which use slightly different methods to collect their data. Both games begin with\nthe same basic setup: two players work together to locate features in an object image that they believe\nare important for categorizing it. As one of the players selects important image regions, those regions\nare filled into a blank canvas for the other observer to see and categorize the image as quickly as\npossible. In Clicktionary [22], both players are humans, whereas in ClickMe [20], the player selecting\nfeatures is a human and the player recognizing images is a DNN (VGG16 [49]). For both games,\nfeature importance maps depicting the average object category diagnosticity of every pixel was\ncomputed as the probability of it being clicked by a participant. In total, Clicktionary [22] contained\nfeature importance maps for 200 images from the ImageNet validation set, whereas ClickMe [20]\ncontained feature importance maps for a non-overlapping set of 196,499 images from ImageNet\ntraining and validation sets. Thus, ClickMe has far more data than Clicktionary, but Clicktionary\ndata has more reliable human feature importance data than ClickMe. Our experiments measure\nthe alignment between human and DNN visual strategies using ClickMe and Clicktionary feature\nimportance maps captured on the ImageNet validation set. As we describe in \u00a74, ClickMe feature\nimportance maps from the ImageNet training set are used to implement our neural harmonizer.\n\n3\n\n\fPsychophysics participants and dataset. We complemented the feature importance maps from\nClicktionary and ClickMe with psychophysics experiments on rapid visual categorization. We\nrecruited 199 participants from Amazon Mechanical Turk (mturk.com) to complete the experiments.\nParticipants viewed a psychophysics dataset consisting of the 100 animal and 100 non-animal images\nin the Clicktionary game taken from the ImageNet validation set [22]. We used the feature importance\nmaps for each image as masks for the object images, allowing us to control the proportion of important\nfeatures observers were shown when asked to recognize objects (Fig. 5a). We generated versions of\neach image that reveal anywhere between 1% to 100% (at log-scale spaced intervals) of the important\nobject pixels against a phase scrambled noise background (see Appendix \u00a71 for details on mask\ngeneration). The total number of revealed pixels was equal for every image at a given level of image\nmasking, and the revealed pixels were centered against the noise background. Each participant saw\nonly one masked version of each object image.\n\nPsychophysics experiment. Participants were instructed to categorize images in the psychophysics\ndataset as animals or non-animals as quickly and accurately as possible. Each experimental trial\nconsisted of the following sequence of events overlaid onto a white background (SI Fig. 1): (i)\na fixation cross displayed for a variable time (1,100\u20131,600ms); (ii) an image for 400ms; (iii) an\nadditional 150ms of response time. In other words, the experiment forced participants to perform\nrapid object categorization. They were given a total of 550ms to view an image and press a button\nto indicate its category (feedback was provided on trials in which responses were not provided\nwithin this time limit). Images were sized at 256 x 256 pixel resolution, which is equivalent to a\nstimulus size approximately between 5 \u2013 11 degrees of visual angle across a likely range of possible\ndisplay and seating setups we expect participants used for the experiment. Similar paradigms and\ntiming parameters have been shown to capture pre-attentive visual system processing [31, 50\u201352].\nParticipants provided informed consent electronically and were compensated $3.00 for their time (\u223c\n10\u201315 min; approximately $15.00/hr).\n\nModels. We compared humans with 84 different DNNs representing the variety of approaches\nused in the field today: 50 CNNs trained on ImageNet [1, 49, 53\u201364, 64\u201373], 6 CNNs trained on\nother datasets in addition to ImageNet (which we refer to as \u201cCNN extra data\u201d) [1, 66, 74], 10 vision\ntransformers [75\u201379], 6 CNNs trained with self-supervision [80, 81], and 13 models trained for\nrobustness to noise or adversarial examples [82, 83]. We used pretrained weights for each of these\nmodels supplied by their authors, with a variety of licenses (detailed in SI \u00a72), implemented in\nTensorflow 2.0, Keras, or PyTorch.\n\n\n\nBased on the section(s), please answer the following question with yes, no, or n/a and provide a brief justification for your answer.\nQuestion: If the authors curate/release new assets (e.g., code, data, models), do the authors discuss whether and how consent was obtained from people whose data they are using/curating?"}, "4e": {"role": "user", "content": "The following is the methods section of the paper you are reviewing:\n\n\nHuman feature importance datasets. We focused on the ImageNet dataset to compare the visual\nstrategies of humans and DNNs for object recognition at scale. We relied on the two significant\nefforts for gathering feature importance data from humans on ImageNet: the Clicktionary [22] and\nClickMe [20] games, which use slightly different methods to collect their data. Both games begin with\nthe same basic setup: two players work together to locate features in an object image that they believe\nare important for categorizing it. As one of the players selects important image regions, those regions\nare filled into a blank canvas for the other observer to see and categorize the image as quickly as\npossible. In Clicktionary [22], both players are humans, whereas in ClickMe [20], the player selecting\nfeatures is a human and the player recognizing images is a DNN (VGG16 [49]). For both games,\nfeature importance maps depicting the average object category diagnosticity of every pixel was\ncomputed as the probability of it being clicked by a participant. In total, Clicktionary [22] contained\nfeature importance maps for 200 images from the ImageNet validation set, whereas ClickMe [20]\ncontained feature importance maps for a non-overlapping set of 196,499 images from ImageNet\ntraining and validation sets. Thus, ClickMe has far more data than Clicktionary, but Clicktionary\ndata has more reliable human feature importance data than ClickMe. Our experiments measure\nthe alignment between human and DNN visual strategies using ClickMe and Clicktionary feature\nimportance maps captured on the ImageNet validation set. As we describe in \u00a74, ClickMe feature\nimportance maps from the ImageNet training set are used to implement our neural harmonizer.\n\n3\n\n\fPsychophysics participants and dataset. We complemented the feature importance maps from\nClicktionary and ClickMe with psychophysics experiments on rapid visual categorization. We\nrecruited 199 participants from Amazon Mechanical Turk (mturk.com) to complete the experiments.\nParticipants viewed a psychophysics dataset consisting of the 100 animal and 100 non-animal images\nin the Clicktionary game taken from the ImageNet validation set [22]. We used the feature importance\nmaps for each image as masks for the object images, allowing us to control the proportion of important\nfeatures observers were shown when asked to recognize objects (Fig. 5a). We generated versions of\neach image that reveal anywhere between 1% to 100% (at log-scale spaced intervals) of the important\nobject pixels against a phase scrambled noise background (see Appendix \u00a71 for details on mask\ngeneration). The total number of revealed pixels was equal for every image at a given level of image\nmasking, and the revealed pixels were centered against the noise background. Each participant saw\nonly one masked version of each object image.\n\nPsychophysics experiment. Participants were instructed to categorize images in the psychophysics\ndataset as animals or non-animals as quickly and accurately as possible. Each experimental trial\nconsisted of the following sequence of events overlaid onto a white background (SI Fig. 1): (i)\na fixation cross displayed for a variable time (1,100\u20131,600ms); (ii) an image for 400ms; (iii) an\nadditional 150ms of response time. In other words, the experiment forced participants to perform\nrapid object categorization. They were given a total of 550ms to view an image and press a button\nto indicate its category (feedback was provided on trials in which responses were not provided\nwithin this time limit). Images were sized at 256 x 256 pixel resolution, which is equivalent to a\nstimulus size approximately between 5 \u2013 11 degrees of visual angle across a likely range of possible\ndisplay and seating setups we expect participants used for the experiment. Similar paradigms and\ntiming parameters have been shown to capture pre-attentive visual system processing [31, 50\u201352].\nParticipants provided informed consent electronically and were compensated $3.00 for their time (\u223c\n10\u201315 min; approximately $15.00/hr).\n\nModels. We compared humans with 84 different DNNs representing the variety of approaches\nused in the field today: 50 CNNs trained on ImageNet [1, 49, 53\u201364, 64\u201373], 6 CNNs trained on\nother datasets in addition to ImageNet (which we refer to as \u201cCNN extra data\u201d) [1, 66, 74], 10 vision\ntransformers [75\u201379], 6 CNNs trained with self-supervision [80, 81], and 13 models trained for\nrobustness to noise or adversarial examples [82, 83]. We used pretrained weights for each of these\nmodels supplied by their authors, with a variety of licenses (detailed in SI \u00a72), implemented in\nTensorflow 2.0, Keras, or PyTorch.\n\n\n\nBased on the section(s), please answer the following question with yes, no, or n/a and provide a brief justification for your answer.\nQuestion: If the authors curate/release new assets (e.g., code, data, models), do the authors discuss whether the data they are using/curating contains personally identifiable information or offensive content?"}, "5a": {"role": "user", "content": "The following is the methods section of the paper you are reviewing:\n\n\nHuman feature importance datasets. We focused on the ImageNet dataset to compare the visual\nstrategies of humans and DNNs for object recognition at scale. We relied on the two significant\nefforts for gathering feature importance data from humans on ImageNet: the Clicktionary [22] and\nClickMe [20] games, which use slightly different methods to collect their data. Both games begin with\nthe same basic setup: two players work together to locate features in an object image that they believe\nare important for categorizing it. As one of the players selects important image regions, those regions\nare filled into a blank canvas for the other observer to see and categorize the image as quickly as\npossible. In Clicktionary [22], both players are humans, whereas in ClickMe [20], the player selecting\nfeatures is a human and the player recognizing images is a DNN (VGG16 [49]). For both games,\nfeature importance maps depicting the average object category diagnosticity of every pixel was\ncomputed as the probability of it being clicked by a participant. In total, Clicktionary [22] contained\nfeature importance maps for 200 images from the ImageNet validation set, whereas ClickMe [20]\ncontained feature importance maps for a non-overlapping set of 196,499 images from ImageNet\ntraining and validation sets. Thus, ClickMe has far more data than Clicktionary, but Clicktionary\ndata has more reliable human feature importance data than ClickMe. Our experiments measure\nthe alignment between human and DNN visual strategies using ClickMe and Clicktionary feature\nimportance maps captured on the ImageNet validation set. As we describe in \u00a74, ClickMe feature\nimportance maps from the ImageNet training set are used to implement our neural harmonizer.\n\n3\n\n\fPsychophysics participants and dataset. We complemented the feature importance maps from\nClicktionary and ClickMe with psychophysics experiments on rapid visual categorization. We\nrecruited 199 participants from Amazon Mechanical Turk (mturk.com) to complete the experiments.\nParticipants viewed a psychophysics dataset consisting of the 100 animal and 100 non-animal images\nin the Clicktionary game taken from the ImageNet validation set [22]. We used the feature importance\nmaps for each image as masks for the object images, allowing us to control the proportion of important\nfeatures observers were shown when asked to recognize objects (Fig. 5a). We generated versions of\neach image that reveal anywhere between 1% to 100% (at log-scale spaced intervals) of the important\nobject pixels against a phase scrambled noise background (see Appendix \u00a71 for details on mask\ngeneration). The total number of revealed pixels was equal for every image at a given level of image\nmasking, and the revealed pixels were centered against the noise background. Each participant saw\nonly one masked version of each object image.\n\nPsychophysics experiment. Participants were instructed to categorize images in the psychophysics\ndataset as animals or non-animals as quickly and accurately as possible. Each experimental trial\nconsisted of the following sequence of events overlaid onto a white background (SI Fig. 1): (i)\na fixation cross displayed for a variable time (1,100\u20131,600ms); (ii) an image for 400ms; (iii) an\nadditional 150ms of response time. In other words, the experiment forced participants to perform\nrapid object categorization. They were given a total of 550ms to view an image and press a button\nto indicate its category (feedback was provided on trials in which responses were not provided\nwithin this time limit). Images were sized at 256 x 256 pixel resolution, which is equivalent to a\nstimulus size approximately between 5 \u2013 11 degrees of visual angle across a likely range of possible\ndisplay and seating setups we expect participants used for the experiment. Similar paradigms and\ntiming parameters have been shown to capture pre-attentive visual system processing [31, 50\u201352].\nParticipants provided informed consent electronically and were compensated $3.00 for their time (\u223c\n10\u201315 min; approximately $15.00/hr).\n\nModels. We compared humans with 84 different DNNs representing the variety of approaches\nused in the field today: 50 CNNs trained on ImageNet [1, 49, 53\u201364, 64\u201373], 6 CNNs trained on\nother datasets in addition to ImageNet (which we refer to as \u201cCNN extra data\u201d) [1, 66, 74], 10 vision\ntransformers [75\u201379], 6 CNNs trained with self-supervision [80, 81], and 13 models trained for\nrobustness to noise or adversarial examples [82, 83]. We used pretrained weights for each of these\nmodels supplied by their authors, with a variety of licenses (detailed in SI \u00a72), implemented in\nTensorflow 2.0, Keras, or PyTorch.\n\n\n\nThe following is the appendix section of the paper you are reviewing:\n\n\nThe psychophysics experiments of \u00a74.2 were implemented with the psiTurk framework [1] and\ncustom javascript functions. Each trial sequence was converted to a HTML5-compatible video for\nthe fastest reliable presentation time possible in a web browser. Videos were cached before each\ntrial to optimize reliability of experiment timing within the web browser. A photo-diode verified\nthe reliability of stimulus timing in our experiment was consistently accurate within \u223c 10ms across\ndifferent operating system, web browser, and display type configurations.\n\nParticipants: We recruited 199 participants from Amazon Mechanical Turk (mturk.com) for the\nexperiments. Participants were based in the United States, used either the Firefox or Chrome browser\non a non-mobile device, and had a minimal average approval rating of 95% on past Mechanical Turk\ntasks.\n\nStimuli: Experiment images were taken from the Clicktionary dataset [2]. Images were sampled\nfrom 5 target and 5 distractor categories: border collie, sorrel (horse), great white shark, bald eagle,\nand panther; trailer truck, sports car, speedboat, airliner, and school bus. Images were presented to\nhuman participants (and DNNs) either intact or with a perceptual phase scrambled mask that exposed\na proportion of their most important visual features, as described in the main text. Images were cast\nto greyscale to control for trivial color-based cues for classification and blend the scrambled mask\nbackground into the foreground. Responses to intact images were used to normalize the performance\nof each observer on masked images relative to their maximum performance on these images.\n\nImage masks were created for each image to reveal only a proportion of the most important visual\nfeatures. For each image, we created masks that revealed between 1% and 100% (at log-scale spaced\nintervals) of the object pixels in the corresponding image\u2019s Clicktionary feature importance map. We\ngenerated these masks in two steps. First, we computed a phase-scrambled version of the image [3, 4].\nNext, we used a novel \u201cstochastic flood-fill\u201d algorithm to reveal a contiguous region of the most\nimportant visual features in the image according to humans. Our flood-fill algorithm was seeded on\nthe pixel deemed most important by humans in the image, then grew outwards anisotropically and\nbiased towards pixels with higher feature importance scores (Figure 1). The revealed region was\nalways centered on the image. Each participant saw every category exemplar only once, with its\namount of image revelation randomly selected from all possible configurations.\n\nAfter providing online consent, participants were instructed to complete a rapid visual categorization\ntask in which they had to classify stimuli revealing a portion of the most diagnostic object features\n(Fig. 3). Each experimental trial began with a cross for participants to fixate for a variable time\n(1,100\u20131,600ms), then a stimulus for 400ms, then another cross and additional time for participants\nto render a decision. Participants were instructed to provide a decision after the first fixation cross,\n\n*These authors contributed equally.\n1Department of Cognitive, Linguistic, & Psychological Sciences, Brown University, Providence, RI\n2Artificial and Natural Intelligence Toulouse Institute (ANITI), Toulouse, France\n3Carney Institute for Brain Science, Brown University, Providence, RI\n\n36th Conference on Neural Information Processing Systems (NeurIPS 2022).\n\n\fFigure 1: Overview of the psychophysics paradigm. Participants performed a rapid animals vs.\nvehicles categorization paradigm (top). Stimuli were created using feature importance maps derived\nfrom humans or DNNs via a \u201cstochastic flood-fill\u201d algorithm that revealed image regions of different\nsizes centered on important features. Sample stimuli are shown (bottom) for different percentages of\nimage revelation. Note that 100% revelation corresponds to all non-zero pixels in a feature importance\nmap.\n\nbut that they only had 650ms to answer. If they were too slow to respond they were told to respond\nfaster and the trial was discarded.\n\n2 Harmonization loss\n\nThe neural harmonizer loss Fig. 2 uses several components crucial to its performance: a pyramidal\nrepresentation of decision explanation maps and normalizing those maps.\n\n2\n\nFigure 1TargetDistractorPercent image revelation1%25%63%100%FullProcessed featureimportance map++Please respondfaster!Pre-stimulus \ufb01xation(1,100 \u2013 1,600ms)Stimulus400msExtra response150ms\u2026if windowIs reachedExperiment trial \fFigure 2: Computing the neural harmonizer loss..\n\nFigure 3: Psychophysics experiment instructions.\n\nWhen computing the difference between model explanations for an image and the human feature\nimportance map for that image, we rely on a pyramid representation of each to compute these\ndifferences Fig. 2). This pyramid allows for a model to align its feature representations with humans\nat multiple scales and corrects for an important problem in datasets like ClickMe: the human data is\nan approximation and not precise at the pixel level. This lack of precision can present optimization\nissues, and computing a pyramid representation alleviates those issues because it allows a model to\nlearn to focus on regions that are important for humans without pixel-level precision.\n\nStandardization tackles a similar problem: because of the imprecision of human data, we choose\nto focus harmonization on only the most important areas selected by humans in ClickMe. By\nstandardizing then rectifying before comparing human and model explanations, we reduce noise in\nthe harmonization procedure.\n\n3\n\n+ReLUReLUdistanceGaussianPyramidClickMemap(    )StandardizationaStandardizationa\fFigure 4: Example ClickMe feature importance maps on ImageNet images.\n\n3 Additional Results\n\n3.1 ClickMe\n\nThe ClickMe game by [5] was used to identify category diagnostic features in ImageNet images.\nThese feature importance maps largely focus on object regions rather than context, and in contrast to\nsegmentation maps select features on the \u201cfront\u201d or \u201cface\u201d of objects (Fig. 4).\n\nAs discussed in the main text, we found a trade-off between DNN top-1 ImageNet accuracy and\nthe alignment of their feature importance maps with humans importance maps from ClickMe. This\ntrade-off persists across multiple scales of feature importance maps, including 4\u00d7 (Fig. 6) and 16\u00d7\n(Fig. 7) sub-sampled maps, meaning that simple smoothing is not sufficient to fix the trade-off.\n\n3.2 ViT attention\n\nWhile in the main text we investigate alignment between humans and models using gradient feature\nimportance visualizations, the attention maps in transformer models like the ViT provide another\navenue for investigation. To understand whether or not attention maps from ViT are more aligned\nwith humans than their gradient-based decision explanation maps, we computed attention rollouts\nfor harmonized and unharmonized ViTs [6]. We found that both versions of the ViT had similar\ncorrelations between their attention rollouts and human ClickMe maps: 0.38 for the harmonized ViT\nand 0.393 for the unharmonized model. This surprising result suggests that the harmonizer affects the\nprocess by which ViTs integrate visual information into their decisions rather than how they allocate\nattention. Through manipulating ViT decision making processes, the harmonizer can induce the large\nchanges in gradient-based visualizations and psychophysics that we describe in the main text.\n\n3.3 Correlations between measurements of human visual strategies\n\nOur results rely on three independent datasets measuring different features of human visual strategies:\nClickMe, Clicktionary, and the psychophysics experiments we introduce in this manuscript. The fact\nthat all three evoke similar trade-offs between top-1 accuracy and human alignment is a surprising\nresult that deserves further attention. We investigated these trade-offs by measuring the correlation be-\ntween human alignment on each dataset, with and without models trained with the neural harmonizer.\nWe found that correlations between datasets were lower across the board when neural harmonizer\nmodels were not included. The association between model alignments with Clicktionary versus\npsychophysics results were not significant (\u03c1 = 0.21, n.s.; Fig. 9), but the associations between\n\n4\n\nHuman feature preferenceLessMore\fFigure 5: Feature importance maps of humans, harmonized, and unharmonized models on\nImageNet.\n\nmodel alignments with ClickMe versus psychophysics (\u03c1 = 0.51, p < 0.001; Fig. 8) and ClickMe\nversus Clicktionary (\u03c1 = 0.77, p < 0.001; Fig. 10) were both significant. Each correlation improved\nwhen the neural harmonizer models were included in the calculation. This finding indicates that the\nneural harmonizer successfully aligned visual strategies between humans and DNNs, and was not\nmerely benefiting from either where humans versus DNNs considered important visual features to be\nor how humans versus DNNs incorporated those features into their decisions.\n\n5\n\nSnakeShovelVanBrown bearAssaultrifleImageClickMeResNet50SimCLRRobustResNet50ConvNextHarmonizedViTHareSoccer ballLeopardMLP-MixerViTHarmonized ViTHuman (ClickMe)MLP-MixerConvNextViTResNet50Robust ResNet50SnakeBearHareLeopardBallSimCLRRi\ufb02eVanShovel\fFigure 6: The neural harmonizer\u2019s effect is robust across image scales. Here, we show that the\ntrade-off between ImageNet accuracy and alignment with humans holds across downsizing by a\nfactor of 4. The Neural harmonizer once again yields the model with the best alignment with humans.\nGrey-shaded area captures the trade-off between accuracy and alignment in standard DNNs. Error\nbars are bootstrapped standard deviations over feature alignment.\n\n6\n\nTop-1 ImageNet Accuracy (%)607080556575858040200CNNTransformerSelf-supervisedRobustCNN extra dataNeural harmonizer60100Human feature alignment (%)\u2020CNNTransformerSelf-supervisedRobustCNN extra dataNeural harmonizer\fFigure 7: The neural harmonizer\u2019s effect is robust across image scales. Here, we show that the\ntrade-off between ImageNet accuracy and alignment with humans holds across downsizing by a factor\nof 16. The Neural harmonizer once again yields the model with the best alignment with humans.\nGrey-shaded area captures the trade-off between accuracy and alignment in standard DNNs. Error\nbars are bootstrapped standard deviations over feature alignment.\n\nFigure 8: The association between ClickMe alignment versus psychophysics alignment. These\nscores are significantly correlated, \u03c1 = 0.68, p < 0.001. Error bars are bootstrapped standard\ndeviations over feature alignment.\n\n7\n\nTop-1 ImageNet Accuracy (%)607080556575858040200CNNTransformerSelf-supervisedRobustCNN extra dataNeural harmonizer60100Human feature alignment (%)-log(mean squared error)843CNNTransformerSelf-supervisedRobustCNN extra dataNeural harmonizer657Human feature alignment (%)10204060705030\fFigure 9: The association between Clicktionary alignment versus psychophysics alignment.\nThese scores are significantly correlated, \u03c1 = 0.53, p < 0.001.\n\nFigure 10: The association between ClickMe alignment versus Clicktionary alignment. These\nscores are significantly correlated, \u03c1 = 0.85, p < 0.001. Error bars are bootstrapped standard\ndeviations over feature alignment.\n\n8\n\n843CNNTransformerSelf-supervisedRobustCNN extra dataNeural harmonizer657Human feature alignment (%)1020304050-log(mean squared error)Clicktionary feature alignment (%)10020CNNTransformerSelf-supervisedRobustCNN extra dataNeural harmonizer604080Clickme feature alignment (%)20408010060\fFigure 11: The mean of ClickMe feature importance maps exhibits a center bias, likely due\nto the positioning of objects in ImageNet images rather than a purely spatial bias of human\nparticipants (compare to individual maps shown in Fig. 4).\n\n9\n\n\f\n\nBased on the section(s), please answer the following question with yes, no, or n/a and provide a brief justification for your answer.\nQuestion: If the authors used crowdsourcing or conducted research with human subjects, do the authors include the full text of instructions given to participants and screenshots, if applicable?"}, "5b": {"role": "user", "content": "The following is the methods section of the paper you are reviewing:\n\n\nHuman feature importance datasets. We focused on the ImageNet dataset to compare the visual\nstrategies of humans and DNNs for object recognition at scale. We relied on the two significant\nefforts for gathering feature importance data from humans on ImageNet: the Clicktionary [22] and\nClickMe [20] games, which use slightly different methods to collect their data. Both games begin with\nthe same basic setup: two players work together to locate features in an object image that they believe\nare important for categorizing it. As one of the players selects important image regions, those regions\nare filled into a blank canvas for the other observer to see and categorize the image as quickly as\npossible. In Clicktionary [22], both players are humans, whereas in ClickMe [20], the player selecting\nfeatures is a human and the player recognizing images is a DNN (VGG16 [49]). For both games,\nfeature importance maps depicting the average object category diagnosticity of every pixel was\ncomputed as the probability of it being clicked by a participant. In total, Clicktionary [22] contained\nfeature importance maps for 200 images from the ImageNet validation set, whereas ClickMe [20]\ncontained feature importance maps for a non-overlapping set of 196,499 images from ImageNet\ntraining and validation sets. Thus, ClickMe has far more data than Clicktionary, but Clicktionary\ndata has more reliable human feature importance data than ClickMe. Our experiments measure\nthe alignment between human and DNN visual strategies using ClickMe and Clicktionary feature\nimportance maps captured on the ImageNet validation set. As we describe in \u00a74, ClickMe feature\nimportance maps from the ImageNet training set are used to implement our neural harmonizer.\n\n3\n\n\fPsychophysics participants and dataset. We complemented the feature importance maps from\nClicktionary and ClickMe with psychophysics experiments on rapid visual categorization. We\nrecruited 199 participants from Amazon Mechanical Turk (mturk.com) to complete the experiments.\nParticipants viewed a psychophysics dataset consisting of the 100 animal and 100 non-animal images\nin the Clicktionary game taken from the ImageNet validation set [22]. We used the feature importance\nmaps for each image as masks for the object images, allowing us to control the proportion of important\nfeatures observers were shown when asked to recognize objects (Fig. 5a). We generated versions of\neach image that reveal anywhere between 1% to 100% (at log-scale spaced intervals) of the important\nobject pixels against a phase scrambled noise background (see Appendix \u00a71 for details on mask\ngeneration). The total number of revealed pixels was equal for every image at a given level of image\nmasking, and the revealed pixels were centered against the noise background. Each participant saw\nonly one masked version of each object image.\n\nPsychophysics experiment. Participants were instructed to categorize images in the psychophysics\ndataset as animals or non-animals as quickly and accurately as possible. Each experimental trial\nconsisted of the following sequence of events overlaid onto a white background (SI Fig. 1): (i)\na fixation cross displayed for a variable time (1,100\u20131,600ms); (ii) an image for 400ms; (iii) an\nadditional 150ms of response time. In other words, the experiment forced participants to perform\nrapid object categorization. They were given a total of 550ms to view an image and press a button\nto indicate its category (feedback was provided on trials in which responses were not provided\nwithin this time limit). Images were sized at 256 x 256 pixel resolution, which is equivalent to a\nstimulus size approximately between 5 \u2013 11 degrees of visual angle across a likely range of possible\ndisplay and seating setups we expect participants used for the experiment. Similar paradigms and\ntiming parameters have been shown to capture pre-attentive visual system processing [31, 50\u201352].\nParticipants provided informed consent electronically and were compensated $3.00 for their time (\u223c\n10\u201315 min; approximately $15.00/hr).\n\nModels. We compared humans with 84 different DNNs representing the variety of approaches\nused in the field today: 50 CNNs trained on ImageNet [1, 49, 53\u201364, 64\u201373], 6 CNNs trained on\nother datasets in addition to ImageNet (which we refer to as \u201cCNN extra data\u201d) [1, 66, 74], 10 vision\ntransformers [75\u201379], 6 CNNs trained with self-supervision [80, 81], and 13 models trained for\nrobustness to noise or adversarial examples [82, 83]. We used pretrained weights for each of these\nmodels supplied by their authors, with a variety of licenses (detailed in SI \u00a72), implemented in\nTensorflow 2.0, Keras, or PyTorch.\n\n\n\nThe following is the appendix section of the paper you are reviewing:\n\n\nThe psychophysics experiments of \u00a74.2 were implemented with the psiTurk framework [1] and\ncustom javascript functions. Each trial sequence was converted to a HTML5-compatible video for\nthe fastest reliable presentation time possible in a web browser. Videos were cached before each\ntrial to optimize reliability of experiment timing within the web browser. A photo-diode verified\nthe reliability of stimulus timing in our experiment was consistently accurate within \u223c 10ms across\ndifferent operating system, web browser, and display type configurations.\n\nParticipants: We recruited 199 participants from Amazon Mechanical Turk (mturk.com) for the\nexperiments. Participants were based in the United States, used either the Firefox or Chrome browser\non a non-mobile device, and had a minimal average approval rating of 95% on past Mechanical Turk\ntasks.\n\nStimuli: Experiment images were taken from the Clicktionary dataset [2]. Images were sampled\nfrom 5 target and 5 distractor categories: border collie, sorrel (horse), great white shark, bald eagle,\nand panther; trailer truck, sports car, speedboat, airliner, and school bus. Images were presented to\nhuman participants (and DNNs) either intact or with a perceptual phase scrambled mask that exposed\na proportion of their most important visual features, as described in the main text. Images were cast\nto greyscale to control for trivial color-based cues for classification and blend the scrambled mask\nbackground into the foreground. Responses to intact images were used to normalize the performance\nof each observer on masked images relative to their maximum performance on these images.\n\nImage masks were created for each image to reveal only a proportion of the most important visual\nfeatures. For each image, we created masks that revealed between 1% and 100% (at log-scale spaced\nintervals) of the object pixels in the corresponding image\u2019s Clicktionary feature importance map. We\ngenerated these masks in two steps. First, we computed a phase-scrambled version of the image [3, 4].\nNext, we used a novel \u201cstochastic flood-fill\u201d algorithm to reveal a contiguous region of the most\nimportant visual features in the image according to humans. Our flood-fill algorithm was seeded on\nthe pixel deemed most important by humans in the image, then grew outwards anisotropically and\nbiased towards pixels with higher feature importance scores (Figure 1). The revealed region was\nalways centered on the image. Each participant saw every category exemplar only once, with its\namount of image revelation randomly selected from all possible configurations.\n\nAfter providing online consent, participants were instructed to complete a rapid visual categorization\ntask in which they had to classify stimuli revealing a portion of the most diagnostic object features\n(Fig. 3). Each experimental trial began with a cross for participants to fixate for a variable time\n(1,100\u20131,600ms), then a stimulus for 400ms, then another cross and additional time for participants\nto render a decision. Participants were instructed to provide a decision after the first fixation cross,\n\n*These authors contributed equally.\n1Department of Cognitive, Linguistic, & Psychological Sciences, Brown University, Providence, RI\n2Artificial and Natural Intelligence Toulouse Institute (ANITI), Toulouse, France\n3Carney Institute for Brain Science, Brown University, Providence, RI\n\n36th Conference on Neural Information Processing Systems (NeurIPS 2022).\n\n\fFigure 1: Overview of the psychophysics paradigm. Participants performed a rapid animals vs.\nvehicles categorization paradigm (top). Stimuli were created using feature importance maps derived\nfrom humans or DNNs via a \u201cstochastic flood-fill\u201d algorithm that revealed image regions of different\nsizes centered on important features. Sample stimuli are shown (bottom) for different percentages of\nimage revelation. Note that 100% revelation corresponds to all non-zero pixels in a feature importance\nmap.\n\nbut that they only had 650ms to answer. If they were too slow to respond they were told to respond\nfaster and the trial was discarded.\n\n2 Harmonization loss\n\nThe neural harmonizer loss Fig. 2 uses several components crucial to its performance: a pyramidal\nrepresentation of decision explanation maps and normalizing those maps.\n\n2\n\nFigure 1TargetDistractorPercent image revelation1%25%63%100%FullProcessed featureimportance map++Please respondfaster!Pre-stimulus \ufb01xation(1,100 \u2013 1,600ms)Stimulus400msExtra response150ms\u2026if windowIs reachedExperiment trial \fFigure 2: Computing the neural harmonizer loss..\n\nFigure 3: Psychophysics experiment instructions.\n\nWhen computing the difference between model explanations for an image and the human feature\nimportance map for that image, we rely on a pyramid representation of each to compute these\ndifferences Fig. 2). This pyramid allows for a model to align its feature representations with humans\nat multiple scales and corrects for an important problem in datasets like ClickMe: the human data is\nan approximation and not precise at the pixel level. This lack of precision can present optimization\nissues, and computing a pyramid representation alleviates those issues because it allows a model to\nlearn to focus on regions that are important for humans without pixel-level precision.\n\nStandardization tackles a similar problem: because of the imprecision of human data, we choose\nto focus harmonization on only the most important areas selected by humans in ClickMe. By\nstandardizing then rectifying before comparing human and model explanations, we reduce noise in\nthe harmonization procedure.\n\n3\n\n+ReLUReLUdistanceGaussianPyramidClickMemap(    )StandardizationaStandardizationa\fFigure 4: Example ClickMe feature importance maps on ImageNet images.\n\n3 Additional Results\n\n3.1 ClickMe\n\nThe ClickMe game by [5] was used to identify category diagnostic features in ImageNet images.\nThese feature importance maps largely focus on object regions rather than context, and in contrast to\nsegmentation maps select features on the \u201cfront\u201d or \u201cface\u201d of objects (Fig. 4).\n\nAs discussed in the main text, we found a trade-off between DNN top-1 ImageNet accuracy and\nthe alignment of their feature importance maps with humans importance maps from ClickMe. This\ntrade-off persists across multiple scales of feature importance maps, including 4\u00d7 (Fig. 6) and 16\u00d7\n(Fig. 7) sub-sampled maps, meaning that simple smoothing is not sufficient to fix the trade-off.\n\n3.2 ViT attention\n\nWhile in the main text we investigate alignment between humans and models using gradient feature\nimportance visualizations, the attention maps in transformer models like the ViT provide another\navenue for investigation. To understand whether or not attention maps from ViT are more aligned\nwith humans than their gradient-based decision explanation maps, we computed attention rollouts\nfor harmonized and unharmonized ViTs [6]. We found that both versions of the ViT had similar\ncorrelations between their attention rollouts and human ClickMe maps: 0.38 for the harmonized ViT\nand 0.393 for the unharmonized model. This surprising result suggests that the harmonizer affects the\nprocess by which ViTs integrate visual information into their decisions rather than how they allocate\nattention. Through manipulating ViT decision making processes, the harmonizer can induce the large\nchanges in gradient-based visualizations and psychophysics that we describe in the main text.\n\n3.3 Correlations between measurements of human visual strategies\n\nOur results rely on three independent datasets measuring different features of human visual strategies:\nClickMe, Clicktionary, and the psychophysics experiments we introduce in this manuscript. The fact\nthat all three evoke similar trade-offs between top-1 accuracy and human alignment is a surprising\nresult that deserves further attention. We investigated these trade-offs by measuring the correlation be-\ntween human alignment on each dataset, with and without models trained with the neural harmonizer.\nWe found that correlations between datasets were lower across the board when neural harmonizer\nmodels were not included. The association between model alignments with Clicktionary versus\npsychophysics results were not significant (\u03c1 = 0.21, n.s.; Fig. 9), but the associations between\n\n4\n\nHuman feature preferenceLessMore\fFigure 5: Feature importance maps of humans, harmonized, and unharmonized models on\nImageNet.\n\nmodel alignments with ClickMe versus psychophysics (\u03c1 = 0.51, p < 0.001; Fig. 8) and ClickMe\nversus Clicktionary (\u03c1 = 0.77, p < 0.001; Fig. 10) were both significant. Each correlation improved\nwhen the neural harmonizer models were included in the calculation. This finding indicates that the\nneural harmonizer successfully aligned visual strategies between humans and DNNs, and was not\nmerely benefiting from either where humans versus DNNs considered important visual features to be\nor how humans versus DNNs incorporated those features into their decisions.\n\n5\n\nSnakeShovelVanBrown bearAssaultrifleImageClickMeResNet50SimCLRRobustResNet50ConvNextHarmonizedViTHareSoccer ballLeopardMLP-MixerViTHarmonized ViTHuman (ClickMe)MLP-MixerConvNextViTResNet50Robust ResNet50SnakeBearHareLeopardBallSimCLRRi\ufb02eVanShovel\fFigure 6: The neural harmonizer\u2019s effect is robust across image scales. Here, we show that the\ntrade-off between ImageNet accuracy and alignment with humans holds across downsizing by a\nfactor of 4. The Neural harmonizer once again yields the model with the best alignment with humans.\nGrey-shaded area captures the trade-off between accuracy and alignment in standard DNNs. Error\nbars are bootstrapped standard deviations over feature alignment.\n\n6\n\nTop-1 ImageNet Accuracy (%)607080556575858040200CNNTransformerSelf-supervisedRobustCNN extra dataNeural harmonizer60100Human feature alignment (%)\u2020CNNTransformerSelf-supervisedRobustCNN extra dataNeural harmonizer\fFigure 7: The neural harmonizer\u2019s effect is robust across image scales. Here, we show that the\ntrade-off between ImageNet accuracy and alignment with humans holds across downsizing by a factor\nof 16. The Neural harmonizer once again yields the model with the best alignment with humans.\nGrey-shaded area captures the trade-off between accuracy and alignment in standard DNNs. Error\nbars are bootstrapped standard deviations over feature alignment.\n\nFigure 8: The association between ClickMe alignment versus psychophysics alignment. These\nscores are significantly correlated, \u03c1 = 0.68, p < 0.001. Error bars are bootstrapped standard\ndeviations over feature alignment.\n\n7\n\nTop-1 ImageNet Accuracy (%)607080556575858040200CNNTransformerSelf-supervisedRobustCNN extra dataNeural harmonizer60100Human feature alignment (%)-log(mean squared error)843CNNTransformerSelf-supervisedRobustCNN extra dataNeural harmonizer657Human feature alignment (%)10204060705030\fFigure 9: The association between Clicktionary alignment versus psychophysics alignment.\nThese scores are significantly correlated, \u03c1 = 0.53, p < 0.001.\n\nFigure 10: The association between ClickMe alignment versus Clicktionary alignment. These\nscores are significantly correlated, \u03c1 = 0.85, p < 0.001. Error bars are bootstrapped standard\ndeviations over feature alignment.\n\n8\n\n843CNNTransformerSelf-supervisedRobustCNN extra dataNeural harmonizer657Human feature alignment (%)1020304050-log(mean squared error)Clicktionary feature alignment (%)10020CNNTransformerSelf-supervisedRobustCNN extra dataNeural harmonizer604080Clickme feature alignment (%)20408010060\fFigure 11: The mean of ClickMe feature importance maps exhibits a center bias, likely due\nto the positioning of objects in ImageNet images rather than a purely spatial bias of human\nparticipants (compare to individual maps shown in Fig. 4).\n\n9\n\n\f\n\nBased on the section(s), please answer the following question with yes, no, or n/a and provide a brief justification for your answer.\nQuestion: If the authors used crowdsourcing or conducted research with human subjects, do the authors describe any potential participant risks, with links to Institutional Review Board (IRB) approvals, if applicable?"}, "5c": {"role": "user", "content": "The following is the methods section of the paper you are reviewing:\n\n\nHuman feature importance datasets. We focused on the ImageNet dataset to compare the visual\nstrategies of humans and DNNs for object recognition at scale. We relied on the two significant\nefforts for gathering feature importance data from humans on ImageNet: the Clicktionary [22] and\nClickMe [20] games, which use slightly different methods to collect their data. Both games begin with\nthe same basic setup: two players work together to locate features in an object image that they believe\nare important for categorizing it. As one of the players selects important image regions, those regions\nare filled into a blank canvas for the other observer to see and categorize the image as quickly as\npossible. In Clicktionary [22], both players are humans, whereas in ClickMe [20], the player selecting\nfeatures is a human and the player recognizing images is a DNN (VGG16 [49]). For both games,\nfeature importance maps depicting the average object category diagnosticity of every pixel was\ncomputed as the probability of it being clicked by a participant. In total, Clicktionary [22] contained\nfeature importance maps for 200 images from the ImageNet validation set, whereas ClickMe [20]\ncontained feature importance maps for a non-overlapping set of 196,499 images from ImageNet\ntraining and validation sets. Thus, ClickMe has far more data than Clicktionary, but Clicktionary\ndata has more reliable human feature importance data than ClickMe. Our experiments measure\nthe alignment between human and DNN visual strategies using ClickMe and Clicktionary feature\nimportance maps captured on the ImageNet validation set. As we describe in \u00a74, ClickMe feature\nimportance maps from the ImageNet training set are used to implement our neural harmonizer.\n\n3\n\n\fPsychophysics participants and dataset. We complemented the feature importance maps from\nClicktionary and ClickMe with psychophysics experiments on rapid visual categorization. We\nrecruited 199 participants from Amazon Mechanical Turk (mturk.com) to complete the experiments.\nParticipants viewed a psychophysics dataset consisting of the 100 animal and 100 non-animal images\nin the Clicktionary game taken from the ImageNet validation set [22]. We used the feature importance\nmaps for each image as masks for the object images, allowing us to control the proportion of important\nfeatures observers were shown when asked to recognize objects (Fig. 5a). We generated versions of\neach image that reveal anywhere between 1% to 100% (at log-scale spaced intervals) of the important\nobject pixels against a phase scrambled noise background (see Appendix \u00a71 for details on mask\ngeneration). The total number of revealed pixels was equal for every image at a given level of image\nmasking, and the revealed pixels were centered against the noise background. Each participant saw\nonly one masked version of each object image.\n\nPsychophysics experiment. Participants were instructed to categorize images in the psychophysics\ndataset as animals or non-animals as quickly and accurately as possible. Each experimental trial\nconsisted of the following sequence of events overlaid onto a white background (SI Fig. 1): (i)\na fixation cross displayed for a variable time (1,100\u20131,600ms); (ii) an image for 400ms; (iii) an\nadditional 150ms of response time. In other words, the experiment forced participants to perform\nrapid object categorization. They were given a total of 550ms to view an image and press a button\nto indicate its category (feedback was provided on trials in which responses were not provided\nwithin this time limit). Images were sized at 256 x 256 pixel resolution, which is equivalent to a\nstimulus size approximately between 5 \u2013 11 degrees of visual angle across a likely range of possible\ndisplay and seating setups we expect participants used for the experiment. Similar paradigms and\ntiming parameters have been shown to capture pre-attentive visual system processing [31, 50\u201352].\nParticipants provided informed consent electronically and were compensated $3.00 for their time (\u223c\n10\u201315 min; approximately $15.00/hr).\n\nModels. We compared humans with 84 different DNNs representing the variety of approaches\nused in the field today: 50 CNNs trained on ImageNet [1, 49, 53\u201364, 64\u201373], 6 CNNs trained on\nother datasets in addition to ImageNet (which we refer to as \u201cCNN extra data\u201d) [1, 66, 74], 10 vision\ntransformers [75\u201379], 6 CNNs trained with self-supervision [80, 81], and 13 models trained for\nrobustness to noise or adversarial examples [82, 83]. We used pretrained weights for each of these\nmodels supplied by their authors, with a variety of licenses (detailed in SI \u00a72), implemented in\nTensorflow 2.0, Keras, or PyTorch.\n\n\n\nBased on the section(s), please answer the following question with yes, no, or n/a and provide a brief justification for your answer.\nQuestion: If the authors used crowdsourcing or conducted research with human subjects, do the authors include the estimated hourly wage paid to participants and the total amount spent on participant compensation?"}}}