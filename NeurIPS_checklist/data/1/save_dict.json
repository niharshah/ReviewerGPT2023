{"paper_index": 1, "title": "Hardness in Markov Decision Processes: Theory and Practice", "abstract": "", "introduction": "", "methods": "\n\nSection 2.1 introduces our notation and important definitions. Section 2.2 presents our survey of\nthe theoretical landscape of measures of hardness, which includes a novel categorization of existing\nmeasures. Section 2.2.3 highlights the weaknesses of existing measures and introduces the concept\nof a complete measure of hardness, which we believe should be the focus of future developments.\n\n2.1 Preliminaries\n\nLet \u2206(X ) denote the set of probability distributions over a set X . A finite Markov decision process\n(MDP) is a tuple M = (S, A, P, P0, R), where S is the finite set of states, A is the finite set of\nactions, P : S \u00d7 A \u2192 \u2206(S) is the transition kernel, P0 \u2208 \u2206(S) is the initial state distribution, and\nR : S \u00d7 A \u2192 \u2206 ([0, 1]) is the reward kernel [6]. Given an optimization horizon T , a reinforcement\nlearning agent aims to find a (possibly stochastic) policy \u03c0 : S \u2192 \u2206(A) that optimizes a reward-\nbased criterion. The MDP M is unknown to the agent but can be learned through experience. The\ninteraction between the agent and the environment starts when the initial state s0 \u223c P0 is drawn.\nFor any 0 \u2264 t < T , the agent samples an action at \u223c \u03c0t(st) from its current policy \u03c0t, and the\nenvironment draws the next state st+1 \u223c P (st, at) and reward rt+1 \u223c R(st, at). An MDP is\nepisodic with time horizon H when the state st+1 is drawn from the initial state distribution whenever\nt + 1 \u2261 0 (mod H) and continuous otherwise. In the continuous setting, a factor \u03b3 \u2208 (0, 1) is used\nto discount future rewards in the discounted setting, and future rewards are averaged across time\nin the undiscounted setting. For a policy \u03c0, an MDP M induces a Markov chain [7] with transition\nprobabilities P\u03c0\na\u2208A \u03c0(a | s)P (s\u2032 | s, a) whose stationary distribution is denoted by\n\u00b5\u03c0. MDPs can be classified into three communication classes [8]. An MDP is ergodic if the Markov\nchain induced by any deterministic policy is ergodic. An MDP is communicating if, for every two\nstates s, s\u2032 \u2208 S, there is a deterministic policy that induces a Markov chain where s is accessible\n\ns\u2192s\u2032(M) = (cid:80)\n\n2\n\n\ffrom s\u2032 and vice versa. An MDP is weakly communicating when there is a partition (C, S \\ C) of the\nset of states S such that, for every two states s, s\u2032 \u2208 C, there is a deterministic policy that induces a\nMarkov chain where s is accessible from s\u2032 and vice versa, and every state s \u2208 S \\ C is transient in\nevery Markov chain induced by any deterministic policy. Every ergodic MDP is communicating, and\nevery communicating MDP is weakly communicating.\n\n(cid:104)(cid:80)H\n\n(cid:105)\nt=h+1 rt|sh = s, ah = a\n\nh,epi(s, a) := E\nh,epi(s) := (cid:80)\n\u03b3 (s, a) := E (cid:2)(cid:80)\u221e\n\u03b3 (s) := (cid:80)\n\nThe episodic state-action value function is given by Q\u03c0\n,\nand the episodic state value function is given by V \u03c0\na \u03c0(a | s)Q\u03c0\nh,epi(s, a). The dis-\nt=1 \u03b3t\u22121rt | s0 = s, a0 = a(cid:3), and\ncounted state-action value function is given by Q\u03c0\nthe discounted state value function is given by V \u03c0\n\u03b3 (s, a). These expectations\nare taken w.r.t. the policy \u03c0, the transition kernel P , and the reward kernel R. In the undiscounted\nsetting, the value of every state(-action) is the same, since rewards are averaged across infinite time\n(cid:80)T\nE [rt], which\nsteps. In that case, we define the expected average reward as \u03c1\u03c0 := limT \u2192\u221e\ns = Ea\u223c\u03c0(s)R(s, a) is the expected\nis also given by \u03c1\u03c0 = \u27e8\u00b5\u03c0, R\u03c0\u27e9, where R\u03c0 is a vector such that R\u03c0\nreward obtained when following \u03c0 from state s. An optimal policy \u03c0\u2217 obtains maximum value for\nevery state. We assume with little loss of generality that the optimal policy is unique. We drop the\nsubscripts when the setting is clear from the context, and write \u2217 to denote \u03c0\u2217 in superscripts.\n\na \u03c0(a | s)Q\u03c0\n\nt=1\n\n1\nT\n\nThe most widely studied performance criteria are the expected cumulative regret, which yields the\nregret minimization setting [9], and the sample efficiency of exploration, which yields the Probably\nApproximately Correct Reinforcement Learning (PAC-RL) setting [10]. In simple terms, the regret\nmeasures the loss in reward due to the execution of a sub-optimal policy. In contrast, the sample\nefficiency measures how many interactions the agent requires to approximately learn \u03c0\u2217 with high\nprobability. The main difference between the two criteria is that the rewards obtained during the\ninteractions with the MDP are not considered in PAC-RL, whereas all rewards contribute to the\ncumulative regret. Therefore, PAC-RL agents can generally afford more aggressive exploration.\n\n2.2 Characterization of hardness\n\nWe distinguish the hardness of MDPs into two kinds of complexity, the visitation complexity and the\nestimation complexity. The visitation complexity relates to the difficulty of visiting all the states, and\nthe estimation complexity relates to the discrepancy between the optimal policy and the best policy\nan agent can derive from a given estimate of the transition and reward kernels. The two complexities\nare complementary in the sense that the former quantifies the hardness of gathering samples from the\nstate space while the latter quantifies the hardness of producing a highly rewarding policy given the\nsamples. In the literature, we identify two approaches that aim to capture the mentioned complexities.\nThe first approach, which we call Markov chain-based, considers that an MDP is an extension of\na Markov chain where transition probabilities can be changed based on direct interventions by an\nagent [11]. This approach is well suited to capture the hardness that comes from the visitation\ncomplexity since it considers the properties of transition kernels. The second approach, which we call\nvalue-based, considers the discrepancy between the optimal policy and the best policy an agent can\nderive from a value function point estimate. Note that the information contained in such point estimate\nis lower than the one in kernels estimate and that the difficulty of obtaining an accurate estimate of\nthe value function is not considered in this approach. Therefore, the value-based approach is only\nable to partially capture the estimation complexity. A striking fact that highlights this shortcoming is\nthat almost every value-based measure of hardness is independent of the variability of the reward\nkernel. Therefore, given an MDP M\u2032 that is obtained by increasing the reward kernel variability of an\nMDP M, value-based measures of hardness assign the same level of hardness to M and M\u2032.\n\nMarkov chain properties and value functions depend on a fixed policy, which presents two natural\nchoices to derive measures of hardness. The first choice considers the optimal policy, which typically\nleads to a measure that considers a best-case scenario. The second choice considers a policy that\nmaximizes a criterion that characterizes a worst-case scenario. For instance, a policy that maximizes\nsuch criterion may spend its time in a region of the state space that is not relevant for learning \u03c0\u2217.\n\n3\n\n\f2.2.1 Markov chain-based measures of hardness\n\nMixing time. The mixing time of a Markov chain with stationary distribution \u00b5 is defined as\n\nt\u00b5 := inf{n | sup\ns\u2208S\n\ndTV (pn\n\ns , \u00b5) \u2264 0.25},\n\n(1)\n\nwhere dTV is the total variation distance between distributions and the vector pn\ns represents the\ndistribution over states after n steps starting from state s. The value 0.25 is conventionally established\nin the Markov chain literature for the definition of the mixing time. For ergodic and aperiodic Markov\nchains, limn\u2192\u221e pn\ns = \u00b5 for every state s, so the mixing time is the number of steps a Markov chain\ntakes to produce samples that are close to being distributed according to the stationary distribution \u00b5.\n\nFor instance, a Markov chain with a non-negligible probability of transitioning from every state to\nevery state is quickly mixing. In contrast, the mixing time can be very long in chains where the state\nspace has several distinct regions each of which is well connected but where transitions between\nregions have low probability [12]. Kearns and Singh [13] propose an extension of the mixing time to\nMDPs that considers the maximum mixing time across policies t := sup\u03c0 t\u00b5\u03c0 in the undiscounted\nsetting. Although the mixing time plays an important role in that setting, since the average reward\nobtained by policy \u03c0 is given by \u03c1\u03c0 = \u27e8\u00b5\u03c0, R\u03c0\u27e9, it is not a generally good measure of hardness. First,\nit does not capture the visitation complexity, since it neglects the fact that the agent may direct its\nexploration through a choice of policy. Second, it does not capture any significant aspect of optimal\npolicy estimation, which does not require stationary distribution samples from every policy.\n\nDiameter. The diameter is fundamentally related to the number of time steps required to transition\nbetween states. In the continuous setting, the diameter D is most commonly defined as\n\nD := sup\ns1\u0338=s2\n\ninf\n\u03c0\n\nT \u03c0\ns1\u2192s2\n\n,\n\n(2)\n\nwhere T \u03c0\ns1\u2192s2 is the expected number of time steps required to reach state s2 from state s1 when\nfollowing policy \u03c0 [14]. Intuitively, D is the worst-case expected number of time steps required to\ntransition between two states when following the best policy for that purpose. Related definitions are\nDworst := sup\n\u03c0\n\nT \u03c0\ns1\u2192s2, which imply D \u2264 Dopt \u2264 Dworst [15].\n\nT \u03c0\ns1\u2192s2 and Dopt := inf\n\u03c0\n\nsup\ns1\u0338=s2\n\nsup\ns1\u0338=s2\n\nIn the episodic setting, we define the diameter by augmenting each state s with the current in-episode\ntime step h and considering the diameter of this augmented MDP in the continuous setting. Note\nthat T \u03c0\n(s1,h1)\u2192(s2,h2) can be larger than the episode length H, which means that (on average) more\nthan one episode may be required to transition from state (s1, h1) to state (s2, h2). This happens\nwhenever h2 < h1, which may be undesirable if the intent is to focus on the expected number of time\nsteps required to transition between states from the same episode. The diameter is always infinite\nin weakly-communicating MDPs if the supremum is not restricted to states in the recurrent class\nC and is always finite in the episodic setting (where every state is reachable within an episode). A\nlarge diameter can be caused by high stochasticity. The diameter is very apt at measuring visitation\ncomplexity, since it captures the effort required to deliberately move between states. However, it\nneglects the reward kernel, and so has limited capacity to measure the estimation complexity.\n\nDistribution mismatch coefficient. The distribution mismatch coefficient (DMC) has been defined\nfor the continuous undiscounted [16] and continuous discounted [17] cases respectively as\n\nDMC := sup\n\n\u03c0\n\n(cid:88)\n\ns\u2208S\n\n\u00b5\u2217\ns\n\u00b5\u03c0\ns\n\nand\n\nDMCs0 := sup\ns\u2208S\n\nd\u2217\n(s)\ns0\nP0(s)\n\n,\n\n(s) = (1 \u2212 \u03b3) (cid:80)\u221e\n\nt=0 \u03b3tPr(st = s | s0) is the discounted state visitation distribution of the\nwhere d\u2217\ns0\noptimal policy given an initial state s0. Note that the DMC is guaranteed to be finite only for ergodic\nMDPs. In communicating MDPs, there is at least one policy \u03c0 whose stationary distribution assigns\nprobability zero to some states. MDPs whose optimal stationary distribution \u00b5\u2217 has its probability\nmass concentrated on a few states tend to have a large DMC. In contrast, when \u00b5\u2217 is closer to being\nuniformly distributed across states, the DMC tends to be small. For small values of DMC, as every\n\u00b5\u03c0 is close to \u00b5\u2217, the agent will gather samples from the optimal stationary distribution regardless of\nits current policy, which may enable quick learning. In contrast, for large values of DMC, the agent\nneeds to actively seek a policy that gathers such samples. The DMC is not well suited to quantify\n\n4\n\n\fthe visitation complexity, since it fails to capture the difficulty of visiting all states. The DMC also\ndoes not capture the estimation complexity, since it does not account for the stochasticity of the\nenvironment, which is related to the number of samples required to make accurate estimations.\n\n2.2.2 Value-based measures of hardness\n\nAction-gap regularity. Given an estimate \u02c6Q\u2217 of the optimal state-action value function Q\u2217, the\ngreedy policy with respect to \u02c6Q\u2217 always chooses an action associated with the highest state-action\nvalue. Whether or not such a policy is optimal depends exclusively on the ordering of the estimates\nfor a given state rather than their accuracy. For instance, assuming that a\u2217 is the optimal action for\nevery state s, a greedy agent would act optimally if \u02c6Q(s, a\u2217) > \u02c6Q(s, a\u2032) for every action a\u2032 \u0338= a\u2217,\neven if |Q\u2217(s, a) \u2212 \u02c6Q(s, a)| \u226b 0 for every action a. The action-gap regularity \u03b6 is a measure of\nhardness that leverages this principle through the theory of hardness for classification algorithms [18].\nHowever, this measure is only defined for two actions, and so has exceptionally limited applicability.\n\nEnvironmental value norm. The (discounted) environmental value norm C \u03c0\n\n\u03b3 is defined as\n\n(cid:114)\n\nC \u03c0\n\n\u03b3 := sup\n(s,a)\n\nVar\ns\u2032\u223cP (s,a)\n\n\u03b3 (s\u2032).\nV \u03c0\n\nThis quantity can be similarly defined in the undiscounted setting [19]. In words, the environmental\nvalue norm captures the one-step variance of the value function V \u03c0 for a given policy \u03c0. In the\nepisodic setting, a closely related measure called maximum per-step conditional variance C \u03c0\nH [20] is\ndefined as\n\n(cid:18)\n\nC \u03c0\n\nH := sup\n(s,a,h)\n\nVar R(s, a) + Var\n\ns\u2032\u223cP (s,a)\n\n(cid:19)\n\nh+1(s\u2032)\nV \u03c0\n\n.\n\nAlternatively, as with the diameter, it is also possible to define this quantity by augmenting each\nstate s with the current in-episode time step h and considering the environmental value norm of\nthis augmented MDP in the continuous setting. For every policy, the environmental value norm is\nequal to zero when the transition kernel is deterministic. However, a highly stochastic MDP may\nstill have a small environmental value norm, since this norm captures the variance of the state value\nfunction rather than the stochasticity of the transition kernel. Maillard et al. [19] suggest using the\nenvironmental value norm of the optimal policy \u03c0\u2217 as a measure of hardness. The main strength of\nsuch measure is that the variability of the optimal value function captures an important aspect of the\nestimation complexity. However, this measure of hardness neglects the visitation complexity.\n\nSub-optimality gap. The sub-optimality gap is defined in the continuous and episodic settings as\n\n\u2206(s, a) := V \u2217(s) \u2212 Q\u2217(s, a)\n\nand \u2206h(s, a) := V \u2217\n\nh (s) \u2212 Q\u2217\n\nh(s, a),\n\n(s,a)|\u2206(s,a)\u0338=0\n\nrespectively. Since V \u2217(s) = maxa Q\u2217(s, a) for every state s, the sub-optimality gap \u2206(s, a) mea-\nsures the difference in expected return between selecting the optimal action for state s and selecting\nthe action a. Intuitively, identifying a suboptimal action a\u2032 in a given state s is easier if the gap \u2206(s, a\u2032)\nis large. Simchowitz and Jamieson [21] identifies the sum of the reciprocals of the sub-optimality\ngaps (cid:80)\n\u2206(s,a) as a measure of hardness. They also demonstrate the importance of the\nsub-optimality gaps by showing that recent optimistic algorithms necessarily incur in a cumulative\nregret proportional to the smallest nonzero sub-optimality gap in the episodic setting. However, note\nthat approximating the optimal value function (and identifying a near-optimal policy) is particularly\neasy when every sub-optimality gap is small. Consequently, the (PAC-RL) sample complexity is\nlikely to decrease when the sum of the reciprocals of the sub-optimality gaps increases. Furthermore,\nthis measure does not explicitly capture visitation complexity and is prone to severe numerical issues.\n\n1\n\nTable 1: Computational complexity of generally applicable measures up to logarithmic factors.\n\nMarkov chain-based measures\n\nValue-based measures\n\nMixing time\n\n?\n\nDiameter\n\u02dcO(|S|3.5|A|)\n\nDistribution mismatch coefficient Environmental value norm Sub-optimality gaps\n\u02dcO(|S|2|A|(1 \u2212 \u03b3)\u22121)\n\n\u02dcO(|S|2|A|(1 \u2212 \u03b3)\u22121)\n\n?\n\n5\n\n\f2.2.3 Future directions\n\nCurrent hardness measures suffer from three principal issues. They are not designed to be efficiently\ncomputable (see Table 1 and Appendix B), they are limited in their ability to simultaneously capture\nvisitation complexity and estimation complexity, and they are oblivious to the distinct challenges\npresented by different performance criteria. For instance, while regret minimizing agents must be\ncautious not to incur in large regret during learning (for example, by not revisiting lowly rewarding\nstates that are not followed by highly rewarding states), PAC-RL agents have the flexibility to incur\nin large regret as long as they end up with a near-optimal policy. Therefore, the visitation complexity\nshould differ across settings. The fact that current measures disregard this distinction is concerning,\nsince they should account for the specific difficulty of the optimization task. These issues are not\ndiscussed in previous work, since measures of hardness have not been considered relevant outside the\ncontext of deriving theoretical performance guarantees for reinforcement learning agents.\n\nIn order to address these issues, we believe that future work should focus on developing efficiently\ncomputable (non-trivial) hardness measures that (approximately) meet the following novel definition.\nDefinition 2.1 (Complete measure of hardness) A measure \u03b8 : M \u2192 R+ is complete for an MDP\nclass M and criterion \u03c8 (sample complexity or cumulative regret) if, for every pair M1, M2 \u2208 M\nand near-optimal agent A\u2217 that achieves the criterion lower bound of class M up to logarithmic\nfactors1, \u03b8(M1) > \u03b8(M2) implies \u02dc\u03c8(M1, A\u2217) > \u02dc\u03c8(M2, A\u2217), where \u02dc\u03c8 hides logarithmic factors.\nCombining existing measures that capture visitation complexity and estimation complexity is a viable\nfirst step in that direction. Recently, Wagenmaker et al. [23] have pioneered this approach in the\nepisodic setting by proposing the gap-visitation complexity,\n\nGVP(\u03f5) :=\n\nH\n(cid:88)\n\nh=0\n\ninf\n\u03c0\n\nsup\ns,a\n\ninf\n\n(cid:18)\n\n1\nh (s, a)\u2206h(s, a)2 ,\nw\u03c0\n\nWh(s)2\nw\u03c0\nh (s, a)\u03f52\n\n(cid:19)\n\n,\n\n(3)\n\nwhere w\u03c0\nh (s, a) := P\u03c0(sh = s, ah = a) is the probability of visiting state-action pairs (s, a) at\nin-episode time step h when following policy \u03c0, Wh(s) := sup\u03c0 P\u03c0(sh = s) is the maximum\nreachability of state s at in-episode time step h, and \u03f5 is a parameter related to the optimality of the\noutput policy in the PAC-RL setting. The strength of this measure is that it weights the sub-optimality\naction gaps with measures of visitation complexity, w\u03c0\nh and Wh. This captures the difficulty induced\nby the critical states that are both hard to reach and for which it is hard to estimate the best action.\nHowever, the gap-visitation complexity fails to be a generally applicable hardness measure. It depends\non the PAC-RL setting-specific parameter \u03f5, it is restricted to the finite horizon setting (and can not\nbe extended to the continuous setting), and is not efficiently computable.\n\n", "experiments": "\n\n3.1 Colosseum\n\nThis section briefly introduces Colosseum, a pioneering Python package that bridges theory and\npractice in tabular reinforcement learning while also being applicable in the non-tabular setting. More\ndetails about the package can be found in Appendix A and in the project website.2\n\nAs a hardness analysis tool, Colosseum identifies the communication class of MDPs, assembles\ninsightful visualizations and logs of interactions between agents and MDPs, computes three measures\nof hardness (environmental value norm, sum of the reciprocals of the sub-optimality gaps, and\ndiameter, whose computation requires a novel solution described in App. A.4). Eight MDP families\nare available for experimentation. Some are traditional families (RiverSwim [24], Taxi [25], and\nFrozenLake) while others are more recent (MiniGid environments [26]). Additionally, DeepSea\n[27] was included as a hard exploration family of problems, and the SimpleGrid family is composed\nof simplified versions of the MG\u2013Empty environment. By controlling the parameters of MDPs from\neach family (further detailed in App. A.3), it is easy to create an MDP with any desired hardness.\n\nAs a benchmarking tool, Colosseum is unique in its strong connection with theory. For instance,\nin contrast to non-tabular benchmarks, Colosseum computes theoretical evaluation criteria such\n\n1For example, the \u2126(|S||A|H 2\u03f5\u22122) sample complexity bound for the episodic communicating setting [22].\n2Available at https://michelangeloconserva.github.io/Colosseum.\n\n6\n\n\fas the expected cumulative regret and the expected average future reward, which can be used to\nexactly evaluate the performance criterion of regret minimizing agents. The benchmark covers the\nmost commonly studied reinforcement learning settings: episodic ergodic, episodic communicating,\ncontinuous ergodic, and continuous communicating. For each setting, we have selected twenty MDPs\nthat are diverse with respect to their diameters and environmental value norms as proxies for different\ncombinations of visitation complexity and estimation complexity. Figure 17 in Appendix E shows\nhow each of these MDPs varies according to these measures, and Section 3.3 empirically validates\nthis selection by showing that harder MDPs correspond to worse agent performance. Notably, the\ntheoretically backed selection of MDPs and the rigorous evaluation criteria make the Colosseum\nbenchmark the most exhaustive in tabular reinforcement learning, since previous evaluations were\nconducted empirically in a few MDPs (such as Taxi or RiverSwim).\n\nColosseum also allows testing of non-tabular agents by leveraging the BlockMDP model [28].\nBlockMDPs equip tabular MDPs with an emission map that is a (possibly stochastic) mapping q : S \u2192\n\u2206(O) from the finite state space S to a (possibly infinite) observation space O. Agents interacting\nwith BlockMDPs are only provided with observations, so non-tabular methods are generally required.\nMany commonly used non-tabular MDPs (such as Minecraft [29]) can be straightforwardly encoded\nas BlockMDPs using the Colosseum MDP families. Colosseum implements a diverse set of\ndeterministic emission maps and allows combining them with different sources of noise. Appendix\nA.2 further details BlockMDPs and the available emission maps.\n\n3.2 Empirical analysis of hardness measures\n\nFor brevity, this section only presents results of hardness measures in the MiniGridEmpty family of\nenvironments in the episodic setting. Appendix D presents the full outcome of the empirical analysis.\n\nA MiniGridEmpty MDP is a grid world where an agent has three available actions: moving forward,\nrotating left, and rotating right. An agent is rewarded for being in a few specific states and receives no\nreward in every other state. Appendix A.3.4 provides more details about this family of environments.\n\nIn our investigation, we consider four scenarios that highlight the different aspects of MDP hardness.\n\nScenario 1. We vary the probability p_rand that an MDP executes a random action instead of the\naction selected by an agent. As p_rand approaches one, value estimation becomes easier, since\noutcomes depend less on agent choices. However, intentionally visiting states becomes harder.\n\nScenario 2. We vary the probability p_lazy that an MDP stays in the same state instead of executing\nthe action selected by an agent. Contrary to increasing p_rand, increasing p_lazy never benefits\nexploration. Increasing p_lazy decreases estimation complexity and increases visitation complexity.\n\nScenario 3 and 4. We vary the number of states across MDPs from the same family. In scenario\n4, we also let p_rand = 0.1 to study the impact of stochasticity. In these scenarios, increasing the\nnumber of states simultaneously increases the estimation complexity and the visitation complexity.\n\nIn every scenario, hardness measures are compared with the cumulative regret of a near-optimal agent\ntuned for each specific MDP (see App. D). This regret serves as an optimistic measure of hardness.\nAppendix C describes how these measures are normalized. Note that, due to normalization, the plots\nshould only be compared in terms of trends (growth rates) rather than absolute values.\n\nAnalysis. Figure 1 presents the empirical results for the episodic MiniGridEmpty family in the four\nscenarios with 95% bootstrapped confidence intervals over twelve random seeds.\n\nThe experiments confirm our claim that the diameter captures visitation rather than estimation\ncomplexity. This measure of hardness grows superlinearly with both p_rand and p_lazy (Figures\n1a and 1b) since deliberate movement between states requires an exponentially increasing number of\ntime steps. Although the diameter highlights the sharply increasing visitation complexity, its trend\noverestimates the increase in cumulative regret of the tuned near-optimal agent, which is explained\nby the unaccounted decrease in estimation complexity. The diameter also increases almost linearly\nwith the number of states (Figures 1c and 1d). For the small p_rand (scenario 4), the relation is still\napproximately linear. This linear trend underestimates the evident non-linear growth in hardness in\nthe regret of the tuned near-optimal agent but is in line with the mild increase in visitation complexity.\n\nThe empirical evidence indicates that the environmental value norm can only capture estimation\ncomplexity. It decreases as p_lazy and p_rand increase (Figures 1a and 1b) because the optimal\n\n7\n\n\fFigure 1: The Colosseum hardness analysis for the episodic MiniGridEmpty family.\n\nvalue of neighboring states becomes closer, which decreases the per-step variability of the optimal\nvalue function. When the number of states increases but the transition and reward structures remain\nthe same (Figures 1c and 1d), the small increase in this variability only generates a sublinear growth.\n\nWe empirically observe that the sum of the reciprocals of the sub-optimality gaps is not particularly\napt at capturing estimation complexity, due to its exclusive focus on optimal policy identification, and\nit also underestimates the increase in hardness induced by an increase in visitation complexity. This\nmeasure increases weakly superlinearly in scenarios 1 and 2 (Figures 1a and 1b). The probability\nof executing the action selected by the agent decreases when p_lazy and p_rand increase, so the\ndifference between the state and the state-action optimal value functions decreases sharply. The\nmeasure increases almost linearly with the number of states (Figures 1c and 1d). This is explained by\nthe fact that the average value of the additional terms in the summation is often similar to the average\nvalue of the existing terms when MDPs have the same structure of reward and transition kernels.\n\n3.3 Colosseum benchmarking\n\nIn this section, we benchmark five tabular agents with theoretical guarantees and four non-tabular\nagents. Besides being valuable on their own, these results help to empirically validate our benchmark.\n\nAgents. The tabular agents are posterior sampling for reinforcement learning (PSRL) for the episodic\nand continuous settings [30, 31], Q-learning with UCB exploration for the episodic setting [32],\nQ-learning with optimism for the continuous setting [16], and UCRL2 for the continuous setting [14].\nThe non-tabular agents (from bsuite) are ActorCritic, ActorCriticRNN, BootDQN, and DQN.\n\nExperimental procedure. We set the total number of time steps to 500 000 with a maximum training\ntime of 10 minutes for the tabular setting and 40 minutes for the non-tabular setting. If an agent\ndoes not reach the maximum number of time steps before this time limit, learning is interrupted,\nand the agent continues interacting using its last best policy. This guarantees a fair comparison\nbetween agents with different computational costs. The performance indicators are computed every\n100 time steps. Each interaction between an agent and an MDP is repeated for 20 seeds. The agents\u2019\nhyperparameters have been chosen by random search to minimize the average regret across MDPs\nwith randomly sampled parameters (see Appendix E). We use a deterministic emission map that\nassigns a uniquely identifying vector to each state (for example, a gridworld coordinate) to derive\nthe non-tabular benchmark MDPs. In Table 2, we report the per-step normalized cumulative regrets\ndivided by the total number of time steps (defined in Appendix C), which allows comparisons across\ndifferent MDPs. We summarize the main findings here and refer to Appendix E for further details.\n\nAnalysis. Table 2 often shows high variability in the performance of the same agent across MDPs of\nthe same family. Therefore, maximising the diversity across diameters and value norms effectively\nproduces diverse challenges even for MDPs with similar transition and reward structures. For example,\nin the continuous communicating case (Table 2d), Q-learning performs well only in some MDPs of\nthe MiniGridEmpty family. This also happens for UCRL2 for the SimpleGrid family.\n\nThe average normalized cumulative regret is lower in ergodic environments compared to commu-\nnicating environments. This indicates that the ergodic setting is generally slightly easier than the\ncommunicating settings. Notably, in the continuous setting, the ergodic setting is more challenging\nthan the communicating setting for Q-learning (Tables 2c and 2d). Designing a naturally ergodic\n\n8\n\n0.00.20.40.6Probability of random action0.00.20.40.60.81.0Normalized values(a) Scenario 10.00.20.40.6Probability of lazy action(b) Scenario 2100200300400Number of states(c) Scenario 3100200300400Number of states(d) Scenario 4DiameterEnvironmental value normSum ofthe reciprocals of the sub-optimality gapsCumulative regret oftuned near-optimal agent\fTable 2: Normalized cumulative regrets of selected agents on the Colosseum benchmark. (a) Episodic\nergodic. (b) Episodic communicating. (c) Continuous ergodic. (d) Continuous communicating.\n\n(a)\n\n(b)\n\n(c)\n\n(d)\n\nQ-learning\n\nPSRL\n\nQ-learning\n\nPSRL\n\nQ-learning\n\nPSRL\n\nUCRL2\n\nQ-learning\n\nPSRL\n\nUCRL2\n\nMDP\n\nDeepSea\n\n.64 \u00b1 .00\n.52 \u00b1 .01\n\n.01 \u00b1 .00\n.00 \u00b1 .00\n\nMDP\n\nDeepSea\n\n.01 \u00b1 .01\n.83 \u00b1 .02\n\n.00 \u00b1 .00\n.54 \u00b1 .01\n\nMDP\n\nMDP\n\nDeepSea\n\n.94 \u00b1 .00\n\n.06 \u00b1 .01\n\n.23 \u00b1 .05\n\nDeepSea\n\nFrozenLake\n\n.83 \u00b1 .03\n\n.01 \u00b1 .03\n\n.01 \u00b1 .02\n\nFrozenLake\n\n.90 \u00b1 .01\n\n.01 \u00b1 .00\n\nFrozenLake\n\n.78 \u00b1 .04\n\n.03 \u00b1 .11\n\nMG-Empty\n\nMG-Empty\n\nMG-Rooms\n\nRiverSwim\n\nSimpleGrid\n\nTaxi\n\n1.00 \u00b1 .00\n1.00 \u00b1 .00\n1.00 \u00b1 .00\n1.00 \u00b1 .00\n1.00 \u00b1 .00\n.92 \u00b1 .04\n.91 \u00b1 .03\n\n.90 \u00b1 .04\n1.00 \u00b1 .00\n.99 \u00b1 .01\n\n.07 \u00b1 .02\n.91 \u00b1 .01\n\n.78 \u00b1 .03\n.79 \u00b1 .03\n.50 \u00b1 .03\n\n.84 \u00b1 .01\n.56 \u00b1 .02\n\n.86 \u00b1 .16\n.94 \u00b1 .07\n.91 \u00b1 .09\n.35 \u00b1 .10\n.44 \u00b1 .12\n.14 \u00b1 .08\n.04 \u00b1 .03\n\n.05 \u00b1 .04\n.54 \u00b1 .36\n.24 \u00b1 .29\n\n.00 \u00b1 .00\n.00 \u00b1 .00\n\n.05 \u00b1 .01\n.79 \u00b1 .03\n.50 \u00b1 .03\n\n.08 \u00b1 .01\n.05 \u00b1 .00\n\nMG-Empty\n\nMG-Rooms\n\nRiverSwim\n\nSimpleGrid\n\nTaxi\n\n.59 \u00b1 .07\n.99 \u00b1 .00\n.99 \u00b1 .01\n1.00 \u00b1 .00\n1.00 \u00b1 .00\n\n.99 \u00b1 .01\n1.00 \u00b1 .00\n1.00 \u00b1 .00\n.94 \u00b1 .05\n\n.87 \u00b1 .00\n.96 \u00b1 .01\n\n.78 \u00b1 .10\n.80 \u00b1 .00\n.50 \u00b1 .00\n.79 \u00b1 .04\n\n.94 \u00b1 .00\n.91 \u00b1 .01\n\n.09 \u00b1 .05\n.24 \u00b1 .15\n.23 \u00b1 .12\n.91 \u00b1 .09\n.93 \u00b1 .09\n\n.21 \u00b1 .29\n.44 \u00b1 .39\n.43 \u00b1 .39\n.04 \u00b1 .04\n\n.00 \u00b1 .00\n.80 \u00b1 .00\n\n.20 \u00b1 .15\n.55 \u00b1 .15\n.11 \u00b1 .01\n.79 \u00b1 .04\n\n.09 \u00b1 .01\n.36 \u00b1 .06\n\n.98 \u00b1 .02\n.98 \u00b1 .02\n.97 \u00b1 .00\n.98 \u00b1 .01\n.96 \u00b1 .01\n.98 \u00b1 .02\n.98 \u00b1 .03\n.98 \u00b1 .01\n\n.98 \u00b1 .03\n.98 \u00b1 .02\n\n.73 \u00b1 .19\n.71 \u00b1 .22\n.90 \u00b1 .06\n.50 \u00b1 .25\n\n.78 \u00b1 .00\n.46 \u00b1 .08\n.49 \u00b1 .00\n\n.99 \u00b1 .01\n.98 \u00b1 .04\n.95 \u00b1 .03\n.99 \u00b1 .01\n.83 \u00b1 .31\n.99 \u00b1 .02\n.99 \u00b1 .01\n.99 \u00b1 .01\n\n.99 \u00b1 .02\n1.00 \u00b1 .00\n\n.00 \u00b1 .00\n.00 \u00b1 .00\n.02 \u00b1 .04\n.01 \u00b1 .00\n\n.70 \u00b1 .19\n.01 \u00b1 .02\n.43 \u00b1 .16\n\n.05 \u00b1 .06\n.03 \u00b1 .05\n.04 \u00b1 .01\n.54 \u00b1 .26\n.01 \u00b1 .00\n.45 \u00b1 .35\n.27 \u00b1 .33\n.93 \u00b1 .09\n\n.18 \u00b1 .29\n.62 \u00b1 .36\n\n.00 \u00b1 .00\n.01 \u00b1 .00\n.01 \u00b1 .01\n.01 \u00b1 .01\n\n.01 \u00b1 .01\n.00 \u00b1 .00\n.00 \u00b1 .00\n\nFrozenLake\n\nMG-Empty\n\nMG-Rooms\n\nRiverSwim\n\nSimpleGrid\n\nMG-Rooms\n\nRiverSwim\n\nSimpleGrid\n\n.78 \u00b1 .00\n.99 \u00b1 .00\n.79 \u00b1 .00\n\n.77 \u00b1 .04\n.84 \u00b1 .04\n\n.51 \u00b1 .23\n.01 \u00b1 .00\n.00 \u00b1 .00\n.35 \u00b1 .17\n.75 \u00b1 .21\n\n.01 \u00b1 .01\n.01 \u00b1 .01\n.02 \u00b1 .02\n\n.16 \u00b1 .03\n.34 \u00b1 .14\n\n.11 \u00b1 .01\n.01 \u00b1 .00\n.15 \u00b1 .01\n.01 \u00b1 .00\n\n.78 \u00b1 .05\n.99 \u00b1 .00\n.79 \u00b1 .04\n\n.01 \u00b1 .04\n.01 \u00b1 .02\n\n.95 \u00b1 .22\n1.00 \u00b1 .00\n.60 \u00b1 .50\n1.00 \u00b1 .00\n1.00 \u00b1 .00\n\n1.00 \u00b1 .00\n1.00 \u00b1 .00\n1.00 \u00b1 .00\n\n.00 \u00b1 .01\n.01 \u00b1 .00\n\n.93 \u00b1 .00\n.45 \u00b1 .15\n.93 \u00b1 .00\n.50 \u00b1 .00\n\n.90 \u00b1 .01\n.99 \u00b1 .00\n.92 \u00b1 .01\n\n.01 \u00b1 .01\n.04 \u00b1 .06\n\n.02 \u00b1 .00\n.02 \u00b1 .00\n.01 \u00b1 .00\n.01 \u00b1 .00\n.08 \u00b1 .20\n\n.78 \u00b1 .40\n.02 \u00b1 .01\n.66 \u00b1 .47\n\n.00 \u00b1 .00\n.02 \u00b1 .01\n\n.01 \u00b1 .00\n.01 \u00b1 .00\n.70 \u00b1 .40\n.33 \u00b1 .24\n\nTaxi\n\n.87 \u00b1 .01\n\n.89 \u00b1 .08\n\n.09 \u00b1 .01\n\nTaxi\n\n.95 \u00b1 .00\n\n.94 \u00b1 .04\n\n.12 \u00b1 .01\n\nAverage\n\n.81 \u00b1 .24\n\n.30 \u00b1 .33\n\nAverage\n\n.83 \u00b1 .23\n\n.35 \u00b1 .30\n\nAverage\n\n.85 \u00b1 .18\n\n.59 \u00b1 .44\n\n.17 \u00b1 .26\n\nAverage\n\n.38 \u00b1 .37\n\n.69 \u00b1 .38\n\n.28 \u00b1 .37\n\nMDP is not straightforward. In fact, the majority of MDPs in the literature are communicating.\nIn Colosseum, ergodicity is induced by setting p_rand > 0 in otherwise communicating MDPs.\nModel-free agents struggle with the resulting increase in variability of the state-action value function.\n\nIn the episodic settings (Tables 2a and 2b), PSRL obtains excellent performances with low variability.\nQ-learning instead performs well in a few MDPs. This often happens since, when the action selected\nby the agent is randomly substituted (due to p_rand > 0) with one with a large sub-optimality gap,\nthe resulting Q-value update introduces a critical error that requires many samples to be corrected.\n\nIn the continuous settings (Tables 2c and 2d), UCRL2 performs best in the ergodic cases when\nQ-learning suffers from the issue caused by p_rand > 0 but is only slightly better than Q-learning\nin the communicating ones. PSRL instead struggles with most MDPs. The reason for its weak\nperformance in this setting is the computationally expensive optimistic sampling procedure required\nfor its worst-case theoretical guarantees. It often breaks the time limit before reaching the first quarter\nof available time steps, meaning that it lacks sufficient samples to estimate the optimal policy.\n\nFigure 2 places the regret of the agents in the continuous ergodic setting (Table 2c) on a position\ncorresponding to the diameter and value norm of the benchmark environments. PSRL and Q-learning\n(Figures 2a and 2b), appear to be impacted more by the value norm than the diameter. This is in line\nwith the lack of sufficient samples for PSRL and the aforementioned issue related to high q estimates\nvariability for Q-learning, which is exacerbated when the estimation complexity is higher. In the\ncase of UCRL2 (Figure 2c), which provides more reliable evidence since it performs well across the\nMDPs, higher regret effectively corresponds to higher diameter and value norm.", "conclusion": "\n\nWe established the usefulness of the theory of hardness in empirical reinforcement learning. Prior\nto our work, hardness measures were limited to providing theoretical guarantees for agents. In\norder to promote a wider understanding of these measures, we presented a systematic survey that\nnewly identified two major approaches for characterizing hardness: Markov chain-based and value-\nbased. These approaches aim to capture complementary aspects of hardness: visitation complexity\nand estimation complexity. Our survey also exposed a relative lack of measures that capture both\naspects, which motivates our definition of complete measures of hardness. Their development is\nimportant theoretically, elucidating what makes a problem hard for a specific performance criterion,\nand empirically, allowing the creation of principled benchmarks for a specific performance criterion.\n\nWe presented the first empirical study of (efficiently computable) hardness measures. This study\nrevealed which aspects of hardness current measures capture and clarified their relationship with\nthe behavior of near-optimal agents. Based on these results, we proposed a benchmark for the most\nwidely studied tabular reinforcement learning settings that contains environments that maximize\ndiversity with respect to two highly distinct measures. Such a principled benchmark is valuable to\ngauge progress in the field. The new benchmark allowed conducting the most exhaustive empirical\ncomparison between theoretically principled tabular reinforcement learning agents to date, which\nrevealed undocumented weaknesses of these agents and further validated our choices of environments.\n\nAs a first step towards principled non-tabular benchmarking, we argued that many commonly used\nenvironments can be encoded as BlockMDPs, which are non-tabular versions of tabular MDPs for\nwhich a partial characterization of hardness is already possible. We observed a clear empirical relation\nbetween two tabular hardness measures and the performance of four non-tabular agents. BlockMDPs\nrepresent a promising starting point for the future development of non-tabular hardness measures\nwhile already being useful to provide relevant insights into the performance of non-tabular agents.\n\nOur work has led to the development of Colosseum, a pioneering tool for empirical but theoretically\nprincipled study of tabular reinforcement learning with experimental non-tabular benchmarking\ncapabilities. Besides implementing the aforementioned tabular benchmark, Colosseum provides\nvaluable analysis tools: regret and hardness computations, communication class identification,\nlogging, and visualizations. Colosseum can also be easily extended and integrated with new agents\nand environments, for which we will actively seek contributions from the community. We strongly\nbelieve that Colosseum has the potential to become a fundamental tool in reinforcement learning.\n\n10\n\n204060800.250.500.751.001.251.501.752.00Value norm20406080100DiameterValue norm20406080100DiameterValue norm20406080100DiameterValue norm(c) BootDQN(a) ActorCritic(b) ActorCriticRNN(d) DQN100DiameterMG-Rooms (1)MG-Rooms (2)MG-Empty (2)MG-Empty (1)MG-Empty (4)MG-Empty (5)MG-Empty (7)MG-Empty (6)RiverSwim (1)MG-Empty (3)MG-Empty (8)SimpleGrid (2)SimpleGrid (3)SimpleGrid (1)FrozenLake (1)Taxi (1)RiverSwim (2)RiverSwim (3)RiverSwim (4)DeepSea (1)MG-Rooms (1)MG-Rooms (2)MG-Empty (2)MG-Empty (1)MG-Empty (4)MG-Empty (5)MG-Empty (7)MG-Empty (6)RiverSwim (1)MG-Empty (3)MG-Empty (8)SimpleGrid (2)SimpleGrid (3)SimpleGrid (1)FrozenLake(1)Taxi (1)RiverSwim (2)RiverSwim (3)RiverSwim (4)DeepSea (1)MG-Rooms (1)MG-Rooms (2)MG-Empty (2)MG-Empty (1)MG-Empty (4)MG-Empty (5)MG-Empty (7)MG-Empty (6)RiverSwim (1)MG-Empty (3)MG-Empty (8)SimpleGrid (2)SimpleGrid (3)SimpleGrid (1)FrozenLake (1)Taxi (1)RiverSwim (2)RiverSwim (3)RiverSwim (4)DeepSea (1)MG-Rooms (1)MG-Rooms (2)MG-Empty (2)MG-Empty (1)MG-Empty (4)MG-Empty (5)MG-Empty (7)MG-Empty (6)RiverSwim (1)MG-Empty (3)MG-Empty (8)SimpleGrid (2)SimpleGrid (3)SimpleGrid (1)FrozenLake (1)Taxi (1)RiverSwim (2)RiverSwim (3)RiverSwim (4)DeepSea (1)\f", "appendix_A": "\n\nColosseum is a pioneering Python package that creates a bridge between theory and practice in\ntabular reinforcement learning with an eye on the non-tabular setting. It allows to empirically, and\nefficiently, investigate the hardness of MDPs, and it implements the first principled benchmark for\ntabular reinforcement learning algorithms. In the following sections, we report some additional\ndetails on the capabilities of Colosseum. However, we invite the reader to check the latest online\ndocumentation along with the tutorials that cover in detail every aspect of the package.3\n\nA.1 Expected performance indicators\n\nEach agent in Colosseum is required to implement a function that returns its current best policy\nestimate \u02c6\u03c0\u2217\nt for any time step t. Using an efficient implementation of the policy evaluation algorithm,\nColosseum can compute the corresponding expected regret and expected average reward, which,\nsummed across time steps, amounts to the expected cumulative reward and expected cumulative regret.\nAlthough it is possible to perform this operation at every time step of the agent/MDP interaction, we\nleave the option to approximate the expected cumulative regret by calculating the expected regret\nevery n time steps and assuming that the policy of the agent in the previous n \u2212 1 time steps would\nhave yielded a similar expected regret. For instance, for n = 100, the expected cumulative regret at\ntime step T = 500 would be approximated as the sum of the expected regrets calculated at time steps\nt = 100, 200, . . . , 500 multiplied by 100.\n\nA.2 Non-tabular capabilities\n\nColosseum is primarily aimed at the tabular reinforcement learning setting. However, as our ultimate\ngoal is to develop principled non-tabular benchmarks, we offer a way to test non-tabular reinforcement\nlearning algorithms on the Colosseum benchmark. Although our benchmark defines a challenge\nthat is well characterized for tabular agents, we believe that it can provide valuable insights into the\nperformance of non-tabular algorithms. In order to do so, we adopt the BlockMDP formalism proposed\nby Du et al. [28]. A BlockMDP is a tuple (S, A, P, P0, R, O, q), where O and q : S \u2192 \u2206(O) are\nrespectively the non-tabular observation space that the agent observes and the (possibly stochastic)\nemission map that associates a distribution over the observation space to each state in the MDP. Note\nthat the agent is not provided with any information on the state space S. Colosseum implements six\ndeterministic emission maps with different properties and four kinds of noise to make the emission\nmaps stochastic, which we describe below. Examples of the emission maps with distinguishable\ncharacteristics for each MDP family will be presented in the corresponding sections.\n\nEmission maps:\n\n\u2022 One-hot encoding. This emission map assigns to each state a feature vector that is filled\n\nwith zeros with the exception of an index that uniquely corresponds to the state.\n\n\u2022 Linear optimal value. This emission map assigns to each state a feature vector \u03d5(s) that\nenables linear representation of the optimal value function. In other words, there is a \u03b8 such\nthat V \u2217(s) = \u03b8T \u03d5(s).\n\n\u2022 Linear random value. This emission map assigns to each state a feature vector \u03d5(s) that\nenables linear representation of the value function of the randomly acting policy. In other\nwords, there is a \u03b8 such that V \u03c0(s) = \u03b8T \u03d5(s), where \u03c0 is the randomly acting policy.\n\n\u2022 State information. This emission map assigns to each state a feature vector that contains\nuniquely identifying information about the state (e.g., coordinates for the DeepSea family).\n\u2022 Image encoding. This emission map assigns to each state a feature matrix that encodes the\n\nvisual representation of the MDP as a grayscale image.\n\n\u2022 Tensor encoding. This emission map assigns to each state a tensor composed of the\nconcatenation of matrices that one-hot encode the presence of a symbol in the corresponding\nindices. For example, for the DeepSea family, the tensor is composed of a matrix that\nencodes the position of the agent and a matrix that encodes the positions of white spaces.\n\nNoise:\n\n3Available at https://michelangeloconserva.github.io/Colosseum.\n\n15\n\n\f\u2022 Uncorrelated light-tailed noise. The output of the emission map is corrupted with element-\n\nwise uncorrelated Gaussian noise.\n\n\u2022 Correlated light-tailed noise. The output of the emission map is corrupted with multivariate\ncorrelated Gaussian noise with a covariance matrix sampled from a Wishart distribution\nwith a pre-specified scale level when the MDP is created. In other words, the correlation\nstructure of the noise remains unchanged while the agent interacts with the MDP.\n\n\u2022 Uncorrelated heavy-tailed noise. The output of the emission map is corrupted with element-\n\nwise uncorrelated Student\u2019s t noise.\n\n\u2022 Correlated heavy-tailed noise. The output of the emission map is corrupted with multivariate\ncorrelated Student\u2019s t noise with covariance matrix sampled from a Wishart distribution\nwith a pre-specified scale level when the MDP is created. In other words, the correlation\nstructure of the noise remains unchanged while the agent interacts with the MDP.\n\nA.3 Colosseum MDP families\n\nColosseum implements eight families of MDPs. When selecting which families to include in\nColosseum, we aimed to balance between traditional environment families (RiverSwim [24], Taxi\n[25], and FrozenLake) and unconventional ones (MiniGid environments [26]). The DeepSea\nfamily [27] was included since it was proposed as an example of a hard exploration problem. The\nSimpleGrid family acts as a simplified version of the MiniGrid-Empty environment.\n\nEach MDP family requires a set of parameters to instantiate an MDP. In addition to individual\nparameters, all MDP families share the following:\n\n\u2022 The size \u2208 N parameter controls the number of states through geometrical properties of\nthe MDP family. For example, in a grid world, it controls the size of the grid. This param-\neter allows increasing the difficulty of an MDP instance without altering the fundamental\nstructure of the MDP family.\n\n\u2022 The p_rand \u2208 [0, 1) parameter controls the probability r that an MDP executes an action\nat random instead of the one selected by the agent. Concretely, the new transition kernel\nis given by P \u2032(st+1 | st, at) = (1 \u2212 r)P (st+1 | st, at) + r\na P (st+1 | st, a). Setting\n|A|\nthis parameter to a non-zero value can make a communicating MDP ergodic.\n\n(cid:80)\n\n\u2022 The lazy \u2208 [0, 1) parameter controls the probability l of an action not being executed.\nConcretely, the new transition kernel is given by P \u2032(st+1 | st, at) = (1 \u2212 l)P (st+1 |\nst, at) + l1(st+1 = st). This parameter can render a deterministic MDP stochastic without\nchanging the communication class.\n\n\u2022 make_reward_stochastic is a boolean parameter to render the rewards stochastic instead\nof deterministic (the default). We opted for a Beta distribution to guarantee rewards bounded\nin a specific range. However, it is possible to specify custom reward distributions using\nscipy random variables.\n\n\u2022 r_min and r_max scale the rewards. The default values are 0 and 1, respectively.\n\nThe hardness analysis presented in Appendix D shows the relationships between the measures of\nhardness and some of these parameters for the Colosseum MDP families. Such relationships can be\neasily exploited to create MDP instances with specific hardness characterization. For example, in\norder to create an MDP instance with low estimation complexity and high visitation complexity, one\ncan force the MDP instance to be deterministic by setting p_rand and p_lazy to zero and the size to\na high value. If instead one wants to increase the estimation complexity while keeping the visitation\ncomplexity fixed, the mean reward of a subset of states can be increased. The scale of the increase\ndepends on the variability of the next state distributions of the selected states. The more variable such\ndistributions are the higher the increase in estimation complexity.\n\n", "appendix_D": "\n\nIn our empirical investigation of the measures of hardness, we consider five MDP families (MG-Empty,\nSimpleGrid, FrozenLake, RiverSwim, and DeepSea) that include different levels of stochasticity\nand challenge. Each MDP family is tested in four scenarios that highlight different aspects of hardness.\nNote that each measure has been normalized (as described in App. C), which solely allows comparing\ntrends (growth rates). Figures 12, 13, 14, 15 and 16 (pg. 28) report the results of our investigation\nalong with the 95% bootstrapped confidence intervals over twelve seeds.\n\nScenario 1. We vary the probability p_rand that an MDP executes a random action instead of the\naction selected by an agent. As p_rand increases, estimating the optimal value function becomes\neasier since every policy yields increasingly similar value functions. This produces a decrease in the\nestimation complexity. However, intentionally visiting states becomes harder.\n\nScenario 2. We vary the probability p_lazy that an MDP stays in the same state instead of\nexecuting the action selected by an agent. Contrary to increasing p_rand, increasing p_lazy never\nbenefits exploration through the execution of random actions. Increasing p_lazy decreases estimation\ncomplexity and increases visitation complexity.\n\nScenario 3 and 4. We vary the number of states across MDPs from the same family. In scenario 4,\nwe also let p_rand = 0.1 to study the impact of stochasticity. In these scenarios, increments in the\nnumber of states increase both estimation complexity and visitation complexity.\n\nCumulative regret of a near-optimal agent.\nIn every scenario, the measures of hardness are com-\npared with the cumulative regret of a near-optimal agent that serves as an optimistic approximation\nof a complete measure of hardness. The near-optimal agents have been chosen between the ones\navailable in Colosseum with the lowest average cumulative regret in the benchmarking results (such\nas PSRL in the episodic setting and UCRL2 in the continuous setting). In order to optimistically\napproximate a complete measure of hardness, we tune the hyperparameters of the agents for each\nMDP in every scenario. Concretely, we perform a random search with the objective of minimizing\nthe average cumulative regret resulting from an interaction of the agent with the MDP that lasts for\n200 000 time steps with a maximum time limit of two minutes across three seeds. The budget for the\nrandom search is 120 samples.\n\nComputational power. The empirical investigation has been carried out on a desktop PC equipped\nwith an AMD Ryzen 9 5950X 16-Core Processor and required less than 24 hours for all the MDP fam-\nilies and scenarios. The most computationally intensive part of the procedure is the hyperparameter\nsearch.\n\nLimitations. The main limitation of our empirical investigation is the selection of the MDP families,\nnear-optimal agents, and scenarios. Although we believe to have proposed a solid methodology, we\nare open to discussing the inclusion of additional experiments to further enhance Colosseum.\n\nD.1 Analysis of results\n\nDiameter. The diameter grows superlinearly with both p_rand and p_lazy since deliberate move-\nment between states requires an exponentially increasing number of time steps. As clearly shown in\nthe figures, this phenomenon is exacerbated in the episodic setting. If the agent is forced to take a\nrandom action or to stay in the same state, it can miss the opportunity to reach the target state in the\ncurrent episode and has to try again in the next episode. Although the diameter highlights this sharply\nincreasing visitation complexity, its trend overestimates the increase in cumulative regret of the\ntuned near-optimal agent, which is explained by the unaccounted decrease in estimation complexity.\nThe diameter also increases almost linearly with the number of states. When p_rand is relatively\nsmall, an approximately linear relationship can still be observed. This linear trend underestimates the\nnon-linear growth in hardness clearly shown in the cumulative regret of the tuned near-optimal agent\nbut is in line with the mild increase in visitation complexity. FrozenLake in the episodic setting\n(Figures 14c and 14d) represents the only exception. Given the extremely high level of stochasticity\nof the MDP, increasing the number of states drastically increases the visitation complexity while\nmaking it easier for the agent to act near-optimally.\n\n26\n\n\fEnvironmental value norm. The environmental value norm decreases as p_lazy and p_rand\nincrease because the optimal value of neighboring states becomes closer, which decreases the per-step\nvariability of the optimal state value function. However, we note that for the MG-Empty and the\nFrozenLake MDP families in the continuous cases (see Figures 12f and 14f) as p_lazy increases,\nthe environmental value norm first decreases and later increases. From a certain value of p_lazy\nonward, there is a significant probability of the agent remaining in the same state. This provokes\nlarge changes in the value of states that are distant from the highly rewarding states and no changes at\nall for highly rewarding states since the lazy transition is comparable to taking the optimal action.\nDue to the large changes in the suboptimal region of the state space and the absence of changes in the\noptimal region of the state space, the overall one-step variability of the state value function increases.\nNote that this does not happen in the episodic case due to the restarting mechanism and whether it\nhappens or not in the continuous case depends on the transition and reward structure of an MDP.\nWhen the number of states increases but the transition and reward structures remain the same, the\nsmall increase in measured variability only causes the environmental value norm to grow sublinearly.\nThese findings are strong evidence that this measure is only suited to capture estimation complexity.\n\nSum of the reciprocals of the sub-optimality gaps. The sum of the reciprocals of the sub-\noptimality gaps increases weakly superlinearly in scenarios 1 and 2. The probability of executing\nthe action selected by the agent decreases when p_lazy and p_rand increase, and so the difference\nbetween the optimal value function and the optimal state-action value function decreases sharply.\nFrozenLake (Figures 14a, 14b, 14e and 14f) represents an exception as the sum of the reciprocals\nof the sub-optimality gaps is almost constant. FrozenLake naturally incorporates an exceptionally\nhigh level of stochasticity and so varying p_lazy and p_rand does not significantly affect the value\nfunctions. The sum of the reciprocals of the sub-optimality gaps increases almost linearly with the\nnumber of states. This is explained by the fact that the average value of the additional terms in the\nsummation is often similar to the average value of the existing terms given the same structure of\nreward and transition kernels. This measure of hardness is not particularly apt at capturing estimation\ncomplexity, since it focuses solely on optimal policy identification. It also underestimates the increase\nin hardness induced by an increase in visitation complexity.\n\nCumulative regret of the tuned near-optimal agent. The trends of the cumulative regret of the\ntuned near-optimal agent present more variability when compared to the theoretical measures of\nhardness. This reflects the fact that this is an approximation of a complete measure of hardness\nbased on agents that have specific strengths and weaknesses. Overall, we note a tendency of\nsuperlinear growth in scenarios 1 and 2. Such tendency is specifically marked for the grid worlds,\nsuch as MG-Empty (see Fig. 12) and SimpleGrid (see Fig. 13). In these MDP families, the highly\nrewarding states are located far from the starting states and therefore the visitation complexity plays\na fundamental role. In the FrokenLake family (see Fig. 14), the trend is linear (episodic setting)\nor sub-linear (continuous setting), which is caused by the relatively low impact of the parameters\np_rand and p_lazy in the already highly stochastic MDPs. The cumulative regret of the tuned\nnear-optimal agent presents a moderately superlinear growth in the episodic case and remains almost\nconstant for the RiverSwim family (see Fig. 15). This results from the fact that, in the continuous\ncase, the absence of the restarting mechanism in combination with the chain structure of the MDP\nallows the agent to suffer only minimal impact from the increasing values of the parameters p_rand\nand p_lazy. Finally, for the DeepSea family, the regret is constant. Increases in p_rand dramatically\nreduce the possibility of visiting the highly rewarding state due to the pyramid structure of the MDP.\nIn scenarios 3 and 4, the overall tendency is still superlinear but less marked compared to scenarios\n1 and 2. The superlinear growth is most evident for the MG-Empty family (see Fig. 12) and for the\nRiverSwim family in the episodic case (see Figures 15c and 15d). For the MG-Empty, when the\ngrid size is increased, and with it the number of states, the MDP becomes increasingly challenging\nto navigate since the agent has to coordinate its rotation with its forward movement in order to\neffectively transition between states. For the RiverSwim family, the challenge comes from the\nrestarting mechanism on a chain structure. The agent is required to take a perfect sequence of actions\nin order to visit the last state of the chain, otherwise it will be reset to the start. In the continuous\nsetting (see Figures 15g and 15h), instead, the trends are mostly linear, similarly to what happens in\nscenarios 1 and 2. The less challenging structure of the SimpleGrid family (see Fig. 13) induces\nweakly superlinear trends of the cumulative regret of the tuned near-optimal agent. We note that,\ncontrary to the theoretical measures of hardness, the episodic setting does not appear to be harder\n(which would be suggested by steeper trends). This discrepancy is particularly noticeable in the\n\n27\n\n\fFrozenLake family (see Fig. 14) which yields a mostly linear trend in the continuous settings and\nclearly sublinear trends in the episodic settings. In the DeepSea family, the cumulative regret of the\ntuned near-optimal agent is almost constant in scenario 3 and almost linear in scenario 4. The main\nchallenge for this family lies in the pyramidal structure of the MDP rather than the number of states.\nHowever, setting p_rand = 0.1 creates a more challenging task for the agent as more time steps are\nrequired to find the highly rewarding state. We also note that the difference in results between the\nepisodic and continuous settings is minimal, which is unsurprising given the MDP structure.", "appendix_E": "\n\nIn the following sections, we provide details on the selection methodology for the environments in\nthe benchmark, and we explain the full benchmarking procedure from the hyperparameters selection\nto the benchmark evaluation.\n\nE.1 Benchmark environments selection\n\nThe environments in the benchmark have been selected to be as diverse as possible with respect to the\ndiameter and the environmental value norm. Based on the theoretical properties of these measures\nand the results of the empirical comparison in Section D, we believe that they represent valid proxies\nfor the visitation complexity and the estimation complexity. The candidate environments have been\nsampled from a set of parameters such that their diameter is less than 100 and the environmental\nvalue norm is less than 3.5. This guarantees a sufficient challenge for the reinforcement learning\nagents while limiting the scale of the environments.\n\nFigure 17 represents the benchmark MDPs placed according to their diameter and environmental\nvalue norm. The selection features MDPs with varying combinations of values of diameter in the\ninterval [20, 100] and environmental value norm in the interval [0, 3.5]\n\nFigure 17: Positions in measure of hardness space of the set of MDPs in the benchmark.\n\nE.2 Hyperparameter selection\n\nThe hyperparameter selection procedure is to be considered an integral component of the Colosseum\nbenchmarking procedure to ensure fair hyperparameter tuning.\n\nEach Colosseum agent is required to define a sampling space for all its parameters. These sampling\nspaces are used by the package to conduct a random search optimization procedure with the objective\nof minimizing the cumulative regret across a set of randomly sampled environments. The random\nsampling procedure for environments is defined for each Colosseum MDP family and aims to provide\na varied set of MDPs of up to moderate scale. Note that the MG-DoorKey family is excluded from\nthe hyperparameter selection as it is weakly communicating in the continuous case. Tutorials on how\nto implement the aforementioned functions for novel agents and environments are available online.\n\nThe hyperparameters for the agents employed in the paper have been obtained with 50 samples from\nthe hyperparameter spaces, which have been evaluated on 12 MDPs from each family, for a total of\n84 MDPs with a training time of 20 minutes and a maximum number of total time steps of 200 000.\n\nE.3 Computational resources\n\nThe experiments for the benchmarking procedure have been carried out using CPUs from the Queen\nMary University of London Apocrita HPC facility. Note that, due to the time constraint imposed\nby Colosseum, the computational resources required to run the benchmark are bounded, and the\nbenchmarking procedure is easily parallelizable.\n\n31\n\n255075100Diameter0.00.51.01.52.02.5Value norm12312123451231212341255075100Diameter0.51.01.52.01112345678121234123150100Diameter01231211234512341212341250100Diameter012312112345671231212312DeepSeaFrozenLakeMiniGridEmptyMiniGridRoomsRiverSwimSimpleGridTaxi\fE.4 Tabular setting\n\nExperimental procedure. We set the total number of time steps to 500 000 with a maximum\ntraining time of 10 minutes for the tabular setting and 40 minutes for the non-tabular case. If an agent\ndoes not reach the maximum number of time steps before this time limit, learning is interrupted,\nand the agent keeps using its last best policy. This guarantees a fair comparison between agents\nwith different computational costs. The performance indicators are computed every 100 time steps.\nEach interaction between an agent and an MDP is repeated for 20 seeds. The per-step normalized\ncumulative regret (defined in App. C) is employed as a performance measure since it provides a\nunified scale across different MDPs.\n\nBenchmark hardness.\nIn order to illustrate how hardness measures relate to cumulative regret in\nthe benchmark, Figures 18a, 18b, 18c, and 18d place the average cumulative regret obtained by each\nagent in each benchmark MDP in a coordinate that corresponds to the diameter and the environmental\nvalue norm of that MDP. In the episodic setting (Figures 18a and 18b), we note that the environmental\nvalue norm has an evident impact on the Q-learning agent, whereas the effect of the diameter is most\nnoticeable in the communicating case. Still, in the episodic setting, the diameter has a small influence\ncompared to the environmental value norm for PSRL. In the continuous setting (Figures 18c and 18d),\nthere is generally a positive relationship between both of these hardness measures and the average\ncumulative regret for UCRL2. For Q-learning and PSRL, the diameter seems to have a generally\nsmaller influence on the average cumulative regret.\n\nCumulative regret plots. Figures 19, 20, 21, and 22 report the expected cumulative regrets for the\nagents during the agent/MDP interactions along with the cumulative regret of an agent that selects\naction at random, which provides an informative baseline. Contrary to the episodic setting, in the\ncontinuous setting, the training of UCRL2 and PSRL is stopped for several MDPs of the benchmark.\nFor PSRL, this typically happens before reaching 10 000 time steps, which is particularly damaging.\nAt this point, the agent has not properly explored the MDP and so it is forced to continue the interaction\nfollowing a policy that yields a regret similar to the one of the random agent. UCRL2, instead, tends\nto terminate the allocated training time at later time steps, which penalizes the performance less.\n\nCumulative regret tables.\nIn Tables 11, 12, 13, and 14, we report the per-step normalized regrets\nwith standard deviations along with the number of seeds for which the agent has been able to complete\nthe total number of training time steps before exceeding the time limit. We highlight in bold the best\nperforming agent for each MDP. The same information has been summarized in Table 2 (", "system_prompts": [{"role": "system", "content": "You are a computer science researcher currently reviewing a paper titled \"Hardness in Markov Decision Processes: Theory and Practice\" for the NeurIPS computer science conference. Your goal is to try to be as objective and truthful as possible in your answers about the paper provided. Your reviews will be used for causal reasoning in determining the quality of the paper."}], "prompts": {"1b": {"role": "user", "content": "The following is the conclusion section of the paper you are reviewing:\n\n\nWe established the usefulness of the theory of hardness in empirical reinforcement learning. Prior\nto our work, hardness measures were limited to providing theoretical guarantees for agents. In\norder to promote a wider understanding of these measures, we presented a systematic survey that\nnewly identified two major approaches for characterizing hardness: Markov chain-based and value-\nbased. These approaches aim to capture complementary aspects of hardness: visitation complexity\nand estimation complexity. Our survey also exposed a relative lack of measures that capture both\naspects, which motivates our definition of complete measures of hardness. Their development is\nimportant theoretically, elucidating what makes a problem hard for a specific performance criterion,\nand empirically, allowing the creation of principled benchmarks for a specific performance criterion.\n\nWe presented the first empirical study of (efficiently computable) hardness measures. This study\nrevealed which aspects of hardness current measures capture and clarified their relationship with\nthe behavior of near-optimal agents. Based on these results, we proposed a benchmark for the most\nwidely studied tabular reinforcement learning settings that contains environments that maximize\ndiversity with respect to two highly distinct measures. Such a principled benchmark is valuable to\ngauge progress in the field. The new benchmark allowed conducting the most exhaustive empirical\ncomparison between theoretically principled tabular reinforcement learning agents to date, which\nrevealed undocumented weaknesses of these agents and further validated our choices of environments.\n\nAs a first step towards principled non-tabular benchmarking, we argued that many commonly used\nenvironments can be encoded as BlockMDPs, which are non-tabular versions of tabular MDPs for\nwhich a partial characterization of hardness is already possible. We observed a clear empirical relation\nbetween two tabular hardness measures and the performance of four non-tabular agents. BlockMDPs\nrepresent a promising starting point for the future development of non-tabular hardness measures\nwhile already being useful to provide relevant insights into the performance of non-tabular agents.\n\nOur work has led to the development of Colosseum, a pioneering tool for empirical but theoretically\nprincipled study of tabular reinforcement learning with experimental non-tabular benchmarking\ncapabilities. Besides implementing the aforementioned tabular benchmark, Colosseum provides\nvaluable analysis tools: regret and hardness computations, communication class identification,\nlogging, and visualizations. Colosseum can also be easily extended and integrated with new agents\nand environments, for which we will actively seek contributions from the community. We strongly\nbelieve that Colosseum has the potential to become a fundamental tool in reinforcement learning.\n\n10\n\n204060800.250.500.751.001.251.501.752.00Value norm20406080100DiameterValue norm20406080100DiameterValue norm20406080100DiameterValue norm(c) BootDQN(a) ActorCritic(b) ActorCriticRNN(d) DQN100DiameterMG-Rooms (1)MG-Rooms (2)MG-Empty (2)MG-Empty (1)MG-Empty (4)MG-Empty (5)MG-Empty (7)MG-Empty (6)RiverSwim (1)MG-Empty (3)MG-Empty (8)SimpleGrid (2)SimpleGrid (3)SimpleGrid (1)FrozenLake (1)Taxi (1)RiverSwim (2)RiverSwim (3)RiverSwim (4)DeepSea (1)MG-Rooms (1)MG-Rooms (2)MG-Empty (2)MG-Empty (1)MG-Empty (4)MG-Empty (5)MG-Empty (7)MG-Empty (6)RiverSwim (1)MG-Empty (3)MG-Empty (8)SimpleGrid (2)SimpleGrid (3)SimpleGrid (1)FrozenLake(1)Taxi (1)RiverSwim (2)RiverSwim (3)RiverSwim (4)DeepSea (1)MG-Rooms (1)MG-Rooms (2)MG-Empty (2)MG-Empty (1)MG-Empty (4)MG-Empty (5)MG-Empty (7)MG-Empty (6)RiverSwim (1)MG-Empty (3)MG-Empty (8)SimpleGrid (2)SimpleGrid (3)SimpleGrid (1)FrozenLake (1)Taxi (1)RiverSwim (2)RiverSwim (3)RiverSwim (4)DeepSea (1)MG-Rooms (1)MG-Rooms (2)MG-Empty (2)MG-Empty (1)MG-Empty (4)MG-Empty (5)MG-Empty (7)MG-Empty (6)RiverSwim (1)MG-Empty (3)MG-Empty (8)SimpleGrid (2)SimpleGrid (3)SimpleGrid (1)FrozenLake (1)Taxi (1)RiverSwim (2)RiverSwim (3)RiverSwim (4)DeepSea (1)\f\n\nThe following is the appendix_D section of the paper you are reviewing:\n\n\nIn our empirical investigation of the measures of hardness, we consider five MDP families (MG-Empty,\nSimpleGrid, FrozenLake, RiverSwim, and DeepSea) that include different levels of stochasticity\nand challenge. Each MDP family is tested in four scenarios that highlight different aspects of hardness.\nNote that each measure has been normalized (as described in App. C), which solely allows comparing\ntrends (growth rates). Figures 12, 13, 14, 15 and 16 (pg. 28) report the results of our investigation\nalong with the 95% bootstrapped confidence intervals over twelve seeds.\n\nScenario 1. We vary the probability p_rand that an MDP executes a random action instead of the\naction selected by an agent. As p_rand increases, estimating the optimal value function becomes\neasier since every policy yields increasingly similar value functions. This produces a decrease in the\nestimation complexity. However, intentionally visiting states becomes harder.\n\nScenario 2. We vary the probability p_lazy that an MDP stays in the same state instead of\nexecuting the action selected by an agent. Contrary to increasing p_rand, increasing p_lazy never\nbenefits exploration through the execution of random actions. Increasing p_lazy decreases estimation\ncomplexity and increases visitation complexity.\n\nScenario 3 and 4. We vary the number of states across MDPs from the same family. In scenario 4,\nwe also let p_rand = 0.1 to study the impact of stochasticity. In these scenarios, increments in the\nnumber of states increase both estimation complexity and visitation complexity.\n\nCumulative regret of a near-optimal agent.\nIn every scenario, the measures of hardness are com-\npared with the cumulative regret of a near-optimal agent that serves as an optimistic approximation\nof a complete measure of hardness. The near-optimal agents have been chosen between the ones\navailable in Colosseum with the lowest average cumulative regret in the benchmarking results (such\nas PSRL in the episodic setting and UCRL2 in the continuous setting). In order to optimistically\napproximate a complete measure of hardness, we tune the hyperparameters of the agents for each\nMDP in every scenario. Concretely, we perform a random search with the objective of minimizing\nthe average cumulative regret resulting from an interaction of the agent with the MDP that lasts for\n200 000 time steps with a maximum time limit of two minutes across three seeds. The budget for the\nrandom search is 120 samples.\n\nComputational power. The empirical investigation has been carried out on a desktop PC equipped\nwith an AMD Ryzen 9 5950X 16-Core Processor and required less than 24 hours for all the MDP fam-\nilies and scenarios. The most computationally intensive part of the procedure is the hyperparameter\nsearch.\n\nLimitations. The main limitation of our empirical investigation is the selection of the MDP families,\nnear-optimal agents, and scenarios. Although we believe to have proposed a solid methodology, we\nare open to discussing the inclusion of additional experiments to further enhance Colosseum.\n\nD.1 Analysis of results\n\nDiameter. The diameter grows superlinearly with both p_rand and p_lazy since deliberate move-\nment between states requires an exponentially increasing number of time steps. As clearly shown in\nthe figures, this phenomenon is exacerbated in the episodic setting. If the agent is forced to take a\nrandom action or to stay in the same state, it can miss the opportunity to reach the target state in the\ncurrent episode and has to try again in the next episode. Although the diameter highlights this sharply\nincreasing visitation complexity, its trend overestimates the increase in cumulative regret of the\ntuned near-optimal agent, which is explained by the unaccounted decrease in estimation complexity.\nThe diameter also increases almost linearly with the number of states. When p_rand is relatively\nsmall, an approximately linear relationship can still be observed. This linear trend underestimates the\nnon-linear growth in hardness clearly shown in the cumulative regret of the tuned near-optimal agent\nbut is in line with the mild increase in visitation complexity. FrozenLake in the episodic setting\n(Figures 14c and 14d) represents the only exception. Given the extremely high level of stochasticity\nof the MDP, increasing the number of states drastically increases the visitation complexity while\nmaking it easier for the agent to act near-optimally.\n\n26\n\n\fEnvironmental value norm. The environmental value norm decreases as p_lazy and p_rand\nincrease because the optimal value of neighboring states becomes closer, which decreases the per-step\nvariability of the optimal state value function. However, we note that for the MG-Empty and the\nFrozenLake MDP families in the continuous cases (see Figures 12f and 14f) as p_lazy increases,\nthe environmental value norm first decreases and later increases. From a certain value of p_lazy\nonward, there is a significant probability of the agent remaining in the same state. This provokes\nlarge changes in the value of states that are distant from the highly rewarding states and no changes at\nall for highly rewarding states since the lazy transition is comparable to taking the optimal action.\nDue to the large changes in the suboptimal region of the state space and the absence of changes in the\noptimal region of the state space, the overall one-step variability of the state value function increases.\nNote that this does not happen in the episodic case due to the restarting mechanism and whether it\nhappens or not in the continuous case depends on the transition and reward structure of an MDP.\nWhen the number of states increases but the transition and reward structures remain the same, the\nsmall increase in measured variability only causes the environmental value norm to grow sublinearly.\nThese findings are strong evidence that this measure is only suited to capture estimation complexity.\n\nSum of the reciprocals of the sub-optimality gaps. The sum of the reciprocals of the sub-\noptimality gaps increases weakly superlinearly in scenarios 1 and 2. The probability of executing\nthe action selected by the agent decreases when p_lazy and p_rand increase, and so the difference\nbetween the optimal value function and the optimal state-action value function decreases sharply.\nFrozenLake (Figures 14a, 14b, 14e and 14f) represents an exception as the sum of the reciprocals\nof the sub-optimality gaps is almost constant. FrozenLake naturally incorporates an exceptionally\nhigh level of stochasticity and so varying p_lazy and p_rand does not significantly affect the value\nfunctions. The sum of the reciprocals of the sub-optimality gaps increases almost linearly with the\nnumber of states. This is explained by the fact that the average value of the additional terms in the\nsummation is often similar to the average value of the existing terms given the same structure of\nreward and transition kernels. This measure of hardness is not particularly apt at capturing estimation\ncomplexity, since it focuses solely on optimal policy identification. It also underestimates the increase\nin hardness induced by an increase in visitation complexity.\n\nCumulative regret of the tuned near-optimal agent. The trends of the cumulative regret of the\ntuned near-optimal agent present more variability when compared to the theoretical measures of\nhardness. This reflects the fact that this is an approximation of a complete measure of hardness\nbased on agents that have specific strengths and weaknesses. Overall, we note a tendency of\nsuperlinear growth in scenarios 1 and 2. Such tendency is specifically marked for the grid worlds,\nsuch as MG-Empty (see Fig. 12) and SimpleGrid (see Fig. 13). In these MDP families, the highly\nrewarding states are located far from the starting states and therefore the visitation complexity plays\na fundamental role. In the FrokenLake family (see Fig. 14), the trend is linear (episodic setting)\nor sub-linear (continuous setting), which is caused by the relatively low impact of the parameters\np_rand and p_lazy in the already highly stochastic MDPs. The cumulative regret of the tuned\nnear-optimal agent presents a moderately superlinear growth in the episodic case and remains almost\nconstant for the RiverSwim family (see Fig. 15). This results from the fact that, in the continuous\ncase, the absence of the restarting mechanism in combination with the chain structure of the MDP\nallows the agent to suffer only minimal impact from the increasing values of the parameters p_rand\nand p_lazy. Finally, for the DeepSea family, the regret is constant. Increases in p_rand dramatically\nreduce the possibility of visiting the highly rewarding state due to the pyramid structure of the MDP.\nIn scenarios 3 and 4, the overall tendency is still superlinear but less marked compared to scenarios\n1 and 2. The superlinear growth is most evident for the MG-Empty family (see Fig. 12) and for the\nRiverSwim family in the episodic case (see Figures 15c and 15d). For the MG-Empty, when the\ngrid size is increased, and with it the number of states, the MDP becomes increasingly challenging\nto navigate since the agent has to coordinate its rotation with its forward movement in order to\neffectively transition between states. For the RiverSwim family, the challenge comes from the\nrestarting mechanism on a chain structure. The agent is required to take a perfect sequence of actions\nin order to visit the last state of the chain, otherwise it will be reset to the start. In the continuous\nsetting (see Figures 15g and 15h), instead, the trends are mostly linear, similarly to what happens in\nscenarios 1 and 2. The less challenging structure of the SimpleGrid family (see Fig. 13) induces\nweakly superlinear trends of the cumulative regret of the tuned near-optimal agent. We note that,\ncontrary to the theoretical measures of hardness, the episodic setting does not appear to be harder\n(which would be suggested by steeper trends). This discrepancy is particularly noticeable in the\n\n27\n\n\fFrozenLake family (see Fig. 14) which yields a mostly linear trend in the continuous settings and\nclearly sublinear trends in the episodic settings. In the DeepSea family, the cumulative regret of the\ntuned near-optimal agent is almost constant in scenario 3 and almost linear in scenario 4. The main\nchallenge for this family lies in the pyramidal structure of the MDP rather than the number of states.\nHowever, setting p_rand = 0.1 creates a more challenging task for the agent as more time steps are\nrequired to find the highly rewarding state. We also note that the difference in results between the\nepisodic and continuous settings is minimal, which is unsurprising given the MDP structure.\n\nBased on the section(s), please answer the following question with yes, no, or n/a and provide a brief justification for your answer.\nQuestion: Do the authors describe the limitations of their work?"}, "2a": {"role": "user", "content": "The following is the methods section of the paper you are reviewing:\n\n\nSection 2.1 introduces our notation and important definitions. Section 2.2 presents our survey of\nthe theoretical landscape of measures of hardness, which includes a novel categorization of existing\nmeasures. Section 2.2.3 highlights the weaknesses of existing measures and introduces the concept\nof a complete measure of hardness, which we believe should be the focus of future developments.\n\n2.1 Preliminaries\n\nLet \u2206(X ) denote the set of probability distributions over a set X . A finite Markov decision process\n(MDP) is a tuple M = (S, A, P, P0, R), where S is the finite set of states, A is the finite set of\nactions, P : S \u00d7 A \u2192 \u2206(S) is the transition kernel, P0 \u2208 \u2206(S) is the initial state distribution, and\nR : S \u00d7 A \u2192 \u2206 ([0, 1]) is the reward kernel [6]. Given an optimization horizon T , a reinforcement\nlearning agent aims to find a (possibly stochastic) policy \u03c0 : S \u2192 \u2206(A) that optimizes a reward-\nbased criterion. The MDP M is unknown to the agent but can be learned through experience. The\ninteraction between the agent and the environment starts when the initial state s0 \u223c P0 is drawn.\nFor any 0 \u2264 t < T , the agent samples an action at \u223c \u03c0t(st) from its current policy \u03c0t, and the\nenvironment draws the next state st+1 \u223c P (st, at) and reward rt+1 \u223c R(st, at). An MDP is\nepisodic with time horizon H when the state st+1 is drawn from the initial state distribution whenever\nt + 1 \u2261 0 (mod H) and continuous otherwise. In the continuous setting, a factor \u03b3 \u2208 (0, 1) is used\nto discount future rewards in the discounted setting, and future rewards are averaged across time\nin the undiscounted setting. For a policy \u03c0, an MDP M induces a Markov chain [7] with transition\nprobabilities P\u03c0\na\u2208A \u03c0(a | s)P (s\u2032 | s, a) whose stationary distribution is denoted by\n\u00b5\u03c0. MDPs can be classified into three communication classes [8]. An MDP is ergodic if the Markov\nchain induced by any deterministic policy is ergodic. An MDP is communicating if, for every two\nstates s, s\u2032 \u2208 S, there is a deterministic policy that induces a Markov chain where s is accessible\n\ns\u2192s\u2032(M) = (cid:80)\n\n2\n\n\ffrom s\u2032 and vice versa. An MDP is weakly communicating when there is a partition (C, S \\ C) of the\nset of states S such that, for every two states s, s\u2032 \u2208 C, there is a deterministic policy that induces a\nMarkov chain where s is accessible from s\u2032 and vice versa, and every state s \u2208 S \\ C is transient in\nevery Markov chain induced by any deterministic policy. Every ergodic MDP is communicating, and\nevery communicating MDP is weakly communicating.\n\n(cid:104)(cid:80)H\n\n(cid:105)\nt=h+1 rt|sh = s, ah = a\n\nh,epi(s, a) := E\nh,epi(s) := (cid:80)\n\u03b3 (s, a) := E (cid:2)(cid:80)\u221e\n\u03b3 (s) := (cid:80)\n\nThe episodic state-action value function is given by Q\u03c0\n,\nand the episodic state value function is given by V \u03c0\na \u03c0(a | s)Q\u03c0\nh,epi(s, a). The dis-\nt=1 \u03b3t\u22121rt | s0 = s, a0 = a(cid:3), and\ncounted state-action value function is given by Q\u03c0\nthe discounted state value function is given by V \u03c0\n\u03b3 (s, a). These expectations\nare taken w.r.t. the policy \u03c0, the transition kernel P , and the reward kernel R. In the undiscounted\nsetting, the value of every state(-action) is the same, since rewards are averaged across infinite time\n(cid:80)T\nE [rt], which\nsteps. In that case, we define the expected average reward as \u03c1\u03c0 := limT \u2192\u221e\ns = Ea\u223c\u03c0(s)R(s, a) is the expected\nis also given by \u03c1\u03c0 = \u27e8\u00b5\u03c0, R\u03c0\u27e9, where R\u03c0 is a vector such that R\u03c0\nreward obtained when following \u03c0 from state s. An optimal policy \u03c0\u2217 obtains maximum value for\nevery state. We assume with little loss of generality that the optimal policy is unique. We drop the\nsubscripts when the setting is clear from the context, and write \u2217 to denote \u03c0\u2217 in superscripts.\n\na \u03c0(a | s)Q\u03c0\n\nt=1\n\n1\nT\n\nThe most widely studied performance criteria are the expected cumulative regret, which yields the\nregret minimization setting [9], and the sample efficiency of exploration, which yields the Probably\nApproximately Correct Reinforcement Learning (PAC-RL) setting [10]. In simple terms, the regret\nmeasures the loss in reward due to the execution of a sub-optimal policy. In contrast, the sample\nefficiency measures how many interactions the agent requires to approximately learn \u03c0\u2217 with high\nprobability. The main difference between the two criteria is that the rewards obtained during the\ninteractions with the MDP are not considered in PAC-RL, whereas all rewards contribute to the\ncumulative regret. Therefore, PAC-RL agents can generally afford more aggressive exploration.\n\n2.2 Characterization of hardness\n\nWe distinguish the hardness of MDPs into two kinds of complexity, the visitation complexity and the\nestimation complexity. The visitation complexity relates to the difficulty of visiting all the states, and\nthe estimation complexity relates to the discrepancy between the optimal policy and the best policy\nan agent can derive from a given estimate of the transition and reward kernels. The two complexities\nare complementary in the sense that the former quantifies the hardness of gathering samples from the\nstate space while the latter quantifies the hardness of producing a highly rewarding policy given the\nsamples. In the literature, we identify two approaches that aim to capture the mentioned complexities.\nThe first approach, which we call Markov chain-based, considers that an MDP is an extension of\na Markov chain where transition probabilities can be changed based on direct interventions by an\nagent [11]. This approach is well suited to capture the hardness that comes from the visitation\ncomplexity since it considers the properties of transition kernels. The second approach, which we call\nvalue-based, considers the discrepancy between the optimal policy and the best policy an agent can\nderive from a value function point estimate. Note that the information contained in such point estimate\nis lower than the one in kernels estimate and that the difficulty of obtaining an accurate estimate of\nthe value function is not considered in this approach. Therefore, the value-based approach is only\nable to partially capture the estimation complexity. A striking fact that highlights this shortcoming is\nthat almost every value-based measure of hardness is independent of the variability of the reward\nkernel. Therefore, given an MDP M\u2032 that is obtained by increasing the reward kernel variability of an\nMDP M, value-based measures of hardness assign the same level of hardness to M and M\u2032.\n\nMarkov chain properties and value functions depend on a fixed policy, which presents two natural\nchoices to derive measures of hardness. The first choice considers the optimal policy, which typically\nleads to a measure that considers a best-case scenario. The second choice considers a policy that\nmaximizes a criterion that characterizes a worst-case scenario. For instance, a policy that maximizes\nsuch criterion may spend its time in a region of the state space that is not relevant for learning \u03c0\u2217.\n\n3\n\n\f2.2.1 Markov chain-based measures of hardness\n\nMixing time. The mixing time of a Markov chain with stationary distribution \u00b5 is defined as\n\nt\u00b5 := inf{n | sup\ns\u2208S\n\ndTV (pn\n\ns , \u00b5) \u2264 0.25},\n\n(1)\n\nwhere dTV is the total variation distance between distributions and the vector pn\ns represents the\ndistribution over states after n steps starting from state s. The value 0.25 is conventionally established\nin the Markov chain literature for the definition of the mixing time. For ergodic and aperiodic Markov\nchains, limn\u2192\u221e pn\ns = \u00b5 for every state s, so the mixing time is the number of steps a Markov chain\ntakes to produce samples that are close to being distributed according to the stationary distribution \u00b5.\n\nFor instance, a Markov chain with a non-negligible probability of transitioning from every state to\nevery state is quickly mixing. In contrast, the mixing time can be very long in chains where the state\nspace has several distinct regions each of which is well connected but where transitions between\nregions have low probability [12]. Kearns and Singh [13] propose an extension of the mixing time to\nMDPs that considers the maximum mixing time across policies t := sup\u03c0 t\u00b5\u03c0 in the undiscounted\nsetting. Although the mixing time plays an important role in that setting, since the average reward\nobtained by policy \u03c0 is given by \u03c1\u03c0 = \u27e8\u00b5\u03c0, R\u03c0\u27e9, it is not a generally good measure of hardness. First,\nit does not capture the visitation complexity, since it neglects the fact that the agent may direct its\nexploration through a choice of policy. Second, it does not capture any significant aspect of optimal\npolicy estimation, which does not require stationary distribution samples from every policy.\n\nDiameter. The diameter is fundamentally related to the number of time steps required to transition\nbetween states. In the continuous setting, the diameter D is most commonly defined as\n\nD := sup\ns1\u0338=s2\n\ninf\n\u03c0\n\nT \u03c0\ns1\u2192s2\n\n,\n\n(2)\n\nwhere T \u03c0\ns1\u2192s2 is the expected number of time steps required to reach state s2 from state s1 when\nfollowing policy \u03c0 [14]. Intuitively, D is the worst-case expected number of time steps required to\ntransition between two states when following the best policy for that purpose. Related definitions are\nDworst := sup\n\u03c0\n\nT \u03c0\ns1\u2192s2, which imply D \u2264 Dopt \u2264 Dworst [15].\n\nT \u03c0\ns1\u2192s2 and Dopt := inf\n\u03c0\n\nsup\ns1\u0338=s2\n\nsup\ns1\u0338=s2\n\nIn the episodic setting, we define the diameter by augmenting each state s with the current in-episode\ntime step h and considering the diameter of this augmented MDP in the continuous setting. Note\nthat T \u03c0\n(s1,h1)\u2192(s2,h2) can be larger than the episode length H, which means that (on average) more\nthan one episode may be required to transition from state (s1, h1) to state (s2, h2). This happens\nwhenever h2 < h1, which may be undesirable if the intent is to focus on the expected number of time\nsteps required to transition between states from the same episode. The diameter is always infinite\nin weakly-communicating MDPs if the supremum is not restricted to states in the recurrent class\nC and is always finite in the episodic setting (where every state is reachable within an episode). A\nlarge diameter can be caused by high stochasticity. The diameter is very apt at measuring visitation\ncomplexity, since it captures the effort required to deliberately move between states. However, it\nneglects the reward kernel, and so has limited capacity to measure the estimation complexity.\n\nDistribution mismatch coefficient. The distribution mismatch coefficient (DMC) has been defined\nfor the continuous undiscounted [16] and continuous discounted [17] cases respectively as\n\nDMC := sup\n\n\u03c0\n\n(cid:88)\n\ns\u2208S\n\n\u00b5\u2217\ns\n\u00b5\u03c0\ns\n\nand\n\nDMCs0 := sup\ns\u2208S\n\nd\u2217\n(s)\ns0\nP0(s)\n\n,\n\n(s) = (1 \u2212 \u03b3) (cid:80)\u221e\n\nt=0 \u03b3tPr(st = s | s0) is the discounted state visitation distribution of the\nwhere d\u2217\ns0\noptimal policy given an initial state s0. Note that the DMC is guaranteed to be finite only for ergodic\nMDPs. In communicating MDPs, there is at least one policy \u03c0 whose stationary distribution assigns\nprobability zero to some states. MDPs whose optimal stationary distribution \u00b5\u2217 has its probability\nmass concentrated on a few states tend to have a large DMC. In contrast, when \u00b5\u2217 is closer to being\nuniformly distributed across states, the DMC tends to be small. For small values of DMC, as every\n\u00b5\u03c0 is close to \u00b5\u2217, the agent will gather samples from the optimal stationary distribution regardless of\nits current policy, which may enable quick learning. In contrast, for large values of DMC, the agent\nneeds to actively seek a policy that gathers such samples. The DMC is not well suited to quantify\n\n4\n\n\fthe visitation complexity, since it fails to capture the difficulty of visiting all states. The DMC also\ndoes not capture the estimation complexity, since it does not account for the stochasticity of the\nenvironment, which is related to the number of samples required to make accurate estimations.\n\n2.2.2 Value-based measures of hardness\n\nAction-gap regularity. Given an estimate \u02c6Q\u2217 of the optimal state-action value function Q\u2217, the\ngreedy policy with respect to \u02c6Q\u2217 always chooses an action associated with the highest state-action\nvalue. Whether or not such a policy is optimal depends exclusively on the ordering of the estimates\nfor a given state rather than their accuracy. For instance, assuming that a\u2217 is the optimal action for\nevery state s, a greedy agent would act optimally if \u02c6Q(s, a\u2217) > \u02c6Q(s, a\u2032) for every action a\u2032 \u0338= a\u2217,\neven if |Q\u2217(s, a) \u2212 \u02c6Q(s, a)| \u226b 0 for every action a. The action-gap regularity \u03b6 is a measure of\nhardness that leverages this principle through the theory of hardness for classification algorithms [18].\nHowever, this measure is only defined for two actions, and so has exceptionally limited applicability.\n\nEnvironmental value norm. The (discounted) environmental value norm C \u03c0\n\n\u03b3 is defined as\n\n(cid:114)\n\nC \u03c0\n\n\u03b3 := sup\n(s,a)\n\nVar\ns\u2032\u223cP (s,a)\n\n\u03b3 (s\u2032).\nV \u03c0\n\nThis quantity can be similarly defined in the undiscounted setting [19]. In words, the environmental\nvalue norm captures the one-step variance of the value function V \u03c0 for a given policy \u03c0. In the\nepisodic setting, a closely related measure called maximum per-step conditional variance C \u03c0\nH [20] is\ndefined as\n\n(cid:18)\n\nC \u03c0\n\nH := sup\n(s,a,h)\n\nVar R(s, a) + Var\n\ns\u2032\u223cP (s,a)\n\n(cid:19)\n\nh+1(s\u2032)\nV \u03c0\n\n.\n\nAlternatively, as with the diameter, it is also possible to define this quantity by augmenting each\nstate s with the current in-episode time step h and considering the environmental value norm of\nthis augmented MDP in the continuous setting. For every policy, the environmental value norm is\nequal to zero when the transition kernel is deterministic. However, a highly stochastic MDP may\nstill have a small environmental value norm, since this norm captures the variance of the state value\nfunction rather than the stochasticity of the transition kernel. Maillard et al. [19] suggest using the\nenvironmental value norm of the optimal policy \u03c0\u2217 as a measure of hardness. The main strength of\nsuch measure is that the variability of the optimal value function captures an important aspect of the\nestimation complexity. However, this measure of hardness neglects the visitation complexity.\n\nSub-optimality gap. The sub-optimality gap is defined in the continuous and episodic settings as\n\n\u2206(s, a) := V \u2217(s) \u2212 Q\u2217(s, a)\n\nand \u2206h(s, a) := V \u2217\n\nh (s) \u2212 Q\u2217\n\nh(s, a),\n\n(s,a)|\u2206(s,a)\u0338=0\n\nrespectively. Since V \u2217(s) = maxa Q\u2217(s, a) for every state s, the sub-optimality gap \u2206(s, a) mea-\nsures the difference in expected return between selecting the optimal action for state s and selecting\nthe action a. Intuitively, identifying a suboptimal action a\u2032 in a given state s is easier if the gap \u2206(s, a\u2032)\nis large. Simchowitz and Jamieson [21] identifies the sum of the reciprocals of the sub-optimality\ngaps (cid:80)\n\u2206(s,a) as a measure of hardness. They also demonstrate the importance of the\nsub-optimality gaps by showing that recent optimistic algorithms necessarily incur in a cumulative\nregret proportional to the smallest nonzero sub-optimality gap in the episodic setting. However, note\nthat approximating the optimal value function (and identifying a near-optimal policy) is particularly\neasy when every sub-optimality gap is small. Consequently, the (PAC-RL) sample complexity is\nlikely to decrease when the sum of the reciprocals of the sub-optimality gaps increases. Furthermore,\nthis measure does not explicitly capture visitation complexity and is prone to severe numerical issues.\n\n1\n\nTable 1: Computational complexity of generally applicable measures up to logarithmic factors.\n\nMarkov chain-based measures\n\nValue-based measures\n\nMixing time\n\n?\n\nDiameter\n\u02dcO(|S|3.5|A|)\n\nDistribution mismatch coefficient Environmental value norm Sub-optimality gaps\n\u02dcO(|S|2|A|(1 \u2212 \u03b3)\u22121)\n\n\u02dcO(|S|2|A|(1 \u2212 \u03b3)\u22121)\n\n?\n\n5\n\n\f2.2.3 Future directions\n\nCurrent hardness measures suffer from three principal issues. They are not designed to be efficiently\ncomputable (see Table 1 and Appendix B), they are limited in their ability to simultaneously capture\nvisitation complexity and estimation complexity, and they are oblivious to the distinct challenges\npresented by different performance criteria. For instance, while regret minimizing agents must be\ncautious not to incur in large regret during learning (for example, by not revisiting lowly rewarding\nstates that are not followed by highly rewarding states), PAC-RL agents have the flexibility to incur\nin large regret as long as they end up with a near-optimal policy. Therefore, the visitation complexity\nshould differ across settings. The fact that current measures disregard this distinction is concerning,\nsince they should account for the specific difficulty of the optimization task. These issues are not\ndiscussed in previous work, since measures of hardness have not been considered relevant outside the\ncontext of deriving theoretical performance guarantees for reinforcement learning agents.\n\nIn order to address these issues, we believe that future work should focus on developing efficiently\ncomputable (non-trivial) hardness measures that (approximately) meet the following novel definition.\nDefinition 2.1 (Complete measure of hardness) A measure \u03b8 : M \u2192 R+ is complete for an MDP\nclass M and criterion \u03c8 (sample complexity or cumulative regret) if, for every pair M1, M2 \u2208 M\nand near-optimal agent A\u2217 that achieves the criterion lower bound of class M up to logarithmic\nfactors1, \u03b8(M1) > \u03b8(M2) implies \u02dc\u03c8(M1, A\u2217) > \u02dc\u03c8(M2, A\u2217), where \u02dc\u03c8 hides logarithmic factors.\nCombining existing measures that capture visitation complexity and estimation complexity is a viable\nfirst step in that direction. Recently, Wagenmaker et al. [23] have pioneered this approach in the\nepisodic setting by proposing the gap-visitation complexity,\n\nGVP(\u03f5) :=\n\nH\n(cid:88)\n\nh=0\n\ninf\n\u03c0\n\nsup\ns,a\n\ninf\n\n(cid:18)\n\n1\nh (s, a)\u2206h(s, a)2 ,\nw\u03c0\n\nWh(s)2\nw\u03c0\nh (s, a)\u03f52\n\n(cid:19)\n\n,\n\n(3)\n\nwhere w\u03c0\nh (s, a) := P\u03c0(sh = s, ah = a) is the probability of visiting state-action pairs (s, a) at\nin-episode time step h when following policy \u03c0, Wh(s) := sup\u03c0 P\u03c0(sh = s) is the maximum\nreachability of state s at in-episode time step h, and \u03f5 is a parameter related to the optimality of the\noutput policy in the PAC-RL setting. The strength of this measure is that it weights the sub-optimality\naction gaps with measures of visitation complexity, w\u03c0\nh and Wh. This captures the difficulty induced\nby the critical states that are both hard to reach and for which it is hard to estimate the best action.\nHowever, the gap-visitation complexity fails to be a generally applicable hardness measure. It depends\non the PAC-RL setting-specific parameter \u03f5, it is restricted to the finite horizon setting (and can not\nbe extended to the continuous setting), and is not efficiently computable.\n\n\n\nBased on the section(s), please answer the following question with yes, no, or n/a and provide a brief justification for your answer.\nQuestion: If the authors include theoretical results, do the authors state the full set of assumptions of all theoretical results?"}, "3a": {"role": "user", "content": "The following is the experiments section of the paper you are reviewing:\n\n\n3.1 Colosseum\n\nThis section briefly introduces Colosseum, a pioneering Python package that bridges theory and\npractice in tabular reinforcement learning while also being applicable in the non-tabular setting. More\ndetails about the package can be found in Appendix A and in the project website.2\n\nAs a hardness analysis tool, Colosseum identifies the communication class of MDPs, assembles\ninsightful visualizations and logs of interactions between agents and MDPs, computes three measures\nof hardness (environmental value norm, sum of the reciprocals of the sub-optimality gaps, and\ndiameter, whose computation requires a novel solution described in App. A.4). Eight MDP families\nare available for experimentation. Some are traditional families (RiverSwim [24], Taxi [25], and\nFrozenLake) while others are more recent (MiniGid environments [26]). Additionally, DeepSea\n[27] was included as a hard exploration family of problems, and the SimpleGrid family is composed\nof simplified versions of the MG\u2013Empty environment. By controlling the parameters of MDPs from\neach family (further detailed in App. A.3), it is easy to create an MDP with any desired hardness.\n\nAs a benchmarking tool, Colosseum is unique in its strong connection with theory. For instance,\nin contrast to non-tabular benchmarks, Colosseum computes theoretical evaluation criteria such\n\n1For example, the \u2126(|S||A|H 2\u03f5\u22122) sample complexity bound for the episodic communicating setting [22].\n2Available at https://michelangeloconserva.github.io/Colosseum.\n\n6\n\n\fas the expected cumulative regret and the expected average future reward, which can be used to\nexactly evaluate the performance criterion of regret minimizing agents. The benchmark covers the\nmost commonly studied reinforcement learning settings: episodic ergodic, episodic communicating,\ncontinuous ergodic, and continuous communicating. For each setting, we have selected twenty MDPs\nthat are diverse with respect to their diameters and environmental value norms as proxies for different\ncombinations of visitation complexity and estimation complexity. Figure 17 in Appendix E shows\nhow each of these MDPs varies according to these measures, and Section 3.3 empirically validates\nthis selection by showing that harder MDPs correspond to worse agent performance. Notably, the\ntheoretically backed selection of MDPs and the rigorous evaluation criteria make the Colosseum\nbenchmark the most exhaustive in tabular reinforcement learning, since previous evaluations were\nconducted empirically in a few MDPs (such as Taxi or RiverSwim).\n\nColosseum also allows testing of non-tabular agents by leveraging the BlockMDP model [28].\nBlockMDPs equip tabular MDPs with an emission map that is a (possibly stochastic) mapping q : S \u2192\n\u2206(O) from the finite state space S to a (possibly infinite) observation space O. Agents interacting\nwith BlockMDPs are only provided with observations, so non-tabular methods are generally required.\nMany commonly used non-tabular MDPs (such as Minecraft [29]) can be straightforwardly encoded\nas BlockMDPs using the Colosseum MDP families. Colosseum implements a diverse set of\ndeterministic emission maps and allows combining them with different sources of noise. Appendix\nA.2 further details BlockMDPs and the available emission maps.\n\n3.2 Empirical analysis of hardness measures\n\nFor brevity, this section only presents results of hardness measures in the MiniGridEmpty family of\nenvironments in the episodic setting. Appendix D presents the full outcome of the empirical analysis.\n\nA MiniGridEmpty MDP is a grid world where an agent has three available actions: moving forward,\nrotating left, and rotating right. An agent is rewarded for being in a few specific states and receives no\nreward in every other state. Appendix A.3.4 provides more details about this family of environments.\n\nIn our investigation, we consider four scenarios that highlight the different aspects of MDP hardness.\n\nScenario 1. We vary the probability p_rand that an MDP executes a random action instead of the\naction selected by an agent. As p_rand approaches one, value estimation becomes easier, since\noutcomes depend less on agent choices. However, intentionally visiting states becomes harder.\n\nScenario 2. We vary the probability p_lazy that an MDP stays in the same state instead of executing\nthe action selected by an agent. Contrary to increasing p_rand, increasing p_lazy never benefits\nexploration. Increasing p_lazy decreases estimation complexity and increases visitation complexity.\n\nScenario 3 and 4. We vary the number of states across MDPs from the same family. In scenario\n4, we also let p_rand = 0.1 to study the impact of stochasticity. In these scenarios, increasing the\nnumber of states simultaneously increases the estimation complexity and the visitation complexity.\n\nIn every scenario, hardness measures are compared with the cumulative regret of a near-optimal agent\ntuned for each specific MDP (see App. D). This regret serves as an optimistic measure of hardness.\nAppendix C describes how these measures are normalized. Note that, due to normalization, the plots\nshould only be compared in terms of trends (growth rates) rather than absolute values.\n\nAnalysis. Figure 1 presents the empirical results for the episodic MiniGridEmpty family in the four\nscenarios with 95% bootstrapped confidence intervals over twelve random seeds.\n\nThe experiments confirm our claim that the diameter captures visitation rather than estimation\ncomplexity. This measure of hardness grows superlinearly with both p_rand and p_lazy (Figures\n1a and 1b) since deliberate movement between states requires an exponentially increasing number of\ntime steps. Although the diameter highlights the sharply increasing visitation complexity, its trend\noverestimates the increase in cumulative regret of the tuned near-optimal agent, which is explained\nby the unaccounted decrease in estimation complexity. The diameter also increases almost linearly\nwith the number of states (Figures 1c and 1d). For the small p_rand (scenario 4), the relation is still\napproximately linear. This linear trend underestimates the evident non-linear growth in hardness in\nthe regret of the tuned near-optimal agent but is in line with the mild increase in visitation complexity.\n\nThe empirical evidence indicates that the environmental value norm can only capture estimation\ncomplexity. It decreases as p_lazy and p_rand increase (Figures 1a and 1b) because the optimal\n\n7\n\n\fFigure 1: The Colosseum hardness analysis for the episodic MiniGridEmpty family.\n\nvalue of neighboring states becomes closer, which decreases the per-step variability of the optimal\nvalue function. When the number of states increases but the transition and reward structures remain\nthe same (Figures 1c and 1d), the small increase in this variability only generates a sublinear growth.\n\nWe empirically observe that the sum of the reciprocals of the sub-optimality gaps is not particularly\napt at capturing estimation complexity, due to its exclusive focus on optimal policy identification, and\nit also underestimates the increase in hardness induced by an increase in visitation complexity. This\nmeasure increases weakly superlinearly in scenarios 1 and 2 (Figures 1a and 1b). The probability\nof executing the action selected by the agent decreases when p_lazy and p_rand increase, so the\ndifference between the state and the state-action optimal value functions decreases sharply. The\nmeasure increases almost linearly with the number of states (Figures 1c and 1d). This is explained by\nthe fact that the average value of the additional terms in the summation is often similar to the average\nvalue of the existing terms when MDPs have the same structure of reward and transition kernels.\n\n3.3 Colosseum benchmarking\n\nIn this section, we benchmark five tabular agents with theoretical guarantees and four non-tabular\nagents. Besides being valuable on their own, these results help to empirically validate our benchmark.\n\nAgents. The tabular agents are posterior sampling for reinforcement learning (PSRL) for the episodic\nand continuous settings [30, 31], Q-learning with UCB exploration for the episodic setting [32],\nQ-learning with optimism for the continuous setting [16], and UCRL2 for the continuous setting [14].\nThe non-tabular agents (from bsuite) are ActorCritic, ActorCriticRNN, BootDQN, and DQN.\n\nExperimental procedure. We set the total number of time steps to 500 000 with a maximum training\ntime of 10 minutes for the tabular setting and 40 minutes for the non-tabular setting. If an agent\ndoes not reach the maximum number of time steps before this time limit, learning is interrupted,\nand the agent continues interacting using its last best policy. This guarantees a fair comparison\nbetween agents with different computational costs. The performance indicators are computed every\n100 time steps. Each interaction between an agent and an MDP is repeated for 20 seeds. The agents\u2019\nhyperparameters have been chosen by random search to minimize the average regret across MDPs\nwith randomly sampled parameters (see Appendix E). We use a deterministic emission map that\nassigns a uniquely identifying vector to each state (for example, a gridworld coordinate) to derive\nthe non-tabular benchmark MDPs. In Table 2, we report the per-step normalized cumulative regrets\ndivided by the total number of time steps (defined in Appendix C), which allows comparisons across\ndifferent MDPs. We summarize the main findings here and refer to Appendix E for further details.\n\nAnalysis. Table 2 often shows high variability in the performance of the same agent across MDPs of\nthe same family. Therefore, maximising the diversity across diameters and value norms effectively\nproduces diverse challenges even for MDPs with similar transition and reward structures. For example,\nin the continuous communicating case (Table 2d), Q-learning performs well only in some MDPs of\nthe MiniGridEmpty family. This also happens for UCRL2 for the SimpleGrid family.\n\nThe average normalized cumulative regret is lower in ergodic environments compared to commu-\nnicating environments. This indicates that the ergodic setting is generally slightly easier than the\ncommunicating settings. Notably, in the continuous setting, the ergodic setting is more challenging\nthan the communicating setting for Q-learning (Tables 2c and 2d). Designing a naturally ergodic\n\n8\n\n0.00.20.40.6Probability of random action0.00.20.40.60.81.0Normalized values(a) Scenario 10.00.20.40.6Probability of lazy action(b) Scenario 2100200300400Number of states(c) Scenario 3100200300400Number of states(d) Scenario 4DiameterEnvironmental value normSum ofthe reciprocals of the sub-optimality gapsCumulative regret oftuned near-optimal agent\fTable 2: Normalized cumulative regrets of selected agents on the Colosseum benchmark. (a) Episodic\nergodic. (b) Episodic communicating. (c) Continuous ergodic. (d) Continuous communicating.\n\n(a)\n\n(b)\n\n(c)\n\n(d)\n\nQ-learning\n\nPSRL\n\nQ-learning\n\nPSRL\n\nQ-learning\n\nPSRL\n\nUCRL2\n\nQ-learning\n\nPSRL\n\nUCRL2\n\nMDP\n\nDeepSea\n\n.64 \u00b1 .00\n.52 \u00b1 .01\n\n.01 \u00b1 .00\n.00 \u00b1 .00\n\nMDP\n\nDeepSea\n\n.01 \u00b1 .01\n.83 \u00b1 .02\n\n.00 \u00b1 .00\n.54 \u00b1 .01\n\nMDP\n\nMDP\n\nDeepSea\n\n.94 \u00b1 .00\n\n.06 \u00b1 .01\n\n.23 \u00b1 .05\n\nDeepSea\n\nFrozenLake\n\n.83 \u00b1 .03\n\n.01 \u00b1 .03\n\n.01 \u00b1 .02\n\nFrozenLake\n\n.90 \u00b1 .01\n\n.01 \u00b1 .00\n\nFrozenLake\n\n.78 \u00b1 .04\n\n.03 \u00b1 .11\n\nMG-Empty\n\nMG-Empty\n\nMG-Rooms\n\nRiverSwim\n\nSimpleGrid\n\nTaxi\n\n1.00 \u00b1 .00\n1.00 \u00b1 .00\n1.00 \u00b1 .00\n1.00 \u00b1 .00\n1.00 \u00b1 .00\n.92 \u00b1 .04\n.91 \u00b1 .03\n\n.90 \u00b1 .04\n1.00 \u00b1 .00\n.99 \u00b1 .01\n\n.07 \u00b1 .02\n.91 \u00b1 .01\n\n.78 \u00b1 .03\n.79 \u00b1 .03\n.50 \u00b1 .03\n\n.84 \u00b1 .01\n.56 \u00b1 .02\n\n.86 \u00b1 .16\n.94 \u00b1 .07\n.91 \u00b1 .09\n.35 \u00b1 .10\n.44 \u00b1 .12\n.14 \u00b1 .08\n.04 \u00b1 .03\n\n.05 \u00b1 .04\n.54 \u00b1 .36\n.24 \u00b1 .29\n\n.00 \u00b1 .00\n.00 \u00b1 .00\n\n.05 \u00b1 .01\n.79 \u00b1 .03\n.50 \u00b1 .03\n\n.08 \u00b1 .01\n.05 \u00b1 .00\n\nMG-Empty\n\nMG-Rooms\n\nRiverSwim\n\nSimpleGrid\n\nTaxi\n\n.59 \u00b1 .07\n.99 \u00b1 .00\n.99 \u00b1 .01\n1.00 \u00b1 .00\n1.00 \u00b1 .00\n\n.99 \u00b1 .01\n1.00 \u00b1 .00\n1.00 \u00b1 .00\n.94 \u00b1 .05\n\n.87 \u00b1 .00\n.96 \u00b1 .01\n\n.78 \u00b1 .10\n.80 \u00b1 .00\n.50 \u00b1 .00\n.79 \u00b1 .04\n\n.94 \u00b1 .00\n.91 \u00b1 .01\n\n.09 \u00b1 .05\n.24 \u00b1 .15\n.23 \u00b1 .12\n.91 \u00b1 .09\n.93 \u00b1 .09\n\n.21 \u00b1 .29\n.44 \u00b1 .39\n.43 \u00b1 .39\n.04 \u00b1 .04\n\n.00 \u00b1 .00\n.80 \u00b1 .00\n\n.20 \u00b1 .15\n.55 \u00b1 .15\n.11 \u00b1 .01\n.79 \u00b1 .04\n\n.09 \u00b1 .01\n.36 \u00b1 .06\n\n.98 \u00b1 .02\n.98 \u00b1 .02\n.97 \u00b1 .00\n.98 \u00b1 .01\n.96 \u00b1 .01\n.98 \u00b1 .02\n.98 \u00b1 .03\n.98 \u00b1 .01\n\n.98 \u00b1 .03\n.98 \u00b1 .02\n\n.73 \u00b1 .19\n.71 \u00b1 .22\n.90 \u00b1 .06\n.50 \u00b1 .25\n\n.78 \u00b1 .00\n.46 \u00b1 .08\n.49 \u00b1 .00\n\n.99 \u00b1 .01\n.98 \u00b1 .04\n.95 \u00b1 .03\n.99 \u00b1 .01\n.83 \u00b1 .31\n.99 \u00b1 .02\n.99 \u00b1 .01\n.99 \u00b1 .01\n\n.99 \u00b1 .02\n1.00 \u00b1 .00\n\n.00 \u00b1 .00\n.00 \u00b1 .00\n.02 \u00b1 .04\n.01 \u00b1 .00\n\n.70 \u00b1 .19\n.01 \u00b1 .02\n.43 \u00b1 .16\n\n.05 \u00b1 .06\n.03 \u00b1 .05\n.04 \u00b1 .01\n.54 \u00b1 .26\n.01 \u00b1 .00\n.45 \u00b1 .35\n.27 \u00b1 .33\n.93 \u00b1 .09\n\n.18 \u00b1 .29\n.62 \u00b1 .36\n\n.00 \u00b1 .00\n.01 \u00b1 .00\n.01 \u00b1 .01\n.01 \u00b1 .01\n\n.01 \u00b1 .01\n.00 \u00b1 .00\n.00 \u00b1 .00\n\nFrozenLake\n\nMG-Empty\n\nMG-Rooms\n\nRiverSwim\n\nSimpleGrid\n\nMG-Rooms\n\nRiverSwim\n\nSimpleGrid\n\n.78 \u00b1 .00\n.99 \u00b1 .00\n.79 \u00b1 .00\n\n.77 \u00b1 .04\n.84 \u00b1 .04\n\n.51 \u00b1 .23\n.01 \u00b1 .00\n.00 \u00b1 .00\n.35 \u00b1 .17\n.75 \u00b1 .21\n\n.01 \u00b1 .01\n.01 \u00b1 .01\n.02 \u00b1 .02\n\n.16 \u00b1 .03\n.34 \u00b1 .14\n\n.11 \u00b1 .01\n.01 \u00b1 .00\n.15 \u00b1 .01\n.01 \u00b1 .00\n\n.78 \u00b1 .05\n.99 \u00b1 .00\n.79 \u00b1 .04\n\n.01 \u00b1 .04\n.01 \u00b1 .02\n\n.95 \u00b1 .22\n1.00 \u00b1 .00\n.60 \u00b1 .50\n1.00 \u00b1 .00\n1.00 \u00b1 .00\n\n1.00 \u00b1 .00\n1.00 \u00b1 .00\n1.00 \u00b1 .00\n\n.00 \u00b1 .01\n.01 \u00b1 .00\n\n.93 \u00b1 .00\n.45 \u00b1 .15\n.93 \u00b1 .00\n.50 \u00b1 .00\n\n.90 \u00b1 .01\n.99 \u00b1 .00\n.92 \u00b1 .01\n\n.01 \u00b1 .01\n.04 \u00b1 .06\n\n.02 \u00b1 .00\n.02 \u00b1 .00\n.01 \u00b1 .00\n.01 \u00b1 .00\n.08 \u00b1 .20\n\n.78 \u00b1 .40\n.02 \u00b1 .01\n.66 \u00b1 .47\n\n.00 \u00b1 .00\n.02 \u00b1 .01\n\n.01 \u00b1 .00\n.01 \u00b1 .00\n.70 \u00b1 .40\n.33 \u00b1 .24\n\nTaxi\n\n.87 \u00b1 .01\n\n.89 \u00b1 .08\n\n.09 \u00b1 .01\n\nTaxi\n\n.95 \u00b1 .00\n\n.94 \u00b1 .04\n\n.12 \u00b1 .01\n\nAverage\n\n.81 \u00b1 .24\n\n.30 \u00b1 .33\n\nAverage\n\n.83 \u00b1 .23\n\n.35 \u00b1 .30\n\nAverage\n\n.85 \u00b1 .18\n\n.59 \u00b1 .44\n\n.17 \u00b1 .26\n\nAverage\n\n.38 \u00b1 .37\n\n.69 \u00b1 .38\n\n.28 \u00b1 .37\n\nMDP is not straightforward. In fact, the majority of MDPs in the literature are communicating.\nIn Colosseum, ergodicity is induced by setting p_rand > 0 in otherwise communicating MDPs.\nModel-free agents struggle with the resulting increase in variability of the state-action value function.\n\nIn the episodic settings (Tables 2a and 2b), PSRL obtains excellent performances with low variability.\nQ-learning instead performs well in a few MDPs. This often happens since, when the action selected\nby the agent is randomly substituted (due to p_rand > 0) with one with a large sub-optimality gap,\nthe resulting Q-value update introduces a critical error that requires many samples to be corrected.\n\nIn the continuous settings (Tables 2c and 2d), UCRL2 performs best in the ergodic cases when\nQ-learning suffers from the issue caused by p_rand > 0 but is only slightly better than Q-learning\nin the communicating ones. PSRL instead struggles with most MDPs. The reason for its weak\nperformance in this setting is the computationally expensive optimistic sampling procedure required\nfor its worst-case theoretical guarantees. It often breaks the time limit before reaching the first quarter\nof available time steps, meaning that it lacks sufficient samples to estimate the optimal policy.\n\nFigure 2 places the regret of the agents in the continuous ergodic setting (Table 2c) on a position\ncorresponding to the diameter and value norm of the benchmark environments. PSRL and Q-learning\n(Figures 2a and 2b), appear to be impacted more by the value norm than the diameter. This is in line\nwith the lack of sufficient samples for PSRL and the aforementioned issue related to high q estimates\nvariability for Q-learning, which is exacerbated when the estimation complexity is higher. In the\ncase of UCRL2 (Figure 2c), which provides more reliable evidence since it performs well across the\nMDPs, higher regret effectively corresponds to higher diameter and value norm.\n\nThe following is the appendix_D section of the paper you are reviewing:\n\n\nIn our empirical investigation of the measures of hardness, we consider five MDP families (MG-Empty,\nSimpleGrid, FrozenLake, RiverSwim, and DeepSea) that include different levels of stochasticity\nand challenge. Each MDP family is tested in four scenarios that highlight different aspects of hardness.\nNote that each measure has been normalized (as described in App. C), which solely allows comparing\ntrends (growth rates). Figures 12, 13, 14, 15 and 16 (pg. 28) report the results of our investigation\nalong with the 95% bootstrapped confidence intervals over twelve seeds.\n\nScenario 1. We vary the probability p_rand that an MDP executes a random action instead of the\naction selected by an agent. As p_rand increases, estimating the optimal value function becomes\neasier since every policy yields increasingly similar value functions. This produces a decrease in the\nestimation complexity. However, intentionally visiting states becomes harder.\n\nScenario 2. We vary the probability p_lazy that an MDP stays in the same state instead of\nexecuting the action selected by an agent. Contrary to increasing p_rand, increasing p_lazy never\nbenefits exploration through the execution of random actions. Increasing p_lazy decreases estimation\ncomplexity and increases visitation complexity.\n\nScenario 3 and 4. We vary the number of states across MDPs from the same family. In scenario 4,\nwe also let p_rand = 0.1 to study the impact of stochasticity. In these scenarios, increments in the\nnumber of states increase both estimation complexity and visitation complexity.\n\nCumulative regret of a near-optimal agent.\nIn every scenario, the measures of hardness are com-\npared with the cumulative regret of a near-optimal agent that serves as an optimistic approximation\nof a complete measure of hardness. The near-optimal agents have been chosen between the ones\navailable in Colosseum with the lowest average cumulative regret in the benchmarking results (such\nas PSRL in the episodic setting and UCRL2 in the continuous setting). In order to optimistically\napproximate a complete measure of hardness, we tune the hyperparameters of the agents for each\nMDP in every scenario. Concretely, we perform a random search with the objective of minimizing\nthe average cumulative regret resulting from an interaction of the agent with the MDP that lasts for\n200 000 time steps with a maximum time limit of two minutes across three seeds. The budget for the\nrandom search is 120 samples.\n\nComputational power. The empirical investigation has been carried out on a desktop PC equipped\nwith an AMD Ryzen 9 5950X 16-Core Processor and required less than 24 hours for all the MDP fam-\nilies and scenarios. The most computationally intensive part of the procedure is the hyperparameter\nsearch.\n\nLimitations. The main limitation of our empirical investigation is the selection of the MDP families,\nnear-optimal agents, and scenarios. Although we believe to have proposed a solid methodology, we\nare open to discussing the inclusion of additional experiments to further enhance Colosseum.\n\nD.1 Analysis of results\n\nDiameter. The diameter grows superlinearly with both p_rand and p_lazy since deliberate move-\nment between states requires an exponentially increasing number of time steps. As clearly shown in\nthe figures, this phenomenon is exacerbated in the episodic setting. If the agent is forced to take a\nrandom action or to stay in the same state, it can miss the opportunity to reach the target state in the\ncurrent episode and has to try again in the next episode. Although the diameter highlights this sharply\nincreasing visitation complexity, its trend overestimates the increase in cumulative regret of the\ntuned near-optimal agent, which is explained by the unaccounted decrease in estimation complexity.\nThe diameter also increases almost linearly with the number of states. When p_rand is relatively\nsmall, an approximately linear relationship can still be observed. This linear trend underestimates the\nnon-linear growth in hardness clearly shown in the cumulative regret of the tuned near-optimal agent\nbut is in line with the mild increase in visitation complexity. FrozenLake in the episodic setting\n(Figures 14c and 14d) represents the only exception. Given the extremely high level of stochasticity\nof the MDP, increasing the number of states drastically increases the visitation complexity while\nmaking it easier for the agent to act near-optimally.\n\n26\n\n\fEnvironmental value norm. The environmental value norm decreases as p_lazy and p_rand\nincrease because the optimal value of neighboring states becomes closer, which decreases the per-step\nvariability of the optimal state value function. However, we note that for the MG-Empty and the\nFrozenLake MDP families in the continuous cases (see Figures 12f and 14f) as p_lazy increases,\nthe environmental value norm first decreases and later increases. From a certain value of p_lazy\nonward, there is a significant probability of the agent remaining in the same state. This provokes\nlarge changes in the value of states that are distant from the highly rewarding states and no changes at\nall for highly rewarding states since the lazy transition is comparable to taking the optimal action.\nDue to the large changes in the suboptimal region of the state space and the absence of changes in the\noptimal region of the state space, the overall one-step variability of the state value function increases.\nNote that this does not happen in the episodic case due to the restarting mechanism and whether it\nhappens or not in the continuous case depends on the transition and reward structure of an MDP.\nWhen the number of states increases but the transition and reward structures remain the same, the\nsmall increase in measured variability only causes the environmental value norm to grow sublinearly.\nThese findings are strong evidence that this measure is only suited to capture estimation complexity.\n\nSum of the reciprocals of the sub-optimality gaps. The sum of the reciprocals of the sub-\noptimality gaps increases weakly superlinearly in scenarios 1 and 2. The probability of executing\nthe action selected by the agent decreases when p_lazy and p_rand increase, and so the difference\nbetween the optimal value function and the optimal state-action value function decreases sharply.\nFrozenLake (Figures 14a, 14b, 14e and 14f) represents an exception as the sum of the reciprocals\nof the sub-optimality gaps is almost constant. FrozenLake naturally incorporates an exceptionally\nhigh level of stochasticity and so varying p_lazy and p_rand does not significantly affect the value\nfunctions. The sum of the reciprocals of the sub-optimality gaps increases almost linearly with the\nnumber of states. This is explained by the fact that the average value of the additional terms in the\nsummation is often similar to the average value of the existing terms given the same structure of\nreward and transition kernels. This measure of hardness is not particularly apt at capturing estimation\ncomplexity, since it focuses solely on optimal policy identification. It also underestimates the increase\nin hardness induced by an increase in visitation complexity.\n\nCumulative regret of the tuned near-optimal agent. The trends of the cumulative regret of the\ntuned near-optimal agent present more variability when compared to the theoretical measures of\nhardness. This reflects the fact that this is an approximation of a complete measure of hardness\nbased on agents that have specific strengths and weaknesses. Overall, we note a tendency of\nsuperlinear growth in scenarios 1 and 2. Such tendency is specifically marked for the grid worlds,\nsuch as MG-Empty (see Fig. 12) and SimpleGrid (see Fig. 13). In these MDP families, the highly\nrewarding states are located far from the starting states and therefore the visitation complexity plays\na fundamental role. In the FrokenLake family (see Fig. 14), the trend is linear (episodic setting)\nor sub-linear (continuous setting), which is caused by the relatively low impact of the parameters\np_rand and p_lazy in the already highly stochastic MDPs. The cumulative regret of the tuned\nnear-optimal agent presents a moderately superlinear growth in the episodic case and remains almost\nconstant for the RiverSwim family (see Fig. 15). This results from the fact that, in the continuous\ncase, the absence of the restarting mechanism in combination with the chain structure of the MDP\nallows the agent to suffer only minimal impact from the increasing values of the parameters p_rand\nand p_lazy. Finally, for the DeepSea family, the regret is constant. Increases in p_rand dramatically\nreduce the possibility of visiting the highly rewarding state due to the pyramid structure of the MDP.\nIn scenarios 3 and 4, the overall tendency is still superlinear but less marked compared to scenarios\n1 and 2. The superlinear growth is most evident for the MG-Empty family (see Fig. 12) and for the\nRiverSwim family in the episodic case (see Figures 15c and 15d). For the MG-Empty, when the\ngrid size is increased, and with it the number of states, the MDP becomes increasingly challenging\nto navigate since the agent has to coordinate its rotation with its forward movement in order to\neffectively transition between states. For the RiverSwim family, the challenge comes from the\nrestarting mechanism on a chain structure. The agent is required to take a perfect sequence of actions\nin order to visit the last state of the chain, otherwise it will be reset to the start. In the continuous\nsetting (see Figures 15g and 15h), instead, the trends are mostly linear, similarly to what happens in\nscenarios 1 and 2. The less challenging structure of the SimpleGrid family (see Fig. 13) induces\nweakly superlinear trends of the cumulative regret of the tuned near-optimal agent. We note that,\ncontrary to the theoretical measures of hardness, the episodic setting does not appear to be harder\n(which would be suggested by steeper trends). This discrepancy is particularly noticeable in the\n\n27\n\n\fFrozenLake family (see Fig. 14) which yields a mostly linear trend in the continuous settings and\nclearly sublinear trends in the episodic settings. In the DeepSea family, the cumulative regret of the\ntuned near-optimal agent is almost constant in scenario 3 and almost linear in scenario 4. The main\nchallenge for this family lies in the pyramidal structure of the MDP rather than the number of states.\nHowever, setting p_rand = 0.1 creates a more challenging task for the agent as more time steps are\nrequired to find the highly rewarding state. We also note that the difference in results between the\nepisodic and continuous settings is minimal, which is unsurprising given the MDP structure.\n\nThe following is the appendix_E section of the paper you are reviewing:\n\n\nIn the following sections, we provide details on the selection methodology for the environments in\nthe benchmark, and we explain the full benchmarking procedure from the hyperparameters selection\nto the benchmark evaluation.\n\nE.1 Benchmark environments selection\n\nThe environments in the benchmark have been selected to be as diverse as possible with respect to the\ndiameter and the environmental value norm. Based on the theoretical properties of these measures\nand the results of the empirical comparison in Section D, we believe that they represent valid proxies\nfor the visitation complexity and the estimation complexity. The candidate environments have been\nsampled from a set of parameters such that their diameter is less than 100 and the environmental\nvalue norm is less than 3.5. This guarantees a sufficient challenge for the reinforcement learning\nagents while limiting the scale of the environments.\n\nFigure 17 represents the benchmark MDPs placed according to their diameter and environmental\nvalue norm. The selection features MDPs with varying combinations of values of diameter in the\ninterval [20, 100] and environmental value norm in the interval [0, 3.5]\n\nFigure 17: Positions in measure of hardness space of the set of MDPs in the benchmark.\n\nE.2 Hyperparameter selection\n\nThe hyperparameter selection procedure is to be considered an integral component of the Colosseum\nbenchmarking procedure to ensure fair hyperparameter tuning.\n\nEach Colosseum agent is required to define a sampling space for all its parameters. These sampling\nspaces are used by the package to conduct a random search optimization procedure with the objective\nof minimizing the cumulative regret across a set of randomly sampled environments. The random\nsampling procedure for environments is defined for each Colosseum MDP family and aims to provide\na varied set of MDPs of up to moderate scale. Note that the MG-DoorKey family is excluded from\nthe hyperparameter selection as it is weakly communicating in the continuous case. Tutorials on how\nto implement the aforementioned functions for novel agents and environments are available online.\n\nThe hyperparameters for the agents employed in the paper have been obtained with 50 samples from\nthe hyperparameter spaces, which have been evaluated on 12 MDPs from each family, for a total of\n84 MDPs with a training time of 20 minutes and a maximum number of total time steps of 200 000.\n\nE.3 Computational resources\n\nThe experiments for the benchmarking procedure have been carried out using CPUs from the Queen\nMary University of London Apocrita HPC facility. Note that, due to the time constraint imposed\nby Colosseum, the computational resources required to run the benchmark are bounded, and the\nbenchmarking procedure is easily parallelizable.\n\n31\n\n255075100Diameter0.00.51.01.52.02.5Value norm12312123451231212341255075100Diameter0.51.01.52.01112345678121234123150100Diameter01231211234512341212341250100Diameter012312112345671231212312DeepSeaFrozenLakeMiniGridEmptyMiniGridRoomsRiverSwimSimpleGridTaxi\fE.4 Tabular setting\n\nExperimental procedure. We set the total number of time steps to 500 000 with a maximum\ntraining time of 10 minutes for the tabular setting and 40 minutes for the non-tabular case. If an agent\ndoes not reach the maximum number of time steps before this time limit, learning is interrupted,\nand the agent keeps using its last best policy. This guarantees a fair comparison between agents\nwith different computational costs. The performance indicators are computed every 100 time steps.\nEach interaction between an agent and an MDP is repeated for 20 seeds. The per-step normalized\ncumulative regret (defined in App. C) is employed as a performance measure since it provides a\nunified scale across different MDPs.\n\nBenchmark hardness.\nIn order to illustrate how hardness measures relate to cumulative regret in\nthe benchmark, Figures 18a, 18b, 18c, and 18d place the average cumulative regret obtained by each\nagent in each benchmark MDP in a coordinate that corresponds to the diameter and the environmental\nvalue norm of that MDP. In the episodic setting (Figures 18a and 18b), we note that the environmental\nvalue norm has an evident impact on the Q-learning agent, whereas the effect of the diameter is most\nnoticeable in the communicating case. Still, in the episodic setting, the diameter has a small influence\ncompared to the environmental value norm for PSRL. In the continuous setting (Figures 18c and 18d),\nthere is generally a positive relationship between both of these hardness measures and the average\ncumulative regret for UCRL2. For Q-learning and PSRL, the diameter seems to have a generally\nsmaller influence on the average cumulative regret.\n\nCumulative regret plots. Figures 19, 20, 21, and 22 report the expected cumulative regrets for the\nagents during the agent/MDP interactions along with the cumulative regret of an agent that selects\naction at random, which provides an informative baseline. Contrary to the episodic setting, in the\ncontinuous setting, the training of UCRL2 and PSRL is stopped for several MDPs of the benchmark.\nFor PSRL, this typically happens before reaching 10 000 time steps, which is particularly damaging.\nAt this point, the agent has not properly explored the MDP and so it is forced to continue the interaction\nfollowing a policy that yields a regret similar to the one of the random agent. UCRL2, instead, tends\nto terminate the allocated training time at later time steps, which penalizes the performance less.\n\nCumulative regret tables.\nIn Tables 11, 12, 13, and 14, we report the per-step normalized regrets\nwith standard deviations along with the number of seeds for which the agent has been able to complete\nthe total number of training time steps before exceeding the time limit. We highlight in bold the best\nperforming agent for each MDP. The same information has been summarized in Table 2 (\n\nBased on the section(s), please answer the following question with yes, no, or n/a and provide a brief justification for your answer.\nQuestion: If the authors ran experiments, do the authors include the code, data, and instructions needed to reproduce the main experimental results (either in the supplemental material or as a URL)?"}, "3b": {"role": "user", "content": "The following is the experiments section of the paper you are reviewing:\n\n\n3.1 Colosseum\n\nThis section briefly introduces Colosseum, a pioneering Python package that bridges theory and\npractice in tabular reinforcement learning while also being applicable in the non-tabular setting. More\ndetails about the package can be found in Appendix A and in the project website.2\n\nAs a hardness analysis tool, Colosseum identifies the communication class of MDPs, assembles\ninsightful visualizations and logs of interactions between agents and MDPs, computes three measures\nof hardness (environmental value norm, sum of the reciprocals of the sub-optimality gaps, and\ndiameter, whose computation requires a novel solution described in App. A.4). Eight MDP families\nare available for experimentation. Some are traditional families (RiverSwim [24], Taxi [25], and\nFrozenLake) while others are more recent (MiniGid environments [26]). Additionally, DeepSea\n[27] was included as a hard exploration family of problems, and the SimpleGrid family is composed\nof simplified versions of the MG\u2013Empty environment. By controlling the parameters of MDPs from\neach family (further detailed in App. A.3), it is easy to create an MDP with any desired hardness.\n\nAs a benchmarking tool, Colosseum is unique in its strong connection with theory. For instance,\nin contrast to non-tabular benchmarks, Colosseum computes theoretical evaluation criteria such\n\n1For example, the \u2126(|S||A|H 2\u03f5\u22122) sample complexity bound for the episodic communicating setting [22].\n2Available at https://michelangeloconserva.github.io/Colosseum.\n\n6\n\n\fas the expected cumulative regret and the expected average future reward, which can be used to\nexactly evaluate the performance criterion of regret minimizing agents. The benchmark covers the\nmost commonly studied reinforcement learning settings: episodic ergodic, episodic communicating,\ncontinuous ergodic, and continuous communicating. For each setting, we have selected twenty MDPs\nthat are diverse with respect to their diameters and environmental value norms as proxies for different\ncombinations of visitation complexity and estimation complexity. Figure 17 in Appendix E shows\nhow each of these MDPs varies according to these measures, and Section 3.3 empirically validates\nthis selection by showing that harder MDPs correspond to worse agent performance. Notably, the\ntheoretically backed selection of MDPs and the rigorous evaluation criteria make the Colosseum\nbenchmark the most exhaustive in tabular reinforcement learning, since previous evaluations were\nconducted empirically in a few MDPs (such as Taxi or RiverSwim).\n\nColosseum also allows testing of non-tabular agents by leveraging the BlockMDP model [28].\nBlockMDPs equip tabular MDPs with an emission map that is a (possibly stochastic) mapping q : S \u2192\n\u2206(O) from the finite state space S to a (possibly infinite) observation space O. Agents interacting\nwith BlockMDPs are only provided with observations, so non-tabular methods are generally required.\nMany commonly used non-tabular MDPs (such as Minecraft [29]) can be straightforwardly encoded\nas BlockMDPs using the Colosseum MDP families. Colosseum implements a diverse set of\ndeterministic emission maps and allows combining them with different sources of noise. Appendix\nA.2 further details BlockMDPs and the available emission maps.\n\n3.2 Empirical analysis of hardness measures\n\nFor brevity, this section only presents results of hardness measures in the MiniGridEmpty family of\nenvironments in the episodic setting. Appendix D presents the full outcome of the empirical analysis.\n\nA MiniGridEmpty MDP is a grid world where an agent has three available actions: moving forward,\nrotating left, and rotating right. An agent is rewarded for being in a few specific states and receives no\nreward in every other state. Appendix A.3.4 provides more details about this family of environments.\n\nIn our investigation, we consider four scenarios that highlight the different aspects of MDP hardness.\n\nScenario 1. We vary the probability p_rand that an MDP executes a random action instead of the\naction selected by an agent. As p_rand approaches one, value estimation becomes easier, since\noutcomes depend less on agent choices. However, intentionally visiting states becomes harder.\n\nScenario 2. We vary the probability p_lazy that an MDP stays in the same state instead of executing\nthe action selected by an agent. Contrary to increasing p_rand, increasing p_lazy never benefits\nexploration. Increasing p_lazy decreases estimation complexity and increases visitation complexity.\n\nScenario 3 and 4. We vary the number of states across MDPs from the same family. In scenario\n4, we also let p_rand = 0.1 to study the impact of stochasticity. In these scenarios, increasing the\nnumber of states simultaneously increases the estimation complexity and the visitation complexity.\n\nIn every scenario, hardness measures are compared with the cumulative regret of a near-optimal agent\ntuned for each specific MDP (see App. D). This regret serves as an optimistic measure of hardness.\nAppendix C describes how these measures are normalized. Note that, due to normalization, the plots\nshould only be compared in terms of trends (growth rates) rather than absolute values.\n\nAnalysis. Figure 1 presents the empirical results for the episodic MiniGridEmpty family in the four\nscenarios with 95% bootstrapped confidence intervals over twelve random seeds.\n\nThe experiments confirm our claim that the diameter captures visitation rather than estimation\ncomplexity. This measure of hardness grows superlinearly with both p_rand and p_lazy (Figures\n1a and 1b) since deliberate movement between states requires an exponentially increasing number of\ntime steps. Although the diameter highlights the sharply increasing visitation complexity, its trend\noverestimates the increase in cumulative regret of the tuned near-optimal agent, which is explained\nby the unaccounted decrease in estimation complexity. The diameter also increases almost linearly\nwith the number of states (Figures 1c and 1d). For the small p_rand (scenario 4), the relation is still\napproximately linear. This linear trend underestimates the evident non-linear growth in hardness in\nthe regret of the tuned near-optimal agent but is in line with the mild increase in visitation complexity.\n\nThe empirical evidence indicates that the environmental value norm can only capture estimation\ncomplexity. It decreases as p_lazy and p_rand increase (Figures 1a and 1b) because the optimal\n\n7\n\n\fFigure 1: The Colosseum hardness analysis for the episodic MiniGridEmpty family.\n\nvalue of neighboring states becomes closer, which decreases the per-step variability of the optimal\nvalue function. When the number of states increases but the transition and reward structures remain\nthe same (Figures 1c and 1d), the small increase in this variability only generates a sublinear growth.\n\nWe empirically observe that the sum of the reciprocals of the sub-optimality gaps is not particularly\napt at capturing estimation complexity, due to its exclusive focus on optimal policy identification, and\nit also underestimates the increase in hardness induced by an increase in visitation complexity. This\nmeasure increases weakly superlinearly in scenarios 1 and 2 (Figures 1a and 1b). The probability\nof executing the action selected by the agent decreases when p_lazy and p_rand increase, so the\ndifference between the state and the state-action optimal value functions decreases sharply. The\nmeasure increases almost linearly with the number of states (Figures 1c and 1d). This is explained by\nthe fact that the average value of the additional terms in the summation is often similar to the average\nvalue of the existing terms when MDPs have the same structure of reward and transition kernels.\n\n3.3 Colosseum benchmarking\n\nIn this section, we benchmark five tabular agents with theoretical guarantees and four non-tabular\nagents. Besides being valuable on their own, these results help to empirically validate our benchmark.\n\nAgents. The tabular agents are posterior sampling for reinforcement learning (PSRL) for the episodic\nand continuous settings [30, 31], Q-learning with UCB exploration for the episodic setting [32],\nQ-learning with optimism for the continuous setting [16], and UCRL2 for the continuous setting [14].\nThe non-tabular agents (from bsuite) are ActorCritic, ActorCriticRNN, BootDQN, and DQN.\n\nExperimental procedure. We set the total number of time steps to 500 000 with a maximum training\ntime of 10 minutes for the tabular setting and 40 minutes for the non-tabular setting. If an agent\ndoes not reach the maximum number of time steps before this time limit, learning is interrupted,\nand the agent continues interacting using its last best policy. This guarantees a fair comparison\nbetween agents with different computational costs. The performance indicators are computed every\n100 time steps. Each interaction between an agent and an MDP is repeated for 20 seeds. The agents\u2019\nhyperparameters have been chosen by random search to minimize the average regret across MDPs\nwith randomly sampled parameters (see Appendix E). We use a deterministic emission map that\nassigns a uniquely identifying vector to each state (for example, a gridworld coordinate) to derive\nthe non-tabular benchmark MDPs. In Table 2, we report the per-step normalized cumulative regrets\ndivided by the total number of time steps (defined in Appendix C), which allows comparisons across\ndifferent MDPs. We summarize the main findings here and refer to Appendix E for further details.\n\nAnalysis. Table 2 often shows high variability in the performance of the same agent across MDPs of\nthe same family. Therefore, maximising the diversity across diameters and value norms effectively\nproduces diverse challenges even for MDPs with similar transition and reward structures. For example,\nin the continuous communicating case (Table 2d), Q-learning performs well only in some MDPs of\nthe MiniGridEmpty family. This also happens for UCRL2 for the SimpleGrid family.\n\nThe average normalized cumulative regret is lower in ergodic environments compared to commu-\nnicating environments. This indicates that the ergodic setting is generally slightly easier than the\ncommunicating settings. Notably, in the continuous setting, the ergodic setting is more challenging\nthan the communicating setting for Q-learning (Tables 2c and 2d). Designing a naturally ergodic\n\n8\n\n0.00.20.40.6Probability of random action0.00.20.40.60.81.0Normalized values(a) Scenario 10.00.20.40.6Probability of lazy action(b) Scenario 2100200300400Number of states(c) Scenario 3100200300400Number of states(d) Scenario 4DiameterEnvironmental value normSum ofthe reciprocals of the sub-optimality gapsCumulative regret oftuned near-optimal agent\fTable 2: Normalized cumulative regrets of selected agents on the Colosseum benchmark. (a) Episodic\nergodic. (b) Episodic communicating. (c) Continuous ergodic. (d) Continuous communicating.\n\n(a)\n\n(b)\n\n(c)\n\n(d)\n\nQ-learning\n\nPSRL\n\nQ-learning\n\nPSRL\n\nQ-learning\n\nPSRL\n\nUCRL2\n\nQ-learning\n\nPSRL\n\nUCRL2\n\nMDP\n\nDeepSea\n\n.64 \u00b1 .00\n.52 \u00b1 .01\n\n.01 \u00b1 .00\n.00 \u00b1 .00\n\nMDP\n\nDeepSea\n\n.01 \u00b1 .01\n.83 \u00b1 .02\n\n.00 \u00b1 .00\n.54 \u00b1 .01\n\nMDP\n\nMDP\n\nDeepSea\n\n.94 \u00b1 .00\n\n.06 \u00b1 .01\n\n.23 \u00b1 .05\n\nDeepSea\n\nFrozenLake\n\n.83 \u00b1 .03\n\n.01 \u00b1 .03\n\n.01 \u00b1 .02\n\nFrozenLake\n\n.90 \u00b1 .01\n\n.01 \u00b1 .00\n\nFrozenLake\n\n.78 \u00b1 .04\n\n.03 \u00b1 .11\n\nMG-Empty\n\nMG-Empty\n\nMG-Rooms\n\nRiverSwim\n\nSimpleGrid\n\nTaxi\n\n1.00 \u00b1 .00\n1.00 \u00b1 .00\n1.00 \u00b1 .00\n1.00 \u00b1 .00\n1.00 \u00b1 .00\n.92 \u00b1 .04\n.91 \u00b1 .03\n\n.90 \u00b1 .04\n1.00 \u00b1 .00\n.99 \u00b1 .01\n\n.07 \u00b1 .02\n.91 \u00b1 .01\n\n.78 \u00b1 .03\n.79 \u00b1 .03\n.50 \u00b1 .03\n\n.84 \u00b1 .01\n.56 \u00b1 .02\n\n.86 \u00b1 .16\n.94 \u00b1 .07\n.91 \u00b1 .09\n.35 \u00b1 .10\n.44 \u00b1 .12\n.14 \u00b1 .08\n.04 \u00b1 .03\n\n.05 \u00b1 .04\n.54 \u00b1 .36\n.24 \u00b1 .29\n\n.00 \u00b1 .00\n.00 \u00b1 .00\n\n.05 \u00b1 .01\n.79 \u00b1 .03\n.50 \u00b1 .03\n\n.08 \u00b1 .01\n.05 \u00b1 .00\n\nMG-Empty\n\nMG-Rooms\n\nRiverSwim\n\nSimpleGrid\n\nTaxi\n\n.59 \u00b1 .07\n.99 \u00b1 .00\n.99 \u00b1 .01\n1.00 \u00b1 .00\n1.00 \u00b1 .00\n\n.99 \u00b1 .01\n1.00 \u00b1 .00\n1.00 \u00b1 .00\n.94 \u00b1 .05\n\n.87 \u00b1 .00\n.96 \u00b1 .01\n\n.78 \u00b1 .10\n.80 \u00b1 .00\n.50 \u00b1 .00\n.79 \u00b1 .04\n\n.94 \u00b1 .00\n.91 \u00b1 .01\n\n.09 \u00b1 .05\n.24 \u00b1 .15\n.23 \u00b1 .12\n.91 \u00b1 .09\n.93 \u00b1 .09\n\n.21 \u00b1 .29\n.44 \u00b1 .39\n.43 \u00b1 .39\n.04 \u00b1 .04\n\n.00 \u00b1 .00\n.80 \u00b1 .00\n\n.20 \u00b1 .15\n.55 \u00b1 .15\n.11 \u00b1 .01\n.79 \u00b1 .04\n\n.09 \u00b1 .01\n.36 \u00b1 .06\n\n.98 \u00b1 .02\n.98 \u00b1 .02\n.97 \u00b1 .00\n.98 \u00b1 .01\n.96 \u00b1 .01\n.98 \u00b1 .02\n.98 \u00b1 .03\n.98 \u00b1 .01\n\n.98 \u00b1 .03\n.98 \u00b1 .02\n\n.73 \u00b1 .19\n.71 \u00b1 .22\n.90 \u00b1 .06\n.50 \u00b1 .25\n\n.78 \u00b1 .00\n.46 \u00b1 .08\n.49 \u00b1 .00\n\n.99 \u00b1 .01\n.98 \u00b1 .04\n.95 \u00b1 .03\n.99 \u00b1 .01\n.83 \u00b1 .31\n.99 \u00b1 .02\n.99 \u00b1 .01\n.99 \u00b1 .01\n\n.99 \u00b1 .02\n1.00 \u00b1 .00\n\n.00 \u00b1 .00\n.00 \u00b1 .00\n.02 \u00b1 .04\n.01 \u00b1 .00\n\n.70 \u00b1 .19\n.01 \u00b1 .02\n.43 \u00b1 .16\n\n.05 \u00b1 .06\n.03 \u00b1 .05\n.04 \u00b1 .01\n.54 \u00b1 .26\n.01 \u00b1 .00\n.45 \u00b1 .35\n.27 \u00b1 .33\n.93 \u00b1 .09\n\n.18 \u00b1 .29\n.62 \u00b1 .36\n\n.00 \u00b1 .00\n.01 \u00b1 .00\n.01 \u00b1 .01\n.01 \u00b1 .01\n\n.01 \u00b1 .01\n.00 \u00b1 .00\n.00 \u00b1 .00\n\nFrozenLake\n\nMG-Empty\n\nMG-Rooms\n\nRiverSwim\n\nSimpleGrid\n\nMG-Rooms\n\nRiverSwim\n\nSimpleGrid\n\n.78 \u00b1 .00\n.99 \u00b1 .00\n.79 \u00b1 .00\n\n.77 \u00b1 .04\n.84 \u00b1 .04\n\n.51 \u00b1 .23\n.01 \u00b1 .00\n.00 \u00b1 .00\n.35 \u00b1 .17\n.75 \u00b1 .21\n\n.01 \u00b1 .01\n.01 \u00b1 .01\n.02 \u00b1 .02\n\n.16 \u00b1 .03\n.34 \u00b1 .14\n\n.11 \u00b1 .01\n.01 \u00b1 .00\n.15 \u00b1 .01\n.01 \u00b1 .00\n\n.78 \u00b1 .05\n.99 \u00b1 .00\n.79 \u00b1 .04\n\n.01 \u00b1 .04\n.01 \u00b1 .02\n\n.95 \u00b1 .22\n1.00 \u00b1 .00\n.60 \u00b1 .50\n1.00 \u00b1 .00\n1.00 \u00b1 .00\n\n1.00 \u00b1 .00\n1.00 \u00b1 .00\n1.00 \u00b1 .00\n\n.00 \u00b1 .01\n.01 \u00b1 .00\n\n.93 \u00b1 .00\n.45 \u00b1 .15\n.93 \u00b1 .00\n.50 \u00b1 .00\n\n.90 \u00b1 .01\n.99 \u00b1 .00\n.92 \u00b1 .01\n\n.01 \u00b1 .01\n.04 \u00b1 .06\n\n.02 \u00b1 .00\n.02 \u00b1 .00\n.01 \u00b1 .00\n.01 \u00b1 .00\n.08 \u00b1 .20\n\n.78 \u00b1 .40\n.02 \u00b1 .01\n.66 \u00b1 .47\n\n.00 \u00b1 .00\n.02 \u00b1 .01\n\n.01 \u00b1 .00\n.01 \u00b1 .00\n.70 \u00b1 .40\n.33 \u00b1 .24\n\nTaxi\n\n.87 \u00b1 .01\n\n.89 \u00b1 .08\n\n.09 \u00b1 .01\n\nTaxi\n\n.95 \u00b1 .00\n\n.94 \u00b1 .04\n\n.12 \u00b1 .01\n\nAverage\n\n.81 \u00b1 .24\n\n.30 \u00b1 .33\n\nAverage\n\n.83 \u00b1 .23\n\n.35 \u00b1 .30\n\nAverage\n\n.85 \u00b1 .18\n\n.59 \u00b1 .44\n\n.17 \u00b1 .26\n\nAverage\n\n.38 \u00b1 .37\n\n.69 \u00b1 .38\n\n.28 \u00b1 .37\n\nMDP is not straightforward. In fact, the majority of MDPs in the literature are communicating.\nIn Colosseum, ergodicity is induced by setting p_rand > 0 in otherwise communicating MDPs.\nModel-free agents struggle with the resulting increase in variability of the state-action value function.\n\nIn the episodic settings (Tables 2a and 2b), PSRL obtains excellent performances with low variability.\nQ-learning instead performs well in a few MDPs. This often happens since, when the action selected\nby the agent is randomly substituted (due to p_rand > 0) with one with a large sub-optimality gap,\nthe resulting Q-value update introduces a critical error that requires many samples to be corrected.\n\nIn the continuous settings (Tables 2c and 2d), UCRL2 performs best in the ergodic cases when\nQ-learning suffers from the issue caused by p_rand > 0 but is only slightly better than Q-learning\nin the communicating ones. PSRL instead struggles with most MDPs. The reason for its weak\nperformance in this setting is the computationally expensive optimistic sampling procedure required\nfor its worst-case theoretical guarantees. It often breaks the time limit before reaching the first quarter\nof available time steps, meaning that it lacks sufficient samples to estimate the optimal policy.\n\nFigure 2 places the regret of the agents in the continuous ergodic setting (Table 2c) on a position\ncorresponding to the diameter and value norm of the benchmark environments. PSRL and Q-learning\n(Figures 2a and 2b), appear to be impacted more by the value norm than the diameter. This is in line\nwith the lack of sufficient samples for PSRL and the aforementioned issue related to high q estimates\nvariability for Q-learning, which is exacerbated when the estimation complexity is higher. In the\ncase of UCRL2 (Figure 2c), which provides more reliable evidence since it performs well across the\nMDPs, higher regret effectively corresponds to higher diameter and value norm.\n\nThe following is the appendix_D section of the paper you are reviewing:\n\n\nIn our empirical investigation of the measures of hardness, we consider five MDP families (MG-Empty,\nSimpleGrid, FrozenLake, RiverSwim, and DeepSea) that include different levels of stochasticity\nand challenge. Each MDP family is tested in four scenarios that highlight different aspects of hardness.\nNote that each measure has been normalized (as described in App. C), which solely allows comparing\ntrends (growth rates). Figures 12, 13, 14, 15 and 16 (pg. 28) report the results of our investigation\nalong with the 95% bootstrapped confidence intervals over twelve seeds.\n\nScenario 1. We vary the probability p_rand that an MDP executes a random action instead of the\naction selected by an agent. As p_rand increases, estimating the optimal value function becomes\neasier since every policy yields increasingly similar value functions. This produces a decrease in the\nestimation complexity. However, intentionally visiting states becomes harder.\n\nScenario 2. We vary the probability p_lazy that an MDP stays in the same state instead of\nexecuting the action selected by an agent. Contrary to increasing p_rand, increasing p_lazy never\nbenefits exploration through the execution of random actions. Increasing p_lazy decreases estimation\ncomplexity and increases visitation complexity.\n\nScenario 3 and 4. We vary the number of states across MDPs from the same family. In scenario 4,\nwe also let p_rand = 0.1 to study the impact of stochasticity. In these scenarios, increments in the\nnumber of states increase both estimation complexity and visitation complexity.\n\nCumulative regret of a near-optimal agent.\nIn every scenario, the measures of hardness are com-\npared with the cumulative regret of a near-optimal agent that serves as an optimistic approximation\nof a complete measure of hardness. The near-optimal agents have been chosen between the ones\navailable in Colosseum with the lowest average cumulative regret in the benchmarking results (such\nas PSRL in the episodic setting and UCRL2 in the continuous setting). In order to optimistically\napproximate a complete measure of hardness, we tune the hyperparameters of the agents for each\nMDP in every scenario. Concretely, we perform a random search with the objective of minimizing\nthe average cumulative regret resulting from an interaction of the agent with the MDP that lasts for\n200 000 time steps with a maximum time limit of two minutes across three seeds. The budget for the\nrandom search is 120 samples.\n\nComputational power. The empirical investigation has been carried out on a desktop PC equipped\nwith an AMD Ryzen 9 5950X 16-Core Processor and required less than 24 hours for all the MDP fam-\nilies and scenarios. The most computationally intensive part of the procedure is the hyperparameter\nsearch.\n\nLimitations. The main limitation of our empirical investigation is the selection of the MDP families,\nnear-optimal agents, and scenarios. Although we believe to have proposed a solid methodology, we\nare open to discussing the inclusion of additional experiments to further enhance Colosseum.\n\nD.1 Analysis of results\n\nDiameter. The diameter grows superlinearly with both p_rand and p_lazy since deliberate move-\nment between states requires an exponentially increasing number of time steps. As clearly shown in\nthe figures, this phenomenon is exacerbated in the episodic setting. If the agent is forced to take a\nrandom action or to stay in the same state, it can miss the opportunity to reach the target state in the\ncurrent episode and has to try again in the next episode. Although the diameter highlights this sharply\nincreasing visitation complexity, its trend overestimates the increase in cumulative regret of the\ntuned near-optimal agent, which is explained by the unaccounted decrease in estimation complexity.\nThe diameter also increases almost linearly with the number of states. When p_rand is relatively\nsmall, an approximately linear relationship can still be observed. This linear trend underestimates the\nnon-linear growth in hardness clearly shown in the cumulative regret of the tuned near-optimal agent\nbut is in line with the mild increase in visitation complexity. FrozenLake in the episodic setting\n(Figures 14c and 14d) represents the only exception. Given the extremely high level of stochasticity\nof the MDP, increasing the number of states drastically increases the visitation complexity while\nmaking it easier for the agent to act near-optimally.\n\n26\n\n\fEnvironmental value norm. The environmental value norm decreases as p_lazy and p_rand\nincrease because the optimal value of neighboring states becomes closer, which decreases the per-step\nvariability of the optimal state value function. However, we note that for the MG-Empty and the\nFrozenLake MDP families in the continuous cases (see Figures 12f and 14f) as p_lazy increases,\nthe environmental value norm first decreases and later increases. From a certain value of p_lazy\nonward, there is a significant probability of the agent remaining in the same state. This provokes\nlarge changes in the value of states that are distant from the highly rewarding states and no changes at\nall for highly rewarding states since the lazy transition is comparable to taking the optimal action.\nDue to the large changes in the suboptimal region of the state space and the absence of changes in the\noptimal region of the state space, the overall one-step variability of the state value function increases.\nNote that this does not happen in the episodic case due to the restarting mechanism and whether it\nhappens or not in the continuous case depends on the transition and reward structure of an MDP.\nWhen the number of states increases but the transition and reward structures remain the same, the\nsmall increase in measured variability only causes the environmental value norm to grow sublinearly.\nThese findings are strong evidence that this measure is only suited to capture estimation complexity.\n\nSum of the reciprocals of the sub-optimality gaps. The sum of the reciprocals of the sub-\noptimality gaps increases weakly superlinearly in scenarios 1 and 2. The probability of executing\nthe action selected by the agent decreases when p_lazy and p_rand increase, and so the difference\nbetween the optimal value function and the optimal state-action value function decreases sharply.\nFrozenLake (Figures 14a, 14b, 14e and 14f) represents an exception as the sum of the reciprocals\nof the sub-optimality gaps is almost constant. FrozenLake naturally incorporates an exceptionally\nhigh level of stochasticity and so varying p_lazy and p_rand does not significantly affect the value\nfunctions. The sum of the reciprocals of the sub-optimality gaps increases almost linearly with the\nnumber of states. This is explained by the fact that the average value of the additional terms in the\nsummation is often similar to the average value of the existing terms given the same structure of\nreward and transition kernels. This measure of hardness is not particularly apt at capturing estimation\ncomplexity, since it focuses solely on optimal policy identification. It also underestimates the increase\nin hardness induced by an increase in visitation complexity.\n\nCumulative regret of the tuned near-optimal agent. The trends of the cumulative regret of the\ntuned near-optimal agent present more variability when compared to the theoretical measures of\nhardness. This reflects the fact that this is an approximation of a complete measure of hardness\nbased on agents that have specific strengths and weaknesses. Overall, we note a tendency of\nsuperlinear growth in scenarios 1 and 2. Such tendency is specifically marked for the grid worlds,\nsuch as MG-Empty (see Fig. 12) and SimpleGrid (see Fig. 13). In these MDP families, the highly\nrewarding states are located far from the starting states and therefore the visitation complexity plays\na fundamental role. In the FrokenLake family (see Fig. 14), the trend is linear (episodic setting)\nor sub-linear (continuous setting), which is caused by the relatively low impact of the parameters\np_rand and p_lazy in the already highly stochastic MDPs. The cumulative regret of the tuned\nnear-optimal agent presents a moderately superlinear growth in the episodic case and remains almost\nconstant for the RiverSwim family (see Fig. 15). This results from the fact that, in the continuous\ncase, the absence of the restarting mechanism in combination with the chain structure of the MDP\nallows the agent to suffer only minimal impact from the increasing values of the parameters p_rand\nand p_lazy. Finally, for the DeepSea family, the regret is constant. Increases in p_rand dramatically\nreduce the possibility of visiting the highly rewarding state due to the pyramid structure of the MDP.\nIn scenarios 3 and 4, the overall tendency is still superlinear but less marked compared to scenarios\n1 and 2. The superlinear growth is most evident for the MG-Empty family (see Fig. 12) and for the\nRiverSwim family in the episodic case (see Figures 15c and 15d). For the MG-Empty, when the\ngrid size is increased, and with it the number of states, the MDP becomes increasingly challenging\nto navigate since the agent has to coordinate its rotation with its forward movement in order to\neffectively transition between states. For the RiverSwim family, the challenge comes from the\nrestarting mechanism on a chain structure. The agent is required to take a perfect sequence of actions\nin order to visit the last state of the chain, otherwise it will be reset to the start. In the continuous\nsetting (see Figures 15g and 15h), instead, the trends are mostly linear, similarly to what happens in\nscenarios 1 and 2. The less challenging structure of the SimpleGrid family (see Fig. 13) induces\nweakly superlinear trends of the cumulative regret of the tuned near-optimal agent. We note that,\ncontrary to the theoretical measures of hardness, the episodic setting does not appear to be harder\n(which would be suggested by steeper trends). This discrepancy is particularly noticeable in the\n\n27\n\n\fFrozenLake family (see Fig. 14) which yields a mostly linear trend in the continuous settings and\nclearly sublinear trends in the episodic settings. In the DeepSea family, the cumulative regret of the\ntuned near-optimal agent is almost constant in scenario 3 and almost linear in scenario 4. The main\nchallenge for this family lies in the pyramidal structure of the MDP rather than the number of states.\nHowever, setting p_rand = 0.1 creates a more challenging task for the agent as more time steps are\nrequired to find the highly rewarding state. We also note that the difference in results between the\nepisodic and continuous settings is minimal, which is unsurprising given the MDP structure.\n\nThe following is the appendix_E section of the paper you are reviewing:\n\n\nIn the following sections, we provide details on the selection methodology for the environments in\nthe benchmark, and we explain the full benchmarking procedure from the hyperparameters selection\nto the benchmark evaluation.\n\nE.1 Benchmark environments selection\n\nThe environments in the benchmark have been selected to be as diverse as possible with respect to the\ndiameter and the environmental value norm. Based on the theoretical properties of these measures\nand the results of the empirical comparison in Section D, we believe that they represent valid proxies\nfor the visitation complexity and the estimation complexity. The candidate environments have been\nsampled from a set of parameters such that their diameter is less than 100 and the environmental\nvalue norm is less than 3.5. This guarantees a sufficient challenge for the reinforcement learning\nagents while limiting the scale of the environments.\n\nFigure 17 represents the benchmark MDPs placed according to their diameter and environmental\nvalue norm. The selection features MDPs with varying combinations of values of diameter in the\ninterval [20, 100] and environmental value norm in the interval [0, 3.5]\n\nFigure 17: Positions in measure of hardness space of the set of MDPs in the benchmark.\n\nE.2 Hyperparameter selection\n\nThe hyperparameter selection procedure is to be considered an integral component of the Colosseum\nbenchmarking procedure to ensure fair hyperparameter tuning.\n\nEach Colosseum agent is required to define a sampling space for all its parameters. These sampling\nspaces are used by the package to conduct a random search optimization procedure with the objective\nof minimizing the cumulative regret across a set of randomly sampled environments. The random\nsampling procedure for environments is defined for each Colosseum MDP family and aims to provide\na varied set of MDPs of up to moderate scale. Note that the MG-DoorKey family is excluded from\nthe hyperparameter selection as it is weakly communicating in the continuous case. Tutorials on how\nto implement the aforementioned functions for novel agents and environments are available online.\n\nThe hyperparameters for the agents employed in the paper have been obtained with 50 samples from\nthe hyperparameter spaces, which have been evaluated on 12 MDPs from each family, for a total of\n84 MDPs with a training time of 20 minutes and a maximum number of total time steps of 200 000.\n\nE.3 Computational resources\n\nThe experiments for the benchmarking procedure have been carried out using CPUs from the Queen\nMary University of London Apocrita HPC facility. Note that, due to the time constraint imposed\nby Colosseum, the computational resources required to run the benchmark are bounded, and the\nbenchmarking procedure is easily parallelizable.\n\n31\n\n255075100Diameter0.00.51.01.52.02.5Value norm12312123451231212341255075100Diameter0.51.01.52.01112345678121234123150100Diameter01231211234512341212341250100Diameter012312112345671231212312DeepSeaFrozenLakeMiniGridEmptyMiniGridRoomsRiverSwimSimpleGridTaxi\fE.4 Tabular setting\n\nExperimental procedure. We set the total number of time steps to 500 000 with a maximum\ntraining time of 10 minutes for the tabular setting and 40 minutes for the non-tabular case. If an agent\ndoes not reach the maximum number of time steps before this time limit, learning is interrupted,\nand the agent keeps using its last best policy. This guarantees a fair comparison between agents\nwith different computational costs. The performance indicators are computed every 100 time steps.\nEach interaction between an agent and an MDP is repeated for 20 seeds. The per-step normalized\ncumulative regret (defined in App. C) is employed as a performance measure since it provides a\nunified scale across different MDPs.\n\nBenchmark hardness.\nIn order to illustrate how hardness measures relate to cumulative regret in\nthe benchmark, Figures 18a, 18b, 18c, and 18d place the average cumulative regret obtained by each\nagent in each benchmark MDP in a coordinate that corresponds to the diameter and the environmental\nvalue norm of that MDP. In the episodic setting (Figures 18a and 18b), we note that the environmental\nvalue norm has an evident impact on the Q-learning agent, whereas the effect of the diameter is most\nnoticeable in the communicating case. Still, in the episodic setting, the diameter has a small influence\ncompared to the environmental value norm for PSRL. In the continuous setting (Figures 18c and 18d),\nthere is generally a positive relationship between both of these hardness measures and the average\ncumulative regret for UCRL2. For Q-learning and PSRL, the diameter seems to have a generally\nsmaller influence on the average cumulative regret.\n\nCumulative regret plots. Figures 19, 20, 21, and 22 report the expected cumulative regrets for the\nagents during the agent/MDP interactions along with the cumulative regret of an agent that selects\naction at random, which provides an informative baseline. Contrary to the episodic setting, in the\ncontinuous setting, the training of UCRL2 and PSRL is stopped for several MDPs of the benchmark.\nFor PSRL, this typically happens before reaching 10 000 time steps, which is particularly damaging.\nAt this point, the agent has not properly explored the MDP and so it is forced to continue the interaction\nfollowing a policy that yields a regret similar to the one of the random agent. UCRL2, instead, tends\nto terminate the allocated training time at later time steps, which penalizes the performance less.\n\nCumulative regret tables.\nIn Tables 11, 12, 13, and 14, we report the per-step normalized regrets\nwith standard deviations along with the number of seeds for which the agent has been able to complete\nthe total number of training time steps before exceeding the time limit. We highlight in bold the best\nperforming agent for each MDP. The same information has been summarized in Table 2 (\n\nBased on the section(s), please answer the following question with yes, no, or n/a and provide a brief justification for your answer.\nQuestion: If the authors ran experiments, do the authors specify all the training details (e.g., data splits, hyperparameters, how they were chosen)?"}, "3c": {"role": "user", "content": "The following is the experiments section of the paper you are reviewing:\n\n\n3.1 Colosseum\n\nThis section briefly introduces Colosseum, a pioneering Python package that bridges theory and\npractice in tabular reinforcement learning while also being applicable in the non-tabular setting. More\ndetails about the package can be found in Appendix A and in the project website.2\n\nAs a hardness analysis tool, Colosseum identifies the communication class of MDPs, assembles\ninsightful visualizations and logs of interactions between agents and MDPs, computes three measures\nof hardness (environmental value norm, sum of the reciprocals of the sub-optimality gaps, and\ndiameter, whose computation requires a novel solution described in App. A.4). Eight MDP families\nare available for experimentation. Some are traditional families (RiverSwim [24], Taxi [25], and\nFrozenLake) while others are more recent (MiniGid environments [26]). Additionally, DeepSea\n[27] was included as a hard exploration family of problems, and the SimpleGrid family is composed\nof simplified versions of the MG\u2013Empty environment. By controlling the parameters of MDPs from\neach family (further detailed in App. A.3), it is easy to create an MDP with any desired hardness.\n\nAs a benchmarking tool, Colosseum is unique in its strong connection with theory. For instance,\nin contrast to non-tabular benchmarks, Colosseum computes theoretical evaluation criteria such\n\n1For example, the \u2126(|S||A|H 2\u03f5\u22122) sample complexity bound for the episodic communicating setting [22].\n2Available at https://michelangeloconserva.github.io/Colosseum.\n\n6\n\n\fas the expected cumulative regret and the expected average future reward, which can be used to\nexactly evaluate the performance criterion of regret minimizing agents. The benchmark covers the\nmost commonly studied reinforcement learning settings: episodic ergodic, episodic communicating,\ncontinuous ergodic, and continuous communicating. For each setting, we have selected twenty MDPs\nthat are diverse with respect to their diameters and environmental value norms as proxies for different\ncombinations of visitation complexity and estimation complexity. Figure 17 in Appendix E shows\nhow each of these MDPs varies according to these measures, and Section 3.3 empirically validates\nthis selection by showing that harder MDPs correspond to worse agent performance. Notably, the\ntheoretically backed selection of MDPs and the rigorous evaluation criteria make the Colosseum\nbenchmark the most exhaustive in tabular reinforcement learning, since previous evaluations were\nconducted empirically in a few MDPs (such as Taxi or RiverSwim).\n\nColosseum also allows testing of non-tabular agents by leveraging the BlockMDP model [28].\nBlockMDPs equip tabular MDPs with an emission map that is a (possibly stochastic) mapping q : S \u2192\n\u2206(O) from the finite state space S to a (possibly infinite) observation space O. Agents interacting\nwith BlockMDPs are only provided with observations, so non-tabular methods are generally required.\nMany commonly used non-tabular MDPs (such as Minecraft [29]) can be straightforwardly encoded\nas BlockMDPs using the Colosseum MDP families. Colosseum implements a diverse set of\ndeterministic emission maps and allows combining them with different sources of noise. Appendix\nA.2 further details BlockMDPs and the available emission maps.\n\n3.2 Empirical analysis of hardness measures\n\nFor brevity, this section only presents results of hardness measures in the MiniGridEmpty family of\nenvironments in the episodic setting. Appendix D presents the full outcome of the empirical analysis.\n\nA MiniGridEmpty MDP is a grid world where an agent has three available actions: moving forward,\nrotating left, and rotating right. An agent is rewarded for being in a few specific states and receives no\nreward in every other state. Appendix A.3.4 provides more details about this family of environments.\n\nIn our investigation, we consider four scenarios that highlight the different aspects of MDP hardness.\n\nScenario 1. We vary the probability p_rand that an MDP executes a random action instead of the\naction selected by an agent. As p_rand approaches one, value estimation becomes easier, since\noutcomes depend less on agent choices. However, intentionally visiting states becomes harder.\n\nScenario 2. We vary the probability p_lazy that an MDP stays in the same state instead of executing\nthe action selected by an agent. Contrary to increasing p_rand, increasing p_lazy never benefits\nexploration. Increasing p_lazy decreases estimation complexity and increases visitation complexity.\n\nScenario 3 and 4. We vary the number of states across MDPs from the same family. In scenario\n4, we also let p_rand = 0.1 to study the impact of stochasticity. In these scenarios, increasing the\nnumber of states simultaneously increases the estimation complexity and the visitation complexity.\n\nIn every scenario, hardness measures are compared with the cumulative regret of a near-optimal agent\ntuned for each specific MDP (see App. D). This regret serves as an optimistic measure of hardness.\nAppendix C describes how these measures are normalized. Note that, due to normalization, the plots\nshould only be compared in terms of trends (growth rates) rather than absolute values.\n\nAnalysis. Figure 1 presents the empirical results for the episodic MiniGridEmpty family in the four\nscenarios with 95% bootstrapped confidence intervals over twelve random seeds.\n\nThe experiments confirm our claim that the diameter captures visitation rather than estimation\ncomplexity. This measure of hardness grows superlinearly with both p_rand and p_lazy (Figures\n1a and 1b) since deliberate movement between states requires an exponentially increasing number of\ntime steps. Although the diameter highlights the sharply increasing visitation complexity, its trend\noverestimates the increase in cumulative regret of the tuned near-optimal agent, which is explained\nby the unaccounted decrease in estimation complexity. The diameter also increases almost linearly\nwith the number of states (Figures 1c and 1d). For the small p_rand (scenario 4), the relation is still\napproximately linear. This linear trend underestimates the evident non-linear growth in hardness in\nthe regret of the tuned near-optimal agent but is in line with the mild increase in visitation complexity.\n\nThe empirical evidence indicates that the environmental value norm can only capture estimation\ncomplexity. It decreases as p_lazy and p_rand increase (Figures 1a and 1b) because the optimal\n\n7\n\n\fFigure 1: The Colosseum hardness analysis for the episodic MiniGridEmpty family.\n\nvalue of neighboring states becomes closer, which decreases the per-step variability of the optimal\nvalue function. When the number of states increases but the transition and reward structures remain\nthe same (Figures 1c and 1d), the small increase in this variability only generates a sublinear growth.\n\nWe empirically observe that the sum of the reciprocals of the sub-optimality gaps is not particularly\napt at capturing estimation complexity, due to its exclusive focus on optimal policy identification, and\nit also underestimates the increase in hardness induced by an increase in visitation complexity. This\nmeasure increases weakly superlinearly in scenarios 1 and 2 (Figures 1a and 1b). The probability\nof executing the action selected by the agent decreases when p_lazy and p_rand increase, so the\ndifference between the state and the state-action optimal value functions decreases sharply. The\nmeasure increases almost linearly with the number of states (Figures 1c and 1d). This is explained by\nthe fact that the average value of the additional terms in the summation is often similar to the average\nvalue of the existing terms when MDPs have the same structure of reward and transition kernels.\n\n3.3 Colosseum benchmarking\n\nIn this section, we benchmark five tabular agents with theoretical guarantees and four non-tabular\nagents. Besides being valuable on their own, these results help to empirically validate our benchmark.\n\nAgents. The tabular agents are posterior sampling for reinforcement learning (PSRL) for the episodic\nand continuous settings [30, 31], Q-learning with UCB exploration for the episodic setting [32],\nQ-learning with optimism for the continuous setting [16], and UCRL2 for the continuous setting [14].\nThe non-tabular agents (from bsuite) are ActorCritic, ActorCriticRNN, BootDQN, and DQN.\n\nExperimental procedure. We set the total number of time steps to 500 000 with a maximum training\ntime of 10 minutes for the tabular setting and 40 minutes for the non-tabular setting. If an agent\ndoes not reach the maximum number of time steps before this time limit, learning is interrupted,\nand the agent continues interacting using its last best policy. This guarantees a fair comparison\nbetween agents with different computational costs. The performance indicators are computed every\n100 time steps. Each interaction between an agent and an MDP is repeated for 20 seeds. The agents\u2019\nhyperparameters have been chosen by random search to minimize the average regret across MDPs\nwith randomly sampled parameters (see Appendix E). We use a deterministic emission map that\nassigns a uniquely identifying vector to each state (for example, a gridworld coordinate) to derive\nthe non-tabular benchmark MDPs. In Table 2, we report the per-step normalized cumulative regrets\ndivided by the total number of time steps (defined in Appendix C), which allows comparisons across\ndifferent MDPs. We summarize the main findings here and refer to Appendix E for further details.\n\nAnalysis. Table 2 often shows high variability in the performance of the same agent across MDPs of\nthe same family. Therefore, maximising the diversity across diameters and value norms effectively\nproduces diverse challenges even for MDPs with similar transition and reward structures. For example,\nin the continuous communicating case (Table 2d), Q-learning performs well only in some MDPs of\nthe MiniGridEmpty family. This also happens for UCRL2 for the SimpleGrid family.\n\nThe average normalized cumulative regret is lower in ergodic environments compared to commu-\nnicating environments. This indicates that the ergodic setting is generally slightly easier than the\ncommunicating settings. Notably, in the continuous setting, the ergodic setting is more challenging\nthan the communicating setting for Q-learning (Tables 2c and 2d). Designing a naturally ergodic\n\n8\n\n0.00.20.40.6Probability of random action0.00.20.40.60.81.0Normalized values(a) Scenario 10.00.20.40.6Probability of lazy action(b) Scenario 2100200300400Number of states(c) Scenario 3100200300400Number of states(d) Scenario 4DiameterEnvironmental value normSum ofthe reciprocals of the sub-optimality gapsCumulative regret oftuned near-optimal agent\fTable 2: Normalized cumulative regrets of selected agents on the Colosseum benchmark. (a) Episodic\nergodic. (b) Episodic communicating. (c) Continuous ergodic. (d) Continuous communicating.\n\n(a)\n\n(b)\n\n(c)\n\n(d)\n\nQ-learning\n\nPSRL\n\nQ-learning\n\nPSRL\n\nQ-learning\n\nPSRL\n\nUCRL2\n\nQ-learning\n\nPSRL\n\nUCRL2\n\nMDP\n\nDeepSea\n\n.64 \u00b1 .00\n.52 \u00b1 .01\n\n.01 \u00b1 .00\n.00 \u00b1 .00\n\nMDP\n\nDeepSea\n\n.01 \u00b1 .01\n.83 \u00b1 .02\n\n.00 \u00b1 .00\n.54 \u00b1 .01\n\nMDP\n\nMDP\n\nDeepSea\n\n.94 \u00b1 .00\n\n.06 \u00b1 .01\n\n.23 \u00b1 .05\n\nDeepSea\n\nFrozenLake\n\n.83 \u00b1 .03\n\n.01 \u00b1 .03\n\n.01 \u00b1 .02\n\nFrozenLake\n\n.90 \u00b1 .01\n\n.01 \u00b1 .00\n\nFrozenLake\n\n.78 \u00b1 .04\n\n.03 \u00b1 .11\n\nMG-Empty\n\nMG-Empty\n\nMG-Rooms\n\nRiverSwim\n\nSimpleGrid\n\nTaxi\n\n1.00 \u00b1 .00\n1.00 \u00b1 .00\n1.00 \u00b1 .00\n1.00 \u00b1 .00\n1.00 \u00b1 .00\n.92 \u00b1 .04\n.91 \u00b1 .03\n\n.90 \u00b1 .04\n1.00 \u00b1 .00\n.99 \u00b1 .01\n\n.07 \u00b1 .02\n.91 \u00b1 .01\n\n.78 \u00b1 .03\n.79 \u00b1 .03\n.50 \u00b1 .03\n\n.84 \u00b1 .01\n.56 \u00b1 .02\n\n.86 \u00b1 .16\n.94 \u00b1 .07\n.91 \u00b1 .09\n.35 \u00b1 .10\n.44 \u00b1 .12\n.14 \u00b1 .08\n.04 \u00b1 .03\n\n.05 \u00b1 .04\n.54 \u00b1 .36\n.24 \u00b1 .29\n\n.00 \u00b1 .00\n.00 \u00b1 .00\n\n.05 \u00b1 .01\n.79 \u00b1 .03\n.50 \u00b1 .03\n\n.08 \u00b1 .01\n.05 \u00b1 .00\n\nMG-Empty\n\nMG-Rooms\n\nRiverSwim\n\nSimpleGrid\n\nTaxi\n\n.59 \u00b1 .07\n.99 \u00b1 .00\n.99 \u00b1 .01\n1.00 \u00b1 .00\n1.00 \u00b1 .00\n\n.99 \u00b1 .01\n1.00 \u00b1 .00\n1.00 \u00b1 .00\n.94 \u00b1 .05\n\n.87 \u00b1 .00\n.96 \u00b1 .01\n\n.78 \u00b1 .10\n.80 \u00b1 .00\n.50 \u00b1 .00\n.79 \u00b1 .04\n\n.94 \u00b1 .00\n.91 \u00b1 .01\n\n.09 \u00b1 .05\n.24 \u00b1 .15\n.23 \u00b1 .12\n.91 \u00b1 .09\n.93 \u00b1 .09\n\n.21 \u00b1 .29\n.44 \u00b1 .39\n.43 \u00b1 .39\n.04 \u00b1 .04\n\n.00 \u00b1 .00\n.80 \u00b1 .00\n\n.20 \u00b1 .15\n.55 \u00b1 .15\n.11 \u00b1 .01\n.79 \u00b1 .04\n\n.09 \u00b1 .01\n.36 \u00b1 .06\n\n.98 \u00b1 .02\n.98 \u00b1 .02\n.97 \u00b1 .00\n.98 \u00b1 .01\n.96 \u00b1 .01\n.98 \u00b1 .02\n.98 \u00b1 .03\n.98 \u00b1 .01\n\n.98 \u00b1 .03\n.98 \u00b1 .02\n\n.73 \u00b1 .19\n.71 \u00b1 .22\n.90 \u00b1 .06\n.50 \u00b1 .25\n\n.78 \u00b1 .00\n.46 \u00b1 .08\n.49 \u00b1 .00\n\n.99 \u00b1 .01\n.98 \u00b1 .04\n.95 \u00b1 .03\n.99 \u00b1 .01\n.83 \u00b1 .31\n.99 \u00b1 .02\n.99 \u00b1 .01\n.99 \u00b1 .01\n\n.99 \u00b1 .02\n1.00 \u00b1 .00\n\n.00 \u00b1 .00\n.00 \u00b1 .00\n.02 \u00b1 .04\n.01 \u00b1 .00\n\n.70 \u00b1 .19\n.01 \u00b1 .02\n.43 \u00b1 .16\n\n.05 \u00b1 .06\n.03 \u00b1 .05\n.04 \u00b1 .01\n.54 \u00b1 .26\n.01 \u00b1 .00\n.45 \u00b1 .35\n.27 \u00b1 .33\n.93 \u00b1 .09\n\n.18 \u00b1 .29\n.62 \u00b1 .36\n\n.00 \u00b1 .00\n.01 \u00b1 .00\n.01 \u00b1 .01\n.01 \u00b1 .01\n\n.01 \u00b1 .01\n.00 \u00b1 .00\n.00 \u00b1 .00\n\nFrozenLake\n\nMG-Empty\n\nMG-Rooms\n\nRiverSwim\n\nSimpleGrid\n\nMG-Rooms\n\nRiverSwim\n\nSimpleGrid\n\n.78 \u00b1 .00\n.99 \u00b1 .00\n.79 \u00b1 .00\n\n.77 \u00b1 .04\n.84 \u00b1 .04\n\n.51 \u00b1 .23\n.01 \u00b1 .00\n.00 \u00b1 .00\n.35 \u00b1 .17\n.75 \u00b1 .21\n\n.01 \u00b1 .01\n.01 \u00b1 .01\n.02 \u00b1 .02\n\n.16 \u00b1 .03\n.34 \u00b1 .14\n\n.11 \u00b1 .01\n.01 \u00b1 .00\n.15 \u00b1 .01\n.01 \u00b1 .00\n\n.78 \u00b1 .05\n.99 \u00b1 .00\n.79 \u00b1 .04\n\n.01 \u00b1 .04\n.01 \u00b1 .02\n\n.95 \u00b1 .22\n1.00 \u00b1 .00\n.60 \u00b1 .50\n1.00 \u00b1 .00\n1.00 \u00b1 .00\n\n1.00 \u00b1 .00\n1.00 \u00b1 .00\n1.00 \u00b1 .00\n\n.00 \u00b1 .01\n.01 \u00b1 .00\n\n.93 \u00b1 .00\n.45 \u00b1 .15\n.93 \u00b1 .00\n.50 \u00b1 .00\n\n.90 \u00b1 .01\n.99 \u00b1 .00\n.92 \u00b1 .01\n\n.01 \u00b1 .01\n.04 \u00b1 .06\n\n.02 \u00b1 .00\n.02 \u00b1 .00\n.01 \u00b1 .00\n.01 \u00b1 .00\n.08 \u00b1 .20\n\n.78 \u00b1 .40\n.02 \u00b1 .01\n.66 \u00b1 .47\n\n.00 \u00b1 .00\n.02 \u00b1 .01\n\n.01 \u00b1 .00\n.01 \u00b1 .00\n.70 \u00b1 .40\n.33 \u00b1 .24\n\nTaxi\n\n.87 \u00b1 .01\n\n.89 \u00b1 .08\n\n.09 \u00b1 .01\n\nTaxi\n\n.95 \u00b1 .00\n\n.94 \u00b1 .04\n\n.12 \u00b1 .01\n\nAverage\n\n.81 \u00b1 .24\n\n.30 \u00b1 .33\n\nAverage\n\n.83 \u00b1 .23\n\n.35 \u00b1 .30\n\nAverage\n\n.85 \u00b1 .18\n\n.59 \u00b1 .44\n\n.17 \u00b1 .26\n\nAverage\n\n.38 \u00b1 .37\n\n.69 \u00b1 .38\n\n.28 \u00b1 .37\n\nMDP is not straightforward. In fact, the majority of MDPs in the literature are communicating.\nIn Colosseum, ergodicity is induced by setting p_rand > 0 in otherwise communicating MDPs.\nModel-free agents struggle with the resulting increase in variability of the state-action value function.\n\nIn the episodic settings (Tables 2a and 2b), PSRL obtains excellent performances with low variability.\nQ-learning instead performs well in a few MDPs. This often happens since, when the action selected\nby the agent is randomly substituted (due to p_rand > 0) with one with a large sub-optimality gap,\nthe resulting Q-value update introduces a critical error that requires many samples to be corrected.\n\nIn the continuous settings (Tables 2c and 2d), UCRL2 performs best in the ergodic cases when\nQ-learning suffers from the issue caused by p_rand > 0 but is only slightly better than Q-learning\nin the communicating ones. PSRL instead struggles with most MDPs. The reason for its weak\nperformance in this setting is the computationally expensive optimistic sampling procedure required\nfor its worst-case theoretical guarantees. It often breaks the time limit before reaching the first quarter\nof available time steps, meaning that it lacks sufficient samples to estimate the optimal policy.\n\nFigure 2 places the regret of the agents in the continuous ergodic setting (Table 2c) on a position\ncorresponding to the diameter and value norm of the benchmark environments. PSRL and Q-learning\n(Figures 2a and 2b), appear to be impacted more by the value norm than the diameter. This is in line\nwith the lack of sufficient samples for PSRL and the aforementioned issue related to high q estimates\nvariability for Q-learning, which is exacerbated when the estimation complexity is higher. In the\ncase of UCRL2 (Figure 2c), which provides more reliable evidence since it performs well across the\nMDPs, higher regret effectively corresponds to higher diameter and value norm.\n\nBased on the section(s), please answer the following question with yes, no, or n/a and provide a brief justification for your answer.\nQuestion: If the authors ran experiments, do the authors report error bars (e.g., with respect to the random seed after running experiments multiple times)?"}, "3d": {"role": "user", "content": "The following is the experiments section of the paper you are reviewing:\n\n\n3.1 Colosseum\n\nThis section briefly introduces Colosseum, a pioneering Python package that bridges theory and\npractice in tabular reinforcement learning while also being applicable in the non-tabular setting. More\ndetails about the package can be found in Appendix A and in the project website.2\n\nAs a hardness analysis tool, Colosseum identifies the communication class of MDPs, assembles\ninsightful visualizations and logs of interactions between agents and MDPs, computes three measures\nof hardness (environmental value norm, sum of the reciprocals of the sub-optimality gaps, and\ndiameter, whose computation requires a novel solution described in App. A.4). Eight MDP families\nare available for experimentation. Some are traditional families (RiverSwim [24], Taxi [25], and\nFrozenLake) while others are more recent (MiniGid environments [26]). Additionally, DeepSea\n[27] was included as a hard exploration family of problems, and the SimpleGrid family is composed\nof simplified versions of the MG\u2013Empty environment. By controlling the parameters of MDPs from\neach family (further detailed in App. A.3), it is easy to create an MDP with any desired hardness.\n\nAs a benchmarking tool, Colosseum is unique in its strong connection with theory. For instance,\nin contrast to non-tabular benchmarks, Colosseum computes theoretical evaluation criteria such\n\n1For example, the \u2126(|S||A|H 2\u03f5\u22122) sample complexity bound for the episodic communicating setting [22].\n2Available at https://michelangeloconserva.github.io/Colosseum.\n\n6\n\n\fas the expected cumulative regret and the expected average future reward, which can be used to\nexactly evaluate the performance criterion of regret minimizing agents. The benchmark covers the\nmost commonly studied reinforcement learning settings: episodic ergodic, episodic communicating,\ncontinuous ergodic, and continuous communicating. For each setting, we have selected twenty MDPs\nthat are diverse with respect to their diameters and environmental value norms as proxies for different\ncombinations of visitation complexity and estimation complexity. Figure 17 in Appendix E shows\nhow each of these MDPs varies according to these measures, and Section 3.3 empirically validates\nthis selection by showing that harder MDPs correspond to worse agent performance. Notably, the\ntheoretically backed selection of MDPs and the rigorous evaluation criteria make the Colosseum\nbenchmark the most exhaustive in tabular reinforcement learning, since previous evaluations were\nconducted empirically in a few MDPs (such as Taxi or RiverSwim).\n\nColosseum also allows testing of non-tabular agents by leveraging the BlockMDP model [28].\nBlockMDPs equip tabular MDPs with an emission map that is a (possibly stochastic) mapping q : S \u2192\n\u2206(O) from the finite state space S to a (possibly infinite) observation space O. Agents interacting\nwith BlockMDPs are only provided with observations, so non-tabular methods are generally required.\nMany commonly used non-tabular MDPs (such as Minecraft [29]) can be straightforwardly encoded\nas BlockMDPs using the Colosseum MDP families. Colosseum implements a diverse set of\ndeterministic emission maps and allows combining them with different sources of noise. Appendix\nA.2 further details BlockMDPs and the available emission maps.\n\n3.2 Empirical analysis of hardness measures\n\nFor brevity, this section only presents results of hardness measures in the MiniGridEmpty family of\nenvironments in the episodic setting. Appendix D presents the full outcome of the empirical analysis.\n\nA MiniGridEmpty MDP is a grid world where an agent has three available actions: moving forward,\nrotating left, and rotating right. An agent is rewarded for being in a few specific states and receives no\nreward in every other state. Appendix A.3.4 provides more details about this family of environments.\n\nIn our investigation, we consider four scenarios that highlight the different aspects of MDP hardness.\n\nScenario 1. We vary the probability p_rand that an MDP executes a random action instead of the\naction selected by an agent. As p_rand approaches one, value estimation becomes easier, since\noutcomes depend less on agent choices. However, intentionally visiting states becomes harder.\n\nScenario 2. We vary the probability p_lazy that an MDP stays in the same state instead of executing\nthe action selected by an agent. Contrary to increasing p_rand, increasing p_lazy never benefits\nexploration. Increasing p_lazy decreases estimation complexity and increases visitation complexity.\n\nScenario 3 and 4. We vary the number of states across MDPs from the same family. In scenario\n4, we also let p_rand = 0.1 to study the impact of stochasticity. In these scenarios, increasing the\nnumber of states simultaneously increases the estimation complexity and the visitation complexity.\n\nIn every scenario, hardness measures are compared with the cumulative regret of a near-optimal agent\ntuned for each specific MDP (see App. D). This regret serves as an optimistic measure of hardness.\nAppendix C describes how these measures are normalized. Note that, due to normalization, the plots\nshould only be compared in terms of trends (growth rates) rather than absolute values.\n\nAnalysis. Figure 1 presents the empirical results for the episodic MiniGridEmpty family in the four\nscenarios with 95% bootstrapped confidence intervals over twelve random seeds.\n\nThe experiments confirm our claim that the diameter captures visitation rather than estimation\ncomplexity. This measure of hardness grows superlinearly with both p_rand and p_lazy (Figures\n1a and 1b) since deliberate movement between states requires an exponentially increasing number of\ntime steps. Although the diameter highlights the sharply increasing visitation complexity, its trend\noverestimates the increase in cumulative regret of the tuned near-optimal agent, which is explained\nby the unaccounted decrease in estimation complexity. The diameter also increases almost linearly\nwith the number of states (Figures 1c and 1d). For the small p_rand (scenario 4), the relation is still\napproximately linear. This linear trend underestimates the evident non-linear growth in hardness in\nthe regret of the tuned near-optimal agent but is in line with the mild increase in visitation complexity.\n\nThe empirical evidence indicates that the environmental value norm can only capture estimation\ncomplexity. It decreases as p_lazy and p_rand increase (Figures 1a and 1b) because the optimal\n\n7\n\n\fFigure 1: The Colosseum hardness analysis for the episodic MiniGridEmpty family.\n\nvalue of neighboring states becomes closer, which decreases the per-step variability of the optimal\nvalue function. When the number of states increases but the transition and reward structures remain\nthe same (Figures 1c and 1d), the small increase in this variability only generates a sublinear growth.\n\nWe empirically observe that the sum of the reciprocals of the sub-optimality gaps is not particularly\napt at capturing estimation complexity, due to its exclusive focus on optimal policy identification, and\nit also underestimates the increase in hardness induced by an increase in visitation complexity. This\nmeasure increases weakly superlinearly in scenarios 1 and 2 (Figures 1a and 1b). The probability\nof executing the action selected by the agent decreases when p_lazy and p_rand increase, so the\ndifference between the state and the state-action optimal value functions decreases sharply. The\nmeasure increases almost linearly with the number of states (Figures 1c and 1d). This is explained by\nthe fact that the average value of the additional terms in the summation is often similar to the average\nvalue of the existing terms when MDPs have the same structure of reward and transition kernels.\n\n3.3 Colosseum benchmarking\n\nIn this section, we benchmark five tabular agents with theoretical guarantees and four non-tabular\nagents. Besides being valuable on their own, these results help to empirically validate our benchmark.\n\nAgents. The tabular agents are posterior sampling for reinforcement learning (PSRL) for the episodic\nand continuous settings [30, 31], Q-learning with UCB exploration for the episodic setting [32],\nQ-learning with optimism for the continuous setting [16], and UCRL2 for the continuous setting [14].\nThe non-tabular agents (from bsuite) are ActorCritic, ActorCriticRNN, BootDQN, and DQN.\n\nExperimental procedure. We set the total number of time steps to 500 000 with a maximum training\ntime of 10 minutes for the tabular setting and 40 minutes for the non-tabular setting. If an agent\ndoes not reach the maximum number of time steps before this time limit, learning is interrupted,\nand the agent continues interacting using its last best policy. This guarantees a fair comparison\nbetween agents with different computational costs. The performance indicators are computed every\n100 time steps. Each interaction between an agent and an MDP is repeated for 20 seeds. The agents\u2019\nhyperparameters have been chosen by random search to minimize the average regret across MDPs\nwith randomly sampled parameters (see Appendix E). We use a deterministic emission map that\nassigns a uniquely identifying vector to each state (for example, a gridworld coordinate) to derive\nthe non-tabular benchmark MDPs. In Table 2, we report the per-step normalized cumulative regrets\ndivided by the total number of time steps (defined in Appendix C), which allows comparisons across\ndifferent MDPs. We summarize the main findings here and refer to Appendix E for further details.\n\nAnalysis. Table 2 often shows high variability in the performance of the same agent across MDPs of\nthe same family. Therefore, maximising the diversity across diameters and value norms effectively\nproduces diverse challenges even for MDPs with similar transition and reward structures. For example,\nin the continuous communicating case (Table 2d), Q-learning performs well only in some MDPs of\nthe MiniGridEmpty family. This also happens for UCRL2 for the SimpleGrid family.\n\nThe average normalized cumulative regret is lower in ergodic environments compared to commu-\nnicating environments. This indicates that the ergodic setting is generally slightly easier than the\ncommunicating settings. Notably, in the continuous setting, the ergodic setting is more challenging\nthan the communicating setting for Q-learning (Tables 2c and 2d). Designing a naturally ergodic\n\n8\n\n0.00.20.40.6Probability of random action0.00.20.40.60.81.0Normalized values(a) Scenario 10.00.20.40.6Probability of lazy action(b) Scenario 2100200300400Number of states(c) Scenario 3100200300400Number of states(d) Scenario 4DiameterEnvironmental value normSum ofthe reciprocals of the sub-optimality gapsCumulative regret oftuned near-optimal agent\fTable 2: Normalized cumulative regrets of selected agents on the Colosseum benchmark. (a) Episodic\nergodic. (b) Episodic communicating. (c) Continuous ergodic. (d) Continuous communicating.\n\n(a)\n\n(b)\n\n(c)\n\n(d)\n\nQ-learning\n\nPSRL\n\nQ-learning\n\nPSRL\n\nQ-learning\n\nPSRL\n\nUCRL2\n\nQ-learning\n\nPSRL\n\nUCRL2\n\nMDP\n\nDeepSea\n\n.64 \u00b1 .00\n.52 \u00b1 .01\n\n.01 \u00b1 .00\n.00 \u00b1 .00\n\nMDP\n\nDeepSea\n\n.01 \u00b1 .01\n.83 \u00b1 .02\n\n.00 \u00b1 .00\n.54 \u00b1 .01\n\nMDP\n\nMDP\n\nDeepSea\n\n.94 \u00b1 .00\n\n.06 \u00b1 .01\n\n.23 \u00b1 .05\n\nDeepSea\n\nFrozenLake\n\n.83 \u00b1 .03\n\n.01 \u00b1 .03\n\n.01 \u00b1 .02\n\nFrozenLake\n\n.90 \u00b1 .01\n\n.01 \u00b1 .00\n\nFrozenLake\n\n.78 \u00b1 .04\n\n.03 \u00b1 .11\n\nMG-Empty\n\nMG-Empty\n\nMG-Rooms\n\nRiverSwim\n\nSimpleGrid\n\nTaxi\n\n1.00 \u00b1 .00\n1.00 \u00b1 .00\n1.00 \u00b1 .00\n1.00 \u00b1 .00\n1.00 \u00b1 .00\n.92 \u00b1 .04\n.91 \u00b1 .03\n\n.90 \u00b1 .04\n1.00 \u00b1 .00\n.99 \u00b1 .01\n\n.07 \u00b1 .02\n.91 \u00b1 .01\n\n.78 \u00b1 .03\n.79 \u00b1 .03\n.50 \u00b1 .03\n\n.84 \u00b1 .01\n.56 \u00b1 .02\n\n.86 \u00b1 .16\n.94 \u00b1 .07\n.91 \u00b1 .09\n.35 \u00b1 .10\n.44 \u00b1 .12\n.14 \u00b1 .08\n.04 \u00b1 .03\n\n.05 \u00b1 .04\n.54 \u00b1 .36\n.24 \u00b1 .29\n\n.00 \u00b1 .00\n.00 \u00b1 .00\n\n.05 \u00b1 .01\n.79 \u00b1 .03\n.50 \u00b1 .03\n\n.08 \u00b1 .01\n.05 \u00b1 .00\n\nMG-Empty\n\nMG-Rooms\n\nRiverSwim\n\nSimpleGrid\n\nTaxi\n\n.59 \u00b1 .07\n.99 \u00b1 .00\n.99 \u00b1 .01\n1.00 \u00b1 .00\n1.00 \u00b1 .00\n\n.99 \u00b1 .01\n1.00 \u00b1 .00\n1.00 \u00b1 .00\n.94 \u00b1 .05\n\n.87 \u00b1 .00\n.96 \u00b1 .01\n\n.78 \u00b1 .10\n.80 \u00b1 .00\n.50 \u00b1 .00\n.79 \u00b1 .04\n\n.94 \u00b1 .00\n.91 \u00b1 .01\n\n.09 \u00b1 .05\n.24 \u00b1 .15\n.23 \u00b1 .12\n.91 \u00b1 .09\n.93 \u00b1 .09\n\n.21 \u00b1 .29\n.44 \u00b1 .39\n.43 \u00b1 .39\n.04 \u00b1 .04\n\n.00 \u00b1 .00\n.80 \u00b1 .00\n\n.20 \u00b1 .15\n.55 \u00b1 .15\n.11 \u00b1 .01\n.79 \u00b1 .04\n\n.09 \u00b1 .01\n.36 \u00b1 .06\n\n.98 \u00b1 .02\n.98 \u00b1 .02\n.97 \u00b1 .00\n.98 \u00b1 .01\n.96 \u00b1 .01\n.98 \u00b1 .02\n.98 \u00b1 .03\n.98 \u00b1 .01\n\n.98 \u00b1 .03\n.98 \u00b1 .02\n\n.73 \u00b1 .19\n.71 \u00b1 .22\n.90 \u00b1 .06\n.50 \u00b1 .25\n\n.78 \u00b1 .00\n.46 \u00b1 .08\n.49 \u00b1 .00\n\n.99 \u00b1 .01\n.98 \u00b1 .04\n.95 \u00b1 .03\n.99 \u00b1 .01\n.83 \u00b1 .31\n.99 \u00b1 .02\n.99 \u00b1 .01\n.99 \u00b1 .01\n\n.99 \u00b1 .02\n1.00 \u00b1 .00\n\n.00 \u00b1 .00\n.00 \u00b1 .00\n.02 \u00b1 .04\n.01 \u00b1 .00\n\n.70 \u00b1 .19\n.01 \u00b1 .02\n.43 \u00b1 .16\n\n.05 \u00b1 .06\n.03 \u00b1 .05\n.04 \u00b1 .01\n.54 \u00b1 .26\n.01 \u00b1 .00\n.45 \u00b1 .35\n.27 \u00b1 .33\n.93 \u00b1 .09\n\n.18 \u00b1 .29\n.62 \u00b1 .36\n\n.00 \u00b1 .00\n.01 \u00b1 .00\n.01 \u00b1 .01\n.01 \u00b1 .01\n\n.01 \u00b1 .01\n.00 \u00b1 .00\n.00 \u00b1 .00\n\nFrozenLake\n\nMG-Empty\n\nMG-Rooms\n\nRiverSwim\n\nSimpleGrid\n\nMG-Rooms\n\nRiverSwim\n\nSimpleGrid\n\n.78 \u00b1 .00\n.99 \u00b1 .00\n.79 \u00b1 .00\n\n.77 \u00b1 .04\n.84 \u00b1 .04\n\n.51 \u00b1 .23\n.01 \u00b1 .00\n.00 \u00b1 .00\n.35 \u00b1 .17\n.75 \u00b1 .21\n\n.01 \u00b1 .01\n.01 \u00b1 .01\n.02 \u00b1 .02\n\n.16 \u00b1 .03\n.34 \u00b1 .14\n\n.11 \u00b1 .01\n.01 \u00b1 .00\n.15 \u00b1 .01\n.01 \u00b1 .00\n\n.78 \u00b1 .05\n.99 \u00b1 .00\n.79 \u00b1 .04\n\n.01 \u00b1 .04\n.01 \u00b1 .02\n\n.95 \u00b1 .22\n1.00 \u00b1 .00\n.60 \u00b1 .50\n1.00 \u00b1 .00\n1.00 \u00b1 .00\n\n1.00 \u00b1 .00\n1.00 \u00b1 .00\n1.00 \u00b1 .00\n\n.00 \u00b1 .01\n.01 \u00b1 .00\n\n.93 \u00b1 .00\n.45 \u00b1 .15\n.93 \u00b1 .00\n.50 \u00b1 .00\n\n.90 \u00b1 .01\n.99 \u00b1 .00\n.92 \u00b1 .01\n\n.01 \u00b1 .01\n.04 \u00b1 .06\n\n.02 \u00b1 .00\n.02 \u00b1 .00\n.01 \u00b1 .00\n.01 \u00b1 .00\n.08 \u00b1 .20\n\n.78 \u00b1 .40\n.02 \u00b1 .01\n.66 \u00b1 .47\n\n.00 \u00b1 .00\n.02 \u00b1 .01\n\n.01 \u00b1 .00\n.01 \u00b1 .00\n.70 \u00b1 .40\n.33 \u00b1 .24\n\nTaxi\n\n.87 \u00b1 .01\n\n.89 \u00b1 .08\n\n.09 \u00b1 .01\n\nTaxi\n\n.95 \u00b1 .00\n\n.94 \u00b1 .04\n\n.12 \u00b1 .01\n\nAverage\n\n.81 \u00b1 .24\n\n.30 \u00b1 .33\n\nAverage\n\n.83 \u00b1 .23\n\n.35 \u00b1 .30\n\nAverage\n\n.85 \u00b1 .18\n\n.59 \u00b1 .44\n\n.17 \u00b1 .26\n\nAverage\n\n.38 \u00b1 .37\n\n.69 \u00b1 .38\n\n.28 \u00b1 .37\n\nMDP is not straightforward. In fact, the majority of MDPs in the literature are communicating.\nIn Colosseum, ergodicity is induced by setting p_rand > 0 in otherwise communicating MDPs.\nModel-free agents struggle with the resulting increase in variability of the state-action value function.\n\nIn the episodic settings (Tables 2a and 2b), PSRL obtains excellent performances with low variability.\nQ-learning instead performs well in a few MDPs. This often happens since, when the action selected\nby the agent is randomly substituted (due to p_rand > 0) with one with a large sub-optimality gap,\nthe resulting Q-value update introduces a critical error that requires many samples to be corrected.\n\nIn the continuous settings (Tables 2c and 2d), UCRL2 performs best in the ergodic cases when\nQ-learning suffers from the issue caused by p_rand > 0 but is only slightly better than Q-learning\nin the communicating ones. PSRL instead struggles with most MDPs. The reason for its weak\nperformance in this setting is the computationally expensive optimistic sampling procedure required\nfor its worst-case theoretical guarantees. It often breaks the time limit before reaching the first quarter\nof available time steps, meaning that it lacks sufficient samples to estimate the optimal policy.\n\nFigure 2 places the regret of the agents in the continuous ergodic setting (Table 2c) on a position\ncorresponding to the diameter and value norm of the benchmark environments. PSRL and Q-learning\n(Figures 2a and 2b), appear to be impacted more by the value norm than the diameter. This is in line\nwith the lack of sufficient samples for PSRL and the aforementioned issue related to high q estimates\nvariability for Q-learning, which is exacerbated when the estimation complexity is higher. In the\ncase of UCRL2 (Figure 2c), which provides more reliable evidence since it performs well across the\nMDPs, higher regret effectively corresponds to higher diameter and value norm.\n\nThe following is the appendix_D section of the paper you are reviewing:\n\n\nIn our empirical investigation of the measures of hardness, we consider five MDP families (MG-Empty,\nSimpleGrid, FrozenLake, RiverSwim, and DeepSea) that include different levels of stochasticity\nand challenge. Each MDP family is tested in four scenarios that highlight different aspects of hardness.\nNote that each measure has been normalized (as described in App. C), which solely allows comparing\ntrends (growth rates). Figures 12, 13, 14, 15 and 16 (pg. 28) report the results of our investigation\nalong with the 95% bootstrapped confidence intervals over twelve seeds.\n\nScenario 1. We vary the probability p_rand that an MDP executes a random action instead of the\naction selected by an agent. As p_rand increases, estimating the optimal value function becomes\neasier since every policy yields increasingly similar value functions. This produces a decrease in the\nestimation complexity. However, intentionally visiting states becomes harder.\n\nScenario 2. We vary the probability p_lazy that an MDP stays in the same state instead of\nexecuting the action selected by an agent. Contrary to increasing p_rand, increasing p_lazy never\nbenefits exploration through the execution of random actions. Increasing p_lazy decreases estimation\ncomplexity and increases visitation complexity.\n\nScenario 3 and 4. We vary the number of states across MDPs from the same family. In scenario 4,\nwe also let p_rand = 0.1 to study the impact of stochasticity. In these scenarios, increments in the\nnumber of states increase both estimation complexity and visitation complexity.\n\nCumulative regret of a near-optimal agent.\nIn every scenario, the measures of hardness are com-\npared with the cumulative regret of a near-optimal agent that serves as an optimistic approximation\nof a complete measure of hardness. The near-optimal agents have been chosen between the ones\navailable in Colosseum with the lowest average cumulative regret in the benchmarking results (such\nas PSRL in the episodic setting and UCRL2 in the continuous setting). In order to optimistically\napproximate a complete measure of hardness, we tune the hyperparameters of the agents for each\nMDP in every scenario. Concretely, we perform a random search with the objective of minimizing\nthe average cumulative regret resulting from an interaction of the agent with the MDP that lasts for\n200 000 time steps with a maximum time limit of two minutes across three seeds. The budget for the\nrandom search is 120 samples.\n\nComputational power. The empirical investigation has been carried out on a desktop PC equipped\nwith an AMD Ryzen 9 5950X 16-Core Processor and required less than 24 hours for all the MDP fam-\nilies and scenarios. The most computationally intensive part of the procedure is the hyperparameter\nsearch.\n\nLimitations. The main limitation of our empirical investigation is the selection of the MDP families,\nnear-optimal agents, and scenarios. Although we believe to have proposed a solid methodology, we\nare open to discussing the inclusion of additional experiments to further enhance Colosseum.\n\nD.1 Analysis of results\n\nDiameter. The diameter grows superlinearly with both p_rand and p_lazy since deliberate move-\nment between states requires an exponentially increasing number of time steps. As clearly shown in\nthe figures, this phenomenon is exacerbated in the episodic setting. If the agent is forced to take a\nrandom action or to stay in the same state, it can miss the opportunity to reach the target state in the\ncurrent episode and has to try again in the next episode. Although the diameter highlights this sharply\nincreasing visitation complexity, its trend overestimates the increase in cumulative regret of the\ntuned near-optimal agent, which is explained by the unaccounted decrease in estimation complexity.\nThe diameter also increases almost linearly with the number of states. When p_rand is relatively\nsmall, an approximately linear relationship can still be observed. This linear trend underestimates the\nnon-linear growth in hardness clearly shown in the cumulative regret of the tuned near-optimal agent\nbut is in line with the mild increase in visitation complexity. FrozenLake in the episodic setting\n(Figures 14c and 14d) represents the only exception. Given the extremely high level of stochasticity\nof the MDP, increasing the number of states drastically increases the visitation complexity while\nmaking it easier for the agent to act near-optimally.\n\n26\n\n\fEnvironmental value norm. The environmental value norm decreases as p_lazy and p_rand\nincrease because the optimal value of neighboring states becomes closer, which decreases the per-step\nvariability of the optimal state value function. However, we note that for the MG-Empty and the\nFrozenLake MDP families in the continuous cases (see Figures 12f and 14f) as p_lazy increases,\nthe environmental value norm first decreases and later increases. From a certain value of p_lazy\nonward, there is a significant probability of the agent remaining in the same state. This provokes\nlarge changes in the value of states that are distant from the highly rewarding states and no changes at\nall for highly rewarding states since the lazy transition is comparable to taking the optimal action.\nDue to the large changes in the suboptimal region of the state space and the absence of changes in the\noptimal region of the state space, the overall one-step variability of the state value function increases.\nNote that this does not happen in the episodic case due to the restarting mechanism and whether it\nhappens or not in the continuous case depends on the transition and reward structure of an MDP.\nWhen the number of states increases but the transition and reward structures remain the same, the\nsmall increase in measured variability only causes the environmental value norm to grow sublinearly.\nThese findings are strong evidence that this measure is only suited to capture estimation complexity.\n\nSum of the reciprocals of the sub-optimality gaps. The sum of the reciprocals of the sub-\noptimality gaps increases weakly superlinearly in scenarios 1 and 2. The probability of executing\nthe action selected by the agent decreases when p_lazy and p_rand increase, and so the difference\nbetween the optimal value function and the optimal state-action value function decreases sharply.\nFrozenLake (Figures 14a, 14b, 14e and 14f) represents an exception as the sum of the reciprocals\nof the sub-optimality gaps is almost constant. FrozenLake naturally incorporates an exceptionally\nhigh level of stochasticity and so varying p_lazy and p_rand does not significantly affect the value\nfunctions. The sum of the reciprocals of the sub-optimality gaps increases almost linearly with the\nnumber of states. This is explained by the fact that the average value of the additional terms in the\nsummation is often similar to the average value of the existing terms given the same structure of\nreward and transition kernels. This measure of hardness is not particularly apt at capturing estimation\ncomplexity, since it focuses solely on optimal policy identification. It also underestimates the increase\nin hardness induced by an increase in visitation complexity.\n\nCumulative regret of the tuned near-optimal agent. The trends of the cumulative regret of the\ntuned near-optimal agent present more variability when compared to the theoretical measures of\nhardness. This reflects the fact that this is an approximation of a complete measure of hardness\nbased on agents that have specific strengths and weaknesses. Overall, we note a tendency of\nsuperlinear growth in scenarios 1 and 2. Such tendency is specifically marked for the grid worlds,\nsuch as MG-Empty (see Fig. 12) and SimpleGrid (see Fig. 13). In these MDP families, the highly\nrewarding states are located far from the starting states and therefore the visitation complexity plays\na fundamental role. In the FrokenLake family (see Fig. 14), the trend is linear (episodic setting)\nor sub-linear (continuous setting), which is caused by the relatively low impact of the parameters\np_rand and p_lazy in the already highly stochastic MDPs. The cumulative regret of the tuned\nnear-optimal agent presents a moderately superlinear growth in the episodic case and remains almost\nconstant for the RiverSwim family (see Fig. 15). This results from the fact that, in the continuous\ncase, the absence of the restarting mechanism in combination with the chain structure of the MDP\nallows the agent to suffer only minimal impact from the increasing values of the parameters p_rand\nand p_lazy. Finally, for the DeepSea family, the regret is constant. Increases in p_rand dramatically\nreduce the possibility of visiting the highly rewarding state due to the pyramid structure of the MDP.\nIn scenarios 3 and 4, the overall tendency is still superlinear but less marked compared to scenarios\n1 and 2. The superlinear growth is most evident for the MG-Empty family (see Fig. 12) and for the\nRiverSwim family in the episodic case (see Figures 15c and 15d). For the MG-Empty, when the\ngrid size is increased, and with it the number of states, the MDP becomes increasingly challenging\nto navigate since the agent has to coordinate its rotation with its forward movement in order to\neffectively transition between states. For the RiverSwim family, the challenge comes from the\nrestarting mechanism on a chain structure. The agent is required to take a perfect sequence of actions\nin order to visit the last state of the chain, otherwise it will be reset to the start. In the continuous\nsetting (see Figures 15g and 15h), instead, the trends are mostly linear, similarly to what happens in\nscenarios 1 and 2. The less challenging structure of the SimpleGrid family (see Fig. 13) induces\nweakly superlinear trends of the cumulative regret of the tuned near-optimal agent. We note that,\ncontrary to the theoretical measures of hardness, the episodic setting does not appear to be harder\n(which would be suggested by steeper trends). This discrepancy is particularly noticeable in the\n\n27\n\n\fFrozenLake family (see Fig. 14) which yields a mostly linear trend in the continuous settings and\nclearly sublinear trends in the episodic settings. In the DeepSea family, the cumulative regret of the\ntuned near-optimal agent is almost constant in scenario 3 and almost linear in scenario 4. The main\nchallenge for this family lies in the pyramidal structure of the MDP rather than the number of states.\nHowever, setting p_rand = 0.1 creates a more challenging task for the agent as more time steps are\nrequired to find the highly rewarding state. We also note that the difference in results between the\nepisodic and continuous settings is minimal, which is unsurprising given the MDP structure.\n\nBased on the section(s), please answer the following question with yes, no, or n/a and provide a brief justification for your answer.\nQuestion: If the authors ran experiments, do the authors include the total amount of compute and the type of resources used (e.g., type of GPUs, internal cluster, or cloud provider)?"}, "4a": {"role": "user", "content": "The following is the experiments section of the paper you are reviewing:\n\n\n3.1 Colosseum\n\nThis section briefly introduces Colosseum, a pioneering Python package that bridges theory and\npractice in tabular reinforcement learning while also being applicable in the non-tabular setting. More\ndetails about the package can be found in Appendix A and in the project website.2\n\nAs a hardness analysis tool, Colosseum identifies the communication class of MDPs, assembles\ninsightful visualizations and logs of interactions between agents and MDPs, computes three measures\nof hardness (environmental value norm, sum of the reciprocals of the sub-optimality gaps, and\ndiameter, whose computation requires a novel solution described in App. A.4). Eight MDP families\nare available for experimentation. Some are traditional families (RiverSwim [24], Taxi [25], and\nFrozenLake) while others are more recent (MiniGid environments [26]). Additionally, DeepSea\n[27] was included as a hard exploration family of problems, and the SimpleGrid family is composed\nof simplified versions of the MG\u2013Empty environment. By controlling the parameters of MDPs from\neach family (further detailed in App. A.3), it is easy to create an MDP with any desired hardness.\n\nAs a benchmarking tool, Colosseum is unique in its strong connection with theory. For instance,\nin contrast to non-tabular benchmarks, Colosseum computes theoretical evaluation criteria such\n\n1For example, the \u2126(|S||A|H 2\u03f5\u22122) sample complexity bound for the episodic communicating setting [22].\n2Available at https://michelangeloconserva.github.io/Colosseum.\n\n6\n\n\fas the expected cumulative regret and the expected average future reward, which can be used to\nexactly evaluate the performance criterion of regret minimizing agents. The benchmark covers the\nmost commonly studied reinforcement learning settings: episodic ergodic, episodic communicating,\ncontinuous ergodic, and continuous communicating. For each setting, we have selected twenty MDPs\nthat are diverse with respect to their diameters and environmental value norms as proxies for different\ncombinations of visitation complexity and estimation complexity. Figure 17 in Appendix E shows\nhow each of these MDPs varies according to these measures, and Section 3.3 empirically validates\nthis selection by showing that harder MDPs correspond to worse agent performance. Notably, the\ntheoretically backed selection of MDPs and the rigorous evaluation criteria make the Colosseum\nbenchmark the most exhaustive in tabular reinforcement learning, since previous evaluations were\nconducted empirically in a few MDPs (such as Taxi or RiverSwim).\n\nColosseum also allows testing of non-tabular agents by leveraging the BlockMDP model [28].\nBlockMDPs equip tabular MDPs with an emission map that is a (possibly stochastic) mapping q : S \u2192\n\u2206(O) from the finite state space S to a (possibly infinite) observation space O. Agents interacting\nwith BlockMDPs are only provided with observations, so non-tabular methods are generally required.\nMany commonly used non-tabular MDPs (such as Minecraft [29]) can be straightforwardly encoded\nas BlockMDPs using the Colosseum MDP families. Colosseum implements a diverse set of\ndeterministic emission maps and allows combining them with different sources of noise. Appendix\nA.2 further details BlockMDPs and the available emission maps.\n\n3.2 Empirical analysis of hardness measures\n\nFor brevity, this section only presents results of hardness measures in the MiniGridEmpty family of\nenvironments in the episodic setting. Appendix D presents the full outcome of the empirical analysis.\n\nA MiniGridEmpty MDP is a grid world where an agent has three available actions: moving forward,\nrotating left, and rotating right. An agent is rewarded for being in a few specific states and receives no\nreward in every other state. Appendix A.3.4 provides more details about this family of environments.\n\nIn our investigation, we consider four scenarios that highlight the different aspects of MDP hardness.\n\nScenario 1. We vary the probability p_rand that an MDP executes a random action instead of the\naction selected by an agent. As p_rand approaches one, value estimation becomes easier, since\noutcomes depend less on agent choices. However, intentionally visiting states becomes harder.\n\nScenario 2. We vary the probability p_lazy that an MDP stays in the same state instead of executing\nthe action selected by an agent. Contrary to increasing p_rand, increasing p_lazy never benefits\nexploration. Increasing p_lazy decreases estimation complexity and increases visitation complexity.\n\nScenario 3 and 4. We vary the number of states across MDPs from the same family. In scenario\n4, we also let p_rand = 0.1 to study the impact of stochasticity. In these scenarios, increasing the\nnumber of states simultaneously increases the estimation complexity and the visitation complexity.\n\nIn every scenario, hardness measures are compared with the cumulative regret of a near-optimal agent\ntuned for each specific MDP (see App. D). This regret serves as an optimistic measure of hardness.\nAppendix C describes how these measures are normalized. Note that, due to normalization, the plots\nshould only be compared in terms of trends (growth rates) rather than absolute values.\n\nAnalysis. Figure 1 presents the empirical results for the episodic MiniGridEmpty family in the four\nscenarios with 95% bootstrapped confidence intervals over twelve random seeds.\n\nThe experiments confirm our claim that the diameter captures visitation rather than estimation\ncomplexity. This measure of hardness grows superlinearly with both p_rand and p_lazy (Figures\n1a and 1b) since deliberate movement between states requires an exponentially increasing number of\ntime steps. Although the diameter highlights the sharply increasing visitation complexity, its trend\noverestimates the increase in cumulative regret of the tuned near-optimal agent, which is explained\nby the unaccounted decrease in estimation complexity. The diameter also increases almost linearly\nwith the number of states (Figures 1c and 1d). For the small p_rand (scenario 4), the relation is still\napproximately linear. This linear trend underestimates the evident non-linear growth in hardness in\nthe regret of the tuned near-optimal agent but is in line with the mild increase in visitation complexity.\n\nThe empirical evidence indicates that the environmental value norm can only capture estimation\ncomplexity. It decreases as p_lazy and p_rand increase (Figures 1a and 1b) because the optimal\n\n7\n\n\fFigure 1: The Colosseum hardness analysis for the episodic MiniGridEmpty family.\n\nvalue of neighboring states becomes closer, which decreases the per-step variability of the optimal\nvalue function. When the number of states increases but the transition and reward structures remain\nthe same (Figures 1c and 1d), the small increase in this variability only generates a sublinear growth.\n\nWe empirically observe that the sum of the reciprocals of the sub-optimality gaps is not particularly\napt at capturing estimation complexity, due to its exclusive focus on optimal policy identification, and\nit also underestimates the increase in hardness induced by an increase in visitation complexity. This\nmeasure increases weakly superlinearly in scenarios 1 and 2 (Figures 1a and 1b). The probability\nof executing the action selected by the agent decreases when p_lazy and p_rand increase, so the\ndifference between the state and the state-action optimal value functions decreases sharply. The\nmeasure increases almost linearly with the number of states (Figures 1c and 1d). This is explained by\nthe fact that the average value of the additional terms in the summation is often similar to the average\nvalue of the existing terms when MDPs have the same structure of reward and transition kernels.\n\n3.3 Colosseum benchmarking\n\nIn this section, we benchmark five tabular agents with theoretical guarantees and four non-tabular\nagents. Besides being valuable on their own, these results help to empirically validate our benchmark.\n\nAgents. The tabular agents are posterior sampling for reinforcement learning (PSRL) for the episodic\nand continuous settings [30, 31], Q-learning with UCB exploration for the episodic setting [32],\nQ-learning with optimism for the continuous setting [16], and UCRL2 for the continuous setting [14].\nThe non-tabular agents (from bsuite) are ActorCritic, ActorCriticRNN, BootDQN, and DQN.\n\nExperimental procedure. We set the total number of time steps to 500 000 with a maximum training\ntime of 10 minutes for the tabular setting and 40 minutes for the non-tabular setting. If an agent\ndoes not reach the maximum number of time steps before this time limit, learning is interrupted,\nand the agent continues interacting using its last best policy. This guarantees a fair comparison\nbetween agents with different computational costs. The performance indicators are computed every\n100 time steps. Each interaction between an agent and an MDP is repeated for 20 seeds. The agents\u2019\nhyperparameters have been chosen by random search to minimize the average regret across MDPs\nwith randomly sampled parameters (see Appendix E). We use a deterministic emission map that\nassigns a uniquely identifying vector to each state (for example, a gridworld coordinate) to derive\nthe non-tabular benchmark MDPs. In Table 2, we report the per-step normalized cumulative regrets\ndivided by the total number of time steps (defined in Appendix C), which allows comparisons across\ndifferent MDPs. We summarize the main findings here and refer to Appendix E for further details.\n\nAnalysis. Table 2 often shows high variability in the performance of the same agent across MDPs of\nthe same family. Therefore, maximising the diversity across diameters and value norms effectively\nproduces diverse challenges even for MDPs with similar transition and reward structures. For example,\nin the continuous communicating case (Table 2d), Q-learning performs well only in some MDPs of\nthe MiniGridEmpty family. This also happens for UCRL2 for the SimpleGrid family.\n\nThe average normalized cumulative regret is lower in ergodic environments compared to commu-\nnicating environments. This indicates that the ergodic setting is generally slightly easier than the\ncommunicating settings. Notably, in the continuous setting, the ergodic setting is more challenging\nthan the communicating setting for Q-learning (Tables 2c and 2d). Designing a naturally ergodic\n\n8\n\n0.00.20.40.6Probability of random action0.00.20.40.60.81.0Normalized values(a) Scenario 10.00.20.40.6Probability of lazy action(b) Scenario 2100200300400Number of states(c) Scenario 3100200300400Number of states(d) Scenario 4DiameterEnvironmental value normSum ofthe reciprocals of the sub-optimality gapsCumulative regret oftuned near-optimal agent\fTable 2: Normalized cumulative regrets of selected agents on the Colosseum benchmark. (a) Episodic\nergodic. (b) Episodic communicating. (c) Continuous ergodic. (d) Continuous communicating.\n\n(a)\n\n(b)\n\n(c)\n\n(d)\n\nQ-learning\n\nPSRL\n\nQ-learning\n\nPSRL\n\nQ-learning\n\nPSRL\n\nUCRL2\n\nQ-learning\n\nPSRL\n\nUCRL2\n\nMDP\n\nDeepSea\n\n.64 \u00b1 .00\n.52 \u00b1 .01\n\n.01 \u00b1 .00\n.00 \u00b1 .00\n\nMDP\n\nDeepSea\n\n.01 \u00b1 .01\n.83 \u00b1 .02\n\n.00 \u00b1 .00\n.54 \u00b1 .01\n\nMDP\n\nMDP\n\nDeepSea\n\n.94 \u00b1 .00\n\n.06 \u00b1 .01\n\n.23 \u00b1 .05\n\nDeepSea\n\nFrozenLake\n\n.83 \u00b1 .03\n\n.01 \u00b1 .03\n\n.01 \u00b1 .02\n\nFrozenLake\n\n.90 \u00b1 .01\n\n.01 \u00b1 .00\n\nFrozenLake\n\n.78 \u00b1 .04\n\n.03 \u00b1 .11\n\nMG-Empty\n\nMG-Empty\n\nMG-Rooms\n\nRiverSwim\n\nSimpleGrid\n\nTaxi\n\n1.00 \u00b1 .00\n1.00 \u00b1 .00\n1.00 \u00b1 .00\n1.00 \u00b1 .00\n1.00 \u00b1 .00\n.92 \u00b1 .04\n.91 \u00b1 .03\n\n.90 \u00b1 .04\n1.00 \u00b1 .00\n.99 \u00b1 .01\n\n.07 \u00b1 .02\n.91 \u00b1 .01\n\n.78 \u00b1 .03\n.79 \u00b1 .03\n.50 \u00b1 .03\n\n.84 \u00b1 .01\n.56 \u00b1 .02\n\n.86 \u00b1 .16\n.94 \u00b1 .07\n.91 \u00b1 .09\n.35 \u00b1 .10\n.44 \u00b1 .12\n.14 \u00b1 .08\n.04 \u00b1 .03\n\n.05 \u00b1 .04\n.54 \u00b1 .36\n.24 \u00b1 .29\n\n.00 \u00b1 .00\n.00 \u00b1 .00\n\n.05 \u00b1 .01\n.79 \u00b1 .03\n.50 \u00b1 .03\n\n.08 \u00b1 .01\n.05 \u00b1 .00\n\nMG-Empty\n\nMG-Rooms\n\nRiverSwim\n\nSimpleGrid\n\nTaxi\n\n.59 \u00b1 .07\n.99 \u00b1 .00\n.99 \u00b1 .01\n1.00 \u00b1 .00\n1.00 \u00b1 .00\n\n.99 \u00b1 .01\n1.00 \u00b1 .00\n1.00 \u00b1 .00\n.94 \u00b1 .05\n\n.87 \u00b1 .00\n.96 \u00b1 .01\n\n.78 \u00b1 .10\n.80 \u00b1 .00\n.50 \u00b1 .00\n.79 \u00b1 .04\n\n.94 \u00b1 .00\n.91 \u00b1 .01\n\n.09 \u00b1 .05\n.24 \u00b1 .15\n.23 \u00b1 .12\n.91 \u00b1 .09\n.93 \u00b1 .09\n\n.21 \u00b1 .29\n.44 \u00b1 .39\n.43 \u00b1 .39\n.04 \u00b1 .04\n\n.00 \u00b1 .00\n.80 \u00b1 .00\n\n.20 \u00b1 .15\n.55 \u00b1 .15\n.11 \u00b1 .01\n.79 \u00b1 .04\n\n.09 \u00b1 .01\n.36 \u00b1 .06\n\n.98 \u00b1 .02\n.98 \u00b1 .02\n.97 \u00b1 .00\n.98 \u00b1 .01\n.96 \u00b1 .01\n.98 \u00b1 .02\n.98 \u00b1 .03\n.98 \u00b1 .01\n\n.98 \u00b1 .03\n.98 \u00b1 .02\n\n.73 \u00b1 .19\n.71 \u00b1 .22\n.90 \u00b1 .06\n.50 \u00b1 .25\n\n.78 \u00b1 .00\n.46 \u00b1 .08\n.49 \u00b1 .00\n\n.99 \u00b1 .01\n.98 \u00b1 .04\n.95 \u00b1 .03\n.99 \u00b1 .01\n.83 \u00b1 .31\n.99 \u00b1 .02\n.99 \u00b1 .01\n.99 \u00b1 .01\n\n.99 \u00b1 .02\n1.00 \u00b1 .00\n\n.00 \u00b1 .00\n.00 \u00b1 .00\n.02 \u00b1 .04\n.01 \u00b1 .00\n\n.70 \u00b1 .19\n.01 \u00b1 .02\n.43 \u00b1 .16\n\n.05 \u00b1 .06\n.03 \u00b1 .05\n.04 \u00b1 .01\n.54 \u00b1 .26\n.01 \u00b1 .00\n.45 \u00b1 .35\n.27 \u00b1 .33\n.93 \u00b1 .09\n\n.18 \u00b1 .29\n.62 \u00b1 .36\n\n.00 \u00b1 .00\n.01 \u00b1 .00\n.01 \u00b1 .01\n.01 \u00b1 .01\n\n.01 \u00b1 .01\n.00 \u00b1 .00\n.00 \u00b1 .00\n\nFrozenLake\n\nMG-Empty\n\nMG-Rooms\n\nRiverSwim\n\nSimpleGrid\n\nMG-Rooms\n\nRiverSwim\n\nSimpleGrid\n\n.78 \u00b1 .00\n.99 \u00b1 .00\n.79 \u00b1 .00\n\n.77 \u00b1 .04\n.84 \u00b1 .04\n\n.51 \u00b1 .23\n.01 \u00b1 .00\n.00 \u00b1 .00\n.35 \u00b1 .17\n.75 \u00b1 .21\n\n.01 \u00b1 .01\n.01 \u00b1 .01\n.02 \u00b1 .02\n\n.16 \u00b1 .03\n.34 \u00b1 .14\n\n.11 \u00b1 .01\n.01 \u00b1 .00\n.15 \u00b1 .01\n.01 \u00b1 .00\n\n.78 \u00b1 .05\n.99 \u00b1 .00\n.79 \u00b1 .04\n\n.01 \u00b1 .04\n.01 \u00b1 .02\n\n.95 \u00b1 .22\n1.00 \u00b1 .00\n.60 \u00b1 .50\n1.00 \u00b1 .00\n1.00 \u00b1 .00\n\n1.00 \u00b1 .00\n1.00 \u00b1 .00\n1.00 \u00b1 .00\n\n.00 \u00b1 .01\n.01 \u00b1 .00\n\n.93 \u00b1 .00\n.45 \u00b1 .15\n.93 \u00b1 .00\n.50 \u00b1 .00\n\n.90 \u00b1 .01\n.99 \u00b1 .00\n.92 \u00b1 .01\n\n.01 \u00b1 .01\n.04 \u00b1 .06\n\n.02 \u00b1 .00\n.02 \u00b1 .00\n.01 \u00b1 .00\n.01 \u00b1 .00\n.08 \u00b1 .20\n\n.78 \u00b1 .40\n.02 \u00b1 .01\n.66 \u00b1 .47\n\n.00 \u00b1 .00\n.02 \u00b1 .01\n\n.01 \u00b1 .00\n.01 \u00b1 .00\n.70 \u00b1 .40\n.33 \u00b1 .24\n\nTaxi\n\n.87 \u00b1 .01\n\n.89 \u00b1 .08\n\n.09 \u00b1 .01\n\nTaxi\n\n.95 \u00b1 .00\n\n.94 \u00b1 .04\n\n.12 \u00b1 .01\n\nAverage\n\n.81 \u00b1 .24\n\n.30 \u00b1 .33\n\nAverage\n\n.83 \u00b1 .23\n\n.35 \u00b1 .30\n\nAverage\n\n.85 \u00b1 .18\n\n.59 \u00b1 .44\n\n.17 \u00b1 .26\n\nAverage\n\n.38 \u00b1 .37\n\n.69 \u00b1 .38\n\n.28 \u00b1 .37\n\nMDP is not straightforward. In fact, the majority of MDPs in the literature are communicating.\nIn Colosseum, ergodicity is induced by setting p_rand > 0 in otherwise communicating MDPs.\nModel-free agents struggle with the resulting increase in variability of the state-action value function.\n\nIn the episodic settings (Tables 2a and 2b), PSRL obtains excellent performances with low variability.\nQ-learning instead performs well in a few MDPs. This often happens since, when the action selected\nby the agent is randomly substituted (due to p_rand > 0) with one with a large sub-optimality gap,\nthe resulting Q-value update introduces a critical error that requires many samples to be corrected.\n\nIn the continuous settings (Tables 2c and 2d), UCRL2 performs best in the ergodic cases when\nQ-learning suffers from the issue caused by p_rand > 0 but is only slightly better than Q-learning\nin the communicating ones. PSRL instead struggles with most MDPs. The reason for its weak\nperformance in this setting is the computationally expensive optimistic sampling procedure required\nfor its worst-case theoretical guarantees. It often breaks the time limit before reaching the first quarter\nof available time steps, meaning that it lacks sufficient samples to estimate the optimal policy.\n\nFigure 2 places the regret of the agents in the continuous ergodic setting (Table 2c) on a position\ncorresponding to the diameter and value norm of the benchmark environments. PSRL and Q-learning\n(Figures 2a and 2b), appear to be impacted more by the value norm than the diameter. This is in line\nwith the lack of sufficient samples for PSRL and the aforementioned issue related to high q estimates\nvariability for Q-learning, which is exacerbated when the estimation complexity is higher. In the\ncase of UCRL2 (Figure 2c), which provides more reliable evidence since it performs well across the\nMDPs, higher regret effectively corresponds to higher diameter and value norm.\n\nThe following is the appendix_A section of the paper you are reviewing:\n\n\nColosseum is a pioneering Python package that creates a bridge between theory and practice in\ntabular reinforcement learning with an eye on the non-tabular setting. It allows to empirically, and\nefficiently, investigate the hardness of MDPs, and it implements the first principled benchmark for\ntabular reinforcement learning algorithms. In the following sections, we report some additional\ndetails on the capabilities of Colosseum. However, we invite the reader to check the latest online\ndocumentation along with the tutorials that cover in detail every aspect of the package.3\n\nA.1 Expected performance indicators\n\nEach agent in Colosseum is required to implement a function that returns its current best policy\nestimate \u02c6\u03c0\u2217\nt for any time step t. Using an efficient implementation of the policy evaluation algorithm,\nColosseum can compute the corresponding expected regret and expected average reward, which,\nsummed across time steps, amounts to the expected cumulative reward and expected cumulative regret.\nAlthough it is possible to perform this operation at every time step of the agent/MDP interaction, we\nleave the option to approximate the expected cumulative regret by calculating the expected regret\nevery n time steps and assuming that the policy of the agent in the previous n \u2212 1 time steps would\nhave yielded a similar expected regret. For instance, for n = 100, the expected cumulative regret at\ntime step T = 500 would be approximated as the sum of the expected regrets calculated at time steps\nt = 100, 200, . . . , 500 multiplied by 100.\n\nA.2 Non-tabular capabilities\n\nColosseum is primarily aimed at the tabular reinforcement learning setting. However, as our ultimate\ngoal is to develop principled non-tabular benchmarks, we offer a way to test non-tabular reinforcement\nlearning algorithms on the Colosseum benchmark. Although our benchmark defines a challenge\nthat is well characterized for tabular agents, we believe that it can provide valuable insights into the\nperformance of non-tabular algorithms. In order to do so, we adopt the BlockMDP formalism proposed\nby Du et al. [28]. A BlockMDP is a tuple (S, A, P, P0, R, O, q), where O and q : S \u2192 \u2206(O) are\nrespectively the non-tabular observation space that the agent observes and the (possibly stochastic)\nemission map that associates a distribution over the observation space to each state in the MDP. Note\nthat the agent is not provided with any information on the state space S. Colosseum implements six\ndeterministic emission maps with different properties and four kinds of noise to make the emission\nmaps stochastic, which we describe below. Examples of the emission maps with distinguishable\ncharacteristics for each MDP family will be presented in the corresponding sections.\n\nEmission maps:\n\n\u2022 One-hot encoding. This emission map assigns to each state a feature vector that is filled\n\nwith zeros with the exception of an index that uniquely corresponds to the state.\n\n\u2022 Linear optimal value. This emission map assigns to each state a feature vector \u03d5(s) that\nenables linear representation of the optimal value function. In other words, there is a \u03b8 such\nthat V \u2217(s) = \u03b8T \u03d5(s).\n\n\u2022 Linear random value. This emission map assigns to each state a feature vector \u03d5(s) that\nenables linear representation of the value function of the randomly acting policy. In other\nwords, there is a \u03b8 such that V \u03c0(s) = \u03b8T \u03d5(s), where \u03c0 is the randomly acting policy.\n\n\u2022 State information. This emission map assigns to each state a feature vector that contains\nuniquely identifying information about the state (e.g., coordinates for the DeepSea family).\n\u2022 Image encoding. This emission map assigns to each state a feature matrix that encodes the\n\nvisual representation of the MDP as a grayscale image.\n\n\u2022 Tensor encoding. This emission map assigns to each state a tensor composed of the\nconcatenation of matrices that one-hot encode the presence of a symbol in the corresponding\nindices. For example, for the DeepSea family, the tensor is composed of a matrix that\nencodes the position of the agent and a matrix that encodes the positions of white spaces.\n\nNoise:\n\n3Available at https://michelangeloconserva.github.io/Colosseum.\n\n15\n\n\f\u2022 Uncorrelated light-tailed noise. The output of the emission map is corrupted with element-\n\nwise uncorrelated Gaussian noise.\n\n\u2022 Correlated light-tailed noise. The output of the emission map is corrupted with multivariate\ncorrelated Gaussian noise with a covariance matrix sampled from a Wishart distribution\nwith a pre-specified scale level when the MDP is created. In other words, the correlation\nstructure of the noise remains unchanged while the agent interacts with the MDP.\n\n\u2022 Uncorrelated heavy-tailed noise. The output of the emission map is corrupted with element-\n\nwise uncorrelated Student\u2019s t noise.\n\n\u2022 Correlated heavy-tailed noise. The output of the emission map is corrupted with multivariate\ncorrelated Student\u2019s t noise with covariance matrix sampled from a Wishart distribution\nwith a pre-specified scale level when the MDP is created. In other words, the correlation\nstructure of the noise remains unchanged while the agent interacts with the MDP.\n\nA.3 Colosseum MDP families\n\nColosseum implements eight families of MDPs. When selecting which families to include in\nColosseum, we aimed to balance between traditional environment families (RiverSwim [24], Taxi\n[25], and FrozenLake) and unconventional ones (MiniGid environments [26]). The DeepSea\nfamily [27] was included since it was proposed as an example of a hard exploration problem. The\nSimpleGrid family acts as a simplified version of the MiniGrid-Empty environment.\n\nEach MDP family requires a set of parameters to instantiate an MDP. In addition to individual\nparameters, all MDP families share the following:\n\n\u2022 The size \u2208 N parameter controls the number of states through geometrical properties of\nthe MDP family. For example, in a grid world, it controls the size of the grid. This param-\neter allows increasing the difficulty of an MDP instance without altering the fundamental\nstructure of the MDP family.\n\n\u2022 The p_rand \u2208 [0, 1) parameter controls the probability r that an MDP executes an action\nat random instead of the one selected by the agent. Concretely, the new transition kernel\nis given by P \u2032(st+1 | st, at) = (1 \u2212 r)P (st+1 | st, at) + r\na P (st+1 | st, a). Setting\n|A|\nthis parameter to a non-zero value can make a communicating MDP ergodic.\n\n(cid:80)\n\n\u2022 The lazy \u2208 [0, 1) parameter controls the probability l of an action not being executed.\nConcretely, the new transition kernel is given by P \u2032(st+1 | st, at) = (1 \u2212 l)P (st+1 |\nst, at) + l1(st+1 = st). This parameter can render a deterministic MDP stochastic without\nchanging the communication class.\n\n\u2022 make_reward_stochastic is a boolean parameter to render the rewards stochastic instead\nof deterministic (the default). We opted for a Beta distribution to guarantee rewards bounded\nin a specific range. However, it is possible to specify custom reward distributions using\nscipy random variables.\n\n\u2022 r_min and r_max scale the rewards. The default values are 0 and 1, respectively.\n\nThe hardness analysis presented in Appendix D shows the relationships between the measures of\nhardness and some of these parameters for the Colosseum MDP families. Such relationships can be\neasily exploited to create MDP instances with specific hardness characterization. For example, in\norder to create an MDP instance with low estimation complexity and high visitation complexity, one\ncan force the MDP instance to be deterministic by setting p_rand and p_lazy to zero and the size to\na high value. If instead one wants to increase the estimation complexity while keeping the visitation\ncomplexity fixed, the mean reward of a subset of states can be increased. The scale of the increase\ndepends on the variability of the next state distributions of the selected states. The more variable such\ndistributions are the higher the increase in estimation complexity.\n\n\n\nBased on the section(s), please answer the following question with yes, no, or n/a and provide a brief justification for your answer.\nQuestion: If the authors use existing assets (e.g., code, data, models), do the authors cite the creators?"}, "4c": {"role": "user", "content": "The following is the experiments section of the paper you are reviewing:\n\n\n3.1 Colosseum\n\nThis section briefly introduces Colosseum, a pioneering Python package that bridges theory and\npractice in tabular reinforcement learning while also being applicable in the non-tabular setting. More\ndetails about the package can be found in Appendix A and in the project website.2\n\nAs a hardness analysis tool, Colosseum identifies the communication class of MDPs, assembles\ninsightful visualizations and logs of interactions between agents and MDPs, computes three measures\nof hardness (environmental value norm, sum of the reciprocals of the sub-optimality gaps, and\ndiameter, whose computation requires a novel solution described in App. A.4). Eight MDP families\nare available for experimentation. Some are traditional families (RiverSwim [24], Taxi [25], and\nFrozenLake) while others are more recent (MiniGid environments [26]). Additionally, DeepSea\n[27] was included as a hard exploration family of problems, and the SimpleGrid family is composed\nof simplified versions of the MG\u2013Empty environment. By controlling the parameters of MDPs from\neach family (further detailed in App. A.3), it is easy to create an MDP with any desired hardness.\n\nAs a benchmarking tool, Colosseum is unique in its strong connection with theory. For instance,\nin contrast to non-tabular benchmarks, Colosseum computes theoretical evaluation criteria such\n\n1For example, the \u2126(|S||A|H 2\u03f5\u22122) sample complexity bound for the episodic communicating setting [22].\n2Available at https://michelangeloconserva.github.io/Colosseum.\n\n6\n\n\fas the expected cumulative regret and the expected average future reward, which can be used to\nexactly evaluate the performance criterion of regret minimizing agents. The benchmark covers the\nmost commonly studied reinforcement learning settings: episodic ergodic, episodic communicating,\ncontinuous ergodic, and continuous communicating. For each setting, we have selected twenty MDPs\nthat are diverse with respect to their diameters and environmental value norms as proxies for different\ncombinations of visitation complexity and estimation complexity. Figure 17 in Appendix E shows\nhow each of these MDPs varies according to these measures, and Section 3.3 empirically validates\nthis selection by showing that harder MDPs correspond to worse agent performance. Notably, the\ntheoretically backed selection of MDPs and the rigorous evaluation criteria make the Colosseum\nbenchmark the most exhaustive in tabular reinforcement learning, since previous evaluations were\nconducted empirically in a few MDPs (such as Taxi or RiverSwim).\n\nColosseum also allows testing of non-tabular agents by leveraging the BlockMDP model [28].\nBlockMDPs equip tabular MDPs with an emission map that is a (possibly stochastic) mapping q : S \u2192\n\u2206(O) from the finite state space S to a (possibly infinite) observation space O. Agents interacting\nwith BlockMDPs are only provided with observations, so non-tabular methods are generally required.\nMany commonly used non-tabular MDPs (such as Minecraft [29]) can be straightforwardly encoded\nas BlockMDPs using the Colosseum MDP families. Colosseum implements a diverse set of\ndeterministic emission maps and allows combining them with different sources of noise. Appendix\nA.2 further details BlockMDPs and the available emission maps.\n\n3.2 Empirical analysis of hardness measures\n\nFor brevity, this section only presents results of hardness measures in the MiniGridEmpty family of\nenvironments in the episodic setting. Appendix D presents the full outcome of the empirical analysis.\n\nA MiniGridEmpty MDP is a grid world where an agent has three available actions: moving forward,\nrotating left, and rotating right. An agent is rewarded for being in a few specific states and receives no\nreward in every other state. Appendix A.3.4 provides more details about this family of environments.\n\nIn our investigation, we consider four scenarios that highlight the different aspects of MDP hardness.\n\nScenario 1. We vary the probability p_rand that an MDP executes a random action instead of the\naction selected by an agent. As p_rand approaches one, value estimation becomes easier, since\noutcomes depend less on agent choices. However, intentionally visiting states becomes harder.\n\nScenario 2. We vary the probability p_lazy that an MDP stays in the same state instead of executing\nthe action selected by an agent. Contrary to increasing p_rand, increasing p_lazy never benefits\nexploration. Increasing p_lazy decreases estimation complexity and increases visitation complexity.\n\nScenario 3 and 4. We vary the number of states across MDPs from the same family. In scenario\n4, we also let p_rand = 0.1 to study the impact of stochasticity. In these scenarios, increasing the\nnumber of states simultaneously increases the estimation complexity and the visitation complexity.\n\nIn every scenario, hardness measures are compared with the cumulative regret of a near-optimal agent\ntuned for each specific MDP (see App. D). This regret serves as an optimistic measure of hardness.\nAppendix C describes how these measures are normalized. Note that, due to normalization, the plots\nshould only be compared in terms of trends (growth rates) rather than absolute values.\n\nAnalysis. Figure 1 presents the empirical results for the episodic MiniGridEmpty family in the four\nscenarios with 95% bootstrapped confidence intervals over twelve random seeds.\n\nThe experiments confirm our claim that the diameter captures visitation rather than estimation\ncomplexity. This measure of hardness grows superlinearly with both p_rand and p_lazy (Figures\n1a and 1b) since deliberate movement between states requires an exponentially increasing number of\ntime steps. Although the diameter highlights the sharply increasing visitation complexity, its trend\noverestimates the increase in cumulative regret of the tuned near-optimal agent, which is explained\nby the unaccounted decrease in estimation complexity. The diameter also increases almost linearly\nwith the number of states (Figures 1c and 1d). For the small p_rand (scenario 4), the relation is still\napproximately linear. This linear trend underestimates the evident non-linear growth in hardness in\nthe regret of the tuned near-optimal agent but is in line with the mild increase in visitation complexity.\n\nThe empirical evidence indicates that the environmental value norm can only capture estimation\ncomplexity. It decreases as p_lazy and p_rand increase (Figures 1a and 1b) because the optimal\n\n7\n\n\fFigure 1: The Colosseum hardness analysis for the episodic MiniGridEmpty family.\n\nvalue of neighboring states becomes closer, which decreases the per-step variability of the optimal\nvalue function. When the number of states increases but the transition and reward structures remain\nthe same (Figures 1c and 1d), the small increase in this variability only generates a sublinear growth.\n\nWe empirically observe that the sum of the reciprocals of the sub-optimality gaps is not particularly\napt at capturing estimation complexity, due to its exclusive focus on optimal policy identification, and\nit also underestimates the increase in hardness induced by an increase in visitation complexity. This\nmeasure increases weakly superlinearly in scenarios 1 and 2 (Figures 1a and 1b). The probability\nof executing the action selected by the agent decreases when p_lazy and p_rand increase, so the\ndifference between the state and the state-action optimal value functions decreases sharply. The\nmeasure increases almost linearly with the number of states (Figures 1c and 1d). This is explained by\nthe fact that the average value of the additional terms in the summation is often similar to the average\nvalue of the existing terms when MDPs have the same structure of reward and transition kernels.\n\n3.3 Colosseum benchmarking\n\nIn this section, we benchmark five tabular agents with theoretical guarantees and four non-tabular\nagents. Besides being valuable on their own, these results help to empirically validate our benchmark.\n\nAgents. The tabular agents are posterior sampling for reinforcement learning (PSRL) for the episodic\nand continuous settings [30, 31], Q-learning with UCB exploration for the episodic setting [32],\nQ-learning with optimism for the continuous setting [16], and UCRL2 for the continuous setting [14].\nThe non-tabular agents (from bsuite) are ActorCritic, ActorCriticRNN, BootDQN, and DQN.\n\nExperimental procedure. We set the total number of time steps to 500 000 with a maximum training\ntime of 10 minutes for the tabular setting and 40 minutes for the non-tabular setting. If an agent\ndoes not reach the maximum number of time steps before this time limit, learning is interrupted,\nand the agent continues interacting using its last best policy. This guarantees a fair comparison\nbetween agents with different computational costs. The performance indicators are computed every\n100 time steps. Each interaction between an agent and an MDP is repeated for 20 seeds. The agents\u2019\nhyperparameters have been chosen by random search to minimize the average regret across MDPs\nwith randomly sampled parameters (see Appendix E). We use a deterministic emission map that\nassigns a uniquely identifying vector to each state (for example, a gridworld coordinate) to derive\nthe non-tabular benchmark MDPs. In Table 2, we report the per-step normalized cumulative regrets\ndivided by the total number of time steps (defined in Appendix C), which allows comparisons across\ndifferent MDPs. We summarize the main findings here and refer to Appendix E for further details.\n\nAnalysis. Table 2 often shows high variability in the performance of the same agent across MDPs of\nthe same family. Therefore, maximising the diversity across diameters and value norms effectively\nproduces diverse challenges even for MDPs with similar transition and reward structures. For example,\nin the continuous communicating case (Table 2d), Q-learning performs well only in some MDPs of\nthe MiniGridEmpty family. This also happens for UCRL2 for the SimpleGrid family.\n\nThe average normalized cumulative regret is lower in ergodic environments compared to commu-\nnicating environments. This indicates that the ergodic setting is generally slightly easier than the\ncommunicating settings. Notably, in the continuous setting, the ergodic setting is more challenging\nthan the communicating setting for Q-learning (Tables 2c and 2d). Designing a naturally ergodic\n\n8\n\n0.00.20.40.6Probability of random action0.00.20.40.60.81.0Normalized values(a) Scenario 10.00.20.40.6Probability of lazy action(b) Scenario 2100200300400Number of states(c) Scenario 3100200300400Number of states(d) Scenario 4DiameterEnvironmental value normSum ofthe reciprocals of the sub-optimality gapsCumulative regret oftuned near-optimal agent\fTable 2: Normalized cumulative regrets of selected agents on the Colosseum benchmark. (a) Episodic\nergodic. (b) Episodic communicating. (c) Continuous ergodic. (d) Continuous communicating.\n\n(a)\n\n(b)\n\n(c)\n\n(d)\n\nQ-learning\n\nPSRL\n\nQ-learning\n\nPSRL\n\nQ-learning\n\nPSRL\n\nUCRL2\n\nQ-learning\n\nPSRL\n\nUCRL2\n\nMDP\n\nDeepSea\n\n.64 \u00b1 .00\n.52 \u00b1 .01\n\n.01 \u00b1 .00\n.00 \u00b1 .00\n\nMDP\n\nDeepSea\n\n.01 \u00b1 .01\n.83 \u00b1 .02\n\n.00 \u00b1 .00\n.54 \u00b1 .01\n\nMDP\n\nMDP\n\nDeepSea\n\n.94 \u00b1 .00\n\n.06 \u00b1 .01\n\n.23 \u00b1 .05\n\nDeepSea\n\nFrozenLake\n\n.83 \u00b1 .03\n\n.01 \u00b1 .03\n\n.01 \u00b1 .02\n\nFrozenLake\n\n.90 \u00b1 .01\n\n.01 \u00b1 .00\n\nFrozenLake\n\n.78 \u00b1 .04\n\n.03 \u00b1 .11\n\nMG-Empty\n\nMG-Empty\n\nMG-Rooms\n\nRiverSwim\n\nSimpleGrid\n\nTaxi\n\n1.00 \u00b1 .00\n1.00 \u00b1 .00\n1.00 \u00b1 .00\n1.00 \u00b1 .00\n1.00 \u00b1 .00\n.92 \u00b1 .04\n.91 \u00b1 .03\n\n.90 \u00b1 .04\n1.00 \u00b1 .00\n.99 \u00b1 .01\n\n.07 \u00b1 .02\n.91 \u00b1 .01\n\n.78 \u00b1 .03\n.79 \u00b1 .03\n.50 \u00b1 .03\n\n.84 \u00b1 .01\n.56 \u00b1 .02\n\n.86 \u00b1 .16\n.94 \u00b1 .07\n.91 \u00b1 .09\n.35 \u00b1 .10\n.44 \u00b1 .12\n.14 \u00b1 .08\n.04 \u00b1 .03\n\n.05 \u00b1 .04\n.54 \u00b1 .36\n.24 \u00b1 .29\n\n.00 \u00b1 .00\n.00 \u00b1 .00\n\n.05 \u00b1 .01\n.79 \u00b1 .03\n.50 \u00b1 .03\n\n.08 \u00b1 .01\n.05 \u00b1 .00\n\nMG-Empty\n\nMG-Rooms\n\nRiverSwim\n\nSimpleGrid\n\nTaxi\n\n.59 \u00b1 .07\n.99 \u00b1 .00\n.99 \u00b1 .01\n1.00 \u00b1 .00\n1.00 \u00b1 .00\n\n.99 \u00b1 .01\n1.00 \u00b1 .00\n1.00 \u00b1 .00\n.94 \u00b1 .05\n\n.87 \u00b1 .00\n.96 \u00b1 .01\n\n.78 \u00b1 .10\n.80 \u00b1 .00\n.50 \u00b1 .00\n.79 \u00b1 .04\n\n.94 \u00b1 .00\n.91 \u00b1 .01\n\n.09 \u00b1 .05\n.24 \u00b1 .15\n.23 \u00b1 .12\n.91 \u00b1 .09\n.93 \u00b1 .09\n\n.21 \u00b1 .29\n.44 \u00b1 .39\n.43 \u00b1 .39\n.04 \u00b1 .04\n\n.00 \u00b1 .00\n.80 \u00b1 .00\n\n.20 \u00b1 .15\n.55 \u00b1 .15\n.11 \u00b1 .01\n.79 \u00b1 .04\n\n.09 \u00b1 .01\n.36 \u00b1 .06\n\n.98 \u00b1 .02\n.98 \u00b1 .02\n.97 \u00b1 .00\n.98 \u00b1 .01\n.96 \u00b1 .01\n.98 \u00b1 .02\n.98 \u00b1 .03\n.98 \u00b1 .01\n\n.98 \u00b1 .03\n.98 \u00b1 .02\n\n.73 \u00b1 .19\n.71 \u00b1 .22\n.90 \u00b1 .06\n.50 \u00b1 .25\n\n.78 \u00b1 .00\n.46 \u00b1 .08\n.49 \u00b1 .00\n\n.99 \u00b1 .01\n.98 \u00b1 .04\n.95 \u00b1 .03\n.99 \u00b1 .01\n.83 \u00b1 .31\n.99 \u00b1 .02\n.99 \u00b1 .01\n.99 \u00b1 .01\n\n.99 \u00b1 .02\n1.00 \u00b1 .00\n\n.00 \u00b1 .00\n.00 \u00b1 .00\n.02 \u00b1 .04\n.01 \u00b1 .00\n\n.70 \u00b1 .19\n.01 \u00b1 .02\n.43 \u00b1 .16\n\n.05 \u00b1 .06\n.03 \u00b1 .05\n.04 \u00b1 .01\n.54 \u00b1 .26\n.01 \u00b1 .00\n.45 \u00b1 .35\n.27 \u00b1 .33\n.93 \u00b1 .09\n\n.18 \u00b1 .29\n.62 \u00b1 .36\n\n.00 \u00b1 .00\n.01 \u00b1 .00\n.01 \u00b1 .01\n.01 \u00b1 .01\n\n.01 \u00b1 .01\n.00 \u00b1 .00\n.00 \u00b1 .00\n\nFrozenLake\n\nMG-Empty\n\nMG-Rooms\n\nRiverSwim\n\nSimpleGrid\n\nMG-Rooms\n\nRiverSwim\n\nSimpleGrid\n\n.78 \u00b1 .00\n.99 \u00b1 .00\n.79 \u00b1 .00\n\n.77 \u00b1 .04\n.84 \u00b1 .04\n\n.51 \u00b1 .23\n.01 \u00b1 .00\n.00 \u00b1 .00\n.35 \u00b1 .17\n.75 \u00b1 .21\n\n.01 \u00b1 .01\n.01 \u00b1 .01\n.02 \u00b1 .02\n\n.16 \u00b1 .03\n.34 \u00b1 .14\n\n.11 \u00b1 .01\n.01 \u00b1 .00\n.15 \u00b1 .01\n.01 \u00b1 .00\n\n.78 \u00b1 .05\n.99 \u00b1 .00\n.79 \u00b1 .04\n\n.01 \u00b1 .04\n.01 \u00b1 .02\n\n.95 \u00b1 .22\n1.00 \u00b1 .00\n.60 \u00b1 .50\n1.00 \u00b1 .00\n1.00 \u00b1 .00\n\n1.00 \u00b1 .00\n1.00 \u00b1 .00\n1.00 \u00b1 .00\n\n.00 \u00b1 .01\n.01 \u00b1 .00\n\n.93 \u00b1 .00\n.45 \u00b1 .15\n.93 \u00b1 .00\n.50 \u00b1 .00\n\n.90 \u00b1 .01\n.99 \u00b1 .00\n.92 \u00b1 .01\n\n.01 \u00b1 .01\n.04 \u00b1 .06\n\n.02 \u00b1 .00\n.02 \u00b1 .00\n.01 \u00b1 .00\n.01 \u00b1 .00\n.08 \u00b1 .20\n\n.78 \u00b1 .40\n.02 \u00b1 .01\n.66 \u00b1 .47\n\n.00 \u00b1 .00\n.02 \u00b1 .01\n\n.01 \u00b1 .00\n.01 \u00b1 .00\n.70 \u00b1 .40\n.33 \u00b1 .24\n\nTaxi\n\n.87 \u00b1 .01\n\n.89 \u00b1 .08\n\n.09 \u00b1 .01\n\nTaxi\n\n.95 \u00b1 .00\n\n.94 \u00b1 .04\n\n.12 \u00b1 .01\n\nAverage\n\n.81 \u00b1 .24\n\n.30 \u00b1 .33\n\nAverage\n\n.83 \u00b1 .23\n\n.35 \u00b1 .30\n\nAverage\n\n.85 \u00b1 .18\n\n.59 \u00b1 .44\n\n.17 \u00b1 .26\n\nAverage\n\n.38 \u00b1 .37\n\n.69 \u00b1 .38\n\n.28 \u00b1 .37\n\nMDP is not straightforward. In fact, the majority of MDPs in the literature are communicating.\nIn Colosseum, ergodicity is induced by setting p_rand > 0 in otherwise communicating MDPs.\nModel-free agents struggle with the resulting increase in variability of the state-action value function.\n\nIn the episodic settings (Tables 2a and 2b), PSRL obtains excellent performances with low variability.\nQ-learning instead performs well in a few MDPs. This often happens since, when the action selected\nby the agent is randomly substituted (due to p_rand > 0) with one with a large sub-optimality gap,\nthe resulting Q-value update introduces a critical error that requires many samples to be corrected.\n\nIn the continuous settings (Tables 2c and 2d), UCRL2 performs best in the ergodic cases when\nQ-learning suffers from the issue caused by p_rand > 0 but is only slightly better than Q-learning\nin the communicating ones. PSRL instead struggles with most MDPs. The reason for its weak\nperformance in this setting is the computationally expensive optimistic sampling procedure required\nfor its worst-case theoretical guarantees. It often breaks the time limit before reaching the first quarter\nof available time steps, meaning that it lacks sufficient samples to estimate the optimal policy.\n\nFigure 2 places the regret of the agents in the continuous ergodic setting (Table 2c) on a position\ncorresponding to the diameter and value norm of the benchmark environments. PSRL and Q-learning\n(Figures 2a and 2b), appear to be impacted more by the value norm than the diameter. This is in line\nwith the lack of sufficient samples for PSRL and the aforementioned issue related to high q estimates\nvariability for Q-learning, which is exacerbated when the estimation complexity is higher. In the\ncase of UCRL2 (Figure 2c), which provides more reliable evidence since it performs well across the\nMDPs, higher regret effectively corresponds to higher diameter and value norm.\n\nBased on the section(s), please answer the following question with yes, no, or n/a and provide a brief justification for your answer.\nQuestion: If the authors curate/release new assets (e.g., code, data, models), do the authors include any new assets either in the supplemental material or as a URL?"}}}