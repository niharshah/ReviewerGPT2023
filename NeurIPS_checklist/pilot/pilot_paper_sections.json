{"paper_index": "pilot", "title": "Local Bayesian optimization via maximizing probability of descent", "abstract": "\nAbstract\nLocal optimization presents a promising approach to expensive, high-dimensional\nblack-box optimization by sidestepping the need to globally explore the search\nspace. For objective functions whose gradient cannot be evaluated directly,\nBayesian optimization offers one solution \u2013 we construct a probabilistic model of\nthe objective, design a policy to learn about the gradient at the current location, and\nuse the resulting information to navigate the objective landscape. Previous work\nhas realized this scheme by minimizing the variance in the estimate of the gradient,\nthen moving in the direction of the expected gradient. In this paper, we re-examine\nand refine this approach. We demonstrate that, surprisingly, the expected value\nof the gradient is not always the direction maximizing the probability of descent,\nand in fact, these directions may be nearly orthogonal. This observation then\ninspires an elegant optimization scheme seeking to maximize the probability of\ndescent while moving in the direction of most-probable descent. Experiments\non both synthetic and real-world objectives show that our method outperforms\nprevious realizations of this optimization scheme and is competitive against other,\nsignificantly more complicated baselines.\n", "introduction": "\n1 Introduction\nThe optimization of expensive-to-evaluate, high-dimensional black-box functions is ubiquitous in\nmachine learning, science, engineering, and beyond; examples range from hyperparameter tuning\n[21] and policy search in reinforcement learning [3, 7], to configuring physics simulations [14].\nHigh-dimensional global optimization faces an inherent difficulty stemming from the curse of\ndimensionality, as a thorough exploration of the search space becomes exponentially more expensive.\nIt is more feasible to seek to locally optimize these high-dimensional objective functions, as we can\nthen sidestep this inherent burden. This is true even in settings where we cannot directly observe the\ngradient of the objective function, as we may appeal to sophisticated techniques such as Bayesian\noptimization to nonetheless learn about the gradient of the objective through noisy observations, and\nthen use this knowledge to navigate the high-dimensional search space locally.\nA realization of this scheme has been proposed by M\u00fcller et al. [18], where a Gaussian process (GP) is\nused to model the objective function, and observations are designed to alternate between minimizing\nthe variance \u2013 and thus uncertainty \u2013 of the GP\u2019s estimate of the gradient of the objective at a given\nlocation, then moving in the direction of the expected gradient. Although this approach seems natural,\nit fails to account for some nuances in the distribution of the directional derivative induced by the GP.\nSpecifically, it turns out that beliefs about the gradient with identical uncertainty may nonetheless have\ndifferent probabilities of descent along the expected gradient. Further and perhaps surprisingly, the\nexpected gradient is not necessarily the direction maximizing the probability of descent \u2013 in fact, these\ndirections can be nearly orthogonal. In other words, simply minimizing the gradient variance and moving in the direction of the expected gradient may lead to suboptimal (local) optimization performance.\nWith this insight, we propose a scheme for local Bayesian optimization that alternates between\nidentifying the direction of most probable descent, then moving in that direction. The result is a\nlocal optimizer that is efficient by design. To this end, we derive a closed-form solution for the\ndirection of most probable descent at a given location in the input space under a GP belief about the\nobjective function. We then design a corresponding closed-form acquisition function that optimizes\n(an upper bound of) the one-step maximum descent probability. Taken together, these components\ncomprise an elegant and efficient optimization scheme. We demonstrate empirically that, across many\nsynthetic and real-world functions, our method outperforms the aforementioned prior realization of\nthis framework and is competitive against other, significantly more complicated baselines.\n", "methods": "\n3 Maximizing probability of descent\nWhat behavior is desirable for a local optimization routine that values sample efficiency? We argue that we should seek to quickly identify directions that will, with high probability, yield progress on the objective function. Pursuing this idea requires reasoning about the probability that a given direction leads \u201cdownhill\u201d from a given location. Although one might guess that the direction most likely to lead downhill is always the (negative) expected gradient, this is not necessarily the case.\nConsider the directional derivative of the objective f with respect to a unit vector v at point x: \u2207vf(x) = v\u22a4\u2207f(x),\nwhich quantifies the rate of change of f at x along the direction of v. According to our GP belief, \u2207f(x) follows a multivariate normal distribution, so the directional derivative \u2207vf(x) is then:\np\udbff\udc00\u2207vf(x) | x,v\udbff\udc01 = N\udbff\udc00v\u22a4\u03bcx,v\u22a4\u03a3xv\udbff\udc01,\nwhere \u03bcx and \u03a3x are the mean and covariance matrix of the normal belief about \u2207f (x), as defined in Eq. (1). This distribution allows us to reason about the probability that we descend on the objective function by moving along the direction of v from x, which is simply the probability that the directional derivative is negative. Thus, we have the following definition.\nDefinition 3.1 (Descent probability and most probable descent direction). Given a unit vector v, the descent probability of the direction v at the location x is given by\n\udbff\udc00 \udbff\udc01 \udbff\udc04 v\u22a4\u03bcx \udbff\udc05\nPr \u2207vf(x)<0|x,v =\u03a6 \u2212\udbff\udc0cv\u22a4\u03a3xv , (2)\nwhere \u03a6 is the CDF of the standard normal distribution. If v\u2217 achieves the maximum descent probability v\u2217 \u2208 arg maxv Pr \udbff\udc00\u2207v f (x) < 0 | x, v\udbff\udc01, then we call v\u2217 a most probable descent direction.\nNote that the definition Eq. (2) is scaling invariant. Thus, the length of v\u2217 does not matter since the descent probability only depends on its direction. Moreover, we note that descent probability depends on both the expected gradient \u03bcx and the gradient uncertainty \u03a3x. Therefore, learning about the gradient by minimizing uncertainty via the trace of the posterior covariance matrix (which does not consider the expected gradient) and moving in the direction of the negative expected gradient (which does not consider uncertainty in the gradient) in a decoupled manner may lead to suboptimal behavior. We first present a simple example to demonstrate the nuances that are not captured by this scheme and to motivate our proposed solution.\n3.1 The (negative) expected gradient does not always maximize descent probability\nIn Fig. 1, we show polar plots of the descent probability Pr \udbff\udc00\u2207v f (x) < 0 | x, v\udbff\udc01 with respect to different beliefs about the gradient. The angles in the polar plots are the angles between v and the vector [1, 0]\u22a4 . Critically for the discussion below, the uncertainty in the gradient, as measured by the trace of the covariance matrix, is identical for all three examples.\nIn the first example in the left panel of Fig. 1, the negative expected gradient happens to maximize the descent probability, and moving in this direction is almost certain to lead downhill. In the middle panel, the expected gradient is the same as in the left panel, but the covariance matrix has been permuted. Here, the negative expected gradient again maximizes the descent probability; however, the largest descent probability is now much lower. In fact, there is non-negligible probability that the descent direction is in the opposite direction. This is because most of the uncertainty we have about the gradient concentrates on the first element of \u03bcx, which determines its direction. We note that the situation in the left panel is inarguably preferable to that in the middle panel, but distinguishing these two is impossible from uncertainty in \u2207f (x) alone.\nFinally, in the right panel, the direction of the expected gradient has rotated with respect to that in the first two panels. Now the (negative) expected gradient is entirely different from the most probable descent direction. Intuitively, the variance in the first coordinate is much smaller than in the second coordinate, and thus the mean in the first coordinate is more likely to have the same sign as the true gradient. However, using negative expected gradient as a descent direction entirely ignores the uncertainty estimate in the gradient. This example shows that, when we reason about the descent of a function, the mean vector \u03bcx and the covariance matrix \u03a3x need to be jointly considered, as the probability of descent depends on both of these quantities (Eq. (2)).\n3.2 Computing the most probable descent direction\nIn light of the above discussion, we propose a local BO algorithm centered entirely around the local descent probability. As a first step, we show in the following how to compute the most probable descentdirectionv\u2217 =argmaxvPr\udbff\udc00\u2207vf(x)<0|x,v\udbff\udc01atagivenlocationgivendata.\nTheorem 3.1. Suppose that the belief about the gradient is p\udbff\udc00\u2207f(x) | x,X,y\udbff\udc01 = N(\u03bcx,\u03a3x), where the posterior covariance \u03a3x is positive definite. Then, the unique (up to scaling) most probable\ndescent direction is\nwith the corresponding maximum descent probability\nmaxPr\udbff\udc00\u2207 f(x) < 0 | x,v\udbff\udc01 = \u03a6\udbff\udc02\udbff\udc0d\u03bc\u22a4\u03a3\u22121\u03bc \udbff\udc03. vvxxx\nProof. As \u03a6 (\u00b7) is monotonic, we can reframe the problem as\nv =argmaxPr \u2207vf(x)<0|x,v =argmax\u03a6 \u2212\udbff\udc0c \u22a4 =argmax\u2212\udbff\udc0c \u22a4 . v v v\u03a3xv v v\u03a3xv\n    Next, we square the objective, and the maximizer is still the same (up to sign). That is, if v\u2217 is the maximizer of the squared objective:\nv\u2217 =argmax v\u22a4\u03bcx\u03bc\u22a4xv, (3) v v\u22a4\u03a3xv\nthen either v\u2217 or \u2212v\u2217 maximizes the descent probability. Let \u03a3x = LL\u22a4 be the Cholesky decompo- sition of \u03a3x, where L has to be nonsingular. A change of variable v = L\u2212\u22a4w gives\nv\u22a4\u03bcx\u03bc\u22a4x v = w\u22a4L\u22121\u03bcx\u03bc\u22a4x L\u2212\u22a4w, v\u22a4\u03a3xv w\u22a4w\n   which is exactly the Rayleigh quotient of L\u22121\u03bcx\u03bc\u22a4x L\u2212\u22a4. Note that this is a rank-1 matrix with top eigenvector L\u22121\u03bc and corresponding eigenvalue \u03bc\u22a4\u03a3\u22121\u03bc . Thus, the maximizer w\u2217 is given by\nx xxx w\u2217 = L\u22121\u03bcx.\nTherefore, the maximizer to Eq. (3) is v\u2217 = L\u2212\u22a4L\u22121w\u2217 = \u03a3\u22121\u03bc . Plug both \u03a3\u22121\u03bc and \u2212\u03a3\u22121\u03bc xxxxxx\nback into Eq. (2). It is easy to check that the direction along \u2212\u03a3\u22121\u03bc is the desired maximizer.\nTheorem 3.1 states that the most probable descent direction can be computed by simply solving a linear system. Being able to compute this quantity allows us to always move within the search space in the direction that most likely improves the objective value, which, as we have seen, is not necessarily the negative expected gradient. This helps us to realize the \u201cupdate\u201d portion of our local BO algorithm, where we iteratively move from the current location x in the most probable descent direction v\u2217. That is, we repeatedly update x with x + \u03b4v\u2217, where \u03b4 is a small constant that acts as a step size. This procedure is iterative in that we do not take one single step along a direction, but multiple small steps, always in the most probable descent direction at the current point, throughout. (Note that we do not observe the value of the objective function at any of these steps.)\nIt is important that we stop this iterative procedure when it becomes uncertain whether we can continue to descend. This is because we aim to move to a new location that decreases the value of the objective function, and thus should only move when descent is likely. A natural approach is to again use the maximum descent probability, which we can compute using Theorem 3.1. Specifically, we stop the iterative update when the maximum descent probability falls below a prespecified threshold p\u2217. Once we have stopped, the final updated x is the location we move to at the current iteration of the BO loop. In our experiments, we set the step size to \u03b4 = 0.001 and the descent probability threshold to p\u2217 = 65%, which we find to work well empirically.\n3.3 Acquisition function via look-ahead maximum descent probability\nWhen the maximum descent probability falls below the threshold p\u2217, we begin selecting queries to learn about the gradient in the current location so as to maximize the probability of descent. Here we derive an acquisition function seeking data that will, in expectation, best improve the highest descent probability. For maximum flexibility, we consider the batch setting where we may gather multiple measurements simultaneously, although we only use the sequential case in our experiments.\nIn particular, the acquisition function we would like to use for a batch of potential query points Z is: \udbff\udc00\udbff\udc01\udbff\udc0a\udbff\udc00 \udbff\udc01\udbff\udc0b\n\u03b10 Z =Ey|Z maxPr \u2207vf(x)<0|x,Z v\n\udbff\udc0a\udbff\udc02\udbff\udc0d\u22a4\u22121 \udbff\udc03\udbff\udc0b (4) = Ey|Z \u03a6 \u03bcx|Z\u03a3x|Z\u03bcx|Z ,\n where \u03bcx|Z and \u03a3x|Z are the posterior mean and covariance of the belief about \u2207f (x), conditioned on a batch of observations at Z and a previously collected training set (X, y) which we have omitted for notational clarity. Note that the second equality is due to Theorem 3.1. The above acquisition function is exactly the look-ahead maximum descent probability. Namely, \u03b10 (Z) is the expected maximum descent probability after querying Z.\nUnfortunately, this expectation is challenging to compute, so we opt for another acquisition function that approximates Eq. (4) via computing the expectation of an upper bound:\n\udbff\udc0a\u22a4 \u22121 \udbff\udc0b\n\u03b1 (Z) = Ey|Z \u03bcx|Z\u03a3x|Z\u03bcx|Z . (5)\nWe discard the (monotonic and concave) transformation given by the normal CDF and square root, thus optimizing an upper bound by Jensen\u2019s inequality. The advantage to this acquisition function \u03b1 is that, remarkably, it has a closed-form expression, as we show below.\nNote that \u03bc =\u03bc +\u03a3 \u03a3\u22121(y \u2212\u03bc ), where y \u223c N(\u03bc ,\u03a3 ). Thus, the acquisition x|ZxxZZZZ Z ZZ\nfunction in Eq. (5) is an expectation of a quadratic function over a Gaussian distribution. Let LL\u22a4 = \u03a3Z be the Cholesky decomposition of \u03a3Z and denote A = \u03a3xZL\u2212\u22a4. Then, the acquisition function can be written as an expectation over a standard normal \u03b6:\n\udbff\udc0a \u22a4\u22121 \udbff\udc0b \u03b1(Z)=E\u03b6\u223cN(0,I) (\u03bcx +A\u03b6) \u03a3x|Z(\u03bcx +A\u03b6) .\nExpanding, we have:\n(\u03bc +A\u03b6)\u22a4\u03a3\u22121 (\u03bc +A\u03b6)=\u03bc\u22a4\u03a3\u22121 \u03bc +2\u03bc\u22a4\u03a3\u22121 A\u03b6+\u03b6\u22a4A\u22a4\u03a3\u22121 A\u03b6. x x|Z x x x|Z x x x|Z x|Z\nThe expectation of each term can be computed in closed form. The first term is a constant and the second term vanishes. Finally, the third term is the expectation of a quadratic form, yielding:\n\u03b1(Z)=\u03bc\u22a4\u03a3\u22121 \u03bc +tr\udbff\udc02A\u22a4\u03a3\u22121 A\udbff\udc03.\nAlgorithm 1 Local BO via MPD\n 1:\n2: 3: 4: 5: 6: 7: 8: 9:\n10: 11:\n12: 13: 14: 15:\ninputsstartinglocationx,numberofiterationsN,numberofsamplesforlearningthegradient M, step size \u03b4, and minimum descent probability threshold p\u2217.\nInitialize the GP.\nfor t = 0,...,N do\nObserve the objective value: y = f (x) + \u03b5.\nUpdate the training data D \u2190 D \u222a {(x, y)} and retrain the GP. for m = 1,...,M do\nQuery point: z\u2217 = arg maxz \u03b1(z).\nObserve the objective value: yz = f (z) + \u03b5.\nUpdate the training data D \u2190 D \u222a {(z, yz)} and the GP.\n\u25b7 learning the gradient\nend for\nwhile maxv Pr \udbff\udc00\u2207v f (x) < 0 | x, v\udbff\udc01 > p\u2217 do \u25b7 move by maximizing descent probability\nComputethemostprobabledescentdirectionv\u2217 \u2190argmaxvPr\udbff\udc00\u2207vf(x)<0|x,v\udbff\udc01.\nMove in the most probable descent direction: x \u2190 x + \u03b4v\u2217. end while\nendfor\nThis compact expression gives the closed-form solution to our acquisition function. Note that solving a linear system with respect to \u03a3x|Z can be performed efficiently using low-rank updates to the Cholesky decomposition of \u03a3x. Further, we may differentiate the acquisition function easily via automatic differentiation. This allows us to optimize the acquisition function trivially using any gradient-based optimizer such as L-BFGS with restart.\nThis completes our algorithm, local BO via most-probable descent, or MPD, which is summarized in Alg. 1. The algorithm alternates between learning about the gradient of the objective function using the acquisition function discussed above, and then iteratively moving in the most probable descent direction until further progress is unlikely, as described in Sect. 3.\n", "experiments": "\n4 Experiments\nWe now present results from extensive experiments that evaluate our method MPD against three baselines: (1) GIBO [18], which performs local BO by minimizing the trace of the posterior covariance matrix of the gradient and uses the expected gradient in the update step; (2) ARS [16], which estimates the gradient of the objective via finite difference with random perturbations; and (3) TuRBO [5], a trust region-based Bayesian optimization method.\nM\u00fcller et al. [18] provide code implementation under the MIT license for GIBO, ARS, and various test objectives. We extend this codebase to implement MPD and conduct our own numerical experiments. For the synthetic (Sect. 4.1) and reinforcement learning (Sect. 4.2) objectives, we use the provided experimental settings. For the other objectives (Sect. 4.3), we set the number of samples to learn about the gradient per iteration M = 1. For each objective function tested, we run each algorithm ten times from the same set of starting points sampled from a Sobol sequence over the (box-bounded) domain. In each of the following plots, we show the progressive mean objective values as a function of the number of queries with error bars indicating (plus or minus) one standard error. Experiments were performed on a small cluster built from commodity hardware comprising approximately 200 Intel Xeon CPU cores (no GPUs), with approximately 10 GB of RAM available to each core. Our implementation is available at https://github.com/kayween/local-bo-mpd.\n4.1 Synthetic objectives\nOur first experiments involve maximizing, over the d-dimensional unit hypercube [0, 1]d , synthetic objective functions that are generated by drawing samples from a GP with an RBF kernel. We refer to \u00a74.1 of M\u00fcller et al. [18] for more details regarding the experimental setup. While M\u00fcller et al. [18] tested for dimensions up to 36, we opt for much higher-dimensional objectives: d \u2208 {25, 50, 100}. Each run has a budget of 500 function evaluations. We visualize the results in Fig. 2, which shows that MPD was able to optimize these functions at a faster rate than the other baselines. Note that the difference in performance becomes larger as the dimensionality d grows, indicating that our method scales well to high dimensions.\n4.2 MuJoCo objectives\nThe second set of experiments are reinforcement learning MuJoCo locomotion tasks [23], where each task involves learning a linear policy that maps states to actions to maximize the reward received from the learning environment. We use the same three environments in M\u00fcller et al. [18], CartPole-v1 with 4 parameters, Swimmer-v1 with 16, and Hopper-v1 with 33, to evaluate the methods and show the re- sults in Fig. 3. MPD is competitive in the first two tasks but progresses slower than the other baselines on Hopper-v1. We conduct a thorough investigation into the cause of MPD\u2019s failure on the Hopper func- tion and present our findings in Appx. B. In short, the experiments on Hopper-v1 employ a state nor- malization scheme (described in \u00a73.3 of M\u00fcller et al. [18]) that leads to systematic differences in the behavior of GIBO and MPD. By controlling for the effect of state normalization in our comparison of the two algorithms, we find that the performance of GIBO and that of MPD are statistically comparable.\n4.3 Other objective functions\nWe further evaluate our method on other real-world objective functions. The first two functions represent inverse problems from physics and engineering. The first is from electrical engineering, where we seek to maximize the fit of a theoretical physical model of an electronic circuit to observed data. There are nine parameters in total, and we set the budget to 500 evaluations. The second is a problem from cosmology [20], where we aim to configure a cosmological model/physical simulator to fit data observed from the Universe. In particular, our objective is to maximize the log likelihood of the physical model parameterized by various physics-related constants that are to be tuned. We follow the setting in Eriksson et al. [5], which presents a harder optimization problem with 12 parameters and much larger bounds, and set the budget at 2000 evaluations. Our third objective function uses the rover trajectory planning problem [27]. This involves tuning the locations of 100 points on a two-dimensional space that map the trajectory of a rover to minimize a cost, thus making up a 200-dimensional optimization problem. We set the budget to be 1000 function evaluations.\nTable1: Averageterminaloptimizedobjectivevaluesandstandarderrorsofdifferentvariantsof MPD. Results that are better than those of our baseline GIBO are highlighted bold.\n  MPD\udbff\udc00p\u2217 = 65%, \u03b4 = 10\u22123\udbff\udc01 GIBO\ntrace + MPD\nMPD + expected gradient\nMPD(p\u2217 = 50%) MPD(p\u2217 = 85%) MPD(p\u2217 = 95%)\nMPD\udbff\udc00\u03b4 = 10\u22124\udbff\udc01 MPD\udbff\udc00\u03b4 = 10\u22122\udbff\udc01\n16D Swimmer (maximization)\n360.50 (0.61) 348.88 (10.11)\n350.58 (9.35) 340.12 (12.75)\n342.36 (13.10) 294.67 (38.16) 15.97 (5.46)\n362.06 (0.63) 350.15 (10.92)\n12D cosmo. constant (maximization)\n\u221223.97 (0.34) \u221255.25 (3.23)\n\u221227.72 (1.16) \u221221.24 (0.04)\n\u221224.29 (0.10) \u221231.08 (0.86) \u221231.86 (0.25)\n\u221224.22 (0.53) \u221225.73 (0.39)\n200D rover trajectory (minimization)\n89.89 (3.88) 152.77 (2.26)\n84.17 (2.10) 293.08 (8.12)\n51.48 (3.44) 142.63 (5.57) 140.44 (6.95)\n90.99 (3.29) 98.72 (4.42)\nWe visualize optimization performance on these three objective functions in Fig. 4. Our proposed pol- icy MPD is consistently competitive against both GIBO and TuRBO. Most notably, in the cosmological constant learning problem, MPD was able to make significant progress immediately and ultimately outperforms its closest spiritual competitor GIBO.\n4.4 Ablation study\nWe now present results from various ablation studies to offer insight into the components of our method MPD and its hyperparameters, specifically the descent probability threshold p\u2217 (65% as the default) and the step size \u03b4 (0.001 as the default), as described in Sect. 3.\nFirst, one may reasonably ask which of the two novel components of MPD \u2013 either the learning phase that seeks to maximize expected posterior descent probability, or the update phase that moves in the most probable descent direction \u2013 is responsible for the performance improvement compared to GIBO. We address this question by comparing the performance of MPD against two variants: (1) trace + MPD, which learns about the gradient by minimizing the trace of the posterior covariance matrix and moves in the most probable descent direction, and (2) MPD + expected gradient, which uses our scheme for identifying the most probable descent direction, then moves in the direction of the (negative) expected gradient. The second section of Tab. 1 shows the average terminal objective values of these MPD variants on three tasks that MPD outperforms GIBO: Swimmer-v1, cosmological constant learning, and rover trajectory planning. We observe that swapping out either component of MPD does not consistently improve from GIBO as much as MPD does. This indicates that the two components of our MPD algorithm work in tandem and both are needed to successfully realize our local BO scheme.\nIn particular, the components of our method are coupled: because the expected gradient and the most probable descent direction are not the same in general, spending evaluation budget to learn about one and then using the other to move may not work well. GIBO\u2019s acquisition function minimizes the trace of the posterior covariance and therefore aims to make the expected gradient estimate more accurate, but it is unclear whether it will necessarily estimate the most probable descent direction accurately. On the other hand, our acquisition function focuses on the one-step maximum descent probability directly. GIBO\u2019s \u201cmoving\u201d policy, moving in the direction of the (negative) expected gradient (which may not be the most probable descent direction), may not necessarily benefit from having a descent direction with a high descent probability (which could point in a different direction), and is therefore incompatible with our acquisition function.\nWe also tested MPD with three other values for the minimum descent probability threshold p\u2217 \u2208 {50%, 85%, 95%} (described in Sect. 3). The first variant with p\u2217 = 50% is less conservative when moving to a new location than our default policy with p\u2217 = 65%, while the other two variants are more conservative. In the third section of Tab. 1, we observe that the more conservative variants of MPD are not as competitive. For example, MPD(p\u2217 = 85%) sees a drop in performance on the Swimmer task, while MPD(p\u2217 = 95%) fails to make significant progress altogether. Interestingly, while the less conservative policy with p\u2217 = 50% also does not perform as well on the two Mujoco tasks, we do observe an increase in performance in the rover trajectory planning problem. From our experiments, we find that this rover objective function is piecewise linear within most of its domain, making finding a descent direction \u201ceasier\u201d and allowing a lower value of p\u2217 to perform better.\nThe interpretation of the threshold p\u2217 is quite natural: it sets a threshold of the minimum probability that we would make progress by moving to a new location. Intuitively, this hyperparameter trades off robustness versus optimism, with higher thresholds spending more budget before moving, but being more confident in their moves. While p\u2217 = 65% performs well in our experiments, a user can set their own threshold depending on their use case. As observed with the rover trajectory planning problem, if there are structures within the objective function that make it \u201ceasy\u201d to find a descent direction, MPD may benefit from a lower threshold. We might also consider dynamically setting the value of p\u2217 based on recent optimization progress \u2013 that is, we might increase p\u2217 if we believe that we are approaching a local optimum and therefore that finding a promising descent direction is becoming more challenging.\nFinally, the lower section of Tab. 1 shows the performance of the variants of MPD with two additional step sizes, 10\u22124 and 10\u22122. We observe that MPD with \u03b4 = 10\u22122 occasionally fails to perform better than GIBO, illustrating the potentially detrimental effect of a step size that is too large. This step size parameter \u03b4 balances between faster convergence and taking steps that are too large, analogous to gradient descent, and may even be problem dependent. It would be additionally interesting to analyze whether there are good \u201crules of thumb\u201d for setting \u03b4 based on the length scale of the GP, as smoother functions can likely support larger step sizes.\n", "conclusion": "\nWe develop a principled local Bayesian optimization framework that revolves around maximization\nof the probability of descending on the objective function. This novel scheme is realized with (1)\nan update rule that iteratively moves from the current location in the direction of maximum descent\nprobability, and (2) a mathematically elegant, computationally convenient acquisition function that\naims to maximize the probability of descent prior to our next move. Our extensive experiments show\nthat our policy outperforms natural baselines on a wide range of applications.\n(Local) Bayesian optimization has seen a wide range of applications across science, engineering,\nand beyond; an extensive annotated bibliography of these applications was compiled by Garnett\n[6] [appendix D]. However, it is possible to leverage BO for nefarious purposes as well; a concrete\nexample is constructing adversarial attacks on machine learning models [22, 24]. Further, BO requires\nhuman expertise and ethical considerations in many important applications, and fully automated\noptimization systems may run the risk of perpetuating misaligned goals in machine learning. The\nauthors judge the potential positive impacts on society resulting from better methods for local\noptimization to outweigh the potential negative impacts.\n", "appendix": "", "references": "\nReferences\n[1] Riad Akrour, Dmitry Sorokin, Jan Peters, and Gerhard Neumann. Local Bayesian Optimization of Motor Skills. In Proceedings of the 34th International Conference on Machine Learning, pages 41\u201350, 2017.\n[2] \u00d6mer Deniz Akyildiz, V\u00edctor Elvira, and Joaqu\u00edn M\u00edguez. The Incremental Proximal Method: A Proba- bilistic Perspective. In 2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 4279\u20134283. IEEE, 2018.\n[3] Roberto Calandra, Andr\u00e9 Seyfarth, Jan Peters, and Marc Peter Deisenroth. Bayesian optimization for learning gaits under uncertainty. In Annals of Mathematics and Artificial Intelligence, volume 76, pages 5\u201323. Springer, 2016.\n[4] Youssef Diouane, Victor Picheny, Rodolphe Le Riche, and Alexandre Scotto Di Perrotolo. TREGO: a Trust-Region Framework for Efficient Global Optimization. 2021. arXiv preprint arXiv:2101.06808 [stat.ML].\n[5] David Eriksson, Michael Pearce, Jacob Gardner, Ryan D Turner, and Matthias Poloczek. Scalable Global Optimization via Local Bayesian Optimization. In Advances in Neural Information Processing Systems, volume 32, 2019.\n[6] Roman Garnett. Bayesian Optimization. Cambridge University Press, 2022.\n[7] Aditya Gopalan and Shie Mannor. Thompson Sampling for Learning Parameterized Markov Decision Processes. In Proceedings of The 28th Conference on Learning Theory, volume 40, pages 861\u2013898. PMLR, 2015.\n[8] Nikolaus Hansen. The CMA Evolution Strategy: A Comparing Review. Towards a New Evolutionary Computation, pages 75\u2013102, 2006.\n[9] Philipp Hennig. Fast Probabilistic Optimization from Noisy Gradients. In Proceedings of the 30th International Conference on Machine Learning, pages 62\u201370, 2013.\n[10] Philipp Hennig, Michael A. Osborne, and Hans P. Kersting. Probabilistic Numerics. Cambridge University Press, 2022.\n[11] Donald R Jones, Matthias Schonlau, and William J Welch. Efficient Global Optimization of Expensive Black-Box Functions. Journal of Global Optimization, 13(4):455\u2013492, 1998.\n[12] Nicolas Le Roux, Pierre-Antoine Manzagol, and Yoshua Bengio. Topmoumoute online natural gradient algorithm. In Advances in Neural Information Processing Systems, volume 20, 2007.\n[13] Michael Y Li and Ryan P. Adams. Explainability constraints for bayesian optimization. 6th ICML Workshop on Automated Machine Learning, 2020.\n[14] Wesley Maddox, Qing Feng, and Max Balandat. Optimizing High-Dimensional Physics Simulations via Composite Bayesian Optimization. Fourth Workshop on Machine Learning and the Physical Sciences (NeurIPS 2021), 2021.\n[15] MarenMahsereciandPhilippHennig.ProbabilisticLineSearchesforStochasticOptimization.InAdvances in Neural Information Processing Systems, volume 28, 2015.\n[16] Horia Mania, Aurelia Guy, and Benjamin Recht. Simple random search of static linear policies is competitive for reinforcement learning. In Advances in Neural Information Processing Systems, volume 31, 2018.\n[17] MarkMcLeod,StephenRoberts,andMichaelAOsborne.Optimization,fastandslow:Optimallyswitching between local and bayesian optimization. In Proceedings of the 35th International Conference on Machine Learning, pages 3443\u20133452, 2018.\n[18] Sarah M\u00fcller, Alexander von Rohr, and Sebastian Trimpe. Local policy search with bayesian optimization. In Advances in Neural Information Processing Systems, volume 34, 2021.\n[19] Carl Edward Rasmussen and Christopher K. I. Williams. Gaussian Processes for Machine Learning. The MIT Press, 2006.\n[20] BethAReid,WillJPercival,DanielJEisenstein,LiciaVerde,DavidNSpergel,RaminASkibba,NetaA Bahcall, Tamas Budavari, Joshua A Frieman, Masataka Fukugita, et al. Cosmological constraints from the clustering of the sloan digital sky survey dr7 luminous red galaxies. Monthly Notices of the Royal Astronomical Society, 404(1):60\u201385, 2010.\n[21] JasperSnoek,HugoLarochelle,andRyanP.Adams.PracticalBayesianOptimizationofMachineLearning Algorithms. In Advances in Neural Information Processing Systems, volume 25, 2012.\n[22] Fnu Suya, Jianfeng Chi, David Evans, and Yuan Tian. Hybrid Batch Attacks: Finding Black-box Adversarial Examples with Limited Queries. In 29th USENIX Security Symposium (USENIX Security 20), pages 1327\u20131344, 2020.\n[23] Emanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: A physics engine for model-based control. In IEEE/RSJ International Conference on Intelligent Robots and Systems, pages 5026\u20135033, 2012.\n[24] XingchenWan,HenryKenlay,RobinRu,ArnoBlaas,MichaelAOsborne,andXiaowenDong.Adversarial Attacks on Graph Classifiers via Bayesian Optimisation. In Advances in Neural Information Processing Systems, volume 34, pages 6983\u20136996, 2021.\n[25] XingchenWan,VuNguyen,HuongHa,BinxinRu,CongLu,andMichaelAOsborne.Thinkglobalandact local: Bayesian optimisation over high-dimensional categorical and mixed search spaces. In Proceedings of the 38th International Conference on Machine Learning, pages 10663\u201310674, 2021.\n[26] Linnan Wang, Rodrigo Fonseca, and Yuandong Tian. Learning Search Space Partition for Black-box Optimization using Monte Carlo Tree Search. In Advances in Neural Information Processing Systems, volume 33, pages 19511\u201319522, 2020.\n[27] Zi Wang, Clement Gehring, Pushmeet Kohli, and Stefanie Jegelka. Batched Large-scale Bayesian Optimization in High-dimensional Spaces. In Proceedings of the 21st International Conference on Artificial Intelligence and Statistics, pages 745\u2013754, 2018.\n"}