{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "06797264",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up the notebook\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys\n",
    "import os\n",
    "import logging\n",
    "logging.basicConfig(level=logging.ERROR)\n",
    "from functools import partial\n",
    "import re\n",
    "import openai\n",
    "import json\n",
    "from pdfminer.high_level import extract_text\n",
    "\n",
    "with open(\"./credential.json\", \"r\") as f:\n",
    "    credential = json.load(f)[\"openai\"]\n",
    "openai.api_key = credential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "e4c16807",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FILL IN MANUALLY\n",
    "# paper title\n",
    "title = \"Harmonizing the object recognition strategies of deep neural networks with humans\"\n",
    "# paper should be stored as papers/{paper_index}.pdf\n",
    "paper_index = 11\n",
    "\n",
    "# maps checklist questions to the corresponding sections that should be used as context\n",
    "question_section_mapping = {\n",
    "    \"1b\": [\"conclusion\"],\n",
    "    \"1c\": [\"conclusion\"],\n",
    "    \"2a\": [\"methods\"],\n",
    "    \"2b\": [\"methods\"],\n",
    "    \"3a\": [\"experiments\"],\n",
    "    \"3b\": [\"experiments\"],\n",
    "    \"3c\": [\"experiments\", \"appendix\"],\n",
    "    \"3d\": [\"experiments\"],\n",
    "    \"4a\": [\"methods\"],\n",
    "    \"4b\": [\"methods\"],\n",
    "    \"4c\": [\"introduction\", \"methods\"],\n",
    "    \"4d\": [\"methods\"],\n",
    "    \"4e\": [\"methods\"],\n",
    "    \"5a\": [\"methods\", \"appendix\"],\n",
    "    \"5b\": [\"methods\", \"appendix\"],\n",
    "    \"5c\": [\"methods\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "76efb764",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "abstract 1418\n",
      "introduction 4452\n",
      "methods 4690\n",
      "experiments 14653\n",
      "conclusion 3857\n",
      "appendix 11803\n"
     ]
    }
   ],
   "source": [
    "# get the paper and appendix text\n",
    "file = f'papers/{paper_index}.pdf'\n",
    "text = extract_text(file)\n",
    "supplemental_file = f'papers/{paper_index}supp.pdf'\n",
    "supplemental_text = extract_text(supplemental_file)\n",
    "\n",
    "# get the sections; make sure to remove the checklist section using re.search\n",
    "abstract = re.search(r\"Abstract(.*?)Introduction\", text, re.DOTALL).group(1)\n",
    "introduction = re.search(r\"Introduction(.*?)Do DNNs explain human visual per-\", text, re.DOTALL).group(1)\n",
    "methods = re.search(r\"3 Methods(.*?)4 Results\", text, re.DOTALL).group(1)\n",
    "experiments = re.search(r\"4 Results(.*?)5 Conclusion\", text, re.DOTALL).group(1) + \" higher diameter and value norm.\"\n",
    "conclusion = re.search(r\"5 Conclusion(.*?)Acknowledgments and Disclosure of Funding\", text, re.DOTALL).group(1)\n",
    "appendix = re.search(r\"1 Psychophyics(.*?)References\", supplemental_text, re.DOTALL).group(1)\n",
    "# remove all <latexit text\n",
    "methods = re.sub(r\"<latexit.*?>.*?</latexit>\", \"\", methods)\n",
    "experiments = re.sub(r\"<latexit.*?>.*?</latexit>\", \"\", experiments)\n",
    "# remove all (cid:xx)\n",
    "introduction = re.sub(r\"\\(cid:.*?\\)\", \"\", introduction)\n",
    "appendix = re.sub(r\"\\(cid:.*?\\)\", \"\", appendix)\n",
    "\n",
    "print(\"abstract\", len(abstract))\n",
    "print(\"introduction\", len(introduction))\n",
    "print(\"methods\", len(methods))\n",
    "print(\"experiments\", len(experiments))\n",
    "print(\"conclusion\", len(conclusion))\n",
    "print(\"appendix\", len(appendix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "d1cf6985",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "appendix_A 6488\n",
      "appendix_D 2710\n",
      "appendix_E 2559\n"
     ]
    }
   ],
   "source": [
    "# Only include when appendix is too large to fit into the token limit, thus needing to break it into sections.\n",
    "appendix_1 = re.search(r\"S1.2 Main Assumptions(.*?)S1.5 Supporting Lemmata\", appendix, re.DOTALL).group(1)\n",
    "appendix_2 = re.search(r\"S2 Application to(.*?)Additional Experiments\", appendix, re.DOTALL).group(1) + \" unsurprising given the MDP structure.\"\n",
    "appendix_3 = re.search(r\"Additional Experiments(.*?)Figure S8\", appendix, re.DOTALL).group(1)\n",
    "\n",
    "print(\"appendix_A\", len(appendix_1))\n",
    "print(\"appendix_D\", len(appendix_2))\n",
    "print(\"appendix_E\", len(appendix_3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "e8a4fc18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the save dict\n",
    "save_dict = {}\n",
    "save_dict[\"paper_index\"] = paper_index\n",
    "save_dict[\"title\"] = title\n",
    "save_dict[\"abstract\"] = abstract\n",
    "save_dict[\"introduction\"] = introduction\n",
    "save_dict[\"methods\"] = methods\n",
    "save_dict[\"experiments\"] = experiments\n",
    "save_dict[\"conclusion\"] = conclusion\n",
    "save_dict[\"appendix\"] = appendix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "c014f240",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only include when appendix is too large to fit into the token limit (as above).\n",
    "save_dict[\"appendix_1\"] = appendix_1\n",
    "save_dict[\"appendix_2\"] = appendix_2\n",
    "save_dict[\"appendix_3\"] = appendix_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "40c9c884",
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct the system prompts\n",
    "system_prompt = f\"You are a computer science researcher currently reviewing a paper titled \\\"{title}\\\" for the NeurIPS computer science conference. Your goal is to try to be as objective and truthful as possible in your answers about the paper provided. Your reviews will be used for causal reasoning in determining the quality of the paper.\"\n",
    "\n",
    "def turn_into_input_format(role, prompt):\n",
    "    # role: \"system\", \"user\", or \"assistant\"\n",
    "    return {\"role\": role, \\\n",
    "            \"content\": prompt}\n",
    "\n",
    "system_prompt = turn_into_input_format(\"system\", system_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "3101498d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define checklist questions\n",
    "checklist_questions = {\n",
    "    \"1b\": \"Do the authors describe the limitations of their work?\",\n",
    "    \"1c\": \"Do the authors discuss any potential negative societal impacts of their work?\",\n",
    "    \"2a\": \"If the authors include theoretical results, do the authors state the full set of assumptions of all theoretical results?\",\n",
    "    \"2b\": \"If the authors include theoretical results, do the authors include complete proofs of all theoretical results?\",\n",
    "    \"3a\": \"If the authors ran experiments, do the authors include the code, data, and instructions needed to reproduce the main experimental results (either in the supplemental material or as a URL)?\",\n",
    "    \"3b\": \"If the authors ran experiments, do the authors specify all the training details (e.g., data splits, hyperparameters, how they were chosen)?\",\n",
    "    \"3c\": \"If the authors ran experiments, do the authors report error bars (e.g., with respect to the random seed after running experiments multiple times)?\",\n",
    "    \"3d\": \"If the authors ran experiments, do the authors include the total amount of compute and the type of resources used (e.g., type of GPUs, internal cluster, or cloud provider)?\",\n",
    "    \"4a\": \"If the authors use existing assets (e.g., code, data, models), do the authors cite the creators?\",\n",
    "    \"4b\": \"If the authors use existing assets (e.g., code, data, models) or curate/release new assets, do the authors mention the license of the assets?\",\n",
    "    \"4c\": \"If the authors curate/release new assets (e.g., code, data, models), do the authors include any new assets either in the supplemental material or as a URL?\",\n",
    "    \"4d\": \"If the authors curate/release new assets (e.g., code, data, models), do the authors discuss whether and how consent was obtained from people whose data they are using/curating?\",\n",
    "    \"4e\": \"If the authors curate/release new assets (e.g., code, data, models), do the authors discuss whether the data they are using/curating contains personally identifiable information or offensive content?\",\n",
    "    \"5a\": \"If the authors used crowdsourcing or conducted research with human subjects, do the authors include the full text of instructions given to participants and screenshots, if applicable?\",\n",
    "    \"5b\": \"If the authors used crowdsourcing or conducted research with human subjects, do the authors describe any potential participant risks, with links to Institutional Review Board (IRB) approvals, if applicable?\",\n",
    "    \"5c\": \"If the authors used crowdsourcing or conducted research with human subjects, do the authors include the estimated hourly wage paid to participants and the total amount spent on participant compensation?\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "a2adb1be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the full prompts based on the individual parts\n",
    "prompts = {}\n",
    "for question in question_section_mapping.keys(): # e.g. q1c -> [\"abstract\"]\n",
    "    sections = question_section_mapping[question]\n",
    "\n",
    "    prompt_prefix = \"\"\n",
    "    for section in sections:\n",
    "        prompt_prefix += f'The following is the {section} section of the paper you are reviewing:\\n'\n",
    "        prompt_prefix += save_dict[section] + \"\\n\\n\"\n",
    "    prompt_question = \"Based on the section(s), please answer the following question with yes, no, or n/a and provide a brief justification for your answer.\\nQuestion: \"\n",
    "    prompt = prompt_prefix + prompt_question + checklist_questions[question]\n",
    "    prompts[question] = turn_into_input_format(\"user\", prompt) # put the inputs into input format for openAI API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "3665df0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# query the GPT-4 API\n",
    "def query_problem(system_prompt, alternative, model=\"gpt-4\", max_tokens=150, temperature=1, top_p=1, n=1, verbose=False):\n",
    "    # alternative is the user prompt\n",
    "    messages = [system_prompt, alternative] # first message in the chain of messages\n",
    "    if verbose:\n",
    "        print(\"messages:\", messages)\n",
    "    \n",
    "    completion1 = openai.ChatCompletion.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        max_tokens=max_tokens,\n",
    "        temperature=temperature,\n",
    "        top_p=top_p, \n",
    "        n=n\n",
    "    )\n",
    "\n",
    "    return [completion1[\"choices\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "d6b9125e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the responses across multiple problems\n",
    "import time\n",
    "def get_responses(verbose=False, start_point=None):\n",
    "    # start_point is 0-indexed, with the order (pilot_questions, sys_index, alt_index)\n",
    "    params = (1, 1) # (temperature, top_p)\n",
    "    questions = list(prompts.keys())\n",
    "    \n",
    "    for problem_index in range(len(questions)):\n",
    "        problem = questions[problem_index]\n",
    "        prompt = prompts[problem]\n",
    "\n",
    "        if start_point is not None:\n",
    "            if (start_point > problem_index): continue\n",
    "\n",
    "        print(f'problem {problem_index+1}/{len(questions)}')\n",
    "            \n",
    "        answers = query_problem(system_prompt, prompt, temperature=params[0], top_p=params[1], n=3, verbose=verbose)\n",
    "        # save the answers to a file, create the directory if necessary\n",
    "        if not os.path.exists(f'data/{paper_index}'):\n",
    "            os.makedirs(f'data/{paper_index}')\n",
    "        \n",
    "        with open(f'data/{paper_index}/{problem}.json', \"w\") as f:\n",
    "            json.dump(answers, f)\n",
    "        time.sleep(10) # OpenAI limit is like 40000 tokens/min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "704ecd43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "problem 2/8\n",
      "problem 3/8\n",
      "problem 4/8\n",
      "problem 5/8\n",
      "problem 6/8\n",
      "problem 7/8\n",
      "problem 8/8\n"
     ]
    }
   ],
   "source": [
    "get_responses(verbose=False, start_point=0) # the index of the next one that hasn't been queried"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "33adf9db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the system prompts to the save dict\n",
    "save_dict[\"system_prompts\"] = [system_prompt]\n",
    "\n",
    "# add the prompts for each question to the save dict\n",
    "save_dict[\"prompts\"] = prompts\n",
    "\n",
    "# save the save_dict\n",
    "with open(f'data/{paper_index}/save_dict.json', \"w\") as f:\n",
    "    json.dump(save_dict, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
